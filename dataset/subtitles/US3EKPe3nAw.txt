utt_0000 utt 0.62 2.03 -X HELLO, I'M LINYI JIN,
utt_0001 utt 2.03 4.37 -X AND I'M GLAD TO INTRODUCE OUR PAPER
utt_0002 utt 4.40 7.44 -X PLANAR SURFACE RECONSTRUCTION FROM SPARSE VIEWS.
utt_0004 utt 7.50 9.94 -X THIS IS A JOINT WORK WITH SHENGYI QIAN,
utt_0005 utt 9.94 14.10 -X ANDREW OWENS AND DAVID FOUHEY.
utt_0006 utt 14.10 15.73 -X CONSIDER THE TWO PHOTOS OF A ROOM.
utt_0007 utt 15.73 18.16 -X THERE IS A CHAIR ON ONE SIDE,
utt_0008 utt 18.16 19.83 -X A BEDSIDE TABLE ON THE OTHER,
utt_0009 utt 19.98 21.65 -X A LARGE GLASS WALL,
utt_0010 utt 21.65 22.74 -X AND A FLOOR IN THE MIDDLE.
utt_0011 utt 22.86 25.44 -X THEY ARE TAKEN FROM VERY DIFFERENT CAMERA POSES
utt_0013 utt 25.45 30.87 -X AND VERY LITTLE OF THE SCENE STRUCTURE WITHIN THEM OVERLAPS.
utt_0015 utt 30.87 32.15 -X DESPITE THESE CHALLENGES,
utt_0016 utt 32.15 36.27 -X WE CAN STILL UNDERSTAND SPACES LIKE THESE FROM A FEW ORDINARY PHOTOS.
utt_0019 utt 36.33 40.02 -X THE GOAL OF OUR PAPER IS TO GIVE THE SAME ABILITY TO COMPUTERS.
utt_0021 utt 40.14 43.38 -X GIVEN TWO RGB IMAGES WITH AN UNKNOWN RELATIONSHIP,
utt_0023 utt 43.38 46.35 -X WE WANT A SINGLE AND COHERENT RECONSTRUCTION.
utt_0025 utt 46.45 51.99 -X OUR SYSTEM NEEDS TO OUTPUT A SET OF PLANES AND THE RELATIVE CAMERA POSE OF THE INPUT IMAGES.
utt_0029 utt 53.26 57.65 -X HERE IS A SAMPLE OUTPUT OF OUR SYSTEM RENDERED IN DIFFERENT CAMERA VIEWS.
utt_0031 utt 60.53 66.77 -X PREVIOUSLY, MANY WORKS AIM TO FIND CORRESPONDENCES BETWEEN REGIONS OF IMAGES, HOWEVER,
utt_0034 utt 66.77 69.91 -X PIXEL-WISE CORRESPONDENCES ONLY WORK OVERLAPPING REGIONS,
utt_0036 utt 69.91 74.42 -X AND MOST OF THE AREA IN THIS SCENE, FOR EXAMPLE, IS NOT OVERLAPPING.
utt_0039 utt 74.42 77.75 -X THERE IS NO INFORMATION IN THE NON-OVERLAPPING REGIONS TO RECONSTRUCT.
utt_0042 utt 78.67 84.34 -X MEANWHILE THERE EXISTS MANY WORKS TO MAP twoD IMAGES TO threeD STRUCTURES.
utt_0044 utt 84.34 92.60 -X PLANERCNN, FOR EXAMPLE, USES A DEEP NEURAL NETWORK TO DETECT AND RECONSTRUCT PIECEWISE PLANAR SURFACES FROM A SINGLE RGB IMAGE.
utt_0048 utt 92.91 99.83 -X IN THIS EXAMPLE, PLANERCNN DOES A GOOD JOB OF EXTRACTING DIFFERENT PLANAR REGIONS IN EACH IMAGE.
utt_0051 utt 99.83 102.74 -X HOWEVER, WE STILL DON'T KNOW HOW TO PUT THINGS TOGETHER.
utt_0053 utt 102.80 103.80 -X FOR EXAMPLE,
utt_0054 utt 103.92 108.25 -X WE WILL HAVE TWO PIECES OF THE SAME GLASS WALL FROM THE TWO INPUT IMAGES,
utt_0056 utt 108.47 113.21 -X AND WE NEED TO KNOW HOW TO STICK THEM TOGETHER TO PRODUCE A COHERENT OUTPUT.
utt_0058 utt 113.62 121.22 -X IN GENERAL, SINGLE VIEW RECONSTRUCTION SYSTEMS LIKE PLANERCNN WILL PRODUCE MULTIPLE SLICES OF THE SAME OBJECT
utt_0063 utt 121.30 123.29 -X IF IT APPEARS ACROSS VIEWS.
utt_0064 utt 124.02 129.24 -X AS WE NEED ONE COHERENT RECONSTRUCTION INSTEAD OF A SIMPLE MIXTURE OF PER-VIEW SEGMENTS,
utt_0068 utt 129.27 132.70 -X GROUPING WITHIN THE OVERLAPPING REGION IS IMPORTANT.
utt_0070 utt 134.26 136.92 -X THE RELATIONSHIP BETWEEN RECONSTRUCTION,
utt_0071 utt 136.92 142.74 -X CORRESPONDENCE, AND CAMERA POSE HAS LONG BEEN WELL UNDERSTOOD IN THE MULTIVIEW threeD COMMUNITY.
utt_0075 utt 142.96 145.08 -X OUR WORK IS DIFFERENT FROM NERF,
utt_0076 utt 145.08 150.46 -X WHICH GENERALLY WORKS ON ONE SINGLE SCENE AND REQUIRES CAMERA POSE AS INPUT.
utt_0079 utt 150.68 155.16 -X OUR WORK PREDICTS CAMERA POSES AND GENERALIZES TO NOVEL SCENES.
utt_0082 utt 155.67 159.07 -X WHILE WE USE TWO WELL-SEPARATED AND UNKNOWN VIEWS,
utt_0084 utt 159.19 162.65 -X OUR APPROACH IS DIFFERENT COMPARED TO STAND OUT TWO VIEW-STEREO
utt_0086 utt 162.68 166.59 -X OR VISUAL SLAM AND MORE SIMILAR TO WIDE-BASELINE STEREO.
utt_0088 utt 166.74 169.37 -X AND UNLIKE WIDE-BASELINE STEREO SYSTEMS,
utt_0089 utt 169.37 175.39 -X WE ALSO NEED TO PRODUCE RECONSTRUCTIONS FOR PORTIONS OF THE SCENE THAT ARE SEEN IN ONLY ONE CAMERA.
utt_0092 utt 176.31 179.52 -X THE MOST SIMILAR WORK IN THIS DIRECTION IS ASSOCIATIVEthreeD.
utt_0094 utt 179.52 186.65 -X IT RECONSTRUCTS VOLUMETRIC OBJECTS USING MULTIPLE NETWORKS AND A HEURISTIC RANSAC-LIKE OPTIMIZATION FROM TWO IMAGES.
utt_0098 utt 186.78 190.68 -X WHILE EFFECTIVE ON A SIX- OBJECT SUBSET OF A SYNTHETIC DATASET,
utt_0100 utt 190.68 193.34 -X THESE COMPONENTS FALL SHORT ON THE MORE CHALLENGING,
utt_0102 utt 193.34 196.67 -X REALISTIC MATTERPORTthreeD DATASET THAT OUR WORK USES.
utt_0104 utt 196.95 198.14 -X BY CONTRAST,
utt_0105 utt 198.14 203.84 -X WE PROPOSE A MORE PRINCIPLED OPTIMIZATION STRATEGY TO HANDLE PLANAR SEGMENTS IN REALISTIC DATA.
utt_0109 utt 203.90 208.22 -X ADDITIONALLY, OUR APPROACH HAS ONE BACKBONE NETWORK FORWARD PASS PER IMAGE PAIR.
utt_0112 utt 209.05 212.83 -X OUR INSIGHT IS THAT THE THREE RELATED CHALLENGES,
utt_0114 utt 212.83 214.11 -X PER-VIEW RECONSTRUCTION,
utt_0115 utt 214.11 215.77 -X INTER-VIEW CORRESPONDENCE,
utt_0116 utt 215.77 219.23 -X AND INTER-VIEW CAMERA POSE SHOULD BE JOINTLY TACKLED.
utt_0118 utt 219.32 227.07 -X WE ADOPT PLANERCNN TO ESTIMATE PER-VIEW RECONSTRUCTION WHILE JOINLY ESTIMATE CORRESPONDENCES AND CAMERA POSE.
utt_0122 utt 228.15 230.85 -X NOW LET'S GET INTO THE DETAILS OF OUR WORK,
utt_0124 utt 230.85 233.41 -X THE INPUT TO OUR SYSTEM IS TWO IMAGES.
utt_0125 utt 233.72 237.09 -X THE SYSTEM PRODUCES BACKBONE FEATURES USING RESNETfifty-FPN.
utt_0127 utt 238.59 240.32 -X FOLLOWING PLANERCNN,
utt_0128 utt 240.32 247.68 -X WE USE A REGION PROPOSAL NETWORK TO PROPOSE BOXES AND THEN INFER PLANE MASKS AND PARAMETERS FROM THE ROIALIGNED FEATURES.
utt_0133 utt 248.89 252.29 -X WE ADDITIONALLY PREDICT AN EMBEDDING VECTOR FOR EACH PLANE,
utt_0135 utt 252.29 258.18 -X WHICH SERVES AS AN APPEARANCE FEATURE TO COMPARE THE SIMILARITY OF SURFACE TEXTURE ACROSS VIEWS.
utt_0138 utt 259.45 264.54 -X OUR CAMERA POSE MODULE ESTIMATES A DISTRIBUTION OVER THE RELATIVE CAMERA POSE BETWEEN VIEWS.
utt_0142 utt 264.89 269.66 -X THIS ENABLES JOINT REASONING BETWEEN A HOLISTIC ESTIMATE OF THE CAMERA,
utt_0145 utt 269.66 273.38 -X AS WELL AS VALUABLE CUES IN THE ESTIMATED GEOMETRY.
utt_0147 utt 274.14 281.15 -X WE ATTEND THE Pthree FEATURE FROM THE SAME BACKBONE TO CAPTURE SIMILARITY BETWEEN IMAGES AT EACH PIXEL IN A FEATURE MAP.
utt_0152 utt 281.95 287.49 -X AFTER THAT, WE USE CONVOLUTIONAL AND LINEAR LAYERS TO PREDICT THE CAMERA DISTRIBUTION.
utt_0155 utt 287.93 294.75 -X WE FIND THIS ATTENTION TO BE SUPERIOR COMPARED TO THE STRATEGY OF CONCATENATING AVERAGE-POOLED VECTOR OUTPUTS,
utt_0159 utt 294.75 296.07 -X AS IN PRIOR WORKS.
utt_0161 utt 299.59 300.61 -X PLANAR SURFACES,
utt_0162 utt 300.61 304.19 -X CORRESPONDENCES AND CAMERA POSES FROM A SINGLE NETWORK.
utt_0164 utt 304.19 309.80 -X HOWEVER, WE ARE NOT DONE SINCE THE NETWORK MAY NOT PREDICT THE OPTIMAL SOLUTION DIRECTLY.
utt_0167 utt 309.80 317.41 -X WE NEED AN OPTIMIZATION STEP TO PRODUCE THE FINAL OUTPUT TO JOINTLY REASON OVER CORRESPONDENCE AND CAMERAS.
utt_0171 utt 317.41 322.15 -X SUPPOSE OUR NETWORK PREDICTS M PLANES FROM THE LEFT IMAGE AND N PLANES FROM THE RIGHT IMAGE.
utt_0174 utt 322.66 324.76 -X EACH PLANE HAS A PLANE PARAMETER
utt_0175 utt 324.77 327.04 -X AND AN APPEARANCE FEATURE.
utt_0176 utt 327.04 331.11 -X WE ALSO PREDICT THE CROSS-VIEW CAMERA DISTRIBUTION OVER K BINS.
utt_0178 utt 332.22 338.56 -X WE FIRST SOLVE A DISCRETE PROBLEM THAT SELECTS A CAMERA FROM THE K OPTIONS AND THE PLANE CORRESPONDENCE MATRIX C.
utt_0182 utt 339.04 341.41 -X ASSUMING CAMERA K HAS BEEN SELECTED,
utt_0183 utt 341.51 345.86 -X WE CAN FIND A MATCH MATRIX BASED ON THE SCORE MATRIX FOR THIS CAMERA.
utt_0186 utt 346.24 351.88 -X THE SCORE MATRIX IS AN M BY N MATRIX ENCODING THE QUALITY OF A PLANE CORRESPONDENCE.
utt_0189 utt 352.13 353.41 -X BASED ON THE SCORE,
utt_0190 utt 353.41 356.97 -X WE CAN USE HUNGARIAN ALGORITHM TO FIND THE FINAL MATCH MATRIX.
utt_0192 utt 358.91 370.21 -X EACH ENTRY OF THE SCORE MATRIX JOINTLY CONSIDER THE twoD APPEARANCE DISTANCE IN THE BLUE BOX AND THE threeD GEOMETRY DISTANCE IN THE RED BOX, AND LINK THE TWO BY WEIGHTED ADDITION.
utt_0199 utt 370.21 373.13 -X THE SUM OF THEM ARE ZERO WITH PERFECT MATCHES.
utt_0201 utt 374.02 377.42 -X THE OVERALL OBJECTIVE FUNCTION CONTAINS THE MATCH COST,
utt_0203 utt 377.83 381.64 -X WHICH IS THE DOT PRODUCT BETWEEN THE SCORE MATRIX AND THE MATCH MATRIX.
utt_0206 utt 381.76 387.27 -X WE ALSO COMBINE IT WITH TWO REGULARIZING TERMS: ONE REWARDS MATCHING OBJECTS
utt_0208 utt 387.68 390.38 -X AND THE OTHER PENALIZES UNLIKELY CAMERAS.
utt_0209 utt 390.88 398.57 -X THE TERMS IN THE BLUE BOX IS SOLVED PER CAMERA BY THE HUNGARIAN ALGORITHM AND THE RED TERM IS A SCALAR PER CAMERA.
utt_0213 utt 398.63 404.10 -X THE OVERALL OBJECTIVE FUNCTION IS MINIMIZED OVER A SET OF HUNGARIAN ALGORITHM PROBLEMS.
utt_0216 utt 404.10 409.10 -X WE PICK THE BEST CORRESPONDENCE AND CAMERA THAT OPTIMIZE THE OBJECTIVE FUNCTION,
utt_0219 utt 409.10 411.72 -X WHICH ENCOURAGE SELECTING A LIKELY CAMERA,
utt_0221 utt 411.72 413.58 -X AS MANY PLANES AS POSSIBLE,
utt_0222 utt 413.58 418.09 -X AND THE CORRESPONDENCE THAT IS CONSISTENT IN BOTH APPEARANCE AND GEOMETRY.
utt_0225 utt 419.46 423.05 -X HAVING SELECTED THE BEST CAMERA BIN AND PLANE CORRESPONDENCES,
utt_0227 utt 423.11 428.89 -X WE USE NONLINEAR OPTIMIZATION TO CONTINUOUSLY REFINE THE CAMERA AND THE PLANE PARAMETERS
utt_0230 utt 428.93 431.50 -X BASED ON THE PIXEL-WISE CORRESPONDENCES
utt_0231 utt 431.59 433.80 -X TO PRODUCE OUR FINAL RECONSTRUCTION.
utt_0232 utt 435.05 438.19 -X WE GENERATE A DATASET SPECIFIC FOR THIS TASK.
utt_0234 utt 438.50 444.84 -X WE FIT GROUND TRUTH PLANES ON THE MATTERPORTthreeD MESH USING SEMANTIC LABELS TO CONSTRAIN THE PLANES.
utt_0238 utt 445.29 453.04 -X WE SAMPLE CAMERA PAIRS TO CONTAIN SPARSE VIEWS WITH AN AVERAGE OF twenty-one PERCENT OVERLAP IN EACH CAMERA PAIR.
utt_0242 utt 453.04 454.48 -X HERE WE QUALITATIVELY SHOW
utt_0245 utt 457.26 459.98 -X WE SHOW OUR RECONSTRUCTION FROM TWO NOVEL VIEWS.
utt_0247 utt 459.98 464.14 -X THE BLUE AND RED FRUSTUMS SHOW CAMERAS FOR IMAGE ONE AND TWO.
utt_0249 utt 465.26 470.64 -X HERE IS ANOTHER EXAMPLE WHERE THE CAMERAS ARE FACING EACH OTHER TO CAPTURE BOTH SIDES OF THE BED.
utt_0253 utt 471.18 477.01 -X OUR SYSTEM CAN RECOVER LARGE AREAS OF THE ROOM JUST FROM TWO VIEWS WITH A LARGE VIEWPOINT CHANGE.
utt_0257 utt 477.39 479.95 -X HERE IS A RECONSTRUCTION OF A KITCHEN,
utt_0258 utt 479.95 485.04 -X AND WE COMPARE OUR RECONSTRUCTION ON THE BOTTOM LEFT WITH THE GROUND TRUTH ON THE BOTTOM RIGHT.
utt_0262 utt 486.47 492.40 -X WE COMPARE WITH ALTERNATIVE APPROACHES AND ABLATIONS THAT TEST THE SYSTEM'S CONTRIBUTIONS.
utt_0265 utt 492.59 494.48 -X TO THE BEST OF OUR KNOWLEDGE,
utt_0266 utt 494.57 497.17 -X NO EXISTING WORK SOLVES OUR TASK.
utt_0267 utt 497.39 500.40 -X SO WE TEST THE FUSIONS OF EXISTING SYSTEMS.
utt_0269 utt 500.71 503.21 -X THERE ARE TWO COMPONENTS IN THE BASELINES:
utt_0271 utt 503.34 506.64 -X PLANE RECONSTRUCTION AND CAMERA ESTIMATIONS.
utt_0273 utt 507.50 518.45 -X WE USE PLANERCNN OR MANHATTAN-WORLD STEREO FOR PLANE RECONSTRUCTION, AND RGBD PLANE ODOMETRY, SUPERGLUE OR OUR MODIFIED RPNET
utt_0276 utt 518.60 520.53 -X FOR CAMERA ESTIMATIONS.
utt_0277 utt 522.12 526.90 -X WE FIRST SHOW A QUALITATIVE COMPARISON OF THIS IMAGE PAIR AS INPUT.
utt_0279 utt 528.08 535.06 -X MANHATTAN-WORLD STEREO PLUS RGBD ODOMETRY FAILS ON THIS SCENE EVEN WITH ADDITIONAL GROUND TRUTH DEPTH AS INPUT
utt_0283 utt 535.21 540.27 -X BECAUSE THEY MAINLY USE GEOMETRY AND IS DESIGNED TO WORK ON LARGE OVERLAPPING IMAGE PAIRS.
utt_0286 utt 540.94 547.38 -X PLANERCNN PLUS SUPERGLUE CAN HAVE A REASONABLE RECONSTRUCTION GIVEN A GROUND TRUTH CAMERA TRANSLATION SCALE,
utt_0290 utt 547.44 550.55 -X BUT THEY FAIL TO MERGE THE WALLS TOGETHER ACROSS VIEWS.
utt_0292 utt 551.47 555.19 -X OUR METHOD PREDICTS ACCURATE CAMERA POSES FROM THE INPUTS,
utt_0294 utt 555.34 561.71 -X MERGES PLANES ACROSS DIFFERENT VIEWS AND ALIGNS THEIR TEXTURES TO PRODUCE A COHERENT RECONSTRUCTION.
utt_0298 utt 561.97 564.18 -X AND HERE WE SHOW THE GROUND TRUTH.
utt_0299 utt 564.97 567.54 -X NEXT, WE SHOW QUANTITATIVE EVALUATION.
utt_0300 utt 567.54 572.76 -X WE TREAT THE FULL PROBLEM AS A DETECTION TASK AND EVALUATE USING AVERAGE PRECISION.
utt_0303 utt 572.82 582.00 -X WE DEFINE A TRUE POSITIVE AS A DETECTION SATISFYING THREE CRITERIA, INCLUDING MASK IOU, SURFACE NORMAL DISTANCE AND OFFSET DISTANCE.
utt_0307 utt 583.44 587.06 -X PLANE ODOMETRY FINDS POSE PRIMARILY WITH PLANE NORMALS.
utt_0309 utt 587.06 593.88 -X ITS CORRESPONDANCE IS OFTEN WRONG BECAUSE OUR LOW OVERLAP SETTING LIMITS THE INFORMATION IN GEOMETRIC CUES ALONE.
utt_0312 utt 594.45 602.61 -X SUPERGLUE OR RPNET WITH PLANERCNN OUTPERFORMS PLANE ODOMETRY BY USING DEEP FEATURES TO FIND CORRESPONDENCES OR CAMERA POSES.
utt_0316 utt 603.63 609.11 -X OUR PROPOSED APPROACH FURTHER OUTPERFORMS ALL THE OTHER BASELINES BY A LARGE MARGIN IN AP,
utt_0319 utt 609.17 615.73 -X BECAUSE IT ELIMINATES DUPLICATE COPIES OF PLANES AND REDUCES INCONSISTENCY BETWEEN PREDICTIONS.
utt_0323 utt 616.50 620.95 -X NEXT, WE EVALUATE HOW WELL WE ESTIMATE THE RELATIVE CAMERA POSES.
utt_0326 utt 621.01 628.31 -X DUE TO LIMITED INFORMATION AVAILABLE IN GEOMETRY ALONE, PLANE ODOMETRY METHOD HAS A LARGE ERROR.
utt_0330 utt 628.31 634.97 -X SUPERGLUE CAN BE EXTRAORDINARY ACCURATE IN ROTATION WHEN TEXTURES PROVIDE STRONG CORRESPONDENCES ACROSS VIEWS,
utt_0334 utt 635.03 638.23 -X BUT ALSO CAN FAIL TO MAKE INFERENCES ON HARD SCENES.
utt_0336 utt 639.03 640.82 -X USING ATTENTION FEATURES,
utt_0337 utt 640.82 647.13 -X OUR MOST LIKELY CAMERA FROM THE CAMERA BRANCH SUBSTANTIALLY OUTPERFORMS THE PREVIOUS METHOD OF ASSOCIATIVEthreeD.
utt_0341 utt 648.34 654.23 -X OUR OPTIMIZATION FURTHER IMPROVES TRANSLATION SUBSTANTIALLY AND ROTATION WITH SMALL GAINS.
utt_0345 utt 656.18 664.15 -X IN CONCLUSION, WE'VE PRESENTED A LEARNING BASED SYSTEM TO PRODUCE A COHERENT PLANAR SURFACE RECONSTRUCTION FROM TWO UNKNOWN VIEWS.
utt_0350 utt 665.01 674.49 -X FUTURE DIRECTIONS INCLUDE USING OUR FRAMEWORK TO RECONSTRUCT A MORE COMPLETE SCENE WITH FEWER FRAMES THAN TRADITIONAL STRUCTURE FROM MOTION.
utt_0355 utt 674.49 682.01 -X HERE WE SHOW EXAMPLES THAT EXTEND OUR SYSTEM TO MORE THAN TWO VIEWS BY INCREMENTALLY STITCHING NEW VIEWS ON RECONSTRUCTED VIEWS.
utt_0360 utt 683.35 684.89 -X FOR MORE INFORMATION,
utt_0361 utt 684.89 686.78 -X PLEASE VISIT OUR PROJECT PAGE.
utt_0362 utt 686.84 689.37 -X WE HAVE ALSO RELEASED OUR CODE ON GITHUB.
utt_0363 utt 689.46 693.59 -X WE THANK TOYOTA RESEARCH INSTITUTE FOR SPONSORING THIS RESEARCH.
