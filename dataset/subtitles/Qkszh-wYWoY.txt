utt_0000 utt 0.91 4.85 -X HI. I AM SUKRUT RAO, FROM THE MAX PLANCK INSTITUTE FOR INFORMATICS.
utt_0001 utt 4.85 9.87 -X I WILL BE PRESENTING OUR PAPER “ADVERSARIAL TRAINING AGAINST LOCATION-OPTIMIZED ADVERSARIAL PATCHES”.
utt_0003 utt 10.09 14.45 -X THIS IS JOINT WORK WITH DAVID STUTZ AND BERNT SCHIELE.
utt_0004 utt 14.70 21.71 -X I WILL FIRST PROVIDE A SHORT OVERVIEW OF OUR WORK, WHICH WILL BE FOLLOWED BY A MORE DETAILED PRESENTATION.
utt_0006 utt 21.71 26.22 -X A WELL-TRAINED IMAGE CLASSIFIER CAN ACCURATELY PREDICT THE CLASS LABEL FOR AN IMAGE WITH HIGH CONFIDENCE.
utt_0008 utt 27.37 33.14 -X HOWEVER, IT IS POSSIBLE TO CONSTRUCT A PATCH OF PIXELS WHICH WHEN PLACED ON AN IMAGE CAUSES THE CLASSIFIER TO FAIL.
utt_0010 utt 33.29 35.57 -X THIS IS KNOWN AS THE ADVERSARIAL PATCH ATTACK.
utt_0011 utt 37.61 43.19 -X ADVERSARIAL TRAINING IS A DEFENSE MECHANISM WHERE A NETWORK IS TRAINED WITH ADVERSARIAL EXAMPLES TO MAKE IT ROBUST AGAINST THEM.
utt_0013 utt 43.28 50.67 -X IN OUR WORK, WE PERFORM ADVERSARIAL TRAINING TO DEFEND AGAINST ADVERSARIAL PATCHES AND SHOW THAT IT CAN IMPROVE MODEL ROBUSTNESS AGAINST SUCH ATTACKS.
utt_0016 utt 52.40 56.50 -X AN ADVERSARIAL PATCH ATTACK MAY BE MORE EFFECTIVE ON SOME IMAGE LOCATIONS THAN OTHERS.
utt_0017 utt 56.69 60.11 -X FINDING THE OPTIMAL LOCATION ON AN IMAGE WOULD MAKE THE ATTACK STRONGER.
utt_0018 utt 61.74 70.48 -X FOR THIS, WE DEVISE A SIMPLE LOCATION OPTIMIZATION SCHEME TO JOINTLY OPTIMIZE PATCH LOCATION AND VALUES WHEN PERFORMING THE ATTACK, AND FIND THAT THIS LEADS TO A STRONGER ATTACK.
utt_0020 utt 72.69 80.53 -X WE THEN PERFORM ADVERSARIAL TRAINING AGAINST THIS STRONGER ATTACK, AND SHOW THAT THIS IN TURN LEADS TO A STRONGER DEFENSE, RESULTING IN A MORE ROBUST MODEL.
utt_0022 utt 84.05 86.16 -X I WILL NOW PRESENT OUR WORK IN DETAIL.
utt_0023 utt 86.25 89.59 -X FIRST, I WILL OUTLINE THE OBJECTIVE AND CONTRIBUTIONS.
utt_0024 utt 89.59 95.12 -X THEN, I WILL DESCRIBE THE ADVERSARIAL PATCH ATTACK PROCEDURE FOLLOWED BY THE USE OF ADVERSARIAL TRAINING AGAINST SUCH ATTACKS.
utt_0026 utt 95.28 100.82 -X FINALLY, I WILL PROVIDE A BRIEF OVERVIEW OF THE EXPERIMENTAL RESULTS.
utt_0028 utt 100.82 107.57 -X AN ADVERSARIAL PATCH IS A SMALL CONTIGUOUS PATCH OF PIXEL VALUES GENERATED TO CAUSE A CLASSIFIER TO MISCLASSIFY WHEN PLACED ON AN IMAGE.
utt_0030 utt 107.95 114.07 -X UNLIKE IMPERCEPTIBLE ATTACKS, IT IS MORE PRACTICAL SINCE SUCH PATCHES CAN BE PRINTED OUT AS STICKERS AND PLACED IN THE REAL WORLD.
utt_0032 utt 114.67 120.15 -X SUCH ATTACKS COULD BE AN IMPORTANT CONSIDERATION FOR APPLICATIONS SUCH AS NAVIGATION SYSTEMS OF AUTONOMOUS VEHICLES.
utt_0034 utt 122.51 127.67 -X THE OBJECTIVE OF OUR WORK IS TO ASSESS THE EFFECTIVENESS OF ADVERSARIAL TRAINING AGAINST ADVERSARIAL PATCH ATTACKS.
utt_0036 utt 128.62 133.14 -X WE FIRST PROPOSE AN ATTACK STRATEGY THAT JOINTLY OPTIMIZES PATCH VALUES AND LOCATION.
utt_0037 utt 133.26 139.48 -X THEN, WE EVALUATE THE PERFORMANCE OF ADVERSARIAL TRAINING AS A DEFENSE AGAINST SUCH ATTACKS.
utt_0039 utt 139.73 145.14 -X FOR USING ADVERSARIAL TRAINING, A DESIRED PROPERTY IS TO HAVE THE STRONGEST POSSIBLE ATTACK FOR EACH IMAGE.
utt_0041 utt 145.87 152.56 -X THIS IS BECAUSE A NETWORK TRAINED USING EXAMPLES RESULTING FROM A STRONG ATTACK CAN BE EXPECTED TO BE ALSO ROBUST AGAINST WEAKER ATTACKS.
utt_0043 utt 153.84 158.20 -X TO ACHIEVE THIS, WE MAKE THE FOLLOWING DESIGN CHOICES FOR THE PATCHES USED IN OUR ATTACK.
utt_0044 utt 158.22 163.06 -X FIRST, THE PATCHES ARE IMAGE-SPECIFIC, WITH A SEPARATE PATCH TAILORED TO EACH INDIVIDUAL IMAGE.
utt_0046 utt 163.28 166.74 -X SUCH ATTACKS HAVE BEEN SHOWN TO BE STRONGER THAN UNIVERSAL PATCH ATTACKS.
utt_0047 utt 167.50 179.16 -X SECOND, THEY ARE UNTARGETED, I.E., WE ONLY OPTIMIZE THE PATCH TO PUSH THE PREDICTION OF THE CLASSIFIER AWAY FROM THE TRUE CLASS, AND DO NOT TRY TO FORCE IT TO PREDICT ANY SPECIFIC ALTERNATIVE CLASS.
utt_0050 utt 179.16 188.24 -X THIRD, THEY ARE LOCATION OPTIMIZED, WHERE WE USE A HEURISTIC APPROACH IN AN ATTEMPT TO APPROXIMATELY FIND THE OPTIMAL PATCH LOCATION.
utt_0052 utt 188.24 192.66 -X OPTIMIZING PATCH LOCATION CAN BE HELPFUL SINCE NOT ALL PATCH LOCATIONS MAY BE EQUALLY EFFECTIVE.
utt_0054 utt 192.85 196.98 -X WE WOULD LIKE TO FIND THAT LOCATION FOR ANY IMAGE WHERE THE ATTACK IS THE STRONGEST.
utt_0056 utt 197.42 207.61 -X HOWEVER, AT THE SAME TIME, WE WOULD LIKE TO ENSURE THAT THE PATCH DOES NOT COMPLETELY OCCLUDE FEATURES OF THE IMAGE THAT ARE VITAL FOR THE CLASSIFIER TO CLASSIFY ACCURATELY, SINCE SUCH AN ATTACK WOULD NOT BE VERY INTERESTING.
utt_0059 utt 208.15 217.46 -X AS AN APPROXIMATION FOR ENFORCING THIS, IN OUR LOCATION OPTIMIZATION SCHEME WE ALLOW PATCHES TO BE PLACED ANYWHERE EXCEPT IN THE CENTER OF THE IMAGE.
utt_0061 utt 217.46 222.52 -X NEXT, I WILL DESCRIBE THE SETTINGS USED FOR SPECIFYING THE PATCH LOCATION.
utt_0062 utt 222.52 227.77 -X FIRST, WE USE TWO STRATEGIES TO DECIDE THE INITIAL LOCATION OF THE PATCH AT THE START OF THE ATTACK PROCEDURE.
utt_0064 utt 228.31 232.37 -X THE FIRST IS TO PLACE THE PATCH AT A LOCATION NEAR A CORNER OF THE IMAGE,
utt_0065 utt 232.37 234.10 -X AND KEEP IT FIXED THERE THROUGHOUT THE ATTACK.
utt_0066 utt 234.51 238.68 -X THIS ALSO SERVES AS A BASELINE TO COMPARE AGAINST THE USE OF LOCATION OPTIMIZATION.
utt_0067 utt 239.70 246.29 -X THE SECOND IS TO PLACE THE PATCH AT ANY RANDOM LOCATION ON THE IMAGE, AS LONG AS IT DOES NOT INTERSECT WITH THE DESIGNATED CENTER REGION.
utt_0069 utt 246.83 255.00 -X WITH THIS INITIALIZATION, WE CONSIDER BOTH THE CASE OF KEEPING THE PATCH FIXED AS WELL AS OPTIMIZING THE LOCATION.
utt_0071 utt 255.00 262.04 -X NEXT, TO OPTIMIZE LOCATION, WE USE A SIMPLE APPROACH: WE CHECK IF A LOCATION IN THE NEIGHBORHOOD OF THE CURRENT PATCH LOCATION IS BETTER.
utt_0073 utt 262.04 269.43 -X THIS IS DONE BY MOVING THE PATCH TO EACH LOCATION IN THE NEIGHBORHOOD AND CHECKING IF IT IS MORE EFFECTIVE, I.E., IF IT LEADS TO A HIGHER CLASSIFICATION LOSS.
utt_0076 utt 269.43 272.85 -X IF SUCH A LOCATION EXISTS, THE PATCH IS MOVED TO THAT LOCATION.
utt_0077 utt 272.98 276.12 -X BASED ON THE DEFINITION OF NEIGHBORHOOD, WE USE TWO APPROACHES.
utt_0079 utt 276.15 284.95 -X IN THE FIRST APPROACH, FULL LOCATION OPTIMIZATION, WE SEARCH BY MOVING THE PATCH BY A FIXED NUMBER OF PIXELS IN EACH OF THE FOUR DIRECTIONS: UP, DOWN, LEFT AND RIGHT.
utt_0082 utt 286.04 293.40 -X IN THE SECOND APPROACH, RANDOM LOCATION OPTIMIZATION, WE SEARCH BY RANDOMLY SELECTING A DIRECTION IN EACH STEP AND ONLY CHECKING ALONG THAT DIRECTION.
utt_0084 utt 294.00 305.91 -X THIS MAY MISS SOME OPPORTUNITIES TO IMPROVE PATCH LOCATION BUT BENEFITS FROM A LOWER COMPUTATIONAL COST, SINCE IT REQUIRES JUST ONE ADDITIONAL FORWARD PASS OF THE NETWORK AS COMPARED TO FOUR ADDITIONAL FORWARD PASSES FOR FULL LOCATION OPTIMIZATION.
utt_0087 utt 308.72 311.32 -X THE FOLLOWING IS THE OPTIMIZATION FUNCTION FOR THE ATTACK.
utt_0088 utt 311.73 315.99 -X THE PATCH IS DEFINED BY DELTA, THAT REPRESENTS THE PERTURBATIONS, OR PATCH VALUES,
utt_0089 utt 316.24 319.67 -X AND M, THAT REPRESENTS THE MASK, WHICH INDICATES THE PATCH LOCATION.
utt_0090 utt 320.02 325.21 -X THE GOAL IS THEN TO FIND DELTA AND M THAT MAXIMIZE THE CROSS ENTROPY CLASSIFICATION LOSS OF THE NETWORK.
utt_0092 utt 326.80 329.91 -X FINALLY, THE FOLLOWING IS THE ATTACK PROCEDURE.
utt_0093 utt 329.91 334.42 -X THE PATCH IS INITIALIZED WITH RANDOM VALUES AND PLACED EITHER AT A FIXED OR RANDOM LOCATION.
utt_0095 utt 334.42 339.80 -X THEN, PATCH VALUES AND LOCATION ARE OPTIMIZED IN ALTERNATING STEPS.
utt_0097 utt 339.80 347.06 -X NEXT, I WILL SHOW A VISUAL EXAMPLE OF THE ATTACK PROCEDURE WHERE THE PATCH IS PLACED AT A RANDOM LOCATION AND OPTIMIZED USING FULL LOCATION OPTIMIZATION.
utt_0099 utt 348.88 354.39 -X WE HAVE AN INPUT IMAGE AND A TRAINED CLASSIFIER WHICH GIVES THE FOLLOWING OUTPUT.
utt_0100 utt 354.39 359.26 -X LET US ASSUME THAT THERE ARE ONLY FOUR CLASSES TO PREDICT FROM, AND HERE, DOG IS THE CORRECT CLASS.
utt_0102 utt 359.77 366.39 -X THE PLOT ON THE RIGHT TOP SHOWS THE CLASSIFICATION LOSS, AND PLOT ON THE RIGHT BOTTOM SHOWS THE CONFIDENCE OF THE NETWORK FOR EACH CLASS.
utt_0104 utt 368.12 374.59 -X THE FIRST STEP IS TO INITIALIZE THE PATCH, WHICH IS DONE AT RANDOM, AND IT IS THEN PLACED AT A RANDOM LOCATION OUTSIDE THE CENTER REGION,
utt_0106 utt 374.59 376.99 -X AS SHOWN.
utt_0107 utt 376.99 380.06 -X THEN, IN THE FIRST ITERATION, WE FIRST PERFORM A FORWARD PASS.
utt_0108 utt 380.21 384.54 -X THIS MAY LEAD TO A SMALL INCREASE IN LOSS DUE TO IT BLOCKING PARTS OF THE IMAGE.
utt_0110 utt 384.54 393.66 -X BUT IN GENERAL, THIS IN ITSELF IS NOT ENOUGH TO CAUSE A MISCLASSIFICATION AS THE CLASSIFIER IS USUALLY ROBUST TO SMALL OCCLUSIONS.
utt_0112 utt 393.66 400.19 -X THEN, WE PERFORM A BACKWARD PASS AND COMPUTE GRADIENTS, AND UPDATE THE PATCH VALUES.
utt_0113 utt 400.19 402.55 -X NEXT, WE PERFORM THE LOCATION OPTIMIZATION STEP.
utt_0114 utt 403.83 407.99 -X WE FIRST MOVE THE PATCH UP BY A FIXED STRIDE, WHICH IN THIS EXAMPLE IS one PIXEL.
utt_0115 utt 408.15 411.03 -X WE THEN PERFORM A FORWARD PASS, AND COMPUTE THE LOSS,
utt_0116 utt 411.03 414.52 -X AS SHOWN.
utt_0117 utt 414.52 417.34 -X NEXT, WE MOVE THE PATCH RIGHT BY one PIXEL.
utt_0118 utt 417.34 422.84 -X ONCE AGAIN, WE PERFORM A FORWARD PASS AND COMPUTE THE LOSS.
utt_0119 utt 422.84 425.59 -X WE REPEAT THIS FOR ALL FOUR DIRECTIONS.
utt_0120 utt 425.59 427.67 -X NOTE THAT THE LOSS NEED NOT ALWAYS INCREASE.
utt_0121 utt 430.23 435.87 -X FINALLY, WE CHOOSE THE DIRECTION THAT LED TO THE HIGHEST LOSS INCREASE, AND MOVE THE PATCH IN THAT DIRECTION.
utt_0123 utt 435.87 438.49 -X IN THIS CASE, WE MOVE THE PATCH RIGHT.
utt_0124 utt 438.49 446.14 -X IN GENERAL, IT IS ALSO POSSIBLE THAT NO DIRECTION LEADS TO A LOSS INCREASE, IN WHICH CASE THE PATCH IS NOT MOVED AT ALL IN THAT ITERATION.
utt_0126 utt 446.62 449.40 -X THIS ENDS THE FIRST ITERATION.
utt_0127 utt 449.40 455.61 -X IN THE SECOND ITERATION, THE PATCH VALUES ARE AGAIN UPDATED, FOLLOWED BY THE LOCATION OPTIMIZATION STEP.
utt_0129 utt 456.57 458.52 -X WE FIRST MOVE UP, AS BEFORE.
utt_0130 utt 459.61 464.89 -X BUT IN THIS CASE, WE DO NOT MOVE RIGHT, SINCE DOING SO WOULD CAUSE THE PATCH TO INTERSECT WITH THE CENTER REGION.
utt_0132 utt 464.89 467.20 -X THIS IS SKIPPED AND WE MOVE ON TO THE NEXT STEP.
utt_0133 utt 468.89 472.89 -X AGAIN, AT THE END, THE PATCH IS MOVED IN THE DIRECTION OF THE HIGHEST CLASSIFICATION LOSS,
utt_0134 utt 472.89 474.49 -X WHICH IS UP IN THIS EXAMPLE.
utt_0135 utt 477.11 479.36 -X THIS IS REPEATED FOR SEVERAL ITERATIONS.
utt_0136 utt 479.36 487.96 -X AT THE END, WE HAVE OBTAINED AN IMAGE WITH AN ADVERSARIAL PATCH, THAT IN THIS CASE HAS BEEN SUCCESSFUL IN CAUSING THE NETWORK TO MISCLASSIFY THIS IMAGE OF A DOG AS A CAT.
utt_0139 utt 491.03 500.00 -X THE LOCATION OPTIMIZATION PROCEDURE IS ONLY AN APPROXIMATE HEURISTIC, AND WOULD LIKELY NOT ALLOW THE PATCH TO MOVE VERY FAR FROM ITS INITIAL LOCATION DUE TO THE PRESENCE LOCAL OPTIMA.
utt_0142 utt 500.25 503.23 -X MOREOVER, THE PATCH IS INITIALIZED AND PLACED RANDOMLY.
utt_0143 utt 503.67 509.76 -X TO SMOOTH OUT THE EFFECTS OF RANDOMNESS, WE RUN THE ATTACK SEVERAL TIMES AND SELECT THE PATCH THAT LEADS TO THE HIGHEST CLASSIFICATION LOSS.
utt_0145 utt 510.55 515.52 -X SO, WE HAVE THE INPUT IMAGE, THE ATTACK ALGORITHM, AND THE PATCHED IMAGE.
utt_0146 utt 516.28 518.14 -X THIS IS REPEATED SEVERAL TIMES,
utt_0147 utt 521.08 524.22 -X AND THE IMAGE LEADING TO THE HIGHEST CLASSIFICATION LOSS IS CHOSEN,
utt_0148 utt 524.22 526.21 -X WHICH IS THE THIRD IMAGE IN THIS CASE.
utt_0149 utt 527.51 530.53 -X THIS CONCLUDES THE DESCRIPTION OF THE ATTACK PROCEDURE.
utt_0150 utt 530.53 537.21 -X NEXT, I WILL DESCRIBE THE PROCEDURE OF ADVERSARIAL TRAINING AGAINST THIS ATTACK, WHICH WE REFER TO IN SHORT AS ADVERSARIAL PATCH TRAINING.
utt_0152 utt 538.87 543.26 -X THE OBJECTIVE IS TO TRAIN THE NETWORK SO THAT IT IS ABLE TO CORRECTLY CLASSIFY BOTH CLEAN,
utt_0153 utt 543.26 546.27 -X I.E., UNATTACKED, AS WELL AS ADVERSARIALLY PATCHED IMAGES.
utt_0155 utt 547.45 552.22 -X TO DO THIS, WE USE THE FOLLOWING OPTIMIZATION FUNCTION THAT OPTIMIZES WEIGHTS TO MINIMIZE THE LOSS.
utt_0157 utt 552.44 558.56 -X THE FIRST TERM CORRESPONDS TO OPTIMIZING FOR ADVERSARIALLY PATCHED IMAGES, WHILE THE SECOND CORRESPONDS TO OPTIMIZING FOR CLEAN IMAGES.
utt_0159 utt 558.97 567.29 -X TO JOINTLY OPTIMIZE FOR BOTH, WE ATTACK HALF THE IMAGES IN EACH BATCH IN TRAINING AND USE CLEAN IMAGES FOR THE OTHER HALF.
utt_0162 utt 567.29 571.23 -X NEXT, I WILL DESCRIBE A VISUAL EXAMPLE OF THE ADVERSARIAL PATCH TRAINING PROCEDURE.
utt_0163 utt 573.59 576.80 -X THE FOLLOWING IS A BATCH OF FOUR IMAGES ALONG WITH THEIR TRUE LABELS.
utt_0164 utt 577.50 584.80 -X AND THIS IS THE ATTACK ALGORITHM, WHICH USES THE CURRENT ITERATION OF THE TRAINED NETWORK TO GENERATE ADVERSARIAL PATCHES.
utt_0166 utt 584.80 588.61 -X WE PASS HALF THE IMAGES THROUGH THE ALGORITHM AND OBTAIN PATCHED IMAGES FOR THEM.
utt_0167 utt 588.61 590.14 -X THIS IS THE ATTACK STEP.
utt_0168 utt 592.38 594.69 -X THE NETWORK IS THEN USED FOR THE TRAINING STEP.
utt_0169 utt 595.87 601.25 -X THE BATCH OF IMAGES IS PASSED THROUGH THE NETWORK, AND WE OBTAIN PREDICTIONS.
utt_0170 utt 601.25 606.59 -X INITIALLY, EVEN FOR A TRAINED NETWORK, MANY OF THE PREDICTIONS FOR ADVERSARIALLY PATCHED IMAGES WILL LIKELY BE INCORRECT.
utt_0172 utt 608.41 613.76 -X WE THEN PERFORM A BACKWARD PASS AND UPDATE THE WEIGHTS, TRAINING THE NETWORK TO CORRECTLY CLASSIFY PATCHED IMAGES.
utt_0174 utt 615.61 621.89 -X THIS NETWORK WITH UPDATED WEIGHTS IS THEN USED BY THE ATTACK ALGORITHM FOR THE NEXT ITERATION.
utt_0176 utt 621.89 628.51 -X THIS PROCEDURE IS REPEATED FOR SEVERAL ITERATIONS, AND WE FINALLY END UP WITH A NETWORK THAT IS ROBUST TO ADVERSARIALLY PATCHED IMAGES.
utt_0178 utt 630.36 633.31 -X THIS CONCLUDES THE DESCRIPTION OF ADVERSARIAL PATCH TRAINING.
utt_0180 utt 633.50 636.80 -X NEXT, I WILL PROVIDE A BRIEF OVERVIEW OF THE EXPERIMENTAL RESULTS.
utt_0181 utt 637.72 640.00 -X THE FOLLOWING IS THE SETUP.
utt_0182 utt 640.00 647.46 -X WE PERFORM EXPERIMENTS ON THE CIFARten AND THE GERMAN TRAFFIC SIGN RECOGNITION BENCHMARK DATASETS ON A RESNETminus twenty NETWORK ARCHITECTURE.
utt_0184 utt 647.48 650.50 -X WE USE eight X eight SIZED PATCHES ON THE thirty-two X thirty-two SIZED IMAGES.
utt_0185 utt 651.39 659.42 -X THIS IS BECAUSE THIS WAS THE SIZE WHEN THE ROBUST TEST ERROR OF A NORMALLY TRAINED NETWORK, WHICH IS THE CLASSIFICATION ERROR ON ADVERSARIALLY PATCHED IMAGES, SATURATED.
utt_0188 utt 659.71 663.94 -X THIS CAN BE SEEN IN THE FIGURE ON THE RIGHT.
utt_0189 utt 663.94 668.48 -X WE USE FOUR ATTACK CONFIGURATIONS: WITH THE PATCH AT A FIXED LOCATION, CALLED AP-FIXED,
utt_0190 utt 668.48 680.64 -X WITH THE PATCH AT A RANDOM LOCATION WITHOUT LOCATION OPTIMIZATION, CALLED AP-RAND, WITH THE PATCH INITIALIZED AT A RANDOM LOCATION AND OPTIMIZED USING RANDOM LOCATION OPTIMIZATION, CALLED AP-RANDLO, AND WITH THE PATCH INITIALIZED
utt_0193 utt 680.64 684.90 -X AT A RANDOM LOCATION AND OPTIMIZED USING FULL LOCATION OPTIMIZATION, CALLED AP-FULLLO.
utt_0194 utt 687.13 700.39 -X THEN, FOR EACH OF THE ATTACK CONFIGURATIONS, WE TRAIN A MODEL USING ADVERSARIAL PATCH TRAINING, AND OBTAIN FOUR MODELS: AT-FIXED, AT-RAND, AT-RANDLO, AND AT-FULLLO RESPECTIVELY.
utt_0196 utt 700.39 708.03 -X THE COMPUTATIONAL EFFORT FOR PERFORMING THE ATTACK, IN TERMS OF THE NUMBER OF ITERATIONS TIMES THE NUMBER OF ATTEMPTS, IS DIFFERENT WHEN TRAINING AND EVALUATING.
utt_0198 utt 708.03 712.16 -X FOR TRAINING, THIS IS SET TO BE twenty-five, WHILE FOR EVALUATING, THIS IS SET TO BE three thousand.
utt_0199 utt 712.16 715.94 -X THIS IS BECAUSE TRAINING IS MORE COMPUTATIONALLY INTENSIVE.
utt_0200 utt 716.51 722.31 -X USING A MUCH STRONGER ATTACK FOR EVALUATION ALSO INCREASES CONFIDENCE IN THE ROBUSTNESS RESULTS FOR EACH MODEL.
utt_0202 utt 723.87 726.24 -X I WILL NOW PRESENT SOME OF THE RESULTS ON CIFARten.
utt_0203 utt 726.24 728.58 -X FURTHER RESULTS CAN BE FOUND IN THE PAPER.
utt_0204 utt 730.69 735.43 -X THE FOLLOWING TABLE SHOWS THE ROBUST TEST ERROR FOR EACH ATTACK ON EACH MODEL.
utt_0205 utt 735.43 740.83 -X NORMAL REFERS TO A MODEL TRAINED WITHOUT ADVERSARIAL PATCH TRAINING.
utt_0207 utt 740.83 744.45 -X WE SEE THAT GIVEN THE ATTACK EFFORT, EVERY ATTACK IS NEARLY one hundred% SUCCESSFUL AGAINST THE
utt_0208 utt 744.73 747.81 -X NORMAL MODEL.
utt_0209 utt 747.81 752.35 -X THE AT-FIXED AND AT-RAND MODELS SHOW SOME ROBUSTNESS AGAINST THE ATTACKS AS COMPARED TO NORMAL.
utt_0211 utt 752.80 758.08 -X AT-RAND PERFORMS BETTER BECAUSE IT HAS BEEN TRAINED TO DEFEND AGAINST PATCHES AT ANY ALLOWED LOCATION ON THE IMAGE.
utt_0213 utt 758.53 764.87 -X WE ALSO SEE THAT THE ATTACK STRENGTH INCREASES FROM LEFT TO RIGHT, WITH THE ATTACKS USING LOCATION OPTIMIZATION PERFORMING THE BEST.
utt_0215 utt 766.66 774.18 -X FINALLY, THE AT-RANDLO AND AT-FULLLO MODELS PERFORM THE BEST, WITH AT-FULLLO PERFORMING SUBSTANTIALLY BETTER.
utt_0217 utt 774.78 781.22 -X THIS SHOWS THAT FULL LOCATION OPTIMIZATION IS VERY HELPFUL IN IMPROVING THE EFFECTIVENESS OF ADVERSARIAL PATCH TRAINING.
utt_0219 utt 782.50 787.72 -X THIS TABLE SHOWS THE CLEAN TEST ERROR, I.E., THE CLASSIFICATION ERROR ON UNATTACKED IMAGES,
utt_0220 utt 787.72 788.58 -X FOR EACH MODEL.
utt_0221 utt 789.15 796.13 -X WE SEE THAT ADVERSARIAL PATCH TRAINING, WHILE IMPROVING ROBUSTNESS AGAINST PATCHED IMAGES, DOES NOT DECREASE ACCURACY AGAINST CLEAN IMAGES.
utt_0223 utt 798.91 806.63 -X FINALLY, THE FOLLOWING FIGURE SHOWS HEATMAPS OF SUCCESSFUL ATTACKS WITH FULL LOCATION OPTIMIZATION ACROSS one thousand ATTEMPTS FOR A SINGLE IMAGE ON EACH MODEL.
utt_0225 utt 807.58 814.66 -X FOR NORMAL, WE SEE THAT IT IS SPREAD NEARLY UNIFORMLY ACROSS THE ALLOWED REGION, INDICATING THAT ATTACKS ARE SUCCESSFUL NEARLY EVERYWHERE.
utt_0228 utt 815.04 824.64 -X HOWEVER, FOR THE OTHER FOUR MODELS, WE SEE THAT ADVERSARIAL PATCH TRAINING REDUCES THE REGION WHERE THE ATTACK IS SUCCESSFUL, AND ONLY PATCHES AT CERTAIN LOCATIONS ARE SUCCESSFUL IN FOOLING THE NETWORK.
utt_0231 utt 825.28 835.36 -X IN THIS PARTICULAR EXAMPLE, FOR THE AT-FULLLO MODEL, NONE OF THE one thousand ATTEMPTS WERE ABLE TO GENERATE A PATCHED IMAGE THAT COULD FOOL THE NETWORK.
utt_0233 utt 835.36 840.65 -X TO CONCLUDE, IN THIS WORK, WE PROPOSED AN ADVERSARIAL PATCH ATTACK STRATEGY WITH LOCATION OPTIMIZATION.
utt_0235 utt 840.65 850.50 -X WE SHOWED THAT LOCATION OPTIMIZATION STRENGTHENS THE ATTACK, AND FINALLY, WE SHOWED THAT ADVERSARIAL PATCH TRAINING WITH LOCATION-OPTIMIZED PATCHES IMPROVES MODEL ROBUSTNESS.
utt_0238 utt 850.50 854.37 -X THE FOLLOWING ARE LINKS TO THE PAPER, CODE, AND CONTACT INFORMATION.
utt_0239 utt 854.37 856.74 -2.8497 THANK YOU FOR YOUR ATTENTION.
