utt_0000 utt 1.26 7.70 -X HI, MY NAME IS WASI AHMAD AND I AM PRESENTING OUR WORK, “A TRANSFORMER-BASED APPROACH FOR SOURCE CODE SUMMARIZATION”.
utt_0002 utt 8.11 15.70 -X THIS IS JOINT WORK WITH SAIKAT CHAKRABORTY, BAISHAKHI RAY FROM COLUMBIA UNIVERSITY, AND MY ADVISOR KAI-WEI CHANG FROM UCLA.
utt_0004 utt 15.70 23.92 -X SOURCE CODE SUMMARIZATION REFERS TO THE TASK OF CREATING HUMAN-READABLE SUMMARIES THAT DESCRIBE THE FUNCTIONALITY OF A PROGRAM.
utt_0006 utt 24.27 29.04 -X WITH THE PROGRESS OF NATURAL LANGUAGE GENERATION USING NEURAL SEQUENCE-TO-SEQUENCE LEARNING,
utt_0007 utt 29.07 38.29 -X RECENT APPROACHES IN LITERATURE FRAME CODE SUMMARIZATION AS TRANSLATING A PIECE OF SOURCE CODE INTO A SHORT NATURAL LANGUAGE DESCRIPTION.
utt_0009 utt 38.29 46.84 -X FOR EXAMPLE, GIVEN THIS PYTHON SOURCE CODE SNIPPET, A CODE SUMMARIZATION MODEL SHOULD BE ABLE TO GENERATE A SUMMARY, SIMILAR TO THE HUMAN-WRITTEN ONE.
utt_0011 utt 48.24 60.34 -X IN THIS WORK, WE STUDY THE TRANSFORMER, A SEQUENCE GENERATION MODEL THAT HAS BEEN FOUND EFFECTIVE IN MANY NATURAL LANGUAGE GENERATION APPLICATIONS BUT HASN’T BEEN EXPLORED IN SOURCE CODE SUMMARIZATION.
utt_0014 utt 61.74 70.45 -X A NOTABLE AMOUNT OF PRIOR WORKS IN SOURCE CODE SUMMARIZATION HAS LEVERAGED RECURRENT NEURAL NETWORKS TO LEARN SOURCE CODE REPRESENTATIONS.
utt_0016 utt 70.45 78.71 -X TRANSFORMER, IN CONTRAST, ENTIRELY RELIES ON THE SELF-ATTENTION MECHANISM, WHICH IS WELL-KNOWN FOR ITS EFFECTIVENESS IN CAPTURING LONG-RANGE DEPENDENCIES.
utt_0018 utt 79.22 83.96 -X SUCH A CHARACTERISTIC IS ESSENTIAL TO LEARN PROGRAM REPRESENTATIONS.
utt_0019 utt 83.96 92.09 -X BESIDES, THE ORDER OF SOURCE CODE TOKENS PLAYS AN IMPORTANT ROLE IN EMBEDDING THE CODE STRUCTURE INTO LEARNED REPRESENTATIONS.
utt_0021 utt 92.09 101.27 -X HENCE, IN THIS WORK, WE STUDY DIFFERENT POSITIONAL ENCODING SCHEMES TO FIND AN EFFECTIVE WAY TO ENCODE THE SOURCE CODE STRUCTURE.
utt_0023 utt 101.97 107.61 -X WE STUDY THE IMPACT OF THREE DIFFERENT FORMS OF POSITIONAL ENCODING IN LEARNING SOURCE CODE REPRESENTATIONS.
utt_0025 utt 108.02 113.18 -X THE ABSOLUTE POSITION ENCODING SCHEME TREATS SOURCE CODE AS A LINEAR SEQUENCE OF CODE TOKENS,
utt_0026 utt 113.46 120.25 -X WHILE THE RELATIVE POSITION ENCODING CONSIDERS SOURCE CODE AS A FULLY CONNECTED GRAPH, EITHER DIRECTED OR UNDIRECTED.
utt_0028 utt 120.98 122.68 -X WE SHOW AN EXAMPLE HERE.
utt_0029 utt 123.92 131.70 -X FOR THE EXPRESSION, “A + B”, ABSOLUTE POSITION ENCODING USES THE TOKENS’ INDEX TO FORM THEIR RESPECTIVE POSITION REPRESENTATIONS.
utt_0031 utt 131.89 138.52 -X THE RELATIVE POSITION ENCODING, ON THE OTHER HAND, MODELS THE PAIRWISE DISTANCE BETWEEN CODE TOKENS.
utt_0033 utt 138.52 148.60 -X WE CONSIDER TWO VARIANTS OF RELATIVE POSITION ENCODING, WHEREIN ONE VARIANT WE COMPUTE RELATIVE DISTANCES BASED ON WHETHER A TOKEN IS ON THE LEFT OR RIGHT OF THE TARGET TOKEN.
utt_0035 utt 148.60 157.37 -X IN THE SECOND VARIANT, WE SIMPLY IGNORE THE DIRECTION, TREATING THE SOURCE CODE AS AN UNDIRECTED GRAPH.
utt_0037 utt 158.39 163.87 -X WE CONDUCT EXPERIMENTS ON TWO WELL-STUDIED DATASETS IN JAVA AND PYTHON PROGRAMMING LANGUAGES,
utt_0038 utt 163.87 165.75 -X COLLECTED FROM GITHUB.
utt_0039 utt 165.75 168.15 -X THE DATASETS ARE PRE-PROCESSED FOLLOWING A PRIOR WORK.
utt_0040 utt 169.50 179.07 -X IN AN ADDITIONAL PREPROCESSING STEP, WE SUB-TOKENIZE THE SOURCE CODE TOKENS BASED ON CAMEL CASE AND SNAKE CASE THAT IMPROVES CODE SUMMARIZATION SIGNIFICANTLY.
utt_0042 utt 179.07 183.87 -X WE USE BLEU, METEOR, AND ROUGE-L AS THE EVALUATION METRICS.
utt_0043 utt 186.49 191.74 -X WE COMPARE THE TRANSFORMER WITH SIX STATE-OF-THE-ART APPROACHES AS BASELINES THAT UTILIZE RNN-BASED
utt_0044 utt 192.09 193.82 -X SEQUENCE-TO-SEQUENCE LEARNING.
utt_0045 utt 193.98 201.31 -X THE BASELINE APPROACHES MODEL DIFFERENT FORMS OF KNOWLEDGE ABOUT SOURCE CODE, SUCH AS ABSTRACT SYNTAX TREE STRUCTURE, API KNOWLEDGE.
utt_0047 utt 201.31 207.90 -X THEY ALSO COVER A VARIETY OF TECHNIQUES, INCLUDING REINFORCEMENT LEARNING AND DUAL LEARNING.
utt_0048 utt 207.90 216.77 -X IN THIS WORK, WE CONSIDER A SIMPLE SETUP WHERE THE TRANSFORMER TAKES THE SOURCE CODE AS INPUT AND LEARNS TO GENERATE THE SUMMARY WITH SUPERVISED TRAINING.
utt_0050 utt 216.77 225.60 -X THE OVERALL RESULTS SHOW THAT THE TRANSFORMER OUTPERFORMS ALL THE BASELINE APPROACHES BY A SIGNIFICANT MARGIN.
utt_0052 utt 225.60 233.63 -X NEXT, WE SHOW THE IMPACT OF USING ABSOLUTE POSITIONS WHILE ENCODING SOURCE CODE TOKENS AND THE WORDS IN THE NATURAL LANGUAGE SUMMARY.
utt_0054 utt 233.63 240.80 -X ENCODING THE ABSOLUTE POSITION OF THE SUMMARY IS NECESSARY AS THE TRANSFORMER LEARNS TO GENERATE SUMMARIES WORD-BY-WORD.
utt_0056 utt 240.80 250.88 -X IN CONTRAST, WE CAN SEE THAT THE USE OF ABSOLUTE POSITION ENCODING FOR SOURCE CODE HURTS THE PERFORMANCE, WHICH ILLUSTRATES THAT TREATING SOURCE CODE AS A LINEAR SEQUENCE OF TOKENS
utt_0058 utt 250.94 252.26 -X IS NOT ACCURATE.
utt_0059 utt 252.26 266.18 -X SINCE SOURCE CODE HAS A NON-SEQUENTIAL STRUCTURE, WE HYPOTHESIZE RELATIVE POSITION ENCODING FOR SOURCE CODE TOKENS WOULD RESULT IN BETTER CODE REPRESENTATIONS THAT CAN IMPROVE SUMMARIZATION.
utt_0061 utt 266.40 270.21 -X OUR EXPERIMENTAL FINDINGS PROVE OUR HYPOTHESIS.
utt_0062 utt 270.21 280.48 -X IN THIS BAR PLOT, WE COMPARE THE TWO FORMS OF RELATIVE POSITION ENCODING, WHEREIN ONE FORM, SOURCE CODE IS TREATED AS A DIRECTED GRAPH, WHILE IN OTHER, IT IS CONSIDERED AS AN UNDIRECTED GRAPH.
utt_0065 utt 280.99 285.09 -X THE RESULTS SUGGEST THAT SOURCE CODE SHOULD BE VIEWED AS A DIRECTED GRAPH.
utt_0066 utt 285.09 296.93 -X IN OTHER WORDS, WHILE MODELING THE PAIRWISE RELATIONSHIP BETWEEN CODE TOKENS, WHETHER A TOKEN IS IN THE LEFT OR RIGHT OF THE TARGET TOKEN SHOULD BE EMPHASIZED.
utt_0068 utt 296.96 302.12 -X WE STUDY TRANSFORMER FOR SOURCE CODE SUMMARIZATION THAT OUTPERFORMS STATE-OF-THE-ART APPROACHES.
utt_0069 utt 302.34 305.28 -X THE CODE FOR REPRODUCING THE EXPERIMENTS IS IN GITHUB.
utt_0070 utt 305.44 311.49 -X WE HOPE OUR WORK WOULD BE CONSIDERED AS A BASELINE IN FUTURE WORKS.
utt_0071 utt 311.55 317.16 -2.9771 THANK YOU FOR LISTENING!
