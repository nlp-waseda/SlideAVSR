utt_0000 utt 0.59 5.36 -X HELLO EVERYONE, I AM CHUHENG FROM IIIS OF TSINGHUA UNIVERSITY.
utt_0001 utt 5.36 8.66 -X I AM VERY HAPPY HERE TO PRESENT OUR WORK.
utt_0002 utt 8.75 14.45 -X THE TITLE IS POLICY SEARCH BY TARGET DISTRIBUTION LEARNING FOR CONTINUOUS CONTROL.
utt_0003 utt 14.45 18.03 -X IT IS A JOINT WORK WITH YUANQI LI AND JIAN LI.
utt_0004 utt 18.03 25.46 -X THE CONTINUOUS CONTROL PROBLEM IS A REINFORCEMENT LEARNING PROBLEM WHERE THE ACTION IS SPECIFIED BY CONTINUOUS VALUES.
utt_0006 utt 25.81 32.34 -X IT HAS MANY REAL APPLICATIONS, SUCH AS ROBOTIC CONTROL, FINANCE AND HEALTHCARE.
utt_0007 utt 32.34 39.89 -X THERE ARE TWO MAIN STREAMS TO SOLVE THE RL PROBLEM, THE VALUE-BASED METHODS AND THE POLICY-BASED METHODS.
utt_0009 utt 40.14 46.29 -X HOWEVER, VALUE-BASED METHODS ARE NOT WELL-SUITED TO SOLVE THE CONTINUOUS CONTROL PROBLEMS,
utt_0010 utt 46.96 54.48 -X SINCE IS HARD TO CONVERT THE LEARNED VALUE FUNCTIONS INTO AN EXECUTABLE CONTINUOUS CONTROL POLICY.
utt_0012 utt 54.48 60.53 -X ON THE OTHER HAND, POLICY-BASED METHODS ARE SUITABLE WHERE THE POLICY IS LEARNED DIRECTLY.
utt_0013 utt 61.29 71.76 -X CURRENTLY, WE ALREADY HAVE MANY ALGORITHMS FOR CONTINUOUS CONTROL WITH GOOD ASYMPTOTIC PERFORMANCE OR SAMPLE EFFICIENCY, SUCH AS TRPO, PPO, SAC, DDPG.
utt_0015 utt 75.44 82.39 -X HOWEVER, ROBUSTNESS IS ALSO IMPORTANT AND ESPECIALLY IMPORTANT FOR REAL APPLICATIONS.
utt_0016 utt 82.39 90.87 -X IN REAL APPLICATIONS, WE MAY DEPLOY A PRELIMINARY POLICY TO REAL SYSTEMS AND FURTHER TRAIN THE POLICY USING REAL SAMPLES.
utt_0018 utt 91.09 96.82 -X IN THIS CASE, AN UNSTABLE TRAINING PROCESS MAY CAUSE DAMAGE TO REAL SYSTEMS.
utt_0019 utt 97.26 104.18 -X ALSO, HIGH VARIANCE ACROSS DIFFERENT RUNS OR DIFFERENT HYPERPARAMETERS IS UNDESIRABLE,
utt_0020 utt 104.62 110.16 -X SINCE WE MAY HAVE TO RUN MULTIPLE TIMES TO OBTAIN A SATISFACTORY PERFORMANCE.
utt_0021 utt 110.61 120.76 -X THEREFORE, THE OBJECTIVE OF OUR WORK IS TO FIND AN ALGORITHM NOT ONLY WITH GOOD PERFORMANCE BUT ALSO BEING ROBUST.
utt_0023 utt 120.76 127.57 -X IN THIS PAPER, WE CONDUCTED EXPERIMENTS TO DEMONSTRATE A SOURCE OF INSTABILITY THAT EXISTS
utt_0024 utt 127.63 130.48 -X IN SOME POLICY-BASED ALGORITHMS.
utt_0025 utt 130.99 137.69 -X ESPECIALLY WE FOUND THAT SUCH INSTABILITY EXISTS IN A VERY SIMPLE ENVIRONMENT FOR PPO.
utt_0026 utt 138.74 141.21 -X OUR ENVIRONMENT (MISTAKE) IS AS FOLLOWS.
utt_0027 utt 141.46 151.73 -X THE COST, WHICH IS THE NEGATIVE REWARD, EQUALS TO MINUS A SQUARE, WHERE A IS THE ACTION AND IT IS A REAL NUMBER.
utt_0029 utt 152.98 159.19 -X THE STATE S IS ALSO A SINGLE REAL NUMBER UNIFORMLY SAMPLED FROM ZERO AND ONE.
utt_0030 utt 159.19 162.29 -X NOTICE THAT THE COST DOES NOT INVOLVE THE STATE.
utt_0031 utt 162.39 165.91 -X THEREFORE, THE STATE CONTRIBUTES NOTHING TO THE ENVIRONMENT.
utt_0032 utt 166.35 177.94 -X OBVIOUSLY, THE OPTIMAL POLICY FOR THIS ENVIRONMENT IS A EQUALS zero, WHICH MEANS THAT THE AGENT SHOULD OUTPUT ZERO ACTION ALL THE TIME.
utt_0034 utt 177.94 183.67 -X THE POLICY IN PPO RECEIVES A STATE AND OUTPUTS AN ACTION DISTRIBUTION.
utt_0035 utt 183.83 190.74 -X THEN, WE CAN EXECUTE THE POLICY BY SAMPLING AN ACTION FROM THE ACTION DISTRIBUTION.
utt_0036 utt 190.74 196.60 -X HERE THE ACTION DISTRIBUTION IS A GAUSSIAN DISTRIBUTION SPECIFIED BY MU AND SIGMA.
utt_0037 utt 196.63 204.82 -X AFTER SOME MATHEMATICS, WE FOUND THAT THE AVERAGE GRADIENT IS INVERSE PROPORTIONAL TO THE SIGMA OUTPUTTED BY THE POLICY.
utt_0039 utt 204.82 209.14 -X WE WILL SOON FIND THAT THIS LEADS TO THE INSTABILITY.
utt_0040 utt 210.29 214.78 -X LET US LOOK AT THE TRAINING PROCESS OF PPO IN THIS FIGURE.
utt_0041 utt 214.87 217.78 -X THE X-AXIS SHOWS THE TRAINING ITERATIONS.
utt_0042 utt 218.71 223.42 -X THE Y-AXIS HERE IS THE COST OF THE POLICY DURING THE TRAINING.
utt_0043 utt 223.42 224.86 -X THE LOWER THE BETTER.
utt_0044 utt 225.94 231.93 -X THE Y-AXIS HERE IS THE VARIANCE OUTPUTTED BY THE POLICY DURING THE TRAINING.
utt_0045 utt 232.34 238.20 -X SINCE THE OPTIMAL POLICY IS A DETERMINISTIC POLICY, THE VARIANCE SHOULD APPROACH ZERO
utt_0046 utt 238.42 240.31 -X DURING THE TRAINING.
utt_0047 utt 240.41 249.47 -X WE CAN SEE FROM THE FIGURE THAT, AT THE BEGINNING OF THE TRAINING IN PPO, THE COST AND THE VARIANCE OF THE POLICY QUICKLY DECREASE.
utt_0049 utt 250.42 258.07 -X HOWEVER, WHEN THE VARIANCE OF THE POLICY BECOMES SMALLER AND SMALLER, THE AVERAGE GRADIENT BECOMES LARGER.
utt_0051 utt 258.07 260.83 -X THE LARGE GRADIENT RESULTS IN A LARGE UPDATE.
utt_0052 utt 262.07 270.75 -X AND THE LARGE UPDATE MAY DRIVE THE ORIGINAL NEAR-OPTIMAL POLICY FAR AWAY FROM THE OPTIMAL.
utt_0053 utt 270.75 273.31 -X THIS RESULTS IN INSTABILITY.
utt_0054 utt 274.20 281.47 -X WE OBSERVE SUCH INSTABILITY IN OUR EXPERIMENTS THAT WE CAN SEE IN THE FIGURE.
utt_0055 utt 282.90 289.72 -X ON THE OTHER HAND, THE ORANGE AND THE GREEN LINES ARE STABLE AND APPROACH THE OPTIMAL POLICY.
utt_0057 utt 290.52 294.65 -X THEY CORRESPOND TO TWO VARIANTS OF OUR ALGORITHM.
utt_0058 utt 298.58 305.18 -X NEXT, Iâ€™M GOING TO INTRODUCE OUR ALGORITHM, TARGET DISTRIBUTION LEARNING, ABBREVIATED AS TDL.
utt_0060 utt 305.85 318.49 -X OUR ALGORITHM IS BASED ON A CONSERVATIVE POLICY ITERATION FRAMEWORK AND IT IS AN ON-POLICY ALGORITHM, WHERE THE POLICY IS UPDATED ITERATIVELY.
utt_0062 utt 318.49 330.17 -X IN EACH ITERATION, WE COLLECT A NUMBER OF SAMPLES FOLLOWING THE CURRENT POLICY AND THEN WE UPDATE THE POLICY BASED ON THE COLLECTED SAMPLES.
utt_0064 utt 330.17 336.51 -X MANY ALGORITHMS OF THIS KIND DIRECTLY UPDATE THE POLICY NETWORK FOLLOWING THE POLICY GRADIENT.
utt_0065 utt 337.18 346.27 -X TO CONSTRUCT A ROBUST ALGORITHM FOR CONTINUOUS CONTROL, WE PROPOSE TO DECOUPLE THE POLICY GRADIENT BACKWARD PROCESS.
utt_0067 utt 346.75 356.16 -X THAT IS TO FIRST SET THE TARGETS OF THE POLICY BASED ON THE COLLECTED SAMPLES AND THEN UPDATE THE POLICY NETWORK TOWARDS THESE TARGETS.
utt_0069 utt 357.27 362.53 -X SPECIFICALLY, WE SET A TARGET DISTRIBUTION SEPARATELY ON EACH SAMPLE.
utt_0070 utt 363.77 375.26 -X THE TARGET DISTRIBUTION ON EACH STATE SAMPLE TELLS THE ALGORITHM WHAT ACTION DISTRIBUTION THE POLICY SHOULD OUTPUT THE NEXT TIME WHEN THE AGENT FACES A SIMILAR STATE.
utt_0072 utt 376.03 386.78 -X SINCE THE TARGET DISTRIBUTION IS SET SEPARATELY ON EACH SAMPLE, THE OBJECTIVE OF THE TARGET DISTRIBUTION SETTING CAN BE MORE CAREFULLY DESIGNED.
utt_0074 utt 387.45 394.14 -X ONE SUCCESSFUL VARIANT OF OUR ALGORITHM SETS THE TARGETS RELYING ON THE EVOLUTIONARY STRATEGY,
utt_0075 utt 394.49 398.66 -X THE ES, RATHER THAN THE GRADIENT OF THE CUMULATIVE REWARD.
utt_0076 utt 399.23 412.32 -X WE FOUND THAT SETTING THE TARGET THROUGH THE EVOLUTIONARY STRATEGY IS EQUIVALENT TO MAXIMIZING THE PROBABILITY THAT THE POLICY IS IMPROVED OVER THE PREVIOUS POLICY IN THE LAST ITERATION.
utt_0078 utt 412.73 418.11 -X THAT IS TO MAXIMIZE THE LTtwo TERM FOR EACH SAMPLE INDICATED BY T.
utt_0079 utt 418.97 422.56 -X WE ARE TO FIND A NEW ACTION DISTRIBUTION.
utt_0080 utt 422.72 434.82 -X THAT IS A GAUSSIAN DISTRIBUTION SPECIFIED BY MU AND SIGMA, SUCH THAT THE PROBABILITY THAT WE OBTAIN A POSITIVE ADVANTAGE FUNCTION OVER THE OLD POLICY IS MAXIMIZED.
utt_0082 utt 435.42 441.09 -X THIS INDICATES THAT OUR POLICY WILL BE IMPROVED ON THIS SAMPLE.
utt_0083 utt 441.09 447.52 -X ON THE CONTRARY, MANY EXISTING ALGORITHMS MAXIMIZE THE POLICY IMPROVEMENT IN EACH ITERATION.
utt_0084 utt 447.52 449.89 -X THAT IS TO MAXIMIZE THE LTone TERM.
utt_0085 utt 450.65 457.31 -X WE FOUND THAT OUR OBJECTIVE, THE LTtwo TERM, RESULTS IN A MORE ROBUST PERFORMANCE.
utt_0086 utt 458.43 465.63 -X FOLLOWING THE ES ALGORITHM, WE CAN OBTAIN ONE VARIANT OF OUR ALGORITHM, WHICH WE CALL
utt_0087 utt 465.89 466.59 -X TDL-ES.
utt_0088 utt 467.36 475.75 -X NOTICE THAT FOR EACH SAMPLE, THE PREVIOUS POLICY FIRST OUTPUTS AN OLD ACTION DISTRIBUTION,
utt_0089 utt 475.75 479.27 -X WHICH WE SHOW BY THE BLUE DASHED OVAL.
utt_0090 utt 479.27 484.61 -X THEN, WE SAMPLE AN ACTION FROM THIS DISTRIBUTION AND RETURNS TO THE ENVIRONMENT.
utt_0091 utt 485.21 488.10 -X THAT IS THE RED DOT HERE.
utt_0092 utt 488.10 499.01 -X AFTER WE FINISH THE ROLLOUT, WE CAN ESTIMATE THE ADVANTAGE FUNCTION ON THIS ACTION SAMPLE BASED ON THE REWARD FROM THE ENVIRONMENT.
utt_0094 utt 501.18 507.81 -X THE ADVANTAGE INDICATES THE GOODNESS OF THE ACTION SAMPLE OVER THE OLD ACTION DISTRIBUTION.
utt_0095 utt 507.81 522.72 -X BASED ON ALL THESE INFORMATION, WE CAN PROPOSE A BETTER ACTION DISTRIBUTION THE NEXT TIME WHEN WE FACE A SIMILAR STATE, WHICH WE CALL THE TARGET DISTRIBUTION INDICATED BY THE GREEN OVALS HERE.
utt_0098 utt 524.29 529.09 -X IN TDL-ES, WE PROPOSE A TARGET DISTRIBUTION FOR EACH SAMPLE INDICATED BY T.
utt_0099 utt 529.09 540.36 -X WHEN THE ADVANTAGE ESTIMATE ON THIS SAMPLE IS POSITIVE, THIS MEANS THAT THIS ACTION SAMPLE MAY INDICATE A GOOD DIRECTION FOR EXPLORATION.
utt_0101 utt 540.36 551.20 -X THEREFORE, THE NEXT TIME WHEN WE FACE A SIMILAR STATE, WE HOPE THE ACTION DISTRIBUTION COULD BE CLOSER TO THE ACTION AT THAT WE SAMPLED IN THIS ROUND.
utt_0103 utt 553.15 558.92 -X THEREFORE, WE SET THE TARGET DISTRIBUTION TO BE A STEP FORWARD TOWARDS AT.
utt_0104 utt 559.97 564.07 -X NOTICE THAT WE USE A NU TO CONTROL THE STEP SIZE.
utt_0105 utt 564.93 573.89 -X ON THE OTHER HAND, IF THE ADVANTAGE ESTIMATE IS NEGATIVE, SAY, THE ACTION SAMPLE IS WORSE THAN THE OLD ACTION DISTRIBUTION.
utt_0107 utt 573.92 581.70 -X INSTEAD OF PUSHING THE ACTION DISTRIBUTION TO THE OPPOSITE DIRECTION, WE REMAIN THE ACTION DISTRIBUTION THE SAME AS THE OLD ONE.
utt_0109 utt 582.21 585.73 -X WE FOUND THAT THIS CONTRIBUTES TO THE ROBUSTNESS.
utt_0110 utt 586.66 594.15 -X SINCE A BAD ACTION SAMPLE DOES NOT NECESSARILY INDICATE THE CORRECTNESS OF THE OPPOSITE DIRECTION.
utt_0111 utt 594.31 598.89 -X THIS IS ESPECIALLY TRUE WHEN THE DIMENSION OF THE ACTION SPACE IS HIGH.
utt_0112 utt 599.36 606.28 -X IT IS SAFER TO STAY WHERE WE ARE PREVIOUSLY, IF NO MORE INFORMATION IS PROVIDED ABOUT A GOOD DIRECTION.
utt_0114 utt 607.01 613.54 -X BY THE WAY, IT IS QUITE FLEXIBLE FOR US TO DESIGN RULES FOR THE TARGET SETTING AS LONG
utt_0115 utt 614.92 618.98 -X AS THE TARGETS LEAD US TO A LOCALLY OPTIMAL POLICY.
utt_0116 utt 620.13 633.26 -X THEREFORE WE CAN INCORPORATE SOME PRIOR KNOWLEDGE FOR ROBOTIC CONTROL TO THE DESIGN OF THE TARGET SETTING, AND THIS RESULTS IN A REVISED VERSION OF TDL-ES WHICH WE CALL TDL-ESR.
utt_0118 utt 633.99 641.70 -X THE PRIOR KNOWLEDGE IS THAT IN ROBOTIC CONTROL, THE STATE REPRESENTATION DOES NOT CHANGE TOO FAST.
utt_0120 utt 641.83 647.11 -X SO WE CAN SET THE TARGETS BASED ON THE ADJACENT STATE SAMPLES IN THE TRAJECTORY.
utt_0121 utt 648.04 652.07 -X NEXT, WE ARE GOING TO PRESENT FOUR SETS OF OUR EXPERIMENTS.
utt_0122 utt 652.36 658.63 -X THE FIRST SET OF OUR EXPERIMENT IS ON THE STANDARD BENCHMARK TASKS FROM MUJOCO.
utt_0123 utt 659.37 662.86 -X HERE IN OUR SLIDE WE ONLY PUT THREE OF THE TASKS.
utt_0124 utt 663.17 670.01 -X THE LINES IN THE FIGURES ARE THE TRAINING CURVES OF THE ALGORITHMS AND THE STARS INDICATE
utt_0125 utt 670.12 675.02 -X SOME MORE BASIC VERSIONS OR ABLATED VERSIONS OF OUR ALGORITHM.
utt_0126 utt 675.43 684.97 -X THE RESULT SHOWS THAT TDL ACHIEVES A COMPARABLE ASYMPTOTIC PERFORMANCE AND SAMPLE EFFICIENCY TO SOME STATE-OF-THE-ART ALGORITHMS.
utt_0128 utt 685.29 690.83 -X BUT MOST IMPORTANTLY, TDL IS QUITE ROBUST DURING THE TRAINING.
utt_0129 utt 690.83 697.07 -X THE NUMBERS IN THE PARENTHESES INDICATE THE VARIANCE OF THE TRAINING CURVE.
utt_0130 utt 697.54 705.23 -X WE CAN SEE THAT THE NUMBERS FOR TDL ALGORITHMS ARE SMALLER THAN THE PREVIOUS ALGORITHMS.
utt_0131 utt 705.73 708.71 -X HERE IS OUR SECOND SET OF EXPERIMENTS.
utt_0132 utt 708.97 714.03 -X RECALL THAT, WE DECOUPLE THE POLICY GRADIENT BACKWARD PROCESS.
utt_0133 utt 714.34 721.00 -X WE FIRST PROPOSE THE TARGETS IN STEP two, AND THEN UPDATE THE POLICY NETWORK IN STEP three.
utt_0134 utt 722.63 726.70 -X THE ADVANTAGE IS THAT ONCE WE PROPOSE AND FIX THE TARGETS IN STEP two.
utt_0135 utt 726.70 730.19 -X THE HYPERPARAMETERS IN STEP three WILL NOT LARGELY INFLUENCE THE FINAL PERFORMANCE.
utt_0136 utt 730.22 736.75 -X IN ON-POLICY ALGORITHMS, ONCE WE COLLECT A BATCH OF SAMPLES, WE WANT TO MAKE FULL USE OF THEM.
utt_0138 utt 736.75 741.71 -X NATURALLY, WE HOPE TO TRAIN THE NETWORK FOR MORE EPOCHS BASED ON THESE SAMPLES.
utt_0139 utt 741.71 746.16 -X HOWEVER, IF WE DO NOT HAVE FIXED TARGETS, MORE TRAINING EPOCHS WILL DRIVE THE NETWORK AWAY.
utt_0141 utt 746.16 752.11 -X WE HAVE FIXED TARGETS FOR THE POLICY NETWORK TRAINING THEREFORE WE DO NOT SUFFER FROM THE PROBLEM.
utt_0143 utt 752.42 756.17 -X THIS ADVANTAGE IS DEMONSTRATED IN OUR EXPERIMENT.
utt_0144 utt 756.17 761.52 -X WHEN WE INCREASE THE TRAINING EPOCHS IN EACH ITERATION FROM THE BLUE TO THE RED, THE PERFORMANCE
utt_0145 utt 761.87 763.69 -X OF PPO DEGENERATE SIGNIFICANTLY.
utt_0146 utt 763.79 769.20 -X HOWEVER, OUR ALGORITHMS ARE NOT SIGNIFICANTLY INFLUENCED.
utt_0147 utt 769.20 775.53 -X RECALL THAT WE ARE FOLLOWING A CONSERVATIVE POLICY ITERATION FRAMEWORK.
utt_0148 utt 775.53 784.46 -X CONSTRAINING THE KL DIVERGENCE BETWEEN THE NEW AND THE OLD POLICIES IN EACH ITERATION IS THE KEY TO ENSURE THE POLICY IMPROVEMENT FOR EACH ITERATION.
utt_0150 utt 784.46 798.25 -X UNLIKE PREVIOUS METHODS, OUR HYPERPARAMETERS HAVE A DIRECT CONNECTION WITH THE KL DIVERGENCE BETWEEN THE NEW AND OLD POLICIES, AND THEREFORE WE HAVE MORE BASIS TO CHOOSE A PROPER HYPERPARMENTER RATHER THAN SWEEPING OVER ALL POSSIBLE PARAMETERS.
utt_0153 utt 798.25 805.81 -X OUR EXPERIMENT ALSO DEMONSTRATES THAT TDL CAN ACHIEVE A SIMILAR PERFORMANCE WHILE BEING MORE CONSERVATIVE IN TERMS OF THE POLICY UPDATE.
utt_0155 utt 805.81 809.42 -X LOOK AT THE BLUE, ORANGE AND RED LINES IN THE FIGURE.
utt_0156 utt 809.42 815.09 -X THEY INDICATE A LOW KL DIVERGENCE BETWEEN THE NEW AND OLD POLICIES IN EACH ITERATION
utt_0157 utt 815.66 820.43 -X COMPARED WITH OTHER ALGORITHMS THAT BASED ON CONSERVATIVE POLICY ITERATION.
utt_0158 utt 820.43 826.10 -X FINALLY, WE DID EXPERIMENTS ON THE HYPERPARAMETER SENSITIVITY ANALYSIS AND DEMONSTRATE THAT
utt_0159 utt 826.96 831.50 -X OUR ALGORITHM IS ROBUST TO DIFFERENT HYPERPARAMENTER SETTINGS.
utt_0160 utt 831.50 840.72 -X IN CONCLUSION, WE PROPOSED TARGET DISTRIBUTION LEARNING, TDL, A POLICY ITERATION ALGORITHM THAT DECOUPLES THE POLICY GRADIENT PROCESS FOR ROBUST CONTINUOUS CONTROL.
utt_0162 utt 840.72 844.43 -X OUR ALGORITHM ACHIEVES STATE-OF-THE-ART PERFORMANCE ON MUJOCO TASKS.
utt_0163 utt 844.43 849.33 -X MOST IMPORTANTLY, IT IS ROBUST DURING THE TRAINING AND ACROSS DIFFERENT RUNS WITH DIFFERENT RANDOM SEEDS.
utt_0165 utt 849.33 857.78 -X IT IS ROBUST TO THE NUMBER OF OPTIMIZATION EPOCHS, AND THEREFORE WE CAN SAFELY INCREASE THE SAMPLE REUSE FOR THE MAXIMUM SAMPLE EFFICIENCY.
utt_0167 utt 859.37 865.23 -X OUR EXPERIMENTS SHOW THAT TDL UPDATES MORE CONSERVATIVELY WHILE BEING SAMPLE EFFICIENT.
utt_0168 utt 868.69 872.50 -X ALSO, OUR ALGORITHM IS ROBUST ACROSS DIFFERENT HYPERPARAMETER SETTINGS.
utt_0169 utt 873.45 876.69 -X OK, THIS IS THE ORAL PRESENTATION OF OUR WORK.
utt_0170 utt 876.69 880.59 -X YOU CAN CONTACT ME THROUGH THIS EMAIL ADDRESS (FOR QUESTIONS OR DISCUSSION)
utt_0171 utt 880.72 883.79 -X YOU CAN ALSO DOWNLOAD THE SOURCE CODE FROM THIS LINK.
utt_0172 utt 883.79 891.83 -X AND ALSO FULL PAPER WITH SUPPLEMENTARY MATERIALS FROM THIS QR CODE THANK YOU FOR WATCHING THIS VIDEO.
