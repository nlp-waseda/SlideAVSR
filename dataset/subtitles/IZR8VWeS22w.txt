utt_0000 utt 1.77 12.11 -X HELLO, EVERYONE I AM SHIVAM. I'M GOING TO TALK ABOUT OUR RECENT WORK HERE AT KTH , WHERE WE REPLACE ATTENTION WITH NEURAL HIDDEN MARKOV MODELS, OR NEURAL HMMS, IN TEXT-TO-SPEECH.
utt_0002 utt 13.20 16.72 -X IN THIS PAPER, WE INVESTIGATE THE USE OF NEURAL HMMS IN TTS,
utt_0003 utt 16.81 22.32 -X OUR PRIMARY GOALS ARE TO OVERCOME THE SHORTCOMINGS OF BOTH HMMS AND ATTENTION IN TTS SYSTEMS
utt_0005 utt 23.28 28.69 -X TO USE HMMS AND DEEP NEURAL NETWORKS TOGETHER IN A WAY THAT THEY COMPLEMENT EACH OTHER TO GET THE BEST OF BOTH WORLDS
utt_0006 utt 29.17 35.41 -X AND TO PROVIDE A FRAMEWORK FOR FASTER ITERATIONS AND TRAINING ON SMALL DATASETS WHEN DEVELOPING SPEECH SYNTHESISERS
utt_0007 utt 37.36 40.53 -X WHY ARE WE THINKING ABOUT PUTTING HIDDEN MARKOV MODELS IN NEURAL TTS?
utt_0008 utt 40.78 44.40 -X LET’S DO A QUICK OVERVIEW OF THE OLD AND NEW TTS PARADIGMS
utt_0009 utt 45.42 50.90 -X FIRST, WHAT ARE THE APPEALING PROPERTIES OF CLASSICAL STATISTICAL PARAMETRIC SPEECH SYNTHESIS OR SPSS?
utt_0010 utt 51.66 53.20 -X IT HAS A SMALL FOOTPRINT
utt_0011 utt 53.65 55.86 -X IT RAPIDLY LEARNS TO ALIGN TEXT AND AUDIO
utt_0012 utt 56.30 61.59 -X IT REQUIRES LITTLE DATA AND IS ABLE TO SPEAK REASONABLY FROM ONLY A FEW HUNDRED TRAINING UTTERANCES
utt_0013 utt 64.27 66.47 -X BUT SPSS ALSO HAS A NUMBER OF DOWNSIDES
utt_0014 utt 66.77 74.00 -X THE STATEWISE NATURE OF HMMS RESULTS IN WORSE ACOUSTIC QUALITY SINCE EACH STATE HAS A CONSTANT MEAN OUTPUT THAT DOESN'T CHANGES OVER TIME,
utt_0016 utt 74.00 78.80 -X AND WE HAVE TO USE HACKS LIKE DYNAMIC FEATURES TO SMOOTH THE GENERATED FEATURE TRAJECTORIES
utt_0017 utt 79.34 82.13 -X EVEN WITH THIS, WE DO NOT OBTAIN LIFELIKE PROSODY,
utt_0018 utt 82.13 88.88 -X SINCE THE OUTPUT PITCH IS AN AVERAGE OF ALL THE POSSIBLE INTONATION CONTOURS OF THE UTTERANCE WHICH SOUNDS FLAT AND BORING
utt_0020 utt 89.55 93.14 -X EACH HMM STATE ALSO HAS A CONSTANT TRANSITION PROBABILITY.
utt_0021 utt 93.14 97.75 -X MATHEMATICALLY, THIS MEANS THAT SOUND DURATIONS ARE ASSUMED TO FOLLOW A GEOMETRIC DISTRIBUTION,
utt_0022 utt 97.75 99.99 -X WHICH IS A BAD FIT FOR NATURAL SPEECH
utt_0023 utt 100.66 108.09 -X AND EACH LANGUAGE REQUIRES EXTENSIVE FEATURE ENGINEERING TO DEFINE USEFUL LINGUISTIC FEATURE EXTRACTORS THAT ARE INFORMATIVE FOR GOOD SYNTHESIS.
utt_0025 utt 110.83 117.21 -X COMPARED TO SPSS, MODERN SEQUENCE-TO-SEQUENCE NEURAL TTS HAS SEVERAL ADVANTAGES BUT FOR THIS TALK, WE WILL HIGHLIGHT
utt_0027 utt 117.27 121.05 -X THE MUCH HIGHER SYNTHESIS QUALITY THE MORE LIVELY PROSODY
utt_0029 utt 121.36 126.39 -X AND THE FACT THAT THIS IS ACHIEVED WITHOUT THE NEED FOR THE SAME MANUAL FEATURE ENGINEERING AS BEFORE
utt_0031 utt 129.11 131.13 -X WHAT IS THE REASON FOR THESE IMPROVEMENTS?
utt_0032 utt 131.13 135.51 -X PRIOR RESEARCH HAS IDENTIFIED FOUR KEY FACTORS THAT ARE RESPONSIBLE FOR THE DIFFERENCE BETWEEN
utt_0033 utt 135.70 140.28 -X SPSS SYSTEMS LIKE MERLIN AND NEURAL TTS SYSTEMS LIKE TACOTRON two AND DCTTS
utt_0034 utt 141.65 145.21 -X SPSS USES TRADITIONAL VOCODER PARAMETERS WITH AN EXPLICIT Fzero FEATURE
utt_0035 utt 145.56 158.39 -X AND FEED THESE FEATURES INTO A SIGNAL-PROCESSING-BASED VOCODER WHILE THE NEW PARADIGM USES NEURAL VOCODERS DRIVEN BY MEL-SPECTROGRAMS SPSS USES MANUALLY-DESIGNED RULE-BASED TEXT ANALYSIS WHILE NEURAL TTS USES AN ENCODER NETWORK
utt_0039 utt 158.39 162.36 -X WHICH CAN LEARN TO EXTRACT RELEVANT FEATURES DIRECTLY FROM THE INPUT PHONES ITSELF.
utt_0040 utt 164.69 177.02 -X SPSS USES MULTIPLE HMM STATES PER PHONE TO INCREASE THE GRANULARITY OF THE MODELLING WHEREAS TACOTRON AND ITS DERIVATIVES USE AUTOREGRESSION TO ENABLE PREDICTIONS THAT ARE DIFFERENT FOR EVERY TIME FRAME
utt_0043 utt 177.02 181.11 -X SPSS USES LEFT-TO-RIGHT-NO-SKIP HMMS TO ALIGN THE INPUT AND OUTPUT SEQUENCES
utt_0044 utt 182.65 186.33 -X WHILE SEQUENCE-TO-SEQUENCE SYSTEMS USE LEARNED NEURAL ATTENTION INSTEAD.
utt_0045 utt 187.99 196.06 -X AMONG THESE FOUR DIFFERENCES IT TURNS OUT THAT THE USE OF NEURAL VOCODERS DRIVEN BY MEL-SPECTROGRAMS SIGNIFICANTLY IMPROVES THE PERCEIVED NATURALNESS OF THE SYNTHESIS
utt_0048 utt 196.28 204.83 -X THE SAME FOR THE LEARNED TEXT PROCESSING AND ALSO FOR THE USE OF AUTOREGRESSION WHEN COMPARED TO EARLIER MECHANISMS FOR FRAME-LEVEL POSITION ENCODING
utt_0051 utt 205.24 214.46 -X HOWEVER, THE USE OF NEURAL ATTENTION WAS FOUND TO BE PROBLEMATIC AND LISTENERS ACTUALLY PREFERRED TTS USING FIXED ALIGNMENTS TAKEN FROM A CONVENTIONAL LEFT-TO-RIGHT NO-SKIP HMM INSTEAD
utt_0054 utt 214.46 220.44 -X THE KEY ISSUE WITH NEURAL ATTENTION IS ITS NON-MONOTONICITY,
utt_0055 utt 220.44 222.28 -X WHICH LEADS TO A NUMBER OF PROBLEMS
utt_0056 utt 222.90 225.56 -X WE NEED MANY TRAINING UPDATES TO LEARN TO SPEAK
utt_0057 utt 226.14 228.92 -X HERE, EVEN AFTER thirteen zero UPDATES, THE ATTENTION IS NOT WELL-FORMED
utt_0058 utt 229.05 233.56 -X IT CAN TAKE A REALLY LONG TIME TO MAKE ATTENTION-BASED TTS PRODUCE INTELLIGIBLE SPEECH
utt_0059 utt 234.58 238.33 -X EVEN AFTER A LOT OF TRAINING, ATTENTION CAN BREAK DOWN AT SYNTHESIS TIME
utt_0060 utt 238.52 240.96 -X HERE ARE SOME EXAMPLES OF DIFFERENT UTTERANCES,
utt_0061 utt 240.96 244.16 -X WHERE THE TRAINED MODEL PUTS THE ATTENTION ALL OVER THE INPUT SEQUENCE
utt_0062 utt 244.19 248.70 -X THIS SCRAMBLES THE ORDER OF THE PHONES IN THE SENTENCE AND LEADS TO UNINTELLIGIBLE GIBBERISH.
utt_0063 utt 250.23 261.89 -X FINALLY, ATTENTION REQUIRES A SUBSTANTIAL AMOUNT OF DATA TO WORK AT ALL WHEN TRAINED USING ONLY five hundred UTTERANCES FROM LJ SPEECH MONOTONIC ALIGNMENT IS NEVER FORMED AND THE MODEL NEVER PRODUCES INTELLIGIBLE SPEECH.
utt_0066 utt 263.23 266.17 -X BECAUSE OF THESE ISSUES, IMPROVED ATTENTION MECHANISMS
utt_0067 utt 266.62 273.31 -X THAT INCREASE OR ENFORCE MONOTONICITY ARE A HOT RESEARCH TOPIC RIGHT NOW AND WE REVIEW MANY RECENT METHODS IN OUR PAPER
utt_0069 utt 273.37 275.97 -X ONE OF THEM, FROM NAGOYA INSTITUTE OF TECHNOLOGY,
utt_0070 utt 275.97 279.47 -X USES HIDDEN SEMI-MARKOV MODELS TOGETHER WITH STANDARD ATTENTION IN A VAE
utt_0071 utt 279.61 281.89 -X AND IS INCLUDED IN THE SAME SESSION AS OURS
utt_0072 utt 282.01 285.76 -X OUR METHOD, HOWEVER, INSTEAD REPLACES NEURAL ATTENTION COMPLETELY.
utt_0073 utt 285.85 292.16 -X THIS IS NICE BECAUSE IT IS FULLY PROBABILISTIC AND CAN MAXIMISE THE EXACT SEQUENCE LIKELIHOOD WITHOUT APPROXIMATIONS
utt_0075 utt 292.38 296.42 -X AND ALSO DOES NOT NEED ANY EXTERNAL ALIGNMENT TOOLS FOR TRAINING.
utt_0076 utt 297.31 300.45 -X IN A NUTSHELL, OUR PAPER TAKES THE TWO MAIN PARADIGMS FOR TTS
utt_0077 utt 300.89 303.68 -X AND FUSES THEM TOGETHER TO GET NEURAL HMM TTS
utt_0078 utt 304.76 309.79 -X IN THE REST OF THIS PRESENTATION WE DESCRIBE AN EXAMPLE OF HOW NEURAL HMMS CAN REPLACE ATTENTION,
utt_0080 utt 309.79 311.62 -X USING TACOTRON two AS OUR EXAMPLE ARCHITECTURE
utt_0081 utt 312.09 318.98 -X BUT WE EMPHASISE THAT THE SAME IDEA CAN BE USED FOR A LARGE NUMBER OF OTHER NEURAL TTS ARCHITECTURES AS WELL, FOR INSTANCE TRANSFORMER TTS
utt_0083 utt 320.73 323.68 -X BY PUTTING NEURAL HMMS INSIDE SEQUENCE-TO-SEQUENCE NEURAL TTS
utt_0084 utt 324.16 326.00 -X WE OBTAIN THE BEST OF BOTH WORLDS
utt_0085 utt 326.30 331.76 -X WE CAN TAKE ADVANTAGE OF LEARNED TEXT PROCESSING AND WE CAN USE NEURAL VOCODERS FOR BETTER SIGNAL QUALITY
utt_0087 utt 332.22 334.53 -X AUTOREGRESSION REMOVES THE LIMITATIONS OF HMMS.
utt_0088 utt 335.20 343.65 -X AND BECAUSE WE NOW ARE USING A LEFT-TO-RIGHT NO-SKIP HMM INSTEAD OF ATTENTION WE CAN FORCE THE ALIGNMENT TO BE MONOTONIC AND SYNTHESISE ALL OF THE INPUT PHONES IN ORDER.
utt_0090 utt 345.37 354.28 -X ALSO, INSTEAD OF SOMETHING LIKE A STOP TOKEN TO DECIDE WHEN THE UTTERANCE IS OVER WE PREDICT THE STATE TRANSITION PROBABILITY WHICH IS THE PROBABILITY OF MOVING ON TO THE NEXT STATE OF THE HMM
utt_0093 utt 354.82 359.52 -X IN ADDITION, WE REPLACE ONE OF TACOTRON two'S LSTMS WITH A FEEDFORWARD NEURAL NETWORK
utt_0094 utt 359.55 364.61 -X THIS ENSURES THAT THE PROCESS IS MARKOVIAN AND DOES NOT DEPEND ON PREVIOUS HIDDEN STATES.
utt_0095 utt 364.90 367.24 -X FINALLY, WE ALSO REMOVE THE POST-NET,
utt_0096 utt 367.24 375.08 -X WHICH IS NOT COMPATIBLE WITH AUTOREGRESSIVE LIKELIHOOD-BASED MODELS ONE CAN EASILY ADD A POST-NET THAT'S TRAINED SEPARATELY, WHICH WE CONSIDER TO BE FUTURE WORK.
utt_0098 utt 376.86 381.96 -X THE MONOTONICITY OF OUR HMMS GIVES A SIGNIFICANT ADVANTAGE BOTH FOR TRAINING AND SYNTHESIS.
utt_0099 utt 382.34 385.80 -X IT MAKES THE MODEL LEARN SHARP, WELL-DEFINED ALIGNMENTS WITHIN five hundred UPDATES
utt_0100 utt 386.78 388.84 -X AS SEEN IN THE FIRST FIGURE.
utt_0101 utt 388.84 392.68 -X AND OUTPUT SPEECH IS INTELLIGIBLE AFTER LESS THAN two thousand, five hundred TRAINING UPDATES
utt_0102 utt 393.15 396.68 -X AS THE SIMILARITY BETWEEN THESE SYNTHETIC AND NATURAL SPECTROGRAMS SHOWS.
utt_0103 utt 399.49 402.52 -X INTEGRATING AUTOREGRESSION INTO HMMS ADDRESSES
utt_0104 utt 403.20 405.38 -X THE MAIN PROBLEMS THAT HMM-BASED SPSS SYSTEMS HAD
utt_0105 utt 405.70 412.07 -X BY LOOKING AT PREVIOUS OBSERVATIONS WHEN PREDICTING THE OUTPUT PARAMETERS THE ACOUSTICS OF THE PAST FRAMES CAN AFFECT THE NEXT FRAME.
utt_0107 utt 412.19 416.39 -X THIS SOLVES THE ISSUE THAT TRADITIONAL HMM STATES ALWAYS HAVE THE SAME STATISTICS,
utt_0108 utt 416.39 420.84 -X AND SIGNIFICANTLY REDUCES OVERSMOOTHING ACROSS TIME.
utt_0109 utt 422.75 428.84 -X AUTOREGRESSION ALSO GIVES BETTER DURATION DISTRIBUTIONS INSTEAD OF A STATE HAVING THE SAME TRANSITION PROBABILITY ALL THE TIME,
utt_0111 utt 428.84 431.56 -X THE TRANSITION PROBABILITY CAN NOW CHANGE WITH EVERY FRAME
utt_0112 utt 431.59 433.06 -X THIS IS VERY POWERFUL.
utt_0113 utt 433.06 437.70 -X IN FACT, ANY DISCRETE DURATION DISTRIBUTION CAN BE DESCRIBED IN THIS WAY.
utt_0114 utt 437.70 444.33 -X THESE RISING PATTERNS WE SEE IN THE GENERATED TRANSITION PROBABILITIES INDICATE THAT THE MODEL IS AWARE OF ITS PROGRESS THROUGH EACH SPEECH SOUND
utt_0116 utt 444.39 448.97 -X AND THE PROBABILITY OF SWITCHING TO THE NEXT STATE GETS HIGH ONCE WE ARE NEAR THE END OF THE STATE
utt_0117 utt 451.30 455.59 -X THE TRANSITION PROBABILITIES ALSO PROVIDE A NATURAL WAY TO DECIDE WHEN TO STOP SYNTHESIS.
utt_0118 utt 456.04 460.97 -X TACOTRON two ONLY STOPS TALKING WHEN THE PROBABILITY OF ITS STOP TOKEN IS GREATER THAN A HALF.
utt_0119 utt 461.03 464.62 -X THAT MIGHT NEVER HAPPEN, LEADING TO SYNTHESIS THAT NEVER STOPS.
utt_0120 utt 464.68 469.39 -X WE INSTEAD USE AN APPROACH FROM SPSS THAT AGGREGATES TRANSITION PROBABILITIES OVER TIME,
utt_0121 utt 469.41 471.35 -X ENABLING US TO GENERATE THE MEDIAN
utt_0122 utt 471.53 475.50 -X OR RATHER ANY OTHER QUANTILE OF THE DURATION DISTRIBUTION OF EACH STATE
utt_0123 utt 475.78 481.29 -X BY ADJUSTING OUR QUANTILE THRESHOLD UP OR DOWN WE CAN ADJUST THE SPEAKING RATE, EVEN ON A PER-STATE BASIS.
utt_0125 utt 481.29 486.60 -X THIS DURATION-GENERATION GIVES BETTER RESULTS THAN THOSE USED IN EARLIER WORK USING TRANSDUCERS FOR TTS
utt_0127 utt 486.92 490.57 -X WHICH OTHERWISE HAVE MANY MATHEMATICAL SIMILARITIES TO NEURAL HMMS.
utt_0128 utt 492.20 495.40 -X WE'VE TALKED ABOUT THE BENEFITS OF NEURAL HMMS,
utt_0129 utt 495.40 498.86 -X NOW LET’S TAKE A QUICK LOOK AT HOW THE MODEL SYNTHESISES THE MEL-SPECTROGRAM
utt_0130 utt 499.49 500.88 -X WE TAKE THE INPUT TEXT
utt_0131 utt 501.45 503.88 -X RUN IT THROUGH REGULAR TEXT NORMALIZATION
utt_0132 utt 504.23 505.23 -X AND PHONETISE IT
utt_0133 utt 505.74 511.08 -X WE THEN PASS THE PHONETISED TEXT TO THE ENCODER THE ENCODER THEN OUTPUTS A SEQUENCE OF VECTORS,
utt_0135 utt 511.08 513.58 -X WHERE EACH VECTOR DEFINES A STATE IN THE NEURAL HMM
utt_0136 utt 516.58 523.28 -X FOR TIME STEP T, WE SELECT ONE OF THE STATE VECTORS BASED ON THE HMM STATE THE SYNTHESISER IS IN AT THE CURRENT TIME STEP
utt_0138 utt 524.26 530.41 -X THAT STATE IS FED TO THE FEEDFORWARD LAYER IN THE DECODER ALONG WITH AN AUTOREGRESSIVE INPUT BASED ON PREVIOUS FRAMES.
utt_0140 utt 530.41 538.22 -X TO GET THIS INPUT, THE PREVIOUS FRAME IS FIRST PASSED THROUGH A PRE-NET AND THEN THROUGH AN LSTM LAYER THAT, GIVES THE AUTOREGRESSION LONG MEMORY
utt_0142 utt 538.67 549.71 -X THE RESULTING OUTPUT IS THEN PASSED TO THE FEEDFORWARD NETWORK TOGETHER WITH THE STATE VECTOR AT TIME STEP T THE OUTPUTNET, WHICH IS THE UPPER PART OF THE DECODER, DEFINES THE HMM’S EMISSION DISTRIBUTION.
utt_0144 utt 549.71 553.77 -X IN THIS CASE, IT RETURNS THE MEAN AND STANDARD DEVIATION VECTORS OF A GAUSSIAN DISTRIBUTION.
utt_0145 utt 553.77 556.54 -X IT ALSO COMPUTES THE CURRENT TRANSITION PROBABILITY TAU
utt_0146 utt 557.80 560.94 -X WE USE THE MEAN, TO GENERATE THE T^{TH} FAME OF MEL SPECTROGRAM.
utt_0147 utt 560.94 569.13 -X AND THE TRANSITION PROBABILITY DECIDES WHETHER TO SWITCH TO THE NEXT STATE OR STAY IN THE CURRENT STATE DEPENDING ON THE AGGREGATED VALUES OF THE TRANSITION PROBABILITIES OVER TIME
utt_0150 utt 569.87 573.58 -X THUS COMPLETING THE SYNTHESIS OF THE MEL-SPECTROGRAM FRAME AT TIME STEP T.
utt_0151 utt 574.22 576.88 -X WE REPEAT THIS SYNTHESIS PROCESS FOR ALL TIMESTEPS T
utt_0152 utt 577.58 587.34 -X IN ORDER TO SYNTHESISE THE FULL UTTERANCE WE PERFORMED OUR EXPERIMENTS ON LJ SPEECH AND CONSIDERED FOUR MODEL CONFIGURATIONS:
utt_0154 utt 587.34 590.54 -X TWO BASELINES, NAMELY TACOTRON two WITH POSTNET, WHICH WE CALL Ttwo + P,
utt_0155 utt 591.12 594.19 -X AND TACOTRON two WITHOUT POSTNET, WHICH WE CALL Ttwo - P.
utt_0156 utt 594.73 599.60 -X THIS WAS COMPARED TO THE SAME ARCHITECTURE BUT USING THE PROPOSED NEURAL HMMS INSTEAD OF ATTENTION
utt_0157 utt 599.72 602.67 -X EITHER USING two STATES PER PHONE, WHICH WE CALL NHtwo
utt_0158 utt 602.89 605.81 -X OR AN ABLATION WITH ONLY one STATE PER PHONE, CALLED NHone
utt_0159 utt 606.19 611.12 -X WE COMPARED THE INTELLIGIBILITY, NATURALNESS AND THE SIZE OF THE MODELS.
utt_0160 utt 612.24 621.07 -X WE EVALUATED INTELLIGIBILITY BY SYNTHESISING THE VALIDATION SET SENTENCES AT VARIOUS POINTS DURING TRAINING AND RUNNING THAT AUDIO THROUGH AUTOMATIC SPEECH RECOGNITION.
utt_0163 utt 621.13 624.95 -X THIS PLOT SHOWS THE RESULTING WORD ERROR RATE.
utt_0164 utt 625.04 628.56 -X NEURAL HMM TTS WAS ABLE TO GENERATE INTELLIGIBLE SPEECH AFTER one,zero UPDATES,
utt_0165 utt 628.91 632.75 -X WHILE IT TOOK MORE THAN fifteen,zero UPDATES FOR TACOTRON two TO DO THE SAME
utt_0166 utt 633.77 636.56 -X ALSO, WHEN TRAINING ON A SMALL SUBSET OF THE ORIGINAL DATA,
utt_0167 utt 636.88 638.71 -X TACOTRON two NEVER LEARNED TO SPEAK AT ALL,
utt_0168 utt 638.71 642.39 -X WHILE NEURAL HMM TTS WAS ESSENTIALLY UNAFFECTED.
utt_0169 utt 642.70 648.43 -X NOW, LET’S LISTEN TO SOME EXAMPLES OF SYNTHESISED SPEECH AT DIFFERENT UPDATES DURING THE TRAINING PROCESS:
utt_0171 utt 648.43 650.71 -X FIRST, WE START WITH TACOTRON two AT two500 UPDATES,
utt_0172 utt 652.75 654.00 -X *[TACOTRON two BABBLING]*
utt_0173 utt 658.29 664.53 -X AND THIS IS NEURAL HMM AT two thousand, five hundred UPDATES *[NEURAL HMM: THE BIRCH CANOE SLID ON THE SMOOTH PLANKS.]*
utt_0175 utt 665.04 668.24 -X EVEN AFTER fifteen thousand UPDATES TACOTRON two SOUNDS LIKE *[TACOTRON two MUFFLED SYNTHESIS]*
utt_0177 utt 668.24 671.00 -X WHILE THE NEURAL HMM HAS EVEN IMPROVED ON THE PROSODY
utt_0178 utt 671.76 677.97 -X *[NEURAL HMM: THE BIRCH CANOE SLID ON THE [HIGHER INTONATION] SMOOTH [HIGHER INTONATION] PLANKS.]*
utt_0179 utt 680.02 687.19 -X THIS IS A SIGNIFICANT IMPROVEMENT ESPECIALLY WHEN YOU WANT TO ITERATE QUICKLY IN TTS RESEARCH AND DEVELOPMENT.
utt_0181 utt 687.54 690.49 -X FOR NATURALNESS, WE REPORT THE MEAN OPINION SCORES
utt_0182 utt 691.06 694.61 -X WITH ninety-five% CONFIDENCE INTERVALS ON PHONETICALLY BALANCED HARVARD SENTENCES
utt_0183 utt 695.31 699.73 -X THE DIFFERENCE BETWEEN NHtwo AND THE MOST COMPARABLE TACOTRON two CONFIGURATION
utt_0184 utt 700.75 704.66 -X WAS LESS zero point zero zero two, AND WAS NOT STATISTICALLY SIGNIFICANT,
utt_0185 utt 704.69 706.52 -X DESPITE NHtwo BEING MUCH SMALLER.
utt_0186 utt 707.44 710.33 -X NEURAL HMM BENEFITTED FROM USING TWO STATES PER PHONE,
utt_0187 utt 710.74 715.61 -X WHILE TACOTRON two IMPROVED WITH THE USE OF A POST-NET, WHICH WE PLAN TO ADD IN FUTURE WORK.
utt_0188 utt 717.20 720.15 -X WE KNOW THAT ATTENTION TENDS TO BREAK DOWN ON LONG UTTERANCES,
utt_0189 utt 720.31 722.74 -X BUT IT ALSO HAPPENS ON SHORT UTTERANCES AS WELL
utt_0190 utt 722.80 726.01 -X LET’S LISTEN TO TACOTRON two TRYING TO SYNTHESISE “POTATO”
utt_0191 utt 731.12 735.99 -X *[TACOTRON two: POTATO THEN LONG BABBLING WITH PO PO PO TO TO TO]*
utt_0192 utt 737.36 741.08 -X NEURAL HMM TTS CAN SPEAK THIS WORD FLAWLESSLY *[NEURAL HMM: POTATO ]*
utt_0194 utt 741.59 750.84 -X THE SENTENCE “TEST SPEECH” HAS A SIMILAR PROBLEM *[TACOTRON two: TEST SPEECH THEN LONG BABBLING WITH SS SS SH SH SH TS TS TS]*
utt_0196 utt 754.52 757.24 -X WHILE FOR NEURAL HMMS IT IS JUST ANOTHER UTTERANCE.
utt_0197 utt 757.24 758.90 -X *[NEURAL HMM: TEST SPEECH ]*
utt_0198 utt 763.03 769.88 -X FINALLY, HERE ARE SOME EXAMPLES OF CHANGING THE SPEAKING RATE BY ADJUSTING WHICH QUANTILE OF THE DURATION DISTRIBUTION THAT WE GENERATE FROM.
utt_0200 utt 770.17 775.23 -X SOMETHING VERY SLOW *[NEURAL HMM: (VERY SLOW) THE BIRCH CANOE SLID ON THE SMOOTH PLANKS.]*
utt_0202 utt 775.23 776.09 -X LITTLE FASTER
utt_0203 utt 776.37 779.51 -X *[NEURAL HMM: (SLOW) THE BIRCH CANOE SLID ON THE SMOOTH PLANKS.]*
utt_0204 utt 779.51 783.93 -X LITTLE FASTER *[NEURAL HMM: (NORMAL) THE BIRCH CANOE SLID ON THE SMOOTH PLANKS.]*
utt_0206 utt 783.93 784.70 -X LITTLE MORE FAST
utt_0207 utt 784.79 787.71 -X *[NEURAL HMM: (FAST) THE BIRCH CANOE SLID ON THE SMOOTH PLANKS.]*
utt_0208 utt 787.71 788.35 -X AND THE FASTEST
utt_0209 utt 788.50 791.96 -X *[NEURAL HMM: (FASTEST) THE BIRCH CANOE SLID ON THE SMOOTH PLANKS.]*
utt_0210 utt 794.36 799.29 -X IN CONCLUSION, WE HAVE SHOWN HOW TO PUT TOGETHER CLASSIC SPSS AND MODERN NEURAL TTS SYSTEM
utt_0211 utt 799.96 801.61 -X TO GET THE BEST OF BOTH WORLDS
utt_0212 utt 801.69 809.50 -X BY REPLACING ATTENTION WITH A LEFT-RIGHT HMM DEFINED BY A NEURAL NETWORK COMPARED TO AN ATTENTION-BASED NEURAL TTS SYSTEM LIKE TACOTRON two
utt_0214 utt 809.56 812.57 -X THIS GIVES A NUMBER OF ADVANTAGES:
utt_0215 utt 812.57 813.92 -X IT'S SMALLER AND SIMPLER
utt_0216 utt 814.20 816.44 -X IT LEARNS TO SPEAK AND ALIGN MUCH QUICKER
utt_0217 utt 817.59 820.22 -X IT DOES NOT RISK BREAKING DOWN INTO GIBBERISH
utt_0218 utt 820.60 822.24 -X IT'S FULLY PROBABILISTIC
utt_0219 utt 823.03 825.53 -X IT ALLOWS EASY CONTROL OVER SPEAKING RATE
utt_0220 utt 825.94 827.96 -X AND IT ACHIEVES THE SAME NATURALNESS
utt_0221 utt 828.47 836.25 -X SO NEXT TIME WHILE EXPERIMENTING IF YOU NEED SOMETHING WHICH IS FASTER TO TRAIN WHILE DEVELOPING OR YOU DON’T HAVE ENOUGH DATA TO TRAIN AN ATTENTION-BASED MODEL
utt_0224 utt 836.28 837.34 -X USE NEURAL HMMS
utt_0225 utt 839.67 844.00 -X PLEASE SCAN THE QR CODE OR VISIT THE LINK DIRECTLY TO LISTEN TO MORE AUDIO EXAMPLES.
utt_0226 utt 844.00 847.13 -X YOU CAN ALSO FIND OUR CODE AND A PRE-TRAINED MODEL THERE.
utt_0227 utt 847.13 849.89 -6.1372 THANK YOU FOR YOUR ATTENTION *(PUN INTENDED)*.
