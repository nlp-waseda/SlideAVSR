utt_0000 utt 2.57 7.70 -X HELLO, MY NAME IS MOHAMMAD NAMVARPOUR. TODAY I'D LIKE TO DISCUSS SANTOSO AND COLLEAGUES'
utt_0001 utt 7.70 21.91 -X WORK SPEECH EMOTION RECOGNITION BASED ON ATTENTION WEIGHT CORRECTION USING WORD-LEVEL CONFIDENCE MEASURE. THE PURPOSE OF THIS STUDY IS TO PROVIDE A METHOD FOR IMPROVING MULTIMODAL SPEECH EMOTION RECOGNITION.
utt_0004 utt 22.13 33.25 -X SPEECH EMOTION RECOGNITION IS THE PROCESS OF RETRIEVING A SPEAKER'S EMOTIONAL STATE FROM HIS OR HER SPEECH. EMOTION RECOGNITION HAS THE POTENTIAL TO BRING
utt_0006 utt 33.25 45.75 -X HUMAN-LIKE CAPABILITIES TO HUMAN-MACHINE INTERFACES, SUCH AS EMPATHY AND PROPER EMOTION RESPONSE IN THE TEXT TO SPEECH ENGINE. IN THE SAME WAY AS WITH PEOPLE, SPEECH AUDIO
utt_0008 utt 45.75 52.95 -X ALONE IS NOT ENOUGH FOR COMPUTERS TO DETERMINE HOW AN INDIVIDUAL FEELS.
utt_0009 utt 52.95 58.03 -X A PERSON'S EMOTIONAL STATE CAN BE BETTER JUDGED IF WE ALSO KNOW WHAT THEY SAID.
utt_0010 utt 58.83 71.67 -X AUTOMATIC SPEECH RECOGNITION ALLOWS YOU TO COMBINE SPEECH AND TEXT WITHOUT HAVING TO RELY ON HUMAN TRANSCRIPTIONS. THERE ARE NUMEROUS RESEARCH THAT HAVE LOOKED AT COMBINING SER
utt_0012 utt 72.11 84.60 -X WITH ASR RESULTS AS THE TEXTUAL INFORMATION. HOWEVER, THERE IS A PROBLEM: WHEN IT COMES TO SPEECH WITH EMOTION, ASR MODELS STRUGGLE TO COMPREHEND THE SPEAKER! SO, IF WE WANT
utt_0014 utt 84.60 94.93 -X TO USE ASR TO IMPROVE OUR EMOTION RECOGNITION MODELS, WE SHOULD FIND A WAY TO REDUCE ITS ERROR ON EMOTIONAL SPEECH. BUT BEFORE THAT, LET’S SEE HOW A CONVENTIONAL
utt_0016 utt 94.93 101.88 -X MULTI-MODAL SPEECH EMOTION RECOGNIZER WORKS. THIS FIGURE DESCRIBES THE CLASSIFIER STRUCTURE,
utt_0017 utt 102.06 115.54 -X WHICH IS BUILT ON VARIOUS STATE-OF-THE-ART SER MODELS AND INCLUDES BOTH AUDITORY AND TEXT INFORMATION. IT HAS THREE MAIN PARTS: FIRST, AN AUDIO FEATURE EXTRACTOR, WHICH USES
utt_0019 utt 116.08 126.23 -X BLSTM AND A SELF-ATTENTION MECHANISM TO BUILD A SET OF INTERMEDIATE REPRESENTATIONS FROM THE AUDIO CHARACTERISTICS. IF YOU’RE NOT FAMILIAR WITH HOW SELF-ATTENTION
utt_0021 utt 126.23 136.98 -X WORKS, I RECOMMEND YOU WATCH MY VIDEO ON TRANSFORMER ARCHITECTURE, IN WHICH I ALSO COVER SELF ATTENTION IN EXTREME DETAIL. THE SECOND PART IS THE TEXTUAL FEATURE EXTRACTOR,
utt_0023 utt 136.98 146.95 -X WHICH ENCODES TEXTUAL PROPERTIES VIA WORD EMBEDDING AND THEN ANALYZES THEM LIKE AUDIO FEATURE EXTRACTOR. SPEECH TRANSCRIPTION IS OBTAINED USING AN
utt_0025 utt 147.03 157.53 -X ASR MODEL TRAINED ON THE LIBRISPEECH DATASET. AND THE WORD EMBEDDING MODEL IS TRAINED USING BERT THE FINAL EMOTION CLASSIFIER COMBINES THE
utt_0027 utt 157.53 162.62 -X OUTPUT FROM THE TWO PREVIOUS PARTS, AND USES A FULLY CONNECTED NETWORK TO CLASSIFY THEM,
utt_0028 utt 162.71 167.10 -X YIELDING THE FINAL OUTPUT OF THE FINAL EMOTION CLASS.
utt_0029 utt 167.10 176.25 -X NOW THAT WE ARE FAMILIAR WITH THE BASELINE MODEL, WE CAN SEE HOW THE AUTHORS OF THIS PAPER TRIED TO IMPROVE IT BY INTRODUCING CONFIDENCE MEASURE OR CM.
utt_0031 utt 178.26 191.66 -X CM IS A METRIC THAT INDICATES THE RELIABILITY OF ASR DECISIONS. IT HAS LONG BEEN USED IN ASR SYSTEMS FOR EVALUATION OF WORD-LEVEL AND SENTENCE-LEVEL RECOGNITION RESULTS. THE USE
utt_0033 utt 191.66 203.35 -X OF CM IN TEXTUAL FEATURES OF SER MAY SUPPRESS INCORRECTLY RECOGNIZED WORDS OR EMPHASIZE THE MORE CORRECTLY RECOGNIZED WORDS. THE AUTHORS PROPOSED THREE DIFFERENT WAYS
utt_0035 utt 203.35 217.21 -X OF INTEGRATING CM INTO THE BASELINE SER MODEL. IN EARLY FUSION METHOD, CM IS TREATED AS ONE OF THE TEXTUAL EMBEDDING FEATURES AND IS CONCATENATED BEFORE THE TEXTUAL FEATURES HAVE GONE THROUGH
utt_0037 utt 217.21 229.21 -X BLSTM. IN LATE FUSION METHOD, CM IS CONCATENATED AFTER THE TEXTUAL FEATURES HAVE GONE THROUGH BLSTM AND BEFORE THE SELF-ATTENTION MECHANISM.
utt_0039 utt 229.21 236.32 -X IN THE THIRD AND LAST METHOD, CM IS CONCATENATED DIRECTLY TO THE SELF ATTENTION MECHANISM WEIGHTS.
utt_0040 utt 236.32 241.21 -X THE WEIGHTS ARE THEN UPDATED THROUGH A FULLY CONNECTED NETWORK. THIS TECHNIQUE REDUCES
utt_0041 utt 241.30 248.16 -X THE CM'S RELIANCE ON THE TEXTUAL FEATURES AND ALLOWS IT TO TRAIN WITH LESS DATA. BECAUSE
utt_0042 utt 248.50 262.33 -X CM SHOWS HOW TRUSTWORTHY AN ASR RESULT IS, CM VALUES MAY BE USED TO WEIGHT ASR RESULTS IN THE SAME WAY THAT THE SELF-ATTENTION MECHANISM DOES FOR TEXTUAL FEATURE EXTRACTION.
utt_0044 utt 262.33 268.00 -X NOW THAT WE UNDERSTAND HOW THIS MODEL WORKS, LET’S SEE HOW IT HAS PERFORMED IN EXPERIMENTS.
utt_0045 utt 268.47 275.04 -X AS YOU CAN SEE, THE METHODS THAT USE TRANSCRIPT ARE MORE SUCCESSFUL. THIS IS TO BE EXPECTED,
utt_0046 utt 275.04 284.75 -X BECAUSE, AS I MENTIONED PREVIOUSLY IN THIS VIDEO, ASR MODELS HAVE TROUBLE PROCESSING EMOTIONAL SPEECH. HONESTLY, I BELIEVE USING TRANSCRIPTION OF
utt_0048 utt 284.75 296.96 -X SPEECH IN THESE KINDS OF MODELS IS CHEATING. BECAUSE IN A REAL-WORLD SCENARIO, YOU MOST PROBABLY DON’T HAVE A NICE, HUMAN WRITTEN TRANSCRIPTION OF EVERY SPEECH AUDIO. SO, LET’S
utt_0050 utt 296.96 309.73 -X JUST IGNORE THEM. WITH CHEATERS OUT OF SIGHT, THE PROPOSED METHODS HAVE AN OPPORTUNITY TO SHINE! IT SEEMS THAT THE THIRD PROPOSED METHOD, WHICH CONTAINED ATTENTION WEIGHT CORRECTION, IS THE BEST.
utt_0053 utt 310.30 317.09 -X AS YOU CAN SEE HERE, THE MODEL INTRODUCED IN THIS PAPER ALSO BEATS OTHER SIMILAR MODELS.
utt_0054 utt 317.09 329.22 -X THE STRONG SIMILARITY BETWEEN UNWEIGHTED ACCURACY AND WEIGHTED ACCURACY IN THE SUGGESTED METHOD'S OUTCOME SHOWS THAT THE SER IS STABLE. HERE IS AN EXAMPLE OF HOW ATTENTION WEIGHT
utt_0056 utt 329.22 342.40 -X CORRECTION WORKS. AS YOU CAN SEE ON THE BOTTOM OF THIS SLIDE, THE CORRECT TEXT OF THIS UTTERANCE IS “YOU DON’T REALIZE THE STUFF THAT I’VE GOT IN THAT I REALLY NEED TO GET THE SUITCASE
utt_0058 utt 342.40 353.86 -X OKAY” BUT THE ASR MODEL’S OUTPUT IS “YOU DON’T REALIZE THE STUFF IT I GOT IN THAT I’VE REALLY NEED TO GET THE SIOUX CASE OHIO.”
utt_0060 utt 354.62 369.41 -X AS YOU SEE, SOME OF THE WORDS ARE NOT RECOGNIZED CORRECTLY. THESE WORDS ARE HIGHLIGHTED IN THE FIGURE. HERE, THE INCORRECTLY RECOGNIZED “I’VE” IS HIGHLY EMPHASIZED IN THE INITIAL TEXT ATTENTION,
utt_0063 utt 370.43 391.68 -X WHEREAS SOME OTHER CORRECTLY RECOGNIZED WORDS SUCH AS “REALIZE” AND “NEED” ARE NOT AS HEAVILY WEIGHTED. BY APPLYING THE WEIGHT CORRECTION FROM CM, THE WEIGHT FROM “I’VE” AND OTHER INCORRECTLY RECOGNIZED WORDS ARE DECREASED, AND THE WEIGHTS OF CORRECTLY RECOGNIZED WORDS ARE INCREASED. THIS UTTERANCE
utt_0064 utt 391.81 403.40 -X WAS AT FIRST INCORRECTLY RECOGNIZED AS “HAPPY”, BUT CORRECTED AFTERWARD TO “ANGRY”. THIS STUDY SUGGESTS A SER METHOD THAT MAKES
utt_0065 utt 403.40 410.37 -X USE OF ASR RESULTS AS TEXT INFORMATION AND WORD-LEVEL CM AS A BASIS FOR ITS SER PREDICTIONS.
utt_0066 utt 410.75 417.35 -X THE SUGGESTED APPROACH OUTPERFORMS THE MAJORITY OF PRIOR RESEARCH THAT USED THE IEMOCAP DATASET.
utt_0067 utt 418.53 426.92 -X SER AND ASR MAY NOW BE PERFORMED ONLY USING ACOUSTIC FEATURES AND WITHOUT REGARD TO FINE-TUNING FOR EMOTIONAL UTTERANCES.
