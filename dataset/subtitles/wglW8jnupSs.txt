utt_0000 utt 0.38 6.51 -X HI, I’M PAUL-EDOUARD FROM ETH ZURICH AND IN THIS VIDEO I WILL TELL YOU ABOUT MY INTERNSHIP AT META REALITY LABS.
utt_0002 utt 6.64 12.14 -X WE PRESENT ORIENTERNET FOR VISUAL LOCALIZATION IN twoD PUBLIC MAPS.
utt_0003 utt 12.14 16.72 -X HUMANS CAN ORIENT THEMSELVES IN THEIR threeD ENVIRONMENTS USING SIMPLE twoD MAPS.
utt_0004 utt 16.84 26.39 -X DIFFERENTLY, ALGORITHMS FOR VISUAL LOCALIZATION MOSTLY RELY ON COMPLEX threeD POINT CLOUDS THAT ARE EXPENSIVE TO BUILD, STORE, AND MAINTAIN OVER TIME.
utt_0006 utt 26.96 29.78 -X WE BRIDGE THIS GAP BY INTRODUCING ORIENTERNET,
utt_0007 utt 29.78 37.65 -X THE FIRST DEEP NEURAL NETWORK THAT CAN LOCALIZE AN IMAGE WITH SUB-METER ACCURACY USING THE SAME twoD SEMANTIC MAPS THAT HUMANS USE.
utt_0009 utt 38.03 42.74 -X ORIENTERNET LEVERAGES OPEN AND GLOBALLY AVAILABLE MAPS FROM OPENSTREETMAP,
utt_0010 utt 42.74 46.83 -X ENABLING ANYONE TO LOCALIZE ANYWHERE SUCH MAPS ARE AVAILABLE.
utt_0011 utt 47.12 54.00 -X OUR APPROACH ESTIMATES BOTH POSITION AND ORIENTATION BY MATCHING A NEURAL BIRD'S-EYE VIEW WITH A NEURAL MAP.
utt_0013 utt 54.03 59.38 -X IT LEARNS END-TO-END TO PERFORM SEMANTIC MATCHING WITH A WIDE RANGE OF MAP ELEMENTS.
utt_0014 utt 60.56 66.55 -X ORIENTERNET GENERALIZES TO NEW DATASETS CAPTURED BY AUGMENTED REALITY DEVICES OR ROBOTS,
utt_0017 utt 73.14 78.13 -X IN THIS VIDEO, WE WILL FIRST LEARN ABOUT COMMON APPROACHES TO VISUAL LOCALIZATION.
utt_0018 utt 78.16 80.95 -X WE WILL THEN UNDERSTAND HOW ORIENTERNET WORKS.
utt_0019 utt 81.14 85.94 -X FINALLY, WE WILL GET A SENSE OF HOW WELL IT WORKS IN PRACTICE. LET’S DIVE IN
utt_0020 utt 86.54 91.83 -X APPLICATIONS LIKE AUGMENTED REALITY OR ROBOTICS NEED ACCURATE threeD POSITIONING,
utt_0021 utt 91.83 96.34 -X AS TRANSLATION AND ROTATION, IN A GLOBAL REFERENCE FRAME.
utt_0022 utt 96.34 105.36 -X CONSUMER-GRADE GPS AND COMPASS ARE NOT SUFFICIENTLY ACCURATE AND ARE OFTEN UNRELIABLE DUE TO MULTI-PATH EFFECTS OR MAGNETIC ANOMALIES.
utt_0024 utt 105.36 108.41 -X CAMERAS HOWEVER ARE VERSATILE AND ACCURATE.
utt_0025 utt 108.59 111.06 -X TO ESTIMATE THE POSE OF THIS QUERY IMAGE,
utt_0026 utt 111.06 117.67 -X THE STANDARD APPROACH FINDS CORRESPONDENCES BETWEEN THE IMAGE AND A threeD POINT CLOUD RECONSTRUCTED USING MAPPING
utt_0029 utt 121.94 126.74 -X AND SCALES WELL TO LARGE AREAS BUT FACES SEVERAL LIMITATIONS.
utt_0030 utt 127.60 133.78 -X FIRST, BUILDING AND MAINTAINING threeD MAPS AT WORLD SCALE REQUIRES EXPENSIVE SENSOR FLEETS.
utt_0031 utt 133.84 144.69 -X BECAUSE THESE MAPS ENCODE THE LOCAL APPEARANCE, THEY NEED TO BE UPDATED AFTER MAJOR CHANGES LIKE SEASONS, WHICH REQUIRES ACQUIRING NEW MAPPING IMAGES.
utt_0033 utt 144.69 147.83 -X SECOND, SIMPLY STORING threeD MAPS IS VERY EXPENSIVE,
utt_0034 utt 147.83 150.39 -X BECAUSE THEY'RE MADE OF MILLIONS OF POINTS,
utt_0035 utt 150.39 153.43 -X EACH ASSOCIATED WITH A HIGH-DIMENSIONAL VISUAL DESCRIPTOR.
utt_0036 utt 153.62 161.27 -X THIS PREVENTS EXECUTING LOCALIZATION ON-DEVICE AND INSTEAD RELIES ON COSTLY CLOUD-BASED SERVICES.
utt_0037 utt 161.62 170.49 -X LAST, threeD MAPS POSE PRIVACY RISKS, BECAUSE THEIR DESCRIPTORS CAN BE INVERTED TO REVEAL THE APPEARANCE OF SENSITIVE CONTENT IN MAPPING OR QUERY IMAGES.
utt_0039 utt 170.74 175.00 -X THE RESEARCH COMMUNITY HAS RECENTLY PROPOSED SOLUTIONS,
utt_0040 utt 175.00 179.06 -X LIKE SCENE COMPRESSION AND PRIVACY-PRESERVING DESCRIPTORS.
utt_0041 utt 179.06 184.25 -X THESE HOWEVER CONSISTENTLY IMPAIR THE ROBUSTNESS AND ACCURACY OF LOCALIZATION ALGORITHMS.
utt_0042 utt 184.34 189.05 -X IN THIS PAPER, WE SUGGEST TO INSTEAD USE SIMPLER twoD SEMANTIC MAPS.
utt_0043 utt 190.10 195.29 -X twoD MAPS ARE WHAT HUMANS TYPICALLY USE TO FIND OUT WHERE THEY ARE AND TO NAVIGATE THEIR ENVIRONMENT.
utt_0044 utt 195.45 199.10 -X MOBILE MAP APPS ARE USED BY BILLIONS OF PEOPLE EVERYDAY.
utt_0045 utt 199.32 204.41 -X PLANIMETRIC MAPS ENCODE THE POSITION AND twoD SHAPE OF IMPORTANT OBJECTS,
utt_0046 utt 204.41 207.83 -X WHICH ARE REPRESENTED BY POINTS, LINES, OR POLYGONS.
utt_0047 utt 208.02 211.96 -X MAPS FROM OPENSTREETMAP ARE AVAILABLE ALL AROUND THE WORLD,
utt_0048 utt 211.96 215.19 -X FOR FREE, AND ARE MAINTAINED BY AN ACTIVE GLOBAL COMMUNITY.
utt_0049 utt 215.19 221.43 -X THESE MAPS INCLUDE A WIDE DIVERSITY OF OBJECTS COMMONLY FOUND IN URBAN AREAS,
utt_0050 utt 221.43 227.00 -X SUCH AS BUILDINGS, PARKS, TREES, BENCHES, TRASH BINS, AND MANY OTHERS.
utt_0051 utt 227.35 237.08 -X WHEN LOST IN SUCH ENVIRONMENT, HUMANS CAN EASILY ORIENT THEMSELVES BY MATCHING THE twoD MAP WITH A MENTAL MAP DERIVED FROM VISUAL OBSERVATIONS.
utt_0053 utt 237.08 245.08 -X ORIENTERNET LEARNS TO MIMIC THIS BEHAVIOR AND CAN LOCALIZE IN THE VERY SAME MAPS THAT HUMANS USE.
utt_0055 utt 245.08 248.12 -X USING twoD MAPS HAS MULTIPLE BENEFITS OVER threeD MAPS.
utt_0056 utt 248.18 253.92 -X THANKS TO OPENSTREETMAP, ORIENTERNET ENABLES ANYONE TO LOCALIZE ANYWHERE FOR FREE.
utt_0057 utt 253.92 257.95 -X BECAUSE THESE MAPS ENCODE THE SEMANTICS OF THE WORLD BUT NOT ITS APPEARANCE,
utt_0058 utt 257.95 259.87 -X THEY DON'T REQUIRE FREQUENT UPDATES.
utt_0059 utt 260.28 263.64 -X twoD VECTOR MAPS ARE EXTREMELY COMPACT AND EASY TO DOWNLOAD,
utt_0060 utt 263.64 269.24 -X WHICH ENABLES ON-DEVICE LOCALIZATION WITHIN LARGE AREAS, AND ONLY STORE PUBLIC INFORMATION.
utt_0061 utt 270.01 273.12 -X HOW CAN WE LOCALIZE IN threeD USING twoD MAPS?
utt_0062 utt 273.30 279.29 -X IN PRACTICE, THE GRAVITY DIRECTION IS OFTEN MEASURED BY INERTIAL SENSORS THAT ARE UBIQUITOUS IN TODAY'S DEVICES.
utt_0064 utt 279.29 281.85 -X THEY ARE LIKE THE INNER EAR OF HUMANS.
utt_0065 utt 282.30 290.05 -X ADDITIONALLY, THE VERTICAL POSITION OF THE CAMERA IS OFTEN IRRELEVANT BECAUSE MOTIONS IN OUTDOOR SPACES ARE MOSTLY HORIZONTAL.
utt_0067 utt 290.05 293.98 -X SINCE THESE DEVICES OFTEN COMPUTE A LOCAL threeD MAP FOR TRACKING,
utt_0068 utt 293.98 297.60 -X THE HEIGHT CAN ANYWAY BE ESTIMATED AS THE DISTANCE TO THE GROUND.
utt_0069 utt 297.88 302.62 -X THESE ASSUMPTIONS OVERALL REDUCE THE PROBLEM TO ESTIMATING A MUCH SIMPLER three-DOF POSE,
utt_0070 utt 303.67 307.65 -X AS twoD POSITION AND HEADING ANGLE.
utt_0071 utt 307.65 318.43 -X ORIENTERNET ESTIMATES THE POSE OF A QUERY IMAGE WITH KNOWN CAMERA INTRINSICS GIVEN OPENSTREETMAP DATA FOR A LOCAL AREA CENTERED AT A COARSE LOCATION PRIOR,
utt_0073 utt 318.43 321.44 -X FOR EXAMPLE A NOISY GPS POSITION.
utt_0074 utt 321.44 322.62 -X LET'S SEE HOW IT WORKS.
utt_0075 utt 323.36 329.22 -X FIRST, A CONVOLUTIONAL NEURAL NETWORK INFERS A threeD OCCUPANCY VOLUME FROM THE IMAGE.
utt_0076 utt 329.24 334.18 -X THIS OCCUPANCY MAPS IMAGE FEATURES TO A BIRD'S-EYE-VIEW, OR BEV,
utt_0077 utt 334.18 337.31 -X BY LIFTING THEM TO threeD AND POOLING THEM VERTICALLY.
utt_0078 utt 337.34 342.62 -X THIS RESULTS IN A NEURAL BEV WITH AN ASSOCIATED CONFIDENCE MAP.
utt_0079 utt 342.62 348.32 -X IN PARALLEL, THE VECTOR MAP IS RASTERIZED INTO MULTIPLE CHANNELS AND ENCODED INTO A NEURAL MAP.
utt_0080 utt 348.57 352.00 -X WE FINALLY EXHAUSTIVELY MATCH THE BEV AGAINST THE MAP.
utt_0081 utt 352.06 358.98 -X WE DISCRETIZE THE POSE SPACE INTO POSITIONS AND ANGLES ARRANGED IN A GRID AND, FOR EACH POSE,
utt_0082 utt 358.98 364.48 -X WE COMPUTE THE CORRELATION BETWEEN THE BEV AND THE MAP, WEIGHTED BY THE CONFIDENCE.
utt_0083 utt 364.61 370.43 -X THIS RESULTS IN A PROBABILITY VOLUME OVER POSES, FROM WHICH WE CAN EXTRACT A SINGLE ESTIMATE.
utt_0084 utt 370.69 375.65 -X ORIENTERNET IS TRAINED END-TO-END BY MAXIMIZING THE LIKELIHOOD OF THE GROUND-TRUTH POSE.
utt_0086 utt 382.72 388.51 -X EACH COLUMN OF PIXELS THEN MAPS TO A RAY IN THE BEV, WHICH WE DISCRETIZE IN DEPTH PLANES.
utt_0087 utt 388.73 393.41 -X TO MAP AN IMAGE FEATURES TO THE CORRECT PLANE, WE NEED TO ESTIMATE ITS DEPTH.
utt_0088 utt 393.44 396.07 -X WE INSTEAD PREDICT ITS CANONICAL SCALE,
utt_0089 utt 396.07 399.36 -X WHICH WE TRANSLATE TO A DEPTH USING THE KNOWN CAMERA INTRINSICS.
utt_0090 utt 399.39 402.31 -X THIS HELPS GENERALIZING TO ARBITRARY CAMERAS
utt_0091 utt 402.81 405.28 -X BY PREDICTING A DISTRIBUTION OVER SCALES,
utt_0092 utt 405.28 409.44 -X ORIENTERNET LEARNS TO INFER MONOCULAR DEPTH FROM POSE SUPERVISION ONLY.
utt_0093 utt 410.30 416.26 -X OBJECT CLASSES IN OPENSTREETMAP BELONG TO three CATEGORIES: NODES, LINES, AND AREAS.
utt_0094 utt 416.26 419.01 -X WE SELECT THE fifty MOST FREQUENT CLASSES,
utt_0095 utt 419.01 423.65 -X RASTERIZE EACH CATEGORY, AND COMBINE THEM INTO A HIGH-DIMENSIONAL NEURAL MAP.
utt_0096 utt 424.35 430.28 -X WE TRAIN A SINGLE MODEL THAT GENERALIZES TO UNSEEN LOCATIONS WITH ARBITRARY KINDS OF IMAGES.
utt_0097 utt 430.37 439.72 -X USING THE MAPILLARY PLATFORM, WE COLLECT A LARGE, CROWD-SOURCED DATASET OF IMAGES WITH GROUND-TRUTH POSES, CAPTURED IN twelve CITIES IN EUROPE AND THE US
utt_0099 utt 440.06 443.88 -X BY CAMERAS THAT ARE HANDHELD OR MOUNTED ON CARS OR BIKES.
utt_0100 utt 444.00 448.07 -X SUCH VISUAL DIVERSITY IS KEY TO GENERALIZATION.
utt_0101 utt 448.07 451.72 -X LET'S NOW LOOK AT SOME EXAMPLES. WE SHOW THE INPUT IMAGE AND THE MAP,
utt_0102 utt 451.72 454.95 -X WHERE BUILDINGS ARE SHOWN IN BLUE, ROADS IN RED, AND SO ON.
utt_0103 utt 455.04 459.56 -X WE DRAW THE GROUND-TRUTH POSE IN RED AND THE MAX-LIKELIHOOD POSE IN BLACK,
utt_0104 utt 459.56 461.22 -X WHICH ARE WITHIN fortyCM AND three DEGREES.
utt_0105 utt 462.88 469.03 -X THE LIKELIHOOD IS VERY SHARP BUT CAN ALSO ENCODE UNCERTAINTIES WITH MULTIPLE MODES, DUE TO SYMMETRIES.
utt_0107 utt 469.99 478.76 -X PREDICTIONS ARE MORE ACCURATE IN AREAS THAT EXHIBIT DISTINCTIVE AND WELL-LOCALIZED FEATURES, LIKE TREES AND BUILDING BOUNDARIES IN THE TOP EXAMPLE.
utt_0109 utt 478.76 483.08 -X PREDICTIONS HAVE A HIGHER UNCERTAINTY OTHERWISE, LIKE HERE IN THE BOTTOM ONE.
utt_0110 utt 485.44 489.38 -X WE EVALUATE ORIENTERNET FOR DRIVING VIEWPOINTS ON THE KITTI DATASET.
utt_0111 utt 489.60 498.12 -X THE MODEL WAS NOT TRAINED WITH ANY IMAGE FROM THIS CITY BUT GENERALIZES WELL BECAUSE THE MAP DATA IS STANDARDIZED ALL AROUND THE WORLD.
utt_0113 utt 498.12 507.40 -X THE POSITION ERROR IS ONLY seventy CM DESPITE THE LACK OF DISTINCTIVE OBJECTS - THERE A MOSTLY BUILDINGS AND THE ROAD VISIBLE IN THE BOTTOM EXAMPLE.
utt_0115 utt 507.40 511.43 -X WE CONSIDER TYPICAL AUGMENTED REALITY DATA CAPTURED WITH ARIA GLASSES.
utt_0116 utt 511.56 515.18 -X ORIENTERNET PROVIDES MUCH MORE ACCURATE POSITIONING THAN GPS,
utt_0117 utt 515.18 521.54 -X HERE SHOWN IN BLUE, WHICH FALLS SHORT IN URBAN CANYONS LIKE DOWNTOWN SEATTLE.
utt_0118 utt 521.54 527.53 -X SEMANTIC MAPS OFTEN DO NOT CONTAIN SUFFICIENT INFORMATION TO LOCALIZE A SINGLE IMAGE WITH LIMITED FIELD OF VIEW.
utt_0120 utt 527.62 531.46 -X THE MAPS ARE SOMETIMES SPATIALLY INACCURATE OR ARE MISSING SOME OBJECTS.
utt_0121 utt 531.62 536.23 -X IN THE TOP EXAMPLE, TREES ARE CLEARLY VISIBLE IN THE IMAGE BUT NOT IN THE MAP.
utt_0122 utt 536.23 538.51 -X THIS MAKES THE LOCALIZATION AMBIGUOUS.
utt_0123 utt 539.08 543.63 -X LOWER-LEVEL FEATURES LIKE WINDOWS OR BUILDING ENTRANCES ARE NOT IN OPENSTREETMAP,
utt_0124 utt 543.63 545.77 -X SO THE BOTTOM IMAGE CANNOT BE LOCALIZED.
utt_0125 utt 546.66 552.33 -X WE IMPROVE THE LOCALIZATION BY FUSING PREDICTIONS OVER MULTIPLE FRAMES WITH KNOWN RELATIVE POSES,
utt_0127 utt 552.33 556.33 -X WHICH SIMPLY AMOUNTS TO MAXIMIZING THE JOINT LIKELIHOOD.
utt_0128 utt 556.65 561.45 -X ACCUMULATING MORE FRAMES REDUCES THE ERROR THROUGHOUT THE SEQUENCE, HERE IN RED.
utt_0129 utt 563.37 570.19 -X THIS REDUCES THE AMBIGUITY OF MULTI-MODAL PREDICTIONS AND HERE IN THIS EXAMPLE YIELDS ERRORS LOWER THAN thirty CM.
utt_0131 utt 572.58 582.41 -X LOOK AT HOW THE SEQUENCE LIKELIHOOD CONVERGES TO A SINGLE MODE AS WE ACCUMULATE PREDICTIONS FROM ADDITIONAL FRAMES, IN THE RIGHT PANEL.
utt_0133 utt 582.41 585.84 -X FUSING GPS SIGNALS OVER TIME, HERE SHOWN IN ORANGE,
utt_0134 utt 585.84 589.39 -X IS OFTEN NOT RELIABLE BECAUSE THEIR NOISE IS BIASED.
utt_0135 utt 589.54 595.18 -X IN CONTRACT, FUSING PREDICTIONS OF ORIENTERNET, HERE IN RED, IS RELIABLE,
utt_0136 utt 595.18 601.04 -X EVEN THOUGH THE SINGLE-FRAME PREDICTIONS ARE OFTEN MULTIMODAL,
utt_0137 utt 601.04 605.36 -X HERE IS ANOTHER EXAMPLE OF SEQUENCE LOCALIZATION WITH ARIA DEVICES IN DETROIT.
utt_0138 utt 605.61 610.89 -X HERE AGAIN THE GPS, SHOWN IN BLUE, JUMPS OVER TIME AND IS INCONSISTENT,
utt_0139 utt 610.92 616.43 -X WHILE THE SEQUENTIAL PREDICTION IN RED IS CLOSER TO THE GROUND TRUTH.
utt_0140 utt 617.67 623.44 -X WE PERFORM AN EXTENSIVE QUANTITATIVE EVALUATION WITH BOTH KITTI AND ARIA DATASETS.
utt_0141 utt 623.44 628.40 -X HERE, WE PLOT THE RATIO OF IMAGES THAT ARE SUCCESSFULLY LOCALIZED WITHIN one METER OF ERROR.
utt_0142 utt 628.59 639.79 -X ON KITTI, LOCALIZING WITH SEMANTIC MAPS SIGNIFICANTLY IMPROVES OVER EXISTING APPROACHES BASED ON AERIAL IMAGERY, IN WHICH MANY GROUND FEATURES ARE NOT VISIBLE.
utt_0144 utt 639.79 643.57 -X ON ARIA DATA, ORIENTERNET IS MORE ACCURATE THAN GPS.
utt_0145 utt 643.79 648.97 -X THE PERFORMANCE REMAINS LOW, BUT FUSING SEQUENCES INCREASES THE RECALL BY A LARGE AMOUNT.
utt_0146 utt 648.97 652.01 -X SUCH ADDITIONAL INFORMATION DOES NOT BENEFIT THE GPS.
utt_0147 utt 652.27 655.92 -X CHECK OUT OUR PAPER FOR ADDITIONAL RESULTS.
utt_0148 utt 655.92 662.93 -X TO SUMMARIZE, WE DEMONSTRATE THAT twoD SEMANTIC MAPS ARE AN EFFECTIVE REPRESENTATION FOR VISUAL POSITIONING.
utt_0150 utt 662.93 672.02 -X ORIENTERNET GENERALIZES TO AR AND ROBOTICS DATA WITHOUT ANY RE-TRAINING AND ENABLES ANYONE TO LOCALIZE ANYWHERE FOR FREE.
utt_0152 utt 672.02 675.98 -X THE CODE IS PUBLICLY AVAILABLE - TRY IT YOURSELF WITH YOUR OWN PICTURES!
utt_0153 utt 675.98 680.11 -X CHECK OUT THE PAPER FOR QUANTITATIVE RESULTS AND ADDITIONAL VISUALIZATIONS.
utt_0154 utt 680.20 681.52 -2.1269 THANK YOU FOR WATCHING.
