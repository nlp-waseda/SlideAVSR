utt_0003 utt 5.87 6.61 -X THE FIELD.
utt_0005 utt 7.70 10.29 -X I HAVE A FRIEND WHO IS A PHYSICIAN THAT COULD HELP OUT.
utt_0006 utt 10.29 11.02 -X B: OH GREAT!
utt_0007 utt 11.02 14.00 -X CAN THEY LABEL A THOUSAND SENTENCES FOR ME?
utt_0008 utt 14.00 17.90 -X THAT WAY, I CAN UNDERSTAND WHAT’S GOING ON.
utt_0009 utt 17.90 20.14 -X A: MY FRIEND WON’T HAVE THAT MUCH TIME AND EFFORT.
utt_0010 utt 20.14 23.54 -X WHAT IF YOU JUST PICK ten SENTENCES THAT ARE THE MOST CRUCIAL TO LEARN?
utt_0011 utt 23.54 27.79 -X B: HOW DO I DECIDE WHAT’S MOST IMPORTANT TO LEARN?
utt_0012 utt 27.79 31.66 -X A: WELL, YOU SHOULD BE AN ACTIVE LEARNER AND FIGURE OUT WHAT YOU NEED TO KNOW.
utt_0014 utt 35.31 39.41 -X B: I DON’T KNOW ANYTHING ABOUT MEDICINE...SO EVERYTHING CONFUSES ME!
utt_0016 utt 42.23 44.91 -X HAVE YOU HEARD ABOUT THE NEW TRANSFORMER MODELS?
utt_0017 utt 45.13 48.05 -X B: YOU MEAN TRANSFORMER ROBOTS?
utt_0018 utt 48.33 54.03 -X A: NO, I MEAN THE MODELS THAT HAVE REVOLUTIONIZED LANGUAGE UNDERSTANDING FOR ARTIFICIAL INTELLIGENCE.
utt_0019 utt 54.13 59.99 -X THEY CONTAIN LOTS OF GENERAL KNOWLEDGE ABOUT THE WORLD FROM TRAINING ON INFORMATION ACCESSIBLE ON THE WEB.
utt_0022 utt 63.15 69.46 -X WHAT IF YOU USE THAT TO HELP YOU BECOME AN ACTIVE LEARNER?
utt_0023 utt 77.77 82.45 -X LABELING DATA IS A FUNDAMENTAL BOTTLENECK IN MACHINE LEARNING, ESPECIALLY FOR NLP, DUE
utt_0024 utt 82.48 85.23 -X TO ANNOTATION COST AND TIME.
utt_0025 utt 85.23 91.35 -X THE GOAL OF ACTIVE LEARNING IS TO RECOGNIZE THE MOST RELEVANT EXAMPLES AND THEN QUERY THEIR LABELS FROM AN ORACLE.
utt_0027 utt 91.35 95.99 -X THEN, PEOPLE CAN USE THIS SMALL LABELED DATASET TO TRAIN A COMPETENT MODEL.
utt_0028 utt 96.37 102.96 -X A WIDELY-USED ACTIVE LEARNING ALGORITHM IS UNCERTAINTY SAMPLING, WHERE THE LEARNER QUERIES THE MOST UNCERTAIN EXAMPLE.
utt_0030 utt 103.09 110.52 -X TYPICALLY, UNCERTAINTY IS MEASURED BY THE ENTROPY IN THE EXAMPLE’S LABEL DISTRIBUTION.
utt_0031 utt 110.52 120.56 -X ACTIVE LEARNING WOULD BE USEFUL FOR TODAY’S STATE-OF-THE-ART MODELS THAT CAN COST THOUSANDS OF DOLLARS IN CLOUD COMPUTING AND HUNDREDS OF POUNDS IN CARBON EMISSIONS TO TRAIN.
utt_0033 utt 121.36 126.71 -X HOWEVER, TRADITIONAL ACTIVE LEARNING ALGORITHMS, LIKE UNCERTAINTY SAMPLING, FALTER ON DEEP MODELS.
utt_0035 utt 126.71 132.08 -X THESE STRATEGIES USE MODEL CONFIDENCE SCORES, BUT NEURAL NETWORKS ARE POORLY CALIBRATED.
utt_0036 utt 132.50 141.52 -X HIGH CONFIDENCE SCORES DO NOT IMPLY HIGH CORRECTNESS LIKELIHOOD, SO THE SAMPLED EXAMPLES MAY NOT BE THE MOST UNCERTAIN ONES.
utt_0038 utt 142.61 153.91 -X THESE LIMITATIONS OF MODERN NLP MODELS ILLUSTRATE A TWOFOLD EFFECT: THEY SHOW A GREATER NEED FOR ACTIVE LEARNING AND MAKE ACTIVE LEARNING MORE DIFFICULT TO DEPLOY.
utt_0040 utt 153.91 158.32 -X IDEALLY, ACTIVE LEARNING COULD BE MOST USEFUL DURING LOW-RESOURCE SITUATIONS.
utt_0041 utt 159.70 166.96 -X IN REALITY, IT IS IMPRACTICAL TO USE BECAUSE THE ACTIVE LEARNING STRATEGY DEPENDS ON WARM-STARTING THE MODEL WITH INFORMATION ABOUT THE TASK.
utt_0043 utt 167.09 172.79 -X THUS, WE NEED A COLD-START APPROACH, ONE THAT DOES NOT RELY ON CLASSIFICATION LOSS OR CONFIDENCE SCORES.
utt_0045 utt 174.93 179.25 -X DASGUPTA DESCRIBES UNCERTAINTY AND DIVERSITY AS THE “TWO FACES OF ACTIVE LEARNING”.
utt_0048 utt 184.63 189.62 -X THE DECISION BOUNDARY IS DEFINED MORE CLEARLY AS WE SAMPLE THE POINTS THAT ARE MOST CONFUSING FOR THE MODEL TO LEARN.
utt_0050 utt 191.03 195.99 -X ON THE OTHER HAND, DIVERSITY SAMPLING EXPLOITS HETEROGENEITY IN THE EMBEDDING SPACE.
utt_0051 utt 195.99 201.81 -X IF THERE ARE PATTERNS IN THE FEATURE REPRESENTATIONS, THEN WE SHOULD FIND THE EXAMPLES THAT BEST REPRESENT THE DATASET.
utt_0053 utt 201.81 207.93 -X PRIOR WORK SHOW THAT A SUCCESSFUL AL STRATEGY SHOULD INTEGRATE BOTH ASPECTS.
utt_0054 utt 208.18 213.85 -X WE NEED INFORMATION ABOUT THE DOWNSTREAM TASK AND AN UNDERSTANDING OF THE UNDERLYING DATA STRUCTURE.
utt_0056 utt 215.06 222.33 -X HOWEVER, THE EXACT IMPLEMENTATION IS AN OPEN RESEARCH QUESTION.
utt_0057 utt 222.51 225.40 -X NOWADAYS, MODELS LIKE BERT DOMINATE THE LEADERBOARD.
utt_0058 utt 226.16 231.19 -X THE SUCCESS OF BERT LIES IN THE SELF-SUPERVISED OBJECTIVE, WHICH IS MASKED LANGUAGE MODELING.
utt_0059 utt 231.19 238.13 -X TO PRE-TRAIN BERT, INPUT TOKENS ARE RANDOMLY MASKED, AND THE MODEL NEEDS TO PREDICT THE TOKEN LABELS OF THE MASKED TOKENS.
utt_0061 utt 238.64 247.06 -X SINCE WE ARE DEVELOPING ACTIVE LEARNING FOR BERT, WHY NOT USE THE INFORMATION ACCUMULATED DURING PRE-TRAINING FOR ACTIVE LEARNING?
utt_0067 utt 259.35 263.00 -X ALPS OUR PAPER PROPOSES AN ACTIVE LEARNING STRATEGY
utt_0068 utt 263.03 268.25 -X CALLED ALPS, WHICH STANDS FOR ACTIVE LEARNING BY PROCESSING SURPRISAL.
utt_0069 utt 268.25 274.36 -X ALPS SUBSTITUTES THE SUPERVISED LOSS WITH THE SELF-SUPERVISED OBJECTIVE TO ESTIMATE UNCERTAINTY AND SAMPLE EXAMPLES.
utt_0071 utt 276.05 283.58 -X FIRST, WE ITERATE OVER THE UNLABELED DATASET AND COMPUTE A SURPRISAL EMBEDDING FOR EACH EXAMPLE USING PRE-TRAINED BERT.
utt_0073 utt 283.58 286.49 -X SECOND, WE RUN KMEANS CLUSTERING ON THE SURPRISAL EMBEDDINGS.
utt_0074 utt 287.80 291.83 -X THIRD, WE SEARCH FOR THE SENTENCES THAT ARE CLOSEST TO THE CLUSTER CENTERS.
utt_0075 utt 292.92 298.94 -X FINALLY, WE QUERY LABELS FOR THESE NEWLY FOUND SENTENCES AND ADD THEM TO OUR LABELED DATASET FOR MODEL FINE-TUNING.
utt_0077 utt 299.96 303.70 -X NOW, LET’S BREAK THE ALGORITHM DOWN INTO MORE DETAIL.
utt_0078 utt 303.70 307.80 -X TO CONSTRUCT A SURPRISAL EMBEDDING, WE USE THE MASKED LANGUAGE MODELING HEAD OF BERT.
utt_0079 utt 308.44 311.61 -X SUPPOSE WE ARE GIVEN THIS TOY SENTENCE AS AN INPUT.
utt_0080 utt 314.32 324.19 -X WE PASS THIS SENTENCE THROUGH BERT AND ITS MASKED LANGUAGE MODELING HEAD TO GET A REPRESENTATION FOR EACH TOKEN.
utt_0082 utt 324.98 330.14 -X THEN, WE WANT TO PREDICT TOKENS BASED ON THESE REPRESENTATIONS.
utt_0083 utt 330.49 334.62 -X AS IN BERT, WE ONLY PERFORM PREDICTIONS FOR fifteen% OF THE TOKENS IN THE SENTENCE.
utt_0084 utt 337.17 343.19 -X IN THIS CASE, WE ONLY PREDICT THE WORDS: HIGHEST, MOUNTAIN, AND EUROPE.
utt_0085 utt 343.19 346.55 -X WE USE CROSS-ENTROPY LOSS TO SCORE OUR PREDICTIONS.
utt_0086 utt 347.51 352.12 -X AFTER WE RECEIVE A VECTOR OF SCORES, WE NORMALIZE THIS VECTOR FOR CLUSTERING LATER ON.
utt_0087 utt 354.93 359.29 -X IN THE END, WE GET A SPARSE VECTOR OF SURPRISAL SCORES.
utt_0088 utt 359.29 363.58 -X THE NONZERO VALUES CORRESPOND TO THE WORDS, HIGHEST, MOUNTAIN, AND EUROPE, BECAUSE THEY
utt_0089 utt 363.96 368.47 -X WERE PREDICTED WHEN WE COMPUTED CROSS-ENTROPY LOSS.
utt_0090 utt 368.47 374.23 -X NOTICE THAT THERE IS ONE DIFFERENCE BETWEEN BERT PRE-TRAINING AND OUR CONSTRUCTION OF SURPRISAL EMBEDDINGS.
utt_0092 utt 374.23 376.51 -X WE DO NOT MASK ANY TOKENS IN THE INPUT.
utt_0093 utt 376.53 382.17 -X THIS MAINTAINS CONSISTENCY BETWEEN THE ACTIVE LEARNING AND FINE-TUNING STAGES.
utt_0094 utt 382.17 385.53 -X NOW, LET’S SEE HOW ALPS SAMPLES DATA.
utt_0095 utt 385.65 391.83 -X THESE ARE SENTENCES FROM THE PUBMED TRAINING DATASET, A SENTENCE CLASSIFICATION DATASET FOR MEDICAL ABSTRACTS.
utt_0097 utt 392.76 399.45 -X NOTICE THAT THE RED SENTENCE IS HARDER TO UNDERSTAND WHILE THE GREEN ONE IS SHORT AND EASY TO COMPREHEND.
utt_0099 utt 400.41 405.88 -X GIVEN THESE SENTENCES, WE COMPUTE SURPRISAL EMBEDDINGS LIKE BEFORE.
utt_0100 utt 406.71 410.56 -X REMEMBER, ONLY fifteen% OF THE TOKENS ARE EVALUATED WITH MASKED LANGUAGE MODELING.
utt_0101 utt 411.74 417.31 -X NOTICE THAT THE RED VECTOR HAS LARGER SURPRISAL SCORES THAN THE GREEN VECTOR.
utt_0102 utt 417.31 423.55 -X THIS INDICATES THAT THE GREEN SENTENCE SURPRISES THE MODEL MORE BECAUSE THE TOKEN MAY NOT FIT WITHIN ITS SENTENCE CONTEXT.
utt_0104 utt 423.55 430.36 -X NOW, WE PLOT THESE POINTS AND OBSERVE THE KMEANS CLUSTERS OF THE SURPRISAL EMBEDDINGS.
utt_0105 utt 432.57 436.35 -X DIFFERENT SENTENCES CAN SURPRISE THE MODEL IN VARIOUS WAYS.
utt_0106 utt 436.54 445.28 -X BY USING KMEANS TO CLUSTER THE SURPRISAL EMBEDDINGS, WE CAN SEPARATE SENTENCES FROM THE WAY THAT THEY SURPRISE THE MODEL.
utt_0108 utt 445.28 451.04 -X FINALLY, WE TAKE THE SENTENCES CLOSEST TO THE KMEANS CENTERS AS OUR ACTIVE LEARNING SAMPLES.
utt_0110 utt 451.74 456.51 -X SOME SENTENCES ARE EASIER FOR THE MODEL TO LEARN, WHILE OTHERS ARE MORE DIFFICULT.
utt_0111 utt 456.51 466.91 -X THROUGH SAMPLING WITH ALPS, THE MODEL CAN UNDERSTAND THE TASK BY LEARNING LESS SURPRISING EXAMPLES, AND ALSO CHALLENGE ITSELF BY LEARNING VERY SURPRISING EXAMPLES.
utt_0113 utt 468.47 473.37 -X WE RUN ACTIVE LEARNING SIMULATIONS ON SENTENCE CLASSIFICATION DATASETS.
utt_0114 utt 473.37 477.34 -X FOR THIS VIDEO, WE WILL ANALYZE RESULTS OF THE PUBMED DATASET.
utt_0115 utt 477.43 480.77 -X FOR OTHER EXPERIMENTS AND FURTHER ANALYSIS, PLEASE REFER TO OUR PAPER.
utt_0117 utt 486.33 494.81 -X IN OUR EXPERIMENTS, WE SIMULATE AN ACTIVE LEARNER FOR ten ITERATIONS AND SAMPLE ten0 SENTENCES ON EACH ITERATION.
utt_0119 utt 495.55 501.50 -X WE COMPARE AGAINST RANDOM SAMPLING, WHICH IS KNOWN TO BE A STRONG BASELINE,
utt_0120 utt 502.55 505.50 -X ENTROPY SAMPLING, AN UNCERTAINTY-BASED STRATEGY,
utt_0121 utt 506.91 511.90 -X BADGE, A RECENT STATE-OF-THE-ART ACTIVE LEARNING WORK THAT COMBINES UNCERTAINTY AND DIVERSITY
utt_0122 utt 512.47 517.34 -X BERT-KM, WHICH IS KMEANS CLUSTERING OF THE PRE-TRAINED BERT REPRESENTATIONS FOR THE DATASET
utt_0123 utt 518.27 524.03 -X FT-BERT-KM, WHICH IS KMEANS CLUSTERING OF THE FINE-TUNED BERT REPRESENTATIONS FOR THE DATASET
utt_0126 utt 530.11 534.33 -X AS EXPECTED, ENTROPY DOES WORSE DURING COLD-START
utt_0127 utt 537.37 540.22 -X BERT-KM DOESN’T DO BETTER THAN RANDOM.
utt_0128 utt 542.75 545.82 -X FT-BERT-KM ALSO DOESN’T MAKE A DIFFERENCE.
utt_0130 utt 554.75 558.24 -X OUR METHOD ALPS PERFORMS AS WELL AS BADGE
utt_0131 utt 564.35 568.00 -X THE DOTTED LINE INDICATES THE FULL TRAINING DATASET.
utt_0132 utt 568.00 572.67 -X BY THE END OF ten ITERATIONS, ALPS AND BADGE ALMOST REACH THAT MARK.
utt_0133 utt 579.42 586.50 -X LAST BUT NOT LEAST, LET’S SEE HOW QUICKLY THESE SAMPLING ALGORITHMS RUN FOR ONE ITERATION.
utt_0134 utt 587.20 593.47 -X WHILE BADGE MAY SAMPLE THE EXAMPLES NEEDED BY THE MODEL, THE ALGORITHM TAKES VERY LONG TO RUN.
utt_0136 utt 593.66 601.38 -X FOR PUBMED, BADGE TAKES seventy MINUTES TO RUN FOR one ITERATION WHILE ALPS ONLY TAKES twenty-four MINUTES.
utt_0137 utt 602.24 607.74 -X IN OUR PAPER, WE ALSO ANALYZE THE ALGORITHMIC COMPLEXITY OF THE STRATEGIES TO SHOW THAT
utt_0138 utt 607.87 611.91 -3.7302 ALPS IS AN EFFICIENT ACTIVE LEARNING STRATEGY FOR LANGUAGE DATA.
