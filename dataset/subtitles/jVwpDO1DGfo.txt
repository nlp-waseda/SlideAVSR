utt_0000 utt 0.24 14.19 -X HI I'M TERESA FROM VILAB AT EPFL. I'LL BE PRESENTING OUR WORK TITLED ROBUSTNESS VIA CROSS-DOMAIN ENSEMBLES. THIS IS JOINT WORK WITH OGUZHAN AND AMIR. WE HAVE A PROJECT WEBPAGE WHERE WE HAVE MORE QUALITATIVE RESULTS, LINKS TO OUR TRAINING CODE, TO DOWNLOAD PRE-TRAINED MODELS AND
utt_0003 utt 14.19 25.97 -X BASELINES. NEURAL NETWORKS IN THE REAL WORLD WILL ENCOUNTER DATA WITH DISTORTIONS SUCH AS DEFOCUS BLUR, JPEG COMPRESSION, OR ADVERSARIAL SHIFTS. FOR EXAMPLE, THE FIRST COLUMN SHOWS A
utt_0005 utt 25.97 30.80 -X CLEAN UNDISTORTED IMAGE AND THE CORRESPONDING SURFACE NORMALS PREDICTION OF A STANDARD NETWORK
utt_0006 utt 31.12 36.11 -X UNDER SLIGHT DISTORTIONS. THE SECOND AND THIRD COLUMN SHOWS THE PREDICTIONS OF THE SAME MODEL.
utt_0007 utt 36.11 45.62 -X THE GROUND TRUTH IS SHOWN ON THE BOTTOM RIGHT. AS YOU CAN SEE, SEEMINGLY SMALL AND NON-ADVERSARIAL IMAGE DISTORTIONS CAUSE LARGE ERRORS IN THE NEURAL NETWORK PREDICTION.
utt_0009 utt 45.97 50.23 -X THE ERRORS IN THE PREDICTIONS GET WORSE AS THE DISTORTION INTENSITY INCREASES,
utt_0010 utt 50.26 62.20 -X AS SEEN IN THE PLOT IN THE TOP RIGHT. THIS IS THE CHALLENGE THAT NEEDS TO BE SOLVED FOR MODELS TO BE USEFUL IN THE REAL WORLD. OUR PROPOSED METHOD IS BASED ON ENSEMBLES. THE CORE IDEA
utt_0012 utt 62.20 67.44 -X BEHIND ENSEMBLES IS RELYING ON THE DIVERSITY OF INDIVIDUAL MODELS TO DECORRELATE ERRORS.
utt_0013 utt 67.44 72.24 -X SOURCES OF DIVERSITY INCLUDES USING DIFFERENT INITIALIZATIONS LIKE DEEP ENSEMBLES,
utt_0014 utt 72.24 76.98 -X DIFFERENT HYPERPARAMETERS, NETWORK ARCHITECTURES, OR BY MODIFYING THE TRAINING LOSS.
utt_0015 utt 78.96 83.41 -X OUR METHOD IS BASED ON MAKING PREDICTIONS FROM A DIVERSE SET OF CUES THAT WE CALL MIDDLE DOMAINS
utt_0016 utt 84.08 87.80 -X AND WE MERGE THE PREDICTIONS BASED ON THEIR UNCERTAINTIES.
utt_0017 utt 89.23 103.70 -X THIS IS HOW OUR METHOD WORKS ON A SINGLE IMAGE. GIVEN THE DISTORTED INPUT IMAGE WHERE THE DISTORTION IS JPEG COMPRESSION WE EXTRACT THE SET OF MIDDLE DOMAINS. EXAMPLES OF THESE MIDDLE DOMAINS ARE twoD EDGES, LOW PASS FILTERING, GRAYSCALE, AND EMBOSS FILTERING.
utt_0020 utt 103.89 107.03 -X NOTE THAT THESE MIDDLE DOMAINS ARE PROGRAMMATICALLY EXTRACTED,
utt_0021 utt 107.03 115.77 -X SO THEY DO NOT NEED ANY LABELS AND THEY CAN BE OBTAINED FOR ANY DATASET WITHOUT SUPERVISION.
utt_0022 utt 117.04 127.13 -X GIVEN MIDDLE DOMAINS AS INPUTS WE LEARN TO PREDICT THE FINAL TARGET DOMAIN WHICH IS SURFACE NORMALS HERE. EACH MIDDLE DOMAIN HAS CORRESPONDING PREDICTIONS THAT DEFINE A PATH
utt_0024 utt 127.96 132.47 -X AND AS YOU CAN SEE EACH PATH REACTS DIFFERENTLY TO A PARTICULAR DISTRIBUTION SHIFT.
utt_0025 utt 132.76 136.73 -X SO THE PREDICTION QUALITY WILL DIFFER AMONG PATHS.
utt_0026 utt 136.73 142.71 -X IN THIS CASE THE LOW PASS FILTER PATH DOES NOT DEGRADE AS MUCH AS THE OTHERS.
utt_0027 utt 142.71 153.85 -X TO OBTAIN UNCERTAINTIES, WE MODEL THE NETWORK OUTPUT AS A LAPLACIAN DISTRIBUTION WITH PARAMETERS MEAN AND SIGMA WHERE MEAN IS THE PREDICTION AND SIGMA IS THE CORRESPONDING UNCERTAINTY.
utt_0029 utt 153.97 157.21 -X WE THEN TRAIN THE NETWORK WITH A NEGATIVE LOG LIKELIHOOD LOSS.
utt_0030 utt 157.40 160.44 -X IN THE UNCERTAINTY SHOWN HERE DARKER MEANS MORE CONFIDENT.
utt_0031 utt 161.94 166.36 -X WE CONVERT THE UNCERTAINTIES TO MERGING WEIGHTS BY TAKING THE INVERSE OF EACH PATH'S UNCERTAINTY.
utt_0032 utt 168.28 172.86 -X THE IDEA IS THAT EACH PATH WILL CONTRIBUTE TO THE FINAL PREDICTION BASED ON ITS UNCERTAINTY.
utt_0033 utt 175.13 179.26 -X FOR EXAMPLE THIS PATH HAS DEGRADED PREDICTIONS,
utt_0034 utt 182.58 187.36 -X THUS HIGHER UNCERTAINTY WHICH RESULTS IN LOWER MERGING WEIGHTS COMPARED TO THE OTHER PATHS.
utt_0035 utt 189.88 198.30 -X WE MERGE THE PREDICTIONS FROM DIFFERENT PATHS BY USING THE MERGING WEIGHTS AND THIS IS OUR FINAL PREDICTION.
utt_0037 utt 198.30 207.64 -X WE HAVE SEEN AN EXAMPLE ON THE SINGLE IMAGE. WE WILL NOW SEE HOW THIS WORKS ON THE SHORT VIDEO CLIP. OUR ENSEMBLE HAS EIGHT COMPONENTS RESULTING IN EIGHT PATHS. THESE ARE THEIR PREDICTIONS,
utt_0039 utt 207.64 211.87 -X CORRESPONDING UNCERTAINTIES, AND WEIGHTS. THE FINAL PREDICTION IS SHOWN IN THE TOP ROW.
utt_0040 utt 212.63 222.88 -X SEE HOW EACH PATH PREDICTION DIFFERS. FOR EXAMPLE THE LOW PASS PATH HAS LOWER UNCERTAINTIES WITH MORE ACCURATE PREDICTIONS ON THE FLOOR. THIS IS ALSO REFLECTED IN THE HIGHER WEIGHTS
utt_0042 utt 235.03 242.21 -X REMEMBER THAT WE MODEL OUR OUTPUT AS LAPLACIAN WITH PARAMETERS MEAN AND SIGMA WHERE MEAN CORRESPONDS TO THE PREDICTION AND SIGMA IS THE UNCERTAINTY.
utt_0044 utt 242.49 246.50 -X AS SHOWN IN THE VIDEO OUR METHOD RELIES ON THE UNCERTAINTY ESTIMATES TO MERGE PREDICTIONS.
utt_0045 utt 246.50 251.33 -X HOWEVER, UNCERTAINTY ESTIMATES ARE UNCALIBRATED UNDER DISTRIBUTION SHIFTS.
utt_0046 utt 251.42 256.42 -X IN OTHER WORDS, THERE IS A HIGH TENDENCY TO OUTPUT POOR PREDICTIONS WITH HIGH CONFIDENCE.
utt_0047 utt 256.42 266.27 -X TO CALIBRATE THE UNCERTAINTIES, WE PROPOSE AN ADDITIONAL TRAINING STAGE THAT WE CALL SIGMA TRAINING. IN THIS STAGE WE FEED THE NETWORK WITH OUT-OF-DISTRIBUTION INPUTS.
utt_0049 utt 266.27 272.67 -X WE TRAIN WITH TWO LOSS TERMS: THE FIRST TERM HELPS SIGMA TO INCREASE WHEN FACED WITH DISTORTED DATA AND THE SECOND TERM HELPS TO KEEPS PREDICTIONS FIXED.
utt_0051 utt 272.67 285.25 -X THIS IS AN EXAMPLE OF OVERCONFIDENT AND INCORRECT PREDICTIONS. THE IMAGE HAS BEEN DISTORTED BY DIFFERENT LEVELS OF SPECKLE NOISE AND THESE ARE RESHADE PREDICTIONS. BEFORE SIGMA TRAINING,
utt_0053 utt 285.25 289.35 -X WHEN THE MODEL PRODUCES POOR RESULTS THE UNCERTAINTY DOES NOT INCREASE CORRESPONDINGLY.
utt_0054 utt 290.05 293.57 -X THE SAME GOES FOR THE UNCERTAINTY PRODUCED BY DEEP ENSEMBLES.
utt_0055 utt 295.39 305.03 -X AFTER SIGMA TRAINING UNCERTAINTY ESTIMATES ARE BETTER CALIBRATED TO ACCOUNT FOR DEGRADED PREDICTIONS. NOTICE THAT OUR PREDICTIONS DO NOT CHANGE AFTER SIGMA TRAINING. IF YOU COMPARE
utt_0057 utt 305.03 311.75 -X THE PREDICTIONS BEFORE AND AFTER SIGMA TRAINING THEY ARE THE SAME BUT UNCERTAINTIES ARE HIGHER,
utt_0058 utt 311.75 315.04 -X HENCE THE UNCERTAINTIES HAVE BEEN SUCCESSFULLY CALIBRATED.
utt_0059 utt 317.44 321.32 -X WE QUANTIFY THIS WITH A SCATTER PLOT. HERE IT SHOWS THE ERROR AGAINST AVERAGE SIGMA.
utt_0060 utt 321.54 326.44 -X EACH POINT IS AN AVERAGE OF A sixteen thousand TEST IMAGES FOR ONE UNSEEN DISTORTION AND ONE SEVERITY LEVEL.
utt_0061 utt 327.84 332.93 -X BEFORE SIGMA TRAINING SIGMAS DO NOT INCREASE WITH AN INCREASE IN ERROR AND THIS IS THE SAME FOR DEEP ENSEMBLES.
utt_0063 utt 334.40 338.15 -X AFTER SIGMA TRAINING THERE IS A STRONGER CORRELATION BETWEEN UNCERTAINTIES AND ERROR.
utt_0064 utt 340.99 345.99 -X NOW FOR SOME RESULTS. WE OBTAINED IMPROVEMENTS IN ROBUSTNESS COMPARED TO SEVERAL BASELINES
utt_0065 utt 346.11 357.67 -X UNDER NON-ADVERSARIAL SHIFTS AND ADVERSARIAL ONES FOR SEVERAL TASKS AND DATA SETS LIKE REGRESSION ON TASKONOMY, REPLICA, AND HABITAT AND CLASSIFICATION ON IMAGENET AND CIFAR. SO UNLIKE
utt_0067 utt 357.67 362.73 -X MOST EXISTING WORKS ON ROBUSTNESS FOCUSING ON LOW DIMENSIONAL PREDICTION TASKS LIKE CLASSIFICATION,
utt_0068 utt 362.95 373.48 -X WE ALSO EXPERIMENT WITH DENSE PIXEL-WISE TASKS. LET'S LOOK AT SOME QUALITATIVE RESULTS. EACH ROW SHOWS AN IMAGE FROM THE TASKONOMY TEST SET UNDER INCREASING SPECKLE NOISE.
utt_0070 utt 373.92 378.73 -X THESE ARE THE RESULTS FOR THE RESHADING TASK AND THE CORRESPONDING GROUND TRUTH. FOR COMPARISON
utt_0071 utt 379.17 390.79 -X OUR METHOD DEGRADES LESS THAN DEEP ENSEMBLES AND USING A SINGLE MODEL. THE DIFFERENCE IS OBVIOUS AT HIGH DISTORTION INTENSITIES. THIS IS FOR SURFACE NORMALS TASK. THE SAME TREND HOLDS. OUR
utt_0073 utt 390.79 403.08 -X PREDICTIONS DO NOT COLLAPSE AS QUICKLY AS THE BASELINES. FINALLY FOR DEPTH ESTIMATION. THIS DEMONSTRATES THE EFFECTIVENESS OF USING DIFFERENT MIDDLE DOMAINS TO OBTAIN A ROBUST PREDICTION.
utt_0075 utt 403.84 408.65 -X MORE QUALITATIVE RESULTS UNDER FOUR DISTORTIONS: IMPULSE NOISE, DEFOCUS BLUR,
utt_0076 utt 408.65 413.67 -X CONTRAST, AND PIXELATE ON A SAMPLE IMAGE FROM THE REPLICA DATA SET WITH THE SAME DISTORTION LEVEL.
utt_0077 utt 413.67 416.62 -X EACH PREDICTION IS FOLLOWED BY ITS CORRESPONDING ERROR MAP
utt_0078 utt 416.87 420.81 -X AND FOR PIXELATE THE IMPROVEMENTS OVER ENSEMBLE AND SINGLE MODEL IS OBVIOUS.
utt_0079 utt 420.97 424.62 -X FOR THE OTHER DISTORTIONS WE CIRCLE THE IMPROVEMENTS IN FINE-GRAINED DETAILS
utt_0080 utt 426.63 433.35 -X MORE QUALITATIVE RESULTS ON A YOUTUBE VIDEO. THIS VIDEO HAS BEEN CORRUPTED BY INCREASING SHOT NOISE.
utt_0081 utt 433.35 438.44 -X OUR METHOD RETURNS MORE ACCURATE PREDICTIONS WITH LESS FLICKERING.
utt_0082 utt 441.61 460.81 -X THESE ARE THE QUANTITATIVE RESULTS FOR COMMON CORRUPTION DISTORTIONS APPLIED ON THE TASKONOMY
utt_0083 utt 460.81 475.53 -X TEST SET FOR SURFACE NORMALS PREDICTION. EACH POINT SHOWS THE ERROR AVERAGED OVER eleven UNSEEN DISTORTIONS. LOWER IS BETTER. SOME OF THE BASELINES WORTH MENTIONING: YELLOW IS A SINGLE MODEL, DARK BLUE IS DEEP ENSEMBLES, LIGHT BLUE AND DARK BROWN ARE SINGLE MODELS BUT WITH DATA
utt_0086 utt 475.53 480.46 -X AUGMENTATION WHERE LIGHT BLUE IS ADVERSARIAL TRAINING AND DARK BROWN IS WITH STYLE DATA.
utt_0087 utt 481.03 484.08 -X RED IS OUR METHOD WHICH CLEARLY OUTPERFORMS THE BASELINES.
utt_0088 utt 484.23 488.46 -X PURPLE AND GREEN ARE VARIANTS OF OUR METHOD WHERE PURPLE VARIANT LEARNS EMERGING FUNCTION
utt_0089 utt 489.42 504.27 -X AND GREEN IS A SIMPLE AVERAGE. THE TRENDS ARE SIMILAR FOR DEPTH AND RESHADE TASKS TOO. DEEP ENSEMBLES HAVE A SIMILAR PERFORMANCE COMPARED TO USING A SINGLE MODEL. EVEN THOUGH AUGMENTATION METHODS SUCH AS ADVERSARIAL TRAINING AND STYLE ARE COMPETITIVE BASELINES,
utt_0092 utt 504.68 514.10 -X OUR METHOD HAS STRONGER PERFORMANCE COMPARED TO THEM. WE ALSO HAVE QUANTITATIVE RESULTS FOR THE SAME TASK ON OTHER DATASETS SUCH AS REPLICA AND HABITAT IN THE SUPPLEMENTARY MATERIAL.
utt_0094 utt 516.59 523.38 -X THE ROBUSTNESS OF OUR METHOD IS NOT LIMITED TO NON-ADVERSARIAL SHIFTS. THE VIDEO SHOWS AN IMAGE
utt_0095 utt 525.74 530.32 -X UNDER ATTACKS GENERATED BY I-FGSM UNDER INCREASING ATTACK STRENGTH.
utt_0096 utt 530.32 535.89 -X OUR METHOD IS MORE ROBUST TO ATTACKS COMPARED TO BASELINES.
utt_0097 utt 538.44 542.83 -X WE ALSO QUANTIFY THE ADVERSARIAL ROBUSTNESS OF OUR METHOD UNDER DIFFERENT ATTACK STRENGTH
utt_0098 utt 543.21 550.07 -X WITH Lone ERRORS. LOWER IS BETTER, AND OUR METHOD YIELDS LOWER ERROR COMPARED TO BASELINES.
utt_0099 utt 550.07 559.63 -X WE BELIEVE THAT THIS IS BECAUSE USING MIDDLE DOMAINS PROMOTE ENSEMBLE DIVERSITY IN A WAY THAT MAKES IT MORE CHALLENGING TO CREATE AN ATTACK THAT FOOLS ALL PATHS SIMULTANEOUSLY.
utt_0101 utt 560.21 571.15 -X WE HAVE SOME ABLATION STUDIES. THIS ONE LOOKS AT THE EFFECT OF INCREASING THE NUMBER OF PATHS AND THE ROLES OF UNCERTAINTY ON PERFORMANCE. THIS IS PERFORMANCE ON OUT OF DISTRIBUTION DATA.
utt_0103 utt 571.73 577.30 -X EACH POINT SHOWS THE AVERAGE ERROR OVER ALL COMBINATIONS OF A FIXED NUMBER OF PATHS.
utt_0104 utt 577.33 582.45 -X FOR ANY GIVEN NUMBER OF PATHS OUR METHOD, SHOWN IN RED, CONSISTENTLY OUTPERFORMS DEEP ENSEMBLES,
utt_0105 utt 582.45 592.66 -X SHOWN IN BLUE, FOR ALL THREE TASKS. THIS ALSO IMPLIES THAT PERFORMANCE IS NOT SENSITIVE TO THE CHOICE OF MIDDLE DOMAINS AND THE PERFORMANCE GAP INCREASES WITH MORE PATHS.
utt_0107 utt 592.66 602.61 -X COMPARED TO A VARIANT OF OUR METHOD, UNIFORM MERGING, SHOWN IN GREEN WHICH DOES A SIMPLE AVERAGE OF THE PREDICTIONS, MERGING WITH UNCERTAINTIES FURTHER BOOST PERFORMANCE.
utt_0109 utt 602.61 608.53 -X THIS IS ANOTHER ABLATION STUDY TO ANALYZE THE IMPORTANCE OF EACH MIDDLE DOMAIN.
utt_0110 utt 608.53 620.53 -X THE CHART HAS eleven AXES. EACH ONE REPRESENTS ONE DISTORTION. EACH CURVE REPRESENTS A MIDDLE DOMAIN PREDICTION PATH SO THERE ARE EIGHT CURVES. A CURVE GETS ONE ON THE DISTORTION IF IT CONTRIBUTES TO
utt_0112 utt 620.53 631.45 -X LEAST AND EIGHT WHEN IT CONTRIBUTES THE MOST. FOR EXAMPLE THE DIRECT PATH WHICH IS A MAPPING FROM RGB TO THE TARGET TASK IS MOST HELPFUL FOR SATURATE AND twoD EDGES WAS THE LEAST HELPFUL.
utt_0114 utt 631.51 635.21 -X WE SEE THAT THE LOW PASS MIDDLE DOMAIN WAS MOST HELPFUL FOR NOISE DISTORTIONS
utt_0115 utt 636.08 641.30 -X AND THE SHARPENED DOMAIN WAS MOST HELPFUL FOR CONTRAST DISTORTIONS.
utt_0116 utt 641.30 651.86 -X OUR METHOD IS NOT LIMITED TO PIXEL-WISE REGRESSION TASKS. WE SHOW THE RESULTS HERE FOR IMAGENET CLASSIFICATION UNDER CLEAN AND DISTORTED DATA. THE SECOND COLUMN HERE IS THE ERROR FOR CLEAN DATA.
utt_0118 utt 651.96 656.95 -X WE ALSO SHOW THE INDIVIDUAL ERRORS FROM EACH DISTORTION FROM THE DIFFERENT CATEGORIES:
utt_0119 utt 656.95 661.78 -X NOISE, BLUR, WEATHER, AND DIGITAL CORRUPTIONS. AVERAGE HERE IS THE AVERAGE OF THESE NUMBERS.
utt_0120 utt 661.78 665.59 -X OUR ENSEMBLE PREDICTIONS ARE JUST THE SIMPLE AVERAGE OF THE PREDICTIONS FROM ALL PATHS.
utt_0121 utt 665.59 677.56 -X THE ONLY DIFFERENCE BETWEEN DEEP ENSEMBLES AND OUR METHOD IS THE USE OF MIDDLE DOMAINS. WE ALSO HAVE SUPPORTIVE RESULTS ON CIFARminus one hundred AND CIFARminus ten IN THE SUPPLEMENTARY MATERIAL. TO CONCLUDE, USING
utt_0123 utt 677.56 682.43 -X MIDDLE DOMAINS PROMOTE ENSAMPLE DIVERSITY WHICH IMPROVES PERFORMANCE BY DECORRELATING ERRORS,
utt_0124 utt 682.71 693.91 -X AND IT HAS NEGLIGIBLE INCREASE IN COMPUTATIONAL COSTS COMPARED TO DEEP ENSEMBLES. UNCERTAINTY BASED MERGING ALLOWS THE METHOD TO SELECT REGIONS FROM THE BEST PERFORMING PATH. WE OBTAIN
utt_0126 utt 693.91 698.01 -X IMPROVED ROBUSTNESS COMPARED TO BASELINES UNDER DISTRIBUTION SHIFTS SUCH AS COMMON CORRUPTIONS
utt_0127 utt 698.49 707.87 -X OR ADVERSARIAL ATTACKS FOR BOTH CLASSIFICATION AND PIXEL WISE REGRESSION TASKS. AND THIS IMPROVEMENT IN ROBUSTNESS DID NOT COME AT THE EXPENSE OF THE PERFORMANCE OF IN-DISTRIBUTION DATA.
utt_0129 utt 708.73 713.82 -5.0975 THANK YOU FOR LISTENING. VISIT OUR POSTER OR COME AND CHAT WITH US IF YOU HAVE FURTHER QUESTIONS.
