utt_0000 utt 3.44 8.27 -X HI EVERYONE, I’M WEIBO. AND HERE I’M GOING TO INTRODUCE OUR WORK IN CVPR:
utt_0001 utt 8.62 12.82 -X LEAPFROG DIFFUSION MODEL FOR STOCHASTIC TRAJECTORY PREDICTION.
utt_0002 utt 12.82 19.34 -X THIS IS A JOINT WORK WITH CHENXIN, QI, PROFESSOR CHEN, AND PROFESSOR WANG.
utt_0003 utt 19.34 30.16 -X HERE IS A QUICK REVIEW. THIS WORK FOCUS ON ACCELERATING DIFFUSION MODELS FOR STOCHASTIC TRAJECTORY PREDICTION. TO BE SPECIFIC, WE PROPOSE A NOVEL LEAPFROG DIFFUSION MODEL OR LED
utt_0005 utt 30.16 36.21 -X IN BRIEF, WHICH IS A DENOISING-DIFFUSION-BASED STOCHASTIC TRAJECTORY PREDICTION MODEL.
utt_0006 utt 36.21 42.51 -X WE DESIGN A TRAINABLE LEAPFROG INITIALIZER TO DIRECTLY MODEL COMPLEX DENOISED DISTRIBUTIONS,
utt_0007 utt 42.54 51.38 -X ACCELERATING INFERENCE SPEED. OUR METHOD ACHIEVES SOTA PERFORMANCE ON FOUR DATASETS WHILE SPEEDS UP THE INFERENCE BY AROUND twenty TIMES
utt_0009 utt 51.50 57.04 -X COMPARED TO THE STANDARD DIFFUSION MODEL, SATISFYING REAL-TIME PREDICTION NEEDS.
utt_0010 utt 58.86 69.62 -X NOW LET’S DIVE INTO DETAILS. STOCHASTIC TRAJECTORY PREDICTION TAKES THE PAST TRAJECTORIES AS INPUT AND PREDICTS POSSIBLE FUTURE TRAJECTORIES WITH MULTIPLE TRAILS.
utt_0012 utt 70.67 79.06 -X THIS TASK IS MEANINGFUL SINCE THE HUMAN BEHAVIORS HAVE THE INHERENT INDETERMINACY AND FUTURE DISTRIBUTION COULD BE MULTI-MODAL.
utt_0014 utt 81.17 93.81 -X PREVIOUS WORKS HAVE PROPOSED A SERIES OF DEEP GENERATIVE MODELS FOR STOCHASTIC TRAJECTORY PREDICTION, INCLUDING VARIATIONAL AUTO-ENCODER, NORMALIZING FLOW, GENERATIVE ADVERSARIAL NETWORK,
utt_0016 utt 93.81 99.16 -X AND DIFFUSION MODELS. THOUGH DIFFUSION MODEL IS WELL-KNOW FOR ITS HIGH QUALITY AND DIVERSITY,
utt_0017 utt 99.70 105.04 -X IT SUFFERS FROM HEAVY COMPUTATION AND TIME CONSUMPTION, WHERE OUR MODEL TRIES TO FIX.
utt_0018 utt 106.42 119.06 -X HERE WE START WITH A BRIEF DESCRIPTION OF DIFFUSION MODELS. DENOTE X AS THE PAST TRAJECTORY AND Y AS THE FUTURE TRAJECTORY. THE DIFFUSION PROCESS TRIES TO INTENTIONALLY ADD
utt_0020 utt 119.06 124.76 -X A SERIES OF NOISES TO A GROUND-TRUTH FUTURE TRAJECTORY, THROUGH THE FOLLOWING EQUATIONS.
utt_0022 utt 130.58 132.25 -X THE GROUND-TRUTH FUTURE TRAJECTORY
utt_0023 utt 133.04 142.10 -X AND EQUATION (twoB) USES A FORWARD DIFFUSION OPERATION TO SUCCESSIVELY ADD NOISES. NOTE THAT THERE IS NO TRAINABLE PARAMETERS YET,
utt_0025 utt 142.29 147.09 -X AND WE DIFFUSE THE FUTURE TRAJECTORY WITH A FIXED NOISE SCHEDULE.
utt_0026 utt 149.36 155.35 -X FOR THE DENOISING PROCESS, MODEL TRIES TO RECOVER THE FUTURE TRAJECTORY FROM NOISY INPUTS
utt_0027 utt 155.67 162.45 -X CONDITIONED ON PAST TRAJECTORIES. WE SHOW AN EXAMPLE OF DENOISING PROCESS IN THE BOTTOM.
utt_0028 utt 164.95 174.26 -X IN THESE EQUATIONS, (twoC) DRAWS K I.I.D SAMPLES TO INITIALIZE DENOISED TRAJECTORIES FROM A NORMAL DISTRIBUTION.
utt_0030 utt 174.84 186.46 -X AND (twoD) ITERATIVELY APPLIES A DENOISING MODEL TO OBTAIN THE DENOISED TRAJECTORY CONDITIONED ON PAST TRAJECTORIES. NOTE THAT THERE ARE GAMMA DENOISING STEPS,
utt_0032 utt 186.46 190.58 -X WHERE GAMMA USUALLY EQUALS TO one hundred OR five hundred. IT MEANS THAT’S WE WOULD RUN
utt_0033 utt 191.25 198.55 -X THE DENOISING MODEL FOR HUNDREDS OF TIMES, RESULTING IN UNAFFORDABLE TIME CONSUMPTION.
utt_0034 utt 198.64 208.50 -X TO ADDRESS THIS PROBLEM, WE PROPOSE THE LEAPFROG DIFFUSION MODEL. LED USES THE LEAPFROG INITIALIZER TO DIRECTLY ESTIMATE THE DENOISED DISTRIBUTION
utt_0036 utt 208.56 213.47 -X AND SUBSTITUTE A LONG SEQUENCE OF TRADITIONAL DENOISING STEPS.
utt_0037 utt 215.73 224.22 -X MATHEMATICALLY, LED SHARES THE SAME DIFFUSION PROCESS AS DIFFUSION MODELS TO PRESERVE A PROMISING REPRESENTATION ABILITY
utt_0039 utt 226.29 230.52 -X THE MAJOR DIFFERENCE LIES IN THE DENOISING PROCESS. EQUATION (threeC)
utt_0040 utt 231.41 242.91 -X PROPOSES A NOVEL LEAPFROG INITIALIZER TO DIRECTLY MODEL THE TAU-TH DENOISED DISTRIBUTION INSTEAD OF NORMAL DISTRIBUTION, SKIPPING DESNOING STEPS FROM GAMMA TO TAU
utt_0042 utt 245.43 257.05 -X TO EASE THE LEARNING BURDEN OF THE MODULE, WE DISASSEMBLE THE DISTRIBUTION INTO THREE REPRESENTATIVE PARTS: THE MEAN, GLOBAL VARIANCE AND SAMPLE PREDICTION. THE IMAGE IN THE BOTTOM
utt_0044 utt 257.05 262.65 -X RIGHT SHOWS AN EXAMPLE OF THREE PARTS. WHERE THE PINK REPRESENTS THE MEAN ESTIMATION,
utt_0045 utt 262.78 268.92 -X VARIANCE ESTIMATION IS NOTED AS DIGITALS, AND WHITE LINES REPRESENTS THE FINAL PREDICTIONS
utt_0046 utt 271.03 276.51 -X MEAN ESTIMATION: INFER THE MEAN TRAJECTORY AS A BACKBONE OF PREDICTIONS.
utt_0047 utt 276.51 281.79 -X , AND WE USE THE VARIANCE ESTIMATION TO CONTROL THE PREDICTION DIVERSITY
utt_0048 utt 283.42 289.15 -X FINALLY, WE USE THE SAMPLE PREDICTION TO PREDICT K SAMPLES SIMULTANEOUSLY,
utt_0049 utt 289.37 292.92 -X BETTER ALLOCATING SAMPLE POSITION.
utt_0050 utt 293.14 307.49 -X THESE THREE TERMS ARE THEN COMBINED TOGETHER THROUGH REPARAMETERIZATION. AND ALL THREE MODULES SHARE THE SIMILAR NETWORK DESIGN CONSIDERING BOTH SOCIAL INFORMATION AND TEMPORAL INFORMATION.
utt_0052 utt 307.49 312.56 -X WE SUPERVISE THE INITIALIZER THROUGH BEST PREDICTION LOSS AND UNCERTAINTY LOSS
utt_0053 utt 313.75 318.08 -X AND DURING THE INFERENCE PHASE, ONLY TAU STEPS ARE NEEDED.
utt_0054 utt 320.51 328.83 -X NOW LET’S MOVE TO THE EXPERIMENT PART. WE FIRST VALIDATE OUR METHOD ON SPORT DATASETS INCLUDING NBA AND NFL,
utt_0056 utt 331.16 337.66 -X WHERE OUR MODEL ACHIEVES fifteen AND twenty-three PERCENT OF ADE IMPROVEMENT, COMPARED TO PREVIOUS SOTA METHODS.
utt_0057 utt 339.32 345.12 -X WE ALSO VALIDATE ON THE PEDESTRIAN DATASET WHERE OUR METHOD ACHIEVES SOTA PERFORMANCE.
utt_0058 utt 349.79 355.81 -X WE CONDUCT A SERIES OF ABLATION STUDIES TO SHOW THE EFFECTIVENESS OF THREE PROPOSED MODULES,
utt_0059 utt 355.84 360.22 -X INCLUDING MEAN ESTIMATION, VARIANCE ESTIMATION, AND SAMPLE PREDICTION.
utt_0060 utt 360.22 363.55 -X THE CONCLUSION IS THAT EACH COMPONENT IS BENEFICIAL.
utt_0061 utt 365.37 370.62 -X THIS ABLATION EXPERIMENT SHOWS THE TRADE-OFF BETWEEN PERFORMANCE AND INFERENCE TIME.
utt_0062 utt 370.72 374.40 -X LED REDUCES THE INFERENCE TIME FROM eight hundred TO forty-six
utt_0063 utt 375.87 379.14 -X WHILE IMPROVING THE AVERAGE PREDICTION ACCURACY.
utt_0064 utt 380.76 391.33 -X WE ALSO COMPARE OUR METHOD TO OTHER FAST SAMPLING METHODS INCLUDING IMPLICIT MODEL AND PROGRESSIVE DISTILLATION, THIS EXPERIMENT SHOWS THE EFFECTIVENESS OF OUR METHODS.
utt_0066 utt 392.86 397.86 -X THE FOLLOWING ARE VISUALIZATION RESULTS. THIS VISUALIZATION SHOWS AN EXAMPLE
utt_0067 utt 400.86 406.63 -X TO SHOW THE PRECISE PREDICTION GENERATED BY OUR METHOD COMPARED TO OTHER METHODS.
utt_0068 utt 408.51 411.78 -X THE LAST VISUALIZATION IS THE VISUALIZATION COMPARISON
utt_0069 utt 412.22 419.27 -X OF DIFFERENT SAMPLING MECHANISM, WHERE OUR CORRELATED SAMPLING CAN COVER MOST OF THE MODALITIES.
utt_0070 utt 421.82 424.93 -4.3177 THAT’S ALL FOR OUR WORK, THANKS FOR YOUR LISTENING
