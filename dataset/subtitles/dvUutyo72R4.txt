utt_0000 utt 2.00 10.14 -X HI I'M CHAO-WEI HUANG FROM NATIONAL TAIWAN UNIVERSITY I'M PRESENTING OUR WORK LEARNING
utt_0001 utt 10.14 17.71 -X ASR-ROBUST CONTEXTUALIZED EMBEDDINGS FOR SPOKEN LANGUAGE UNDERSTANDING YOU CAN REACH OUT TO
utt_0002 utt 17.71 25.33 -X US VIA EMAIL AND THE SOURCE CODE FOR THIS PROJECT IS AVAILABLE AT THIS LINK THIS KID
utt_0003 utt 25.33 37.03 -X HUB REPOSITORY LET ME SHOW THE HIGHLIGHTS OF THIS WORK FIRST WE SHOW THAT WHEN WE TRAIN AND
utt_0004 utt 37.03 46.29 -X NAME ON THE STANDING MODEL ON SOME INERT ASSET WITH CONTEXTUALIZE EMBEDDINGS LIKE ELMO AND IF
utt_0005 utt 46.29 54.74 -X WE EVALUATE THE MODEL UNSPOKEN DATA SET WITH ASR ARROWS THE MODEL DOESN'T REALLY TRANSFER
utt_0006 utt 54.74 62.26 -X WELL SO THE DOMAIN SHIFT AND IS OUR ARROWS DEGRADE THE PERFORMANCE OF THE CONTEXTUALIZE
utt_0007 utt 62.26 77.46 -X EMBEDDINGS AND SECOND WE PROPOSED A FUNCTIONING METHOD TO TACKLE THIS PROBLEM SO YOU CAN THINK
utt_0008 utt 77.46 86.64 -X OF THIS WORK AS TRYING TO BUILD A BRIDGE BETWEEN ASR SYSTEMS AND A NEW RESISTANCE THE REASON WHY
utt_0009 utt 86.64 94.42 -X WE'RE DOING THIS IS BECAUSE WE IF WE ARE TRYING TO DO SPOKEN LANGUAGE UNDERSTANDING AN INTUITIVE
utt_0010 utt 94.42 105.00 -X WAY IS TO USE A PIPELINE APPROACH SO FIRST WE HAVE AN ASR SYSTEM TO TRANSCRIBE THE SPOKEN
utt_0011 utt 105.00 114.23 -X UTTERANCES AND THEN WE FEED THEIR TRANSCRIPTION INTO AN NAU SYSTEM TO DO THE UNDERSTANDING PART
utt_0012 utt 115.22 127.27 -X THIS METHOD IS PRETTY SIMPLE BUT WITH A CAVEAT WHICH IS IS OUR SYSTEMS HAVE ARROWS AND ARROWS
utt_0013 utt 127.27 136.02 -X WILL PROPAGATE TO THE DOWNSTREAM I KNOW YOU ASKS SO WHICH WILL DEGRADE THE OVERALL SPOKEN LANGUAGE
utt_0014 utt 136.02 147.96 -X UNDERSTANDING PERFORMANCE SO IN THIS WORK WE TRY TO MITIGATE THE EFFECT OF AZAR ARROWS SPECIFICALLY
utt_0015 utt 147.96 162.30 -X WE ARE FOCUSING ON CONTEXTUALIZED MODELS WHICH IS ELMO IN OUR CASE SO WHY DO WE FOCUS ON ELMO IT'S
utt_0016 utt 162.30 169.42 -X BECAUSE PRE-TRAINED LINKED MODELS ARE THE CURRENT STATE-OF-THE-ART MODELS FOR LANGUAGE UNDERSTANDING
utt_0017 utt 170.20 181.50 -X BUT THEY ARE MOSTLY PRE-TRAINED ON WRITTEN TEXT SUCH AS WIKIPEDIA OR SOME BOOKS AND SO WE WANT
utt_0018 utt 181.50 192.12 -X TO RAISE A QUESTION HERE DELAY TRANSFER WELL TO SPOKEN DOMAIN WITH IS OUR ARROWS FOR THIS QUESTION
utt_0019 utt 192.12 202.01 -X WE DID AN EXPERIMENT EXPERIMENT WE RAN A LANGUAGE UNDERSTANDING MODEL ON MANUAL TRANSCRIPTION WHICH
utt_0020 utt 202.01 211.31 -X HAS NO WAYS OUR ARROWS AND THEN EVALUATE IT ON EITHER MANUAL TRANSCRIPTS OR ASR TRANSCRIPTS
utt_0021 utt 214.74 225.31 -X HERE'S THE RESULT FOR THIS EXPERIMENT WE TESTED THIS ON THREE DIFFERENT ASSETS WE HAVE ATTIS SNIPS
utt_0022 utt 225.31 237.23 -X AND SMART LIGHTS SINGS THE SNIPS THEY ARE SET ONLY HAS WRITTEN TEXT WE SING THE THIGHS SPEECH
utt_0023 utt 237.24 248.16 -X WITH A COMMERCIAL TDS SYSTEM WE ALSO SHOW THE WORD ERROR RATE OF ERROR RESULTS HERE THE BLUE
utt_0024 utt 248.16 256.44 -X COLUMNS ARE THE PERFORMANCE WHEN EVALUATING ON MANUAL TRANSCRIPTS AND THE GREEN COLUMNS ARE
utt_0025 utt 256.44 266.40 -X EVALUATED ON ASR TRANSCRIPTS SO AS YOU CAN SEE THE PERFORMANCE DEGRADES A LOT WHEN HEIRS ARE ARROW'S
utt_0026 utt 266.40 275.47 -X PRESENT SO WE CAN ASK ANOTHER QUESTION HERE HOW CAN WE TRANSFER THEM TO SPOKEN DOMAIN BETTER
utt_0027 utt 279.00 287.73 -X LET'S DIVE INTO OUR PROPOSED METHOD WE PROPOSE TO ADD AN ADDITIONAL FINE-TUNING
utt_0028 utt 287.73 296.32 -X STAGE WHEN USING CONTEXTUALIZED EMBEDDINGS LIKE ELMO USUALLY WHEN WE USE ELMO WE HAVE
utt_0029 utt 296.32 303.79 -X TWO STAGES THE FIRST STAGE IS LANGUAGE MODEL PRE-TRAINING AND THE SECOND STAGE IS TRAINING
utt_0030 utt 303.79 312.13 -X A TARGET TASK CLASSIFIER WITH THE PRE TRAINED LIMIT MODEL AS FEATURE EXTRACTOR WE INTRODUCE
utt_0031 utt 312.13 318.88 -X AN ADDITIONAL STAGE IN THE MIDDLE HERE THE GOAL OF THIS STAGE IS TO MAKE THE
utt_0032 utt 318.88 327.30 -X CONTEXTUALIZE EMBEDDINGS MORE ACOUSTIC AWARE AND AFTER FINE-TUNING WE REPLACE THE PREACHER
utt_0033 utt 327.30 338.66 -X MODEL WITH OUR FINE-TUNED MODEL AND TREND TARGET TASK CLASSIFIER AS AS YOU JOE YEAH
utt_0034 utt 338.66 348.80 -X SO HOW DO WE MAKE THEM ACOUSTIC AWARE WE USE A VERY INTUITIVE APPROACH IN THIS WORK WHERE
utt_0035 utt 348.80 356.93 -X WE TRY TO FORCE THE EMBEDDINGS OF CRITICALLY SIMILAR WORDS TO BE CLOSER FOR EXAMPLE WE HAVE
utt_0036 utt 356.93 368.58 -X FOR EMBEDDINGS IN AN EMBEDDING SPACE HERE WHICH ARE FAIR'S AFFAIRS FLIGHTS AND LIGHTS WE KNOW
utt_0037 utt 368.58 379.09 -X THAT FARES AND AFFAIRS SOUND SIMILAR AND ALSO FLIGHTS AND LIGHTS SOUND SIMILAR SO WE CAN USE
utt_0038 utt 379.09 387.06 -X A COSINE DISTANCE LOSS HERE TO FORCE THEM TO BE CLOSER JUST LIKE THIS AND WE CALL THIS LOSS
utt_0039 utt 387.06 397.70 -X FUNCTION CONFUSION LOSS THE PROBLEM HERE IS HOW TO DETERMINE WHICH WORDS TO BRING BRING CLOSER
utt_0040 utt 402.14 409.48 -X WE PROPOSE METHODS FOR TWO SETTINGS IN THE FIRST SETTING WE ASSUME THAT WE HAVE PAIRED
utt_0041 utt 409.48 418.98 -X A CZAR AND MANUAL TRANSCRIPTS WE CALL THIS SETTING SUPERVISED THINGS WE HAVE PAIRED DATA AND IN THE
utt_0042 utt 418.98 425.59 -X SECOND SETTING WE ASSUME THAT WE ONLY HAVE SOME DAYS OUR TRANSCRIPTS AND WE CALL THIS SETTING
utt_0043 utt 425.59 437.69 -X UNSUPERVISED FOR THE FIRST CASE WE CAN ALIGN THE MANUAL TRANSCRIPT WITH A CZAR TRANSCRIPT AND WE
utt_0044 utt 437.69 447.11 -X KNOW THE IRANIAN WORD AND IT'S CORRECT COUNTERPART SO FOR EXAMPLE WE HAVE TWO AUTHOR ANSWERS HERE ONE
utt_0045 utt 447.20 453.77 -X IS GROWN THROUGH STRENGTHENS TRANSCRIPTION AND THE OTHER ONE IS A TSAR TRANSCRIPT WE CAN ALIGN
utt_0046 utt 453.77 462.66 -X THEM SO WE KNOW THE WORD AFFAIRS IS WRONG AND IT SHOULD BE FIERCE SO WE SHOULD BRING
utt_0047 utt 462.66 471.14 -X THE CONTEXTUALIZE EMBEDDINGS OF LEAST TWO WORDS CLOSER SO THAT'S HOW WE EXTRACT THE PAIR'S IN
utt_0048 utt 471.14 479.98 -X THIS SETTING AND IN THE SECOND CASE WE DON'T HAVE THE GROUND TRUTH TRANSCRIPT TO ALIGN SO WE EXTRACT
utt_0049 utt 479.98 487.53 -X WORK CONFUSION NETWORK FROM A CZAR SYSTEM WHICH CONTAINS MULTIPLE HYPOTHESES AND THEY ARE ALREADY
utt_0050 utt 487.53 498.82 -X ALIGNED BY DOING THIS WE KNOW FLIGHTS HERE AND LIGHTS ARE CONFUSING TO THE ASR SYSTEM SO THEY
utt_0051 utt 498.88 507.69 -X ARE PROBABLY ACOUSTICALLY SIMILAR SO WE SHOULD BRING THEIR CONTEXTUALIZE EMBEDDINGS CLOSER AND SO
utt_0052 utt 507.69 523.19 -X THIS IS HOW WE EXTRACT PAIRS IN THIS SETTING AND THINGS WE KNOW UOM FIT PROVED THAT FIND HIM WITH
utt_0053 utt 523.53 531.31 -X LANGUAGE MODELING OBJECTIVE HELPS DOMAIN TRANSFER SO WE INCORPORATE LANGUAGE MODELING LAWS IN THE
utt_0054 utt 531.31 541.45 -X TUNING STAGE SO THE FIND FINAL LOSS FOR FINE TUNING IS A LINEAR COMBINATION OF NECK REMODELING
utt_0055 utt 541.45 553.12 -X LOSS HERE AND OUR PROPOSED CONFUSION LOSS AND HERE WE HAVE THE FINAL LOSS TERM AND AFTER FINE TUNING
utt_0056 utt 553.42 561.68 -X WE CAN TRAIN A TARGET TASK CLASSIFIER WITH THE FINDING OF THE LANGUAGE MODEL AS FEATURE EXTRACTOR
utt_0057 utt 567.18 573.60 -X HERE'S THE RESULTS FOR OUR EXPECT EXPERIMENT ON THREE DIFFERENT INTENT DETECTION DEFICITS
utt_0058 utt 574.92 583.02 -X THE FIRST COLUMN HERE IS THE PERFORMANCE OF USING PRE TRAINED ELMO AND EVALUATING
utt_0059 utt 583.02 590.91 -X ON MANUAL TRANSCRIPTS WHICH CAN BE SEEN AS THE UPPER BOUND PERFORMANCE BECAUSE THERE IS
utt_0060 utt 590.91 599.47 -X NO A HIGHER ROSE THE SECOND COLUMN HERE IS THE PERFORMANCE OF USING PRE TREND ELMO BUT
utt_0061 utt 599.50 610.94 -X EVALUATING ON A ZARA TRANSCRIPTS IN THE THIRD COLUMN HERE WE FIND HIM THE PRETEND ELMO WITH
utt_0062 utt 610.94 621.23 -X ONLY LANGUAGE MODELING LOSS WHICH IS EQUIVALENT TO UL AND FIT AND THE LAST TWO COLUMNS HERE
utt_0063 utt 621.23 630.67 -X ARE THE PERFORMANCE OF OUR PROPOSED METHOD INTO SETTINGS SUPERVISED AND UNSUPERVISED AS YOU CAN
utt_0064 utt 630.67 641.14 -X SEE USING ONLY LINK REMODELING LOSS HERE IN THE THIRD COLUMN TO FIND HIM IMPROVES THE RESULT AND
utt_0065 utt 641.14 650.26 -X AFTER ADDING THE CONFUSION LOSS THAT WE PROPOSED THE PERFORMANCE IMPROVES EVEN MORE THESE RESULTS
utt_0066 utt 650.26 657.31 -X SHOW THAT OUR METHOD CAN LEARN CONTEXTUALIZE EMBEDDINGS LATE OUR MORE ROBUST TO AZAR ARROWS
utt_0067 utt 662.73 670.61 -X SO THE CONCLUSIONS ARE CONTEXTUALIZED MALLOS LIKE ELMO DO NOT TRANSFER WELL
utt_0068 utt 670.61 679.68 -X TO SPOKEN DOMAIN WITH IS OUR ARROWS AND WE INTRODUCE AN ADDITIONAL STAGE FINE-TUNING
utt_0069 utt 679.68 689.97 -X STAGE TO MAKE EMBEDDINGS MORE ACOUSTIC AWARE AND WE ACHIEVE THESE BY FORCING EMBEDDINGS OF
utt_0070 utt 689.97 698.31 -X ACOUSTICALLY SIMILAR WORDS TO BE CLOSER AND WE ALSO PROPOSE TWO METHODS TO EXTRACT THESE PAIRS
utt_0071 utt 698.57 711.44 -X IN DIFFERENT SETTINGS THE EXPERIMENT RESULTS SHOW THAT OUR METHOD CAN MAKE CONTEXTUALIZED
utt_0072 utt 711.44 722.93 -X EMBEDDINGS MORE ROBUST TO ASL ARROWS OUR SOURCE CODE IS AVAILABLE AT THIS GITHUB
utt_0073 utt 722.93 730.52 -1.8620 REPOSITORY HERE AND FEEL FREE TO CONTACT US IF YOU HAVE ANY QUESTIONS THANKS FOR LISTENING
