utt_0000 utt 0.24 4.02 -X HELLO! THIS IS CHUNG-YI WENG FROM UNIVERSITY OF WASHINGTON.
utt_0001 utt 4.11 6.35 -X I WILL PRESENT PERSONNERF.
utt_0002 utt 6.35 9.14 -X THIS IS JOINT WORK WITH GOOGLE RESEARCH.
utt_0003 utt 9.96 12.98 -X TODAY INTERNET IS FULL OF PHOTO RESOURCES.
utt_0004 utt 13.20 16.75 -X FOR EXAMPLE, IF WE SEARCH FOR ROGER FEDERER ON THE INTERNET,
utt_0006 utt 21.23 28.47 -X THIS WORK AIMS TO TAKE AS INPUT THESE VERY DIFFERENT PHOTOS OF ROGER FEDERER AND REBUILD HIS PERSONALIZED SPACE.
utt_0008 utt 30.25 38.55 -X ONCE THE SPACE IS REBUILT, WE CAN RENDER AN ENTIRE SPACE OF FEDERER SPANNED BY CAMERA VIEW, BODY POSE, AND APPEARANCE.
utt_0010 utt 39.31 44.05 -X FOR EXAMPLE, WE CAN NAVIGATE SPACE BY TRAVELING ALONG THE CAMERA VIEW AXIS
utt_0011 utt 44.18 47.86 -X TO RENDER FEDERER VIEWED BY ANY VIEWPOINT.
utt_0012 utt 48.66 52.28 -X ALSO, WE CAN MOVE ALONG THE APPEARANCE AXIS TO SHOW HOW
utt_0015 utt 61.90 64.31 -X OUR WORK IS BUILT UPON HUMANNERF.
utt_0016 utt 64.37 76.95 -X IN HUMANNERF, THEY LEARN A MOTION FIELD THAT MAPS POINTS FROM OBSERVATION TO CANONICAL SPACE AND OPTIMIZE THE SUBJECT'S APPEARANCE IN CANONICAL SPACE TO ACCOUNT FOR LARGE HUMAN MOTIONS.
utt_0019 utt 78.23 82.77 -X THE IMMEDIATE CHALLENGE IS SPARSE OBSERVATIONS.
utt_0020 utt 82.77 85.05 -X HUMANNERF TAKES A VIDEO AS INPUT.
utt_0021 utt 85.05 91.16 -X SO FOR A SINGLE APPEARANCE, THEY USUALLY HAVE MORE THAN five hundred FRAME OBSERVATIONS.
utt_0037 utt 165.93 170.46 -X AND APPEARANCE FILLED WITH OUR RENDERING RESULTS.
utt_0040 utt 183.90 189.47 -X A NAIVE SOLUTION TO THIS IS TO TRAIN DIFFERENT CLOTHING WITH SEPARATE NETWORKS.
utt_0041 utt 190.33 195.10 -X AND IN TEST TIME, WE JUST REPOSE THE LEARNED MODEL TO A TARGET POSE.
utt_0042 utt 195.42 205.41 -X BUT WE FOUND THIS NAIVE SOLUTION PROVIDES UNSATISFIED RESULTS AS THE NETWORK WILL OVERFIT THE OBSERVED BODY POSE.
utt_0046 utt 215.97 221.82 -X BUT IN HUMANNERF THE APPEARANCE MLP NETWORK REQUIRES THE SUBJECT APPEARANCE TO BE IDENTICAL.
utt_0057 utt 261.37 266.11 -X NOW WE HAVE AN ADDITIONAL EMBEDDING AS INPUT OF APPEARANCE MLP.
utt_0060 utt 279.04 285.03 -X IN PERSONNERF, WE DECIDE TO LEARN A SHARED MOTION FIELD ACROSS DIFFERENT CLOTHING.
utt_0061 utt 285.50 293.92 -X THE INTUITION IS ALL OF THESE IMAGES ARE FROM THE SAME PERSON SO SHOULD BE DRIVEN BY A UNIFIED MOTION MODULE.
utt_0063 utt 293.92 306.05 -X BY DOING SO, THE LEARNED MOTION FIELD CAN BE CONSTRAINED BY ALL BODY POSES IN THE DATASET NO MATTER WHETHER THEIR APPEARANCE IS IDENTICAL OR NOT, LEADING TO BETTER POSE GENERALIZATION.
utt_0065 utt 306.75 313.64 -X HERE WE SHOW AN EXAMPLE THAT DEMONSTRATES SINGLE-NETWORK TRAINING IMPROVES APPEARANCE CONSISTENCY.
utt_0067 utt 314.43 319.56 -X WHEN TRAINING WITH SEPARATE NETWORKS, EVEN WITH UNSEEN VIEW REGULARIZATION,
utt_0068 utt 319.56 325.09 -X WE CAN STILL SEE ARTIFACTS FOR UNOBSERVED BODY REGIONS, AS SHOWN ON THE LEFT.
utt_0069 utt 325.67 333.09 -X BUT SINGLE-NETWORK TRAINING GENERALIZES BETTER AND PRODUCES A CONSISTENT APPEARANCE IN THESE REGIONS, AS SHOWN ON THE RIGHT.
utt_0071 utt 333.35 343.53 -X MOREOVER, FOR POSE CONSISTENCY, WHEN TRAINING WITH SEPARATE NETWORKS, IF THE POSE AND APPEARANCE COMBINATION IS NOT OBSERVED IN INPUT,
utt_0073 utt 343.94 349.80 -X THE RENDERED BODY WILL BE UNNATURALLY DISTORTED, AS SHOWN ON THE LEFT.
utt_0074 utt 349.80 354.54 -X HOWEVER, SINGLE-NETWORK TRAINING PRODUCES CONSISTENT RESULTS, AS SHOWN ON THE RIGHT.
utt_0079 utt 376.97 382.31 -X OR WE CAN JUST TRAVERSE ALONG APPEARANCE AXIS TO SWITCH HIS OUTFIT.
utt_0092 utt 451.85 462.86 -X ON THE LEFT IS HUMANNERF, AND ON THE RIGHT IS PERSONNERF.
utt_0094 utt 466.22 473.10 -X STILL, ON THE LEFT IS HUMANNERF. ON THE RIGHT IS PERSONNERF.
utt_0095 utt 476.24 478.32 -2.6606 THANKS FOR YOUR LISTENING.
