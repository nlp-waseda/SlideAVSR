utt_0000 utt 2.22 7.22 -X VIDEO WITH SUBTITLES IS A GREAT SOURCE FOR LARGE-SCALE VISUAL-TEXTUAL REPRESENTATION LEARNING.
utt_0001 utt 7.37 11.83 -X HOWEVER, THE NATURAL LONG-TERM VIDEOS FROM THE INTERNET HAVE ALIGNMENT PROBLEMS.
utt_0002 utt 11.83 14.64 -X LET’S WATCH A VIDEO CLIP FOR EXAMPLE.
utt_0003 utt 23.44 28.02 -X THERE ARE SOME COMMON ALIGNMENT PROBLEMS IN THE EXAMPLE.
utt_0004 utt 28.02 32.56 -X FIRST, THE VIDEO AND THE TEXT DESCRIPTIONS ARE USUALLY NOT SEMANTICALLY SYNCHRONIZED,
utt_0005 utt 32.56 34.80 -X IN OTHER WORDS, THEY ARE MIS-ALIGNED.
utt_0006 utt 34.80 38.80 -X SECOND, MANY OF THE TEXTS ARE UNRELATED TO THE VIDEO, FOR EXAMPLE,
utt_0007 utt 38.80 41.33 -X DESCRIBING FOOD TASTE AND EXPLAINING REASONS.
utt_0008 utt 41.36 44.40 -X THESE TEXTS ARE NOT VISUALLY ALIGNABLE.
utt_0009 utt 44.82 48.37 -X OUR GOAL IN THIS PAPER IS TO CREATE A MODEL THAT CAN DO TWO TASKS:
utt_0010 utt 48.43 51.97 -X FIRST, DETERMINE WHETHER THE TEXTS ARE VISUALLY ALIGNABLE OR NOT
utt_0011 utt 52.08 56.66 -X SECOND, IF THE TEXTS ARE VISUALLY ALIGNABLE, DETERMINE THE CORRESPONDING VIDEO TIMESTAMPS.
utt_0012 utt 56.66 65.59 -X THE MAIN CHALLENGE OF OUR WORK IS THAT WE DO NOT HAVE THE GROUND-TRUTH LABELS FOR EITHER THE ALIGNABILITY OR THE CORRESPONDING TIMESTAMPS.
utt_0014 utt 65.81 70.29 -X ADDITIONALLY, WE AIM TO LEARN FROM THE NOISY WEB-SCALE DATASET, HOWTOone hundredM,
utt_0015 utt 70.90 73.14 -X WHICH CONSISTS OF YOUTUBE VIDEOS.
utt_0016 utt 73.30 79.51 -X THESE UN-CURATED VIDEOS HAVE SUBSTANTIAL NOISES FROM BOTH THE VISUAL SIDE AND THE TEXTUAL SIDE.
utt_0017 utt 79.54 82.55 -X BEFORE EXPLAINING OUR METHOD, LET’S IMAGINE,
utt_0018 utt 82.55 84.95 -X WHAT IF WE HAVE THE GROUND-TRUTH LABELS.
utt_0019 utt 84.95 87.90 -X GIVEN A LONG VIDEO AND A LIST OF SENTENCES AS INPUT,
utt_0020 utt 87.90 91.74 -X WE CAN FIRST EXTRACT THE VISUAL AND TEXTUAL FEATURES.
utt_0021 utt 91.74 94.90 -X IF WE HAVE THE GROUND-TRUTH CORRESPONDING TIMESTAMPS FOR EACH TEXT,
utt_0022 utt 94.90 98.87 -X WE CAN TRAIN THE MODEL WITH AN INFONCE LOSS, IN DETAIL,
utt_0023 utt 98.87 102.30 -X THE CORRESPONDING VISUAL-TEXTUAL FEATURE PAIRS ARE THE POSITIVES,
utt_0024 utt 102.30 105.24 -X AND THE OTHER VISUAL-TEXTUAL FEATURE PAIRS ARE THE NEGATIVES.
utt_0025 utt 105.53 109.05 -X FURTHERMORE, IF WE HAVE THE GROUND-TRUTH LABELS FOR THE ALIGNABILITY,
utt_0026 utt 109.05 112.86 -X WE CAN SIMPLY TRAIN A BINARY CLASSIFIER WITH THE GROUND-TRUTH LABELS.
utt_0027 utt 113.59 120.57 -X THE MAIN IDEA OF OUR METHOD IS TO DETERMINE PSEUDO LABELS TO APPROXIMATE THE GROUND-TRUTH LABELS, AND LEARN FROM THAT.
utt_0029 utt 120.76 122.68 -X OUR MODEL HAS TWO COMPONENTS.
utt_0030 utt 122.68 126.65 -X THE FIRST COMPONENT IS A VISUAL-TEXTUAL DUAL ENCODER.
utt_0031 utt 126.65 130.08 -X IN THIS COMPONENT, THE VIDEO AND TEXT ENCODER DO NOT COMMUNICATE.
utt_0032 utt 130.08 134.75 -X AND WE TRAIN THIS WITH AN INFONCE LOSS ON THE NOISY ASR TIMESTAMPS.
utt_0033 utt 135.13 139.23 -X INTUITIVELY, BECAUSE THE VIDEO AND TEXT ARE ENCODED SEPARATELY,
utt_0034 utt 139.23 141.69 -X AND SOME TEXTS, LIKE ‘IT TASTES GOOD’,
utt_0035 utt 141.75 144.16 -X OCCUR WITH MANY DIFFERENT VIDEO CLIPS,
utt_0036 utt 144.44 146.69 -X AFTER TRAINED WITH THE INFONCE LOSS,
utt_0037 utt 146.69 148.93 -X THEIR TEXT-VIDEO SIMILARITY WILL STILL BE LOW.
utt_0038 utt 149.21 153.60 -X AS A RESULT, DUAL ENCODER CAN FILTER OUT NON-ALIGNABLE TEXTS.
utt_0039 utt 155.03 158.37 -X THE SECOND COMPONENT IS A VISUAL-TEXTUAL JOINT ENCODER.
utt_0040 utt 158.37 162.17 -X IN THIS COMPONENT, THE VIDEO AND TEXT ARE JOINTLY ENCODED.
utt_0041 utt 162.17 167.33 -X SAME AS BEFORE, IT CAN BE TRAINED WITH AN INFONCE LOSS ON THE NOISY ASR TIMESTAMPS.
utt_0042 utt 167.74 173.44 -X INTUITIVELY, THE MODEL ALREADY KNOWS THE SET OF SENTENCES TO ALIGN WITH THE CURRENT VIDEO,
utt_0043 utt 173.44 177.31 -X SO THE JOINT ENCODER MIGHT GIVE MORE ACCURATE TIMESTAMPS.
utt_0044 utt 178.11 184.54 -X WE CAN DETERMINE THE PSEUDO LABELS FROM THE MUTUAL AGREEMENT BETWEEN THESE TWO COMPLEMENTARY ARCHITECTURES.
utt_0046 utt 184.54 190.85 -X IN DETAIL, THE PSEUDO LABELS ARE OBTAINED BY MERGING THE PREDICTED VISUAL-TEXTUAL SIMILARITY MAPS FROM BOTH ENCODERS.
utt_0048 utt 191.01 195.27 -X WE USE THE PSEUDO LABELS TO CONTINUE TRAIN THE DUAL ENCODER AND THE JOINT ENCODER,
utt_0050 utt 195.27 196.71 -X TO IMPROVE BOTH OF THEM.
utt_0051 utt 196.96 200.26 -X NOTE THAT WE USE AN EXPONENTIAL MOVING AVERAGE OF THE MODEL,
utt_0052 utt 200.26 204.35 -X SO THE PSEUDO LABELS CAN SLOWLY UPDATE WHEN THE ENCODERS GET STRONGER.
utt_0053 utt 205.53 206.91 -X AS MENTIONED BEFORE,
utt_0054 utt 206.91 210.11 -X THE SUBTITLES FROM THE WEB VIDEOS CONTAIN LOTS OF NOISE.
utt_0055 utt 210.17 211.94 -X AS A DATA PRE-PROCESSING,
utt_0056 utt 211.94 217.48 -X WE CLEAN-UP THE TEXT BY RESTORING THE PUNCTUATION AND USE THE FULL SENTENCES IN OUR WORK.
utt_0058 utt 219.17 221.32 -X TO EVALUATE THE ALIGNMENT PERFORMANCE,
utt_0059 utt 221.32 223.88 -X WE MANUALLY ANNOTATED A SUBSET OF HOWTOone hundredM.
utt_0060 utt 224.61 228.93 -X NOTE THAT THIS HTM-ALIGN DATASET IS ONLY USED FOR EVALUATION,
utt_0061 utt 228.93 230.44 -X AND NOT USED FOR TRAINING.
utt_0062 utt 232.00 235.30 -X THIS IS AN EXAMPLE VIDEO FROM HTM-ALIGN DATASET.
utt_0063 utt 235.30 239.21 -X WE CAN SEE THAT, MOST OF THE SENTENCES ARE NOT VISUALLY ALIGNABLE,
utt_0064 utt 239.21 240.77 -X AND FOR THOSE ALIGNABLE ONES,
utt_0065 utt 240.77 245.22 -X THEIR VISUALLY CORRESPONDING TIMESTAMPS ARE USUALLY NOT THEIR ORIGINAL TIMESTAMPS.
utt_0066 utt 246.18 250.98 -X THIS IS A ZERO-SHOT QUALITATIVE RESULT OF ONE VIDEO IN HTM-ALIGN DATASET.
utt_0067 utt 251.04 254.41 -X FROM TOP TO BOTTOM, IT SHOWS THE MANUAL ANNOTATION,
utt_0068 utt 254.41 257.42 -X THE OUTPUT OF MIL-NCE AND THE OUTPUT OF OUR TAN.
utt_0069 utt 257.96 264.97 -X WE CAN SEE OUR MODEL GIVES A CLEAR TIMESTAMP PREDICTION AND ALSO A CORRECT ALIGNABILITY PREDICTION.
utt_0071 utt 264.97 270.09 -X WE ALSO EVALUATE THE TAN ON ZERO-SHOT TEXT-TO-VIDEO RETRIEVAL TASK ON YOUCOOKtwo DATASET.
utt_0073 utt 270.09 273.45 -X OUR NETWORK OUTPERFORMS PREVIOUS WORKS ON THIS BENCHMARK.
utt_0074 utt 273.89 278.99 -X LASTLY, SINCE THE OUTPUT OF OUR TEMPORAL ALIGNMENT NETWORK ARE AUTO-ALIGNED VIDEO-TEXT PAIRS,
utt_0076 utt 278.99 282.38 -X WE CAN USE THIS DATA TO IMPROVE THE BACKBONE VISUAL FEATURE.
utt_0077 utt 282.38 285.03 -X IN THIS CASE, WE CAN REMOVE THE TRANSFORMER,
utt_0078 utt 285.03 288.07 -X AND SIMPLY FINETUNE THE VISUAL BACKBONE WITH AN INFONCE LOSS.
utt_0079 utt 288.29 290.67 -X WE SHOW THAT AFTER A QUICK FINETUNING,
utt_0080 utt 290.67 292.94 -X THE QUALITY OF THE VISUAL BACKBONE IS IMPROVED,
utt_0081 utt 292.94 296.01 -X BECAUSE OF THIS AUTO-ALIGNED VIDEO-TEXT PAIRED DATA.
utt_0082 utt 296.39 297.29 -1.9829 THANK YOU!
