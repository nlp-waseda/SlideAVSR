utt_0000 utt 0.27 4.82 -X HELLO I’M AMOS AND I’M A GRADUATE STUDENT AT WEIZMANN INSTITUTE OF SCIENCE.
utt_0001 utt 4.82 9.65 -X I WILL PRESENT OUR RECENT WORK ON IMPLICIT GEOMETRIC REGUALRIZATION FOR LEARNING SHAPES.
utt_0002 utt 10.28 17.11 -X ONE WAY TO REPRESENT twoD OR threeD SHAPES IS IMPLICITLY, AS A ZERO LEVEL SET OF A SCALAR FUNCTION.
utt_0003 utt 17.11 20.85 -X IMPLICIT REPRESENTATIONS FOR SIMPLE SHAPES LIKE SPHERES ARE WELL KNOWN.
utt_0004 utt 21.10 27.54 -X BUT HOW DO WE REPRESENT COMPLICATED SHAPES AS ZERO LEVEL SETS?
utt_0005 utt 27.54 31.54 -X ONE FAVOURABLE FAMILY OF IMPLICIT REPRESENTATIONS ARE SIGNED DISTANCE FUNCTIONS.
utt_0006 utt 31.60 37.36 -X THIS REPRESENTATION ASSUMES THAT THE MODELLED SURFACE IS DEFINED AS A BOUNDARY OF SOME SET OMEGA.
utt_0007 utt 37.97 41.97 -X THEN THE IMPLICIT FUNCTION IS DEFINED BY THE DISTANCE TO THE BOUNDARY
utt_0008 utt 42.03 47.19 -X WITH ADDITIONAL SIGN INFORMATION, DISCRIMINATING INTERIOR FROM EXTERIOR POINTS.
utt_0009 utt 47.79 56.92 -X ONE RECENT APPROACH FOR threeD SHAPE LEARNING IS TO TRAIN A NEURAL NETWORK TO APPROXIMATE A SIGNED DISTANCE FUNCTION.
utt_0011 utt 57.14 61.88 -X OUR GOAL IS TO LEARN A NEURAL SIGNED DISTANCE FUNCTION FROM RAW POINT CLOUD.
utt_0012 utt 64.46 67.92 -X PREVIOUS WORKS SUGGESTED A TWO STAGE SOLUTION:
utt_0013 utt 67.92 72.28 -X FIRST APPROXIMATE A SIGNED DISTANCE FUNCTION ON PRESCRIBED DATA POINTS.
utt_0014 utt 72.28 75.86 -X AND SECOND REGRESS A NEURAL NETWORK TO FIT THIS DATA.
utt_0015 utt 76.40 83.70 -X OUR APPROACH HOWEVER LEARNS AN SDF DIRECTLY FROM RAW POINT CLOUDS IN AN END TO END FASHION.
utt_0017 utt 83.83 90.23 -X WE UTILIZE THE CONNECTION BETWEEN SIGNED DISTANCE FUNCTION AND THE EIKONAL PARTIAL DIFFERENTIAL EQUATION.
utt_0018 utt 90.23 94.55 -X GIVEN A WELL BEHAVED BOUNDARY, THE UNIQUE EIKONAL SOLUTION IS THE SDF TO THE BOUNDARY.
utt_0019 utt 96.43 107.42 -X HOWEVER FOR FINITE POINT CLOUD BOUNDARY, THE EIKONAL IS ILL POSED WITH MULTIPLE SDF SOLUTIONS AS SHOWN HERE FOR EXAMPLE.
utt_0021 utt 107.42 117.66 -X WE SUGGEST THE FOLLOWING SIMPLE LOSS CONSISTING OF A TERM ENCOURAGES THE NETWORK TO VANISH ON THE SURFACE POINTS AND ANOTHER TERM RESPONSIBLE FOR FAVOURING SOLUTIONS THAT SATISFY THE EIKONAL EQUATION.
utt_0023 utt 118.39 123.45 -X SURPRISINGLY, OPTIMISING THIS LOSS FAVOURS NATURAL SIMPLE SOLUTION
utt_0024 utt 123.57 127.70 -X AMONG THE MYRIAD OF POSSIBLE EIKONAL SOLUTIONS.
utt_0025 utt 127.70 129.94 -X THIS INDEED THE CASE FOR OTHER INPUTS AS WELL.
utt_0026 utt 130.93 138.04 -X THIS LEADS US TO THE QUESTION OF IDENTIFYING THE ORIGIN OF THIS PHENOMENA.
utt_0027 utt 138.04 145.47 -X WE BELIEVE THAT OUR MODEL ENJOYS A GEOMETRIC VERSION OF THE WELL KNOWN IMPLICIT REGULARIZATION PHENOMENA IN NEURAL NETWORK OPTIMIZATION.
utt_0029 utt 145.69 147.80 -X WE CALL THIS IN SHORT IGR.
utt_0030 utt 148.57 155.32 -X TO SUPPORT THIS CONJECTURE WE PROVIDE SOME THEOETICAL ANALYSIS FOR THE LINEAR CASE WHICH IS STILL NON-CONVEX.
utt_0032 utt 158.45 166.78 -X WE HAVE WITNESSED ANOTHER IMPORTANT PROPERTY OF IGR AND IT IS ITS ABILITY TO PRODUCE RECONSTRUCTIONS WITH HIGH LEVEL OF DETAILS IN threeD.
utt_0034 utt 172.63 176.96 -X BEFORE WE CONTINUE LET US PLACE OUR WORK IN THE CONTEXT OF PRIOR ART.
utt_0035 utt 177.34 181.98 -X REPRESENTING SHAPES AS SURFACES WITH NEURAL NETWORKS COMES IN TWO MAIN FLAVOURS.
utt_0036 utt 181.98 188.54 -X THE FIRST IS EXPLICIT REPRESENTATIONS, WHERE THE SURFACE IS DEFINED AS A COLLECTION OF CHARTS, MODELLED BY NEURAL NETWORKS.
utt_0038 utt 188.54 194.20 -X THE MAIN CHALLENGE IN THIS FORM OF REPRESENTATION IS IN FINDING A CONSISTENT ATLAS COVERING.
utt_0039 utt 194.20 196.64 -X ON THE OTHER HAND ARE IMPLICIT REPRESENTATIONS.
utt_0040 utt 196.79 203.10 -X ONE IMPORTANT BENEFIT OF IMPLICIT REPRESENTATIONS IS THAT THEY EASILY SUPPORT THE MODELLING OF ARBITRARY SURFACE TOPOLOGY.
utt_0042 utt 203.74 210.11 -X MORE TRADITIONALLY, RESEARCHERS HAVE MODELLED THE IMPLICIT FUNCTION ONLY OVER A FIXED PREDEFINED GRID.
utt_0043 utt 210.23 216.77 -X THEN, THE SHAPE, THAT IS THE ZERO LEVEL SET, IS EXTRACTED BY SOME FORM OF INTERPOLATION BETWEEN GRID POINTS.
utt_0045 utt 216.86 220.51 -X NOTICE THAT THIS IS A NATURAL GENERALISATION OF THE twoD IMAGES MODELLING.
utt_0046 utt 220.51 225.31 -X THUS, ENABLING ALSO THE USAGE OF THE SUCCESSFUL CONVOLUTIONAL NEURAL NETWORKS IN threeD.
utt_0047 utt 227.07 234.24 -X MORE RECENTLY, SOME WORKS HAVE SUGGESTED USING A NEURAL NETWORK DIRECTLY TO MODEL A CONTINUOUS OR EVEN SMOOTH IMPLICIT FUNCTION.
utt_0049 utt 234.52 236.93 -X OUR WORK FALLS IN THIS CATEGORY.
utt_0050 utt 241.66 248.58 -X SO FAR WE CONSIDERED RAW POINT CLOUDS AS INPUTS WHERE OUR SUGGESTED LOSS WAS INSPIRED BY THE EIKONAL EQUATION.
utt_0051 utt 248.58 254.88 -X HOWEVER, SOMETIMES WE ARE PROVIDED WITH ADDITIONAL DATA SUCH AS ORIENTED NORMALS AT THE POINTS.
utt_0052 utt 255.20 264.10 -X THIS CAN BE EASILY PLUGGED INTO THE IGR LOSS BY ADDING A NORMAL RECONSTRUCTION TERM.
utt_0053 utt 264.10 266.88 -X AND NOW MOVE FOR SOME EXPERIMENTAL RESULTS.
utt_0054 utt 266.88 272.03 -X WE START EVALUATING OUR METHOD IN THE TASK OF SURFACE RECONSTRUCTION ON A BENCHMARK FROM BERGER ET AL.
utt_0055 utt 272.29 278.66 -X THE BENCHMARK CONTAINS INPUT POINT CLOUDS WITH NORMAL DATA FROM five SHAPES WITH CHALLENGING PROPERTIES.
utt_0056 utt 279.07 288.90 -X FOR BASELINE WE USE DGP BY WILLIAMS ET AL. WHICH IS A CHART BASED SURFACE RECONSTRUCTION METHOD WHICH PREVIOUSLY ACHIEVED STATE OF THE ART RESULTS ON THIS BENCHMARK.
utt_0058 utt 288.90 292.96 -X ON THE LEFT WE SHOW TWO OF OUR RECONSTRUCTIONS.
utt_0059 utt 292.96 298.53 -X THE TABLE ON THE RIGHT PRESENTS THE TWO SIDED CHAMFER AND HAUSSDORF DISTANCE TO THE GROUND TRUTH.
utt_0060 utt 298.53 303.30 -X AS YOU CAN SEE IGR OUTPERFORMS DGP IN four OUT OF five SHAPES, OFTEN WITH A LARGE MARGIN.
utt_0064 utt 320.45 323.08 -X AND EXTRACTED CORRESPONDING NORMALS.
utt_0070 utt 350.59 353.19 -X SO FAR WE ONLY SHOWED HOW TO REPRESENT A SINGLE SHAPE.
utt_0071 utt 353.19 357.10 -X BUT ANOTHER INTERESTING TASK IS LEARNING A SPACE OF MULTIPLE SHAPES.
utt_0072 utt 357.10 360.87 -X WE DO IT USING AUTO DECODER ARCHITECTURE SUGGESTED IN PARK ET AL.
utt_0073 utt 361.19 364.58 -X NAMELY, A LATENT VECTOR IS ATTACHED TO EVERY SHAPE IN THE DATASET.
utt_0074 utt 364.58 370.28 -X THEN, DURING TRAINING BOTH THE NETWORK WEIGHTS AND THE LATENT CODES ARE OPTIMIZED TO MINIMIZETHE IGR LOSS.
utt_0075 utt 370.85 379.75 -X DURING INFERENCE THE NETWORK WEIGHTS ARE FIXED AND THE LATENT CODE IS OPTIMIZED TO FIT AN INPUT TEST POINT CLOUD.
utt_0077 utt 379.75 383.53 -X WE USED AUTO-DECODER ARCHITECTURE TO LEARN SHAPE SPACE OF THE DYNAMIC FAUST DATA SET.
utt_0078 utt 383.91 388.52 -X WE COMPARE TO SAL BY ATZMON AND LIPMAN WHICH ALSO WORKS DIRECTLY WITH RAW DATA.
utt_0079 utt 388.84 390.73 -X WE PRESENT A TYPICAL TEST RESULT.
utt_0080 utt 390.73 395.53 -X NOTE THAT IGR PRODUCES HIGH FIDELITY RECONSTRUCTION, COMPARED TO PREVIOUS WORKS,
utt_0081 utt 395.75 404.07 -X HOWEVER INTRODUCES AN AUXILIARY BASE DUE TO NOISY NORMALS AT THE FEET, WHICH SAL ABLES TO EVADE SINCE IT DOESN'T USE NORMAL DATA.
utt_0085 utt 414.79 420.75 -X IN THIS EXPERIMENT WE USE A HARDER SPLIT: WE TRAIN ON eight OUT OF ten HUMANS AND TESTED ON THE OTHER two.
utt_0087 utt 420.75 423.08 -X YOU CAN SEE OVER HERE THE TEST RESULTS.
utt_0088 utt 423.08 427.21 -X THE RESULTS ARE QUITE IMPRESSIVE AT LEAST IN FITTING THE POSE.
utt_0090 utt 432.20 437.71 -X SOME FINE DETAILS AND WE BELIEVE TAKING A LARGER DATASET WITH MORE VARIED HUMANS WOULD
utt_0096 utt 464.04 473.55 -X WE MANAGED TO DO SOME THEORETIC ANALYSIS OF IGR TO SUPPORT THE EMPIRICALLY WITNESSED PLANE REPRODUCTION PROPERTY, NAMELY IF THE INPUT POINTS SPAN A Dminus one HYPERPLANE
utt_0098 utt 474.70 480.14 -X IN R^D THEN GRADIENT DESCENT WITH OUR LOSS WILL CONVERGE TO THE CORRECT SIGNED DISTANCE FUNCTION.
utt_0099 utt 480.14 486.51 -X WE PROVE THEORETIC GUARANTEE OF THIS PROPERTY IN THE CASE OF LINEAR MODEL, IN WHICH OUR LOSS TAKES THE FOLLOWING FORM.
utt_0101 utt 487.37 492.82 -X TO GIVE INTUITION TO THE PROOF WE’LL SHOW THE FOLLOWING twoD VISUALIZATION OF LOSS LANDSCAPE.
utt_0102 utt 493.23 496.66 -X ASSUME THAT THE INPUT POINTS SPAN THE Y AXIS.
utt_0103 utt 496.91 503.06 -X WE CAN SEE WE HAVE IN TOTAL five CRITICAL POINTS: zero IS A MAXIMUM, TWO SADDLE POINTS WHICH
utt_0104 utt 505.90 512.53 -X ARE OF THE FORM +-ALPHA E_two FOR SOME ALPHA, AND TWO GLOBAL MINIMA: +-Eone WHICH BOTH FORM
utt_0105 utt 512.88 515.89 -X SIGNED DISTANCE FUNCTIONS W.R. TO Y AXIS.
utt_0106 utt 515.98 524.69 -X FOR ANY STARTING POINT WITH COMPONENT ORTHOGONAL TO THE Y-AXIS GRADIENT DESCENT EVADES THE SADDLE POINTS AND CONVERGE TO ONE OF THE SDF SOLUTION.
utt_0108 utt 524.69 526.96 -X THEREFORE WE CONVERGE WITH PROBABILITY one.
utt_0109 utt 527.69 536.66 -X FOR THE GENERAL CASE WE SHOW THAT THE ARE AT LEAST three CRITICAL POINTS INCLUDING zero WHICH IS A MAXIMA AND THE TWO SDF SOLUTIONS.
utt_0111 utt 537.01 545.04 -X WE SHOW THE REST OF THE CRITICAL POINTS HAVE A STRICTLY NEGATIVE EIGENVALUES THUS THEY ARE EITHER MAXIMA OR STRICT SADDLE POINTS.
utt_0113 utt 546.58 551.38 -X A RESULT BE LEE ET AL. SHOWS THAT GD EVADES STRICT SADDLE POINTS W.P. one WHICH CONCLUDES OUR PROOF.
utt_0114 utt 553.04 558.93 -X WE ALSO EXTEND THESE RESULTS TO THE NOISY CASE WHERE THE DATA IS SAMPLED FROM THE HYPER PLANE WITH ADDITIONAL BOUNDED NOISE.
utt_0116 utt 558.93 567.51 -X WE SHOW THAT ALSO IN THIS CASE WE CAN FIND A SPECIFIC BOUND ON LAMBDA WHICH GRANTEES CONVERGENCE TO A GOOD APPROXIMATION OF THE SDF.
utt_0118 utt 568.75 572.95 -X IMPLICIT NEURAL REPRESENTATIONS HAVE BEEN AN ACTIVE FIELD OF RESEARCH IN THE LAST YEAR,
utt_0119 utt 573.14 576.25 -X LET US MENTION SOME INTERESTING FUTURE RESEARCH DIRECTIONS.
utt_0120 utt 576.34 581.05 -X THE FIRST CHALLENGE I WOULD LIKE TO STATE IS IMPROVING THE RECONSTRUCTION LEVEL OF DETAILS.
utt_0121 utt 581.17 585.78 -X FEW WORKS OBSERVE THAT NEURAL NETWORKS STRUGGLE TO FIT THE HIGH FREQUENCY DETAILS IN SIGNALS.
utt_0122 utt 586.03 588.28 -X AND SUGGEST DIFFERENT METHODS.
utt_0123 utt 588.28 592.95 -X TANCIK ET AL. SUGGEST TO INCORPORATE FOURIER FEATURE VECTORS AT THE INPUT OF THE NETWORK,
utt_0124 utt 592.95 595.32 -X ALSO REFERRED TO AS POSITIONAL ENCODING.
utt_0125 utt 596.02 604.18 -X SITZMANN ET AL. OPTIMISE AN IGR LIKE-LOSS WITH A SINUSOIDAL ACTIVATED MLP, AND REPORT PROMISING RESULTS ON LARGE SCALE SCANS.
utt_0127 utt 604.18 608.18 -X LEADING ME TO THE SECOND CHALLENGE, WHICH IS ROBUSTNESS TO NOISE.
utt_0128 utt 608.69 617.05 -X IN THE D-FAUST EXPERIMENT FOR EXAMPLE, WE WITNESSED OVERFITTING OF NOISY NORMALS, YIELDING AN UN-DESIRABLE RECONSTRUCTED PARTS.
utt_0130 utt 619.00 623.93 -X I BELIEVE THIS COULD BE ADDRESSED BY FURTHER REFINEMENT OF THE IGR LOSS.
utt_0131 utt 623.93 628.22 -X IGR COULD ALSO BE INCORPORATED INTO OTHER threeD LEARNING SCENARIOS.
utt_0132 utt 628.22 636.79 -X FOR EXAMPLE, IGR WAS USED IN ANOTHER RECENT PAPER BY YARIV AND COLLEAGUES TO LEARN threeD GEOMETRY AND MATERIAL OF threeD OBJECTS FROM twoD IMAGES.
utt_0134 utt 637.01 641.72 -X FINALLY IGR COULD BE USED FOR TRAINING threeD GENERATIVE MODELS FROM RAW DATA.
utt_0135 utt 643.48 649.05 -X I’D LIKE TO THANK THE COWRITERS LIOR YARIV, NIV HAIM, MATAN ATZMON AND MY ADVISOR YARON LIPMAN.
utt_0136 utt 649.05 653.43 -X YOU ARE WELCOME TO FOLLOW US ON TWITTER TO HEAR MORE ABOUT OUR RESEARCH.
utt_0137 utt 653.43 655.48 -X WE’D LIKE TO ACKNOWLEDGE OUR FUNDERS.
utt_0138 utt 655.99 657.40 -X THANK YOU ALL FOR LISTENING.
utt_0139 utt 657.40 662.81 -3.8696 I’D BE HAPPY TO MEET YOU AND TALK MORE IN THE Q&AMPA SESSION OR BY MAIL.
