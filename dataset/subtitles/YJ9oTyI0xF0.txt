utt_0000 utt 0.59 5.33 -X HELLO EVERYONE. MY NAME IS MOHAMED EL BANANI. I AM A PHD STUDENT AT THE UNIVERSITY OF MICHIGAN.
utt_0001 utt 5.33 8.72 -X I AM PRESENTING OUR WORK: BOOTSTRAP YOUR OWN CORRESPONDENCES.
utt_0002 utt 8.78 12.85 -X THIS WORK WAS DONE WITH MY ADVISER JUSTIN JOHNSON.
utt_0005 utt 21.97 26.22 -X WE’RE INTERESTED IN DOING THIS WITHOUT RELYING ON ANY SUPERVISION.
utt_0006 utt 26.22 30.29 -X THIS TASK IS TYPICALLY SOLVED BY FINDING CORRESPONDING POINTS BETWEEN THE TWO SCENES,
utt_0007 utt 30.38 41.81 -X FOR EXAMPLE, THE CORNER OF THE PILLOW OR THE TOP LEFT CORNER OF THE COUCH. WHILE THOSE MIGHT BE EASY TO DISCERN FROM A COLORED POINTCLOUD, THE TASK BECOMES MUCH HARDER IF WE ONLY RELY ON
utt_0009 utt 41.81 47.15 -X THE RAW, UNCOLORED POINTCLOUD. ONE SOLUTION IS TO HAVE GOOD FEATURE DESCRIPTORS FOR EACH POINT.
utt_0010 utt 48.30 52.08 -X THIS RESULTED IN A LOT OF WORK ON GEOMETRIC FEATURE EXTRACTION.
utt_0011 utt 52.11 65.52 -X WHILE EARLY WORK RELIED ON HAND-CRAFTED FEATURES WHERE A DESCRIPTOR WAS COMPUTED FOR EACH POINT BASED ON ITS LOCAL threeD NEIGHBORHOOD. MORE RECENT WORK LEVERAGES SUPERVISED LEARNING TO TRAIN NEURAL NETWORKS THAT CAN LEARN THOSE FEATURES DIRECTLY FROM THE DATA.
utt_0014 utt 66.32 78.88 -X GEOMETRIC FEATURE LEARNING HAS BEEN ENABLED BY ARCHITECTURES THAT OPERATE DIRECTLY ON POINT CLOUDS OR ON SPARSE VOXELIZATIONS OF POINT CLOUDS. THOSE ARCHITECTURES HAVE SHOWN A LOT OF SUCCESS IN TASKS SUCH AS POINT CLOUD CLASSIFICATION
utt_0017 utt 78.88 84.15 -X AND SEGMENTATION AND AS A RESULT, HAVE BECOME A POPULAR CHOICE FOR threeD LEARNING.
utt_0018 utt 84.15 93.87 -X TO LEARN LOCAL DESCRIPTORS FOR CORRESPONDENCE, PRIOR WORK TYPICALLY USES TRIPLET OR CONTRASTIVE LOSSES ON GROUND-TRUTH CORRESPONDENCES. BUT HOW DO WE GET THIS SUPERVISION?
utt_0020 utt 94.35 99.03 -X TO ANSWER THIS, LET’S CONSIDER THE COMMON FRAMEWORK FOR GEOMETRIC FEATURE LEARNING.
utt_0021 utt 99.03 110.26 -X TYPICALLY, WE START WITH SPECIAL CAPTURE DEVICES SUCH AS THE MICROSOFT KINECT OR MORE COMPLEX SETUPS SUCH AS THE ONE USED FOR THE REPLICA DATASET. WE THEN TAKE AN RGB-D VIDEO OF THE SCENE,
utt_0023 utt 110.26 120.96 -X MAKING SURE TO CAREFULLY SCAN EVERY SURFACE TO GET FULL COVERAGE OF THE SCENE. THIS VIDEO IS THEN USED TO RECONSTRUCT THE SCENE USING threeD RECONSTRUCTION SYSTEMS SUCH AS BUNDLEFUSION. LONGER
utt_0025 utt 120.96 126.00 -X VIDEOS THAT CAREFULLY COVER THE SCENE RESULT IN MORE ACCURATE RECONSTRUCTIONS. FOR TRAINING,
utt_0026 utt 126.00 131.96 -X WE CAN SAMPLE POINTCLOUD PAIRS FROM EITHER THE VIDEO FRAMES OR CROPS OF THE RECONSTRUCTED SCENE.
utt_0027 utt 131.96 140.88 -X THEN WE CAN TRAIN A GEOMETRIC FEATURE ENCODER USING THE SUPERVISION COMING FROM THE RECONSTRUCTION. WHILE SUCCESSFUL, THIS APPROACHES RELIANCE ON CAREFULLY CAPTURED VIDEO
utt_0029 utt 140.94 153.27 -X AND threeD RECONSTRUCTION SYSTEMS LIMITS ITS SCALABILITY. IDEALLY, WE WOULD BE ABLE TO LEARN DIRECTLY FROM THE RGB-D VIDEO WITHOUT REQUIRING ANY ADDITIONAL PREPROCESSING.
utt_0031 utt 153.27 157.62 -X THIS IS A PRAGMATIC CHOICE AS WE’RE SEEING A RISE IN PHONE CAMERAS WITH DEPTH SENSORS.
utt_0032 utt 157.81 163.86 -X WE CAN EXPECT A NEW STREAM OF RAW RGB-D VIDEO WITHOUT THE ANNOTATIONS NEEDED FOR SUPERVISION.
utt_0033 utt 163.86 167.60 -X FURTHERMORE, SUCH VIDEOS WILL NOT BE CATERED FOR threeD RECONSTRUCTION
utt_0034 utt 167.92 172.21 -X AS THEY WILL LIKELY BE SHORTER VIDEOS WITH PARTIAL COVERAGE OF THE SCENE.
utt_0035 utt 172.72 178.42 -X THIS IS VERY DIFFERENT FROM THE CAREFULLY CAPTURED VIDEOS USED FOR threeD RECONSTRUCTION.
utt_0036 utt 178.42 184.69 -X SELF-SUPERVISED APPROACHES THAT LEARN DIRECTLY FROM VIDEO CAN BETTER LEVERAGE THIS DATA STREAM.
utt_0037 utt 184.69 190.17 -X TO THIS END, WE PROPOSE A METHOD THAT LEARNS DIRECTLY FROM THE RGB-D VIDEO. AT A HIGH LEVEL,
utt_0038 utt 190.17 201.78 -X OUR APPROACH IS FAIRLY SIMPLE. GIVEN AN RGB-D VIDEO, WE FIRST SAMPLE TWO FRAMES. BY SAMPLING CLOSE-BY FRAMES, WE EXCEPT THEM TO CAPTURE SIMILAR PARTS OF THE SCENE. WE THEN REPRESENT THOSE FRAMES
utt_0040 utt 201.78 212.69 -X AS - RGB-D FRAMES OR POINT CLOUDS. FOR BOTH CASES, WE CAN EASILY GENERATE POINT CLOUDS SINCE WE HAVE THE DEPTH AND CAMERA INTRINSICS. WE THEN USE A VISUAL ENCODER TO EXTRACT VISUAL FEATURES FROM
utt_0042 utt 212.69 216.85 -X THE IMAGES, AND A GEOMETRIC ENCODER TO EXTRACT GEOMETRIC FEATURES FROM THE POINT CLOUDS.
utt_0043 utt 217.04 228.57 -X WE CAN THEN SAMPLE POINT CORRESPONDENCES USING THE VISUAL FEATURES, AND USE THEM TO TRAIN THE GEOMETRIC ENCODER. THIS ALLOWS US TO TRAIN THE MODEL WITHOUT REQUIRING ANY SUPERVISION.
utt_0045 utt 228.85 239.93 -X AT TEST TIME, WE ONLY KEEP THE GEOMETRIC ENCODER. THIS ALLOWS US TO OPERATE DIRECTLY ON ANY POINTCLOUD. SIMILAR TO TRAINING TIME, WE GENERATE POINT-WISE GEOMETRIC FEATURES FOR EACH POINTCLOUD,
utt_0047 utt 239.93 248.50 -X ESTIMATE CORRESPONDENCE, AND THEN REGISTER THE SCENE BY ESTIMATE THE TRANSFORMATION THAT BEST ALIGNS THE CORRESPONDENCES. BUT WHY SHOULD THIS WORK?
utt_0049 utt 249.87 254.36 -X WE FIND THAT RANDOMLY-INITIALIZED CNNS ARE SURPRISINGLY GOOD FEATURE EXTRACTORS.
utt_0050 utt 254.67 259.48 -X IF WE CONSIDER THE CORRESPONDENCES EXTRACTED BELOW, WE SEE THAT DESPITE BEING NOISY,
utt_0051 utt 259.57 263.64 -X THE FEATURES ARE GOOD ENOUGH TO FIND CORRESPONDENCES ESPECIALLY AROUND THE DOOR KNOB,
utt_0052 utt 263.73 274.55 -X LIGHT SWITCH, AND CHAIR. MEANWHILE, THE threeD CNN FEATURES ARE FAR NOISIER. OUR CORE INSIGHT IS THAT CORRESPONDENCES FROM A RANDOMLY-INITIALIZED NETWORK
utt_0054 utt 274.58 280.41 -X CAN SERVE AS PSEUDO-LABELS TO BOOTSTRAP VISUAL AND GEOMETRIC FEATURES LEARNING.
utt_0055 utt 280.41 282.39 -X NOW, WE DISCUSS OUR APPROACH IN MORE DETAIL.
utt_0056 utt 283.28 286.81 -X WE FIRST FOCUS ON FEATURE EXTRACTION.
utt_0057 utt 286.81 293.56 -X GIVEN AN RGB-D IMAGE, WE WOULD LIKE TO GENERATE A POINTCLOUD WHERE EACH POINT HAS BOTH A VISUAL AND A GEOMETRIC FEATURE DESCRIPTOR.
utt_0059 utt 293.56 297.66 -X WE FIRST GENERATE THE POINTCLOUD USING THE INPUT DEPTH AND CAMERA INSTRINISICS.
utt_0060 utt 298.26 301.75 -X WE THEN USE A twoD RESNET TO EXTRACT VISUAL FEATURES FROM THE IMAGE,
utt_0061 utt 302.07 305.59 -X AND A SPARSE threeD RESNET TO EXTRACT GEOMETRIC FEATURES FROM THE POINT CLOUD.
utt_0062 utt 306.00 315.00 -X AT THE END OF FEATURE EXTRACTION, EACH VIDEO FRAME IS REPRESENTED BY A POINT CLOUD WHERE EACH POINT HAS A threeD COORDINATE, A GEOMETRIC FEATURE AND A VISUAL FEATURE.
utt_0064 utt 316.21 319.67 -X WE THEN EXTRACT THE CORRESPONDENCES FOR EACH MODALITY SEPARATELY.
utt_0065 utt 320.72 323.99 -X WE FIRST FIND THE NEAREST NEIGHBOR FOR EACH POINT IN FEATURE SPACE.
utt_0066 utt 324.37 328.73 -X WE EXTRACT CORRESPONDENCES FOR ALL POINTS, NOT JUST A SUBSET OF KEYPOINTS,
utt_0067 utt 328.73 330.33 -X MAKING OUR METHOD DETECTOR FREE.
utt_0068 utt 330.61 335.10 -X WE’RE ABLE TO DO THIS QUICKLY THANKS TO A SPECIALIZED KNN CUDA KERNEL.
utt_0069 utt 335.10 338.81 -X HOWEVER, MOST MATCHES WILL BE FALSE POSITIVES AS SEEN BELOW.
utt_0070 utt 339.38 342.78 -X WHILE RECENT APPROACHES TRAIN SEPARATE NETWORKS TO DETERMINE INLIERS,
utt_0071 utt 342.78 346.78 -X WE USE A MUCH SIMPLER APPROACH: A VARIANT OF LOWE’S RATIO TEST.
utt_0072 utt 347.35 351.83 -X THE WEIGHT IS BASED ON THE RATIO OF FEATURE SIMILARITY TO THE FIRST AND SECOND NEAREST NEIGHBORS.
utt_0073 utt 352.21 356.83 -X WE COMPUTE A WEIGHT FOR EACH CORRESPONDENCE AND KEEP THE TOP four hundred CORRESPONDENCES AS INLIERS.
utt_0074 utt 357.21 361.59 -X THIS IS SURPRISINGLY EFFECTIVE AT FILTERING CORRESPONDENCES AS SHOWN BELOW.
utt_0075 utt 362.93 366.30 -X WE THEN APPLY A REGISTRATION LOSS TO EACH SET OF CORRESPONDENCES.
utt_0076 utt 367.25 370.30 -X INTUITIVELY, OUR LEARNING SIGNAL COMES FROM THE QUESTION:
utt_0077 utt 370.30 374.14 -X HOW WELL CAN THE CORRESPONDENCES BE EXPLAINED BY A RIGID BODY TRANSFORM?
utt_0078 utt 374.81 381.59 -X WE DO THIS BY FIRST FINDING THE SE(three) TRANSFORMATION THAT WOULD MINIMIZE THE WEIGHTED RESIDUAL ERROR BETWEEN THE CORRESPONDENCES.
utt_0080 utt 381.97 389.05 -X WE USE THE WEIGHTED PROCRUSTES FORMULATION TO REGISTER THE CORRESPONDENCES WHICH ALLOWS US TO MAINTAIN DIFFERENTIABILITY WITH RESPECT TO THE WEIGHTS.
utt_0082 utt 389.56 394.23 -X ONCE WE REGISTER THE CORRESPONDENCES, WE USE THE SUM OF WEIGHTED RESIDUALS AS OUR LOSS.
utt_0083 utt 394.52 397.43 -X THIS IS APPLIED SEPARATELY TO EACH CORRESPONDENCE SET.
utt_0084 utt 398.93 410.11 -X WE NOTE THAT LOWE’S RATIO WEIGHTS ARE SURPRISINGLY EFFECTIVE FOR BOTH FILTERING CORRESPONDENCES AND WEIGHING THE LOSSES. WE OBSERVE THAT THIS LOSS CAN BE THOUGHT OF AS A WEIGHTED TRIPLET LOSS.
utt_0086 utt 410.81 425.02 -X THIS IS EASIER TO SEE IF WE CONSIDER THAT THE FIRST NEAREST NEIGHBOR IS THE POSITIVE SAMPLE AND THE SECOND NEAREST NEIGHBOR IS THE HARDEST NEGATIVE SAMPLE. WE EMPHASIZE THAT THIS IS DONE WITH ESTIMATED CORRESPONDENCES, NOT GROUND-TRUTH. THE EFFICACY OF THIS LOSS IS
utt_0089 utt 425.02 430.81 -X MORE SURPRISING WHEN CONSIDERING THAT PRIOR WORK TRAINS SEPARATE NETWORKS TO PERFORM THE SAME TASK.
utt_0090 utt 430.81 435.93 -X FINALLY, WE CONSIDER USING VISUAL CORRESPONDENCES TO IMPROVE GEOMETRIC FEATURE LEARNING.
utt_0091 utt 436.31 440.99 -X SINCE VISUAL CORRESPONDENCE ARE MORE ACCURATE, HOW CAN WE USE THEM TO IMPROVE GEOMETRIC LEARNING?
utt_0092 utt 441.14 445.36 -X WE DO THIS BY USING THE SAMPLED VISUAL CORRESPONDENCES TO APPLY A SIMILARITY LOSS
utt_0093 utt 445.40 455.84 -X ON GEOMETRIC FEATURES. HOWEVER, WE FIND THAT SIMPLY MINIMIZING THE DISTANCE RESULTS IN WORSE PERFORMANCE. WE INSTEAD USE THE NON-CONTRASTIVE SELF-SUPERVISED APPROACH PROPOSED BY CHEN AND HE.
utt_0095 utt 456.28 459.80 -X THE MAIN BENEFITS OF THIS APPROACH IS THAT IT DOESN’T REQUIRE NEGATIVE SAMPLING
utt_0096 utt 459.86 467.64 -X NOR DOES IT ADD ANY EXTRA HYPERPARAMETERS. THIS IS IMPORTANT SINCE WHILE THE VISUAL CORRESPONDENCES ONLY PROVIDE US WITH GOOD POSITIVE PAIRS,
utt_0098 utt 467.70 472.38 -X IT’S UNCLEAR HOW ONE WOULD EFFECTIVELY SAMPLE NEGATIVE PAIRS IN OUR SETTING.
utt_0099 utt 472.60 483.13 -X WE CONSIDER TWO VARIANTS OF OUR APPROACH: THE FIRST VARIANT IS OUR FULL MODEL WHICH USES BOTH THE REGISTRATION AND THE VISUAL TO GEOMETRIC TRANSFER LOSSES USING RGB-D VIDEO.
utt_0101 utt 484.12 488.06 -X THE SECOND VARIANT ONLY USES THE GEOMETRIC REGISTRATION LOSS,
utt_0102 utt 488.09 493.37 -X AND AS A RESULT, DOES NOT REQUIRE RGB DATA AT ANY POINT DURING TRAINING OR INFERENCE.
utt_0103 utt 494.81 500.54 -X WE EVALUATE OUR APPROACH ON SCANNET: A LARGE SCALE DATASET OF RGB-D VIDEO OF INDOOR SCENES.
utt_0104 utt 500.89 505.50 -X WE EVALUATE OUR REGISTRATION PERFORMANCE AGAINST TWO SETS OF BASELINES:
utt_0105 utt 505.50 511.04 -X ICP AND HAND-CRAFTED FEATURES USING RANSAC, AS WELL AS SUPERVISED GEOMETRIC REGISTRATION.
utt_0106 utt 512.86 517.31 -X WE FIND THAT ICP AND FPFH, A HAND-CRAFTED FEATURE, ACHIEVE COMPARABLE PERFORMANCE,
utt_0107 utt 517.69 521.47 -X WITH ICP POINT-TO-PLANE PERFORMING BEST.
utt_0108 utt 521.47 531.33 -X MEANWHILE, FCGF – A LEARNED GEOMETRIC FEATURE DESCRIPTOR -- ACHIEVES A MUCH BETTER PERFORMANCE THAN FPFH WHEN USING RASNAC OR WEIGHTED PROCRUSTES AS THEIR ALIGNMENT ALGORITHM.
utt_0110 utt 532.22 537.37 -X APPROACHES THAT LEARN REGISTRATION ON-TOP-OF FCGF ACHIEVE AN EVEN BETTER PERFORMANCE.
utt_0111 utt 537.53 541.79 -X THESE APPROACHES ARE TRAINED ON threeD MATCH AND SHOW IMPRESSIVE GENERALIZATION TO SCANNET.
utt_0112 utt 543.00 550.65 -X WHEN WE TRAIN OUR MODEL ON threeD MATCH, WE PERFORM ON-PAR WITH FCGF WHEN IT USES WEIGHTED PROCRUSTES AS ITS ALIGNMENT ALGORITHM.
utt_0113 utt 551.00 556.29 -X THIS IS DESPITE OUR FEATURE BEING SELF-SUPERVISED WHILE FCGF IS SUPERVISED.
utt_0114 utt 556.29 561.15 -X TRAINING ON A LARGE SCALE DATASET LIKE SCANNET, WE SEE FURTHER IMPROVEMENTS IN PERFORMANCE,
utt_0115 utt 561.15 573.54 -X COMING CLOSE TO METHODS THAT USE SUPERVISION FOR BOTH FEATURE LEARNING AND REGISTRATION. WE ARGUE THAT THIS IS A MORE PRACTICAL COMPARISON SINCE IT COMPARES THE IMPACT OF SUPERVISED LEARNING ON A SMALL DATASET COMPARED TO SELF-SUPERVISED LEARNING ON A LARGE DATASET.
utt_0118 utt 575.80 581.34 -X WE ALSO EVALUATE THE QUALITY OF THE FEATURES LEARNED BY OUR METHOD ON THE threeD MATCH BENCHMARK.
utt_0119 utt 582.08 589.57 -X WE USE THE FEATURE MATCH RECALL METRIC. INTUITIVELY, THIS METRIC MEASURES THE PERCENTAGE OF PAIRS THAT WOULD BE EASILY REGISTERED USING RANSAC.
utt_0121 utt 590.56 593.60 -X WE EVALUATE OUR PERFORMANCE AGAINST HAND-CRAFTED FEATURES,
utt_0122 utt 594.04 597.86 -X SCENE-SUPERVISED FEATURES WHICH TRAIN ON threeD RECONSTRUCTIONS OF THE SCENE,
utt_0123 utt 598.01 602.02 -X AND POSE-SUPERVISED FEATURES WHICH TRAIN ON POSE-ALIGNED POINT CLOUD FRAGMENTS.
utt_0124 utt 602.46 607.52 -X WE NOTE THAT THE LATTER TWO SETS REQUIRE threeD RECONSTRUCTION TO GENERATE THEIR TRAINING DATA.
utt_0125 utt 608.35 613.38 -X HAND-CRAFTED FEATURES ACHIEVE A LOW RECALL PERFORMANCE WITH FPFH PERFORMING BEST.
utt_0126 utt 614.11 618.53 -X SCENE-SUPERVISED APPROACHES ACHIEVE A BETTER PERFORMANCE BY TRAINING ON threeD RECONSTRUCTIONS.
utt_0127 utt 619.84 627.84 -X POSE-SUPERVISED METHODS CAN ACHIEVE STRONGER PERFORMANCE WITH METHODS THAT USE threeD SPARSE RESNET BACKBONES ACHIEVING THE HIGHEST PERFORMANCE.
utt_0129 utt 628.35 642.59 -X WHEN WE TRAIN ON VIDEOS FROM threeD MATCH, WE OUTPERFORM HAND-CRAFTED METHODS BUT FALL-SHORT FROM THE TOP PERFORMING SUPERVISED APPROACHES. TRAINING ON SCANNET, WE CAN GENERALIZE WELL TO threeD MATCH AND PERFORM ON PAR WITH THE BEST SCENE-SUPERVISED METHODS.
utt_0132 utt 642.78 652.32 -X ONE SURPRISING RESULT IS THAT OUR METHOD THAT ONLY USES DEPTH FRAMES FOR TRAINING PERFORMS BETTER THAN THE ONE THAT USING RGB-D VIDEO FOR TRAINING.
utt_0134 utt 652.32 655.39 -X WE EMPHASIZE THAT WHILE OUR APPROACH IS TRAINED DIRECTLY ON VIDEO,
utt_0135 utt 655.42 660.26 -X OTHER LEARNED APPROACHES REQUIRE THE SCENE TO FIRST BE RECONSTRUCTED TO EVEN GENERATE TRAINING DATA.
utt_0136 utt 662.65 671.44 -X WE VISUALIZE OUR LEARNED FEATURES USING TSNE AND OBSERVE THAT THE FEATURES ARE CONSISTENT ACROSS FRAMES WITH CERTAIN ELEMENTS OF THE SCENE SUCH AS THE TOP OF THE TABLE
utt_0138 utt 671.55 675.65 -X OR CHAIRS BEING CLEARLY DELINEATED FROM THE REST OF THE SCENE.
utt_0139 utt 676.22 682.95 -X THIS RESULTS IN ACCURATE CORRESPONDENCE AND REGISTRATION AS SHOWN IN THE RIGHT TWO COLUMNS.
utt_0140 utt 682.95 689.38 -X IN CONCLUSION, WE PROPOSE A SELF-SUPERVISED APPROACH TO POINTCLOUD REGISTRATION THAT TRAINS ON ONLY RGB-D VIDEO.
utt_0142 utt 689.98 694.63 -X WE FIND THAT VIDEO-FRAME CONSISTENCY IS A STRONG LEARNING SIGNAL FOR FEATURE LEARNING.
utt_0143 utt 694.97 702.11 -X WE ALSO FIND THAT RANDOMLY INITIALIZED CNNS PROVIDE US WITH ENOUGH SIGNAL TO BOOTSTRAP FEATURE LEARNING.
utt_0144 utt 702.11 707.84 -X AND THAT THE RATIO-TEST IS SURPRISINGLY EFFECTIVE FOR CORRESPONDENCE ESTIMATION AND FEATURE LEARNING.
utt_0145 utt 707.84 713.16 -X THIS ELIMINATES THE NEED FOR AN INLIER PREDICTION NETWORK AND MAKES THE LEARNING SETUP MUCH SIMPLER.
utt_0146 utt 713.16 717.96 -X THANK YOU FOR YOUR ATTENTION, AND PLEASE VISIT OUR PROJECT PAGE FOR MORE INFORMATION.
