utt_0000 utt 0.46 1.55 -X HELLO EVERYONE.
utt_0001 utt 1.61 7.54 -X MY NAME IS NIKITA AND I'M GOING TO PRESENT OUR WORK ON DENSE UNSUPERVISED LEARNING FOR VIDEO SEGMENTATION.
utt_0003 utt 7.76 11.31 -X THIS IS A JOINT WORK WITH SIMONE SCHAUB-MEYER AND STEFAN ROTH.
utt_0005 utt 16.45 19.50 -X AN INITIAL ANNOTATION IS PROVIDED FOR SOME REFERENCE FRAME.
utt_0006 utt 19.82 26.16 -X THE NOTATION DESIGNATES CERTAIN OBJECTS OF INTEREST, SUCH AS THE BIKER AND THE MOTORCYCLE HERE.
utt_0007 utt 26.16 29.65 -X WE HAVE TO DENSELY TRACK THESE OBJECTS IN THE SUBSEQUENT FRAMES,
utt_0008 utt 29.65 35.73 -X AS THE GROUND TRUTH ANNOTATION HERE SHOWS. THIS IS A CHALLENGING PROBLEM AS CAN BE SEEN IN THIS EXAMPLE:
utt_0010 utt 35.98 41.36 -X THE BIKER AND THE MOTORCYCLE BOTH CHANGE IN THE RELATIVE VIEW ANGLE AND THE DISTANCE TO THE CAMERA.
utt_0011 utt 41.65 47.19 -X THE OBJECT MOTION CAN BE ARBITRARILY FAST AND AFFECT THE DEGREE OF THE PIXEL DISPLACEMENT.
utt_0012 utt 47.19 51.31 -X WE NEED TO LEARN A FEATURE REPRESENTATION THAT IS INVARIANT TO THESE CHANGES,
utt_0013 utt 51.31 54.67 -X SO WE CAN TRACK THE OBJECTS RELIABLY THROUGHOUT THE WHOLE SEQUENCE.
utt_0014 utt 54.86 59.06 -X IN OUR WORK WE LEARN SUCH FEATURES IN A COMPLETELY UNSUPERVISED WAY.
utt_0015 utt 59.47 62.42 -X INSTEAD OF USING GROUND-TRUTH ANNOTATIONS FOR TRAINING,
utt_0016 utt 62.42 63.92 -X AS IN MANY PREVIOUS STUDIES,
utt_0017 utt 64.14 68.85 -X WE INVESTIGATE HOW WE CAN EFFICIENTLY AND EFFECTIVELY LEVERAGE THE LARGE,
utt_0018 utt 68.85 71.99 -X UNLABELLED VIDEO DATA SETS TO LEARN SUCH A REPRESENTATION.
utt_0019 utt 72.24 80.40 -X TO DATE, WE HAVE AMPLE AMOUNTS OF SUCH VIDEOS GATHERED FROM THE PUBLIC DOMAIN INTO DATASETS LIKE YOUTUBE-VOS AND KINETICS.
utt_0021 utt 80.62 87.38 -X FOR A START, WE NEED TO MAKE SOME ASSUMPTIONS ABOUT THE DESIRED PROPERTIES OF THE FEATURE REPRESENTATIONS THAT WE WANT TO LEARN.
utt_0023 utt 88.24 91.38 -X FIRST, WE ASSUME NON-LOCAL FEATURE DIVERSITY.
utt_0024 utt 91.54 97.11 -X THIS MEANS THAT THE SEMANTIC FEATURE REPRESENTATIONS SHOULD BE SPATIALLY DISTINGUISHABLE.
utt_0025 utt 97.11 101.14 -X LET US TAKE AN EXAMPLE OF THIS FRAME EXTRACTED FROM A CAR RACING VIDEO SEQUENCE.
utt_0026 utt 101.14 108.31 -X WE DISCRETISE THE IMAGE PLANE WITH AN OVERIMPOSED COARSER FEATURE GRID, TYPICAL TO THE FULLY CONVOLUTIONAL NEURAL NETWORKS.
utt_0028 utt 108.31 116.63 -X THIS FIRST ASSUMPTION IMPLIES THAT OUR FEATURE REPRESENTATION EXTRACTED FROM ONE GRID CELL SHOULD BE DIFFERENT FROM A FEATURE FROM ANOTHER CELL,
utt_0030 utt 116.66 118.84 -X SAY, IN TERMS OF THE COSINE DISTANCE.
utt_0031 utt 119.22 122.07 -X NATURAL IMAGES TYPICALLY EXHIBIT SUCH PROPERTY.
utt_0032 utt 122.45 124.18 -X THIS ASSUMPTION HAS FEATURED,
utt_0033 utt 124.18 124.85 -X FOR EXAMPLE,
utt_0034 utt 124.85 127.83 -X IN THE PREVIOUS WORK BY JABRI AT AL. (two thousand and twenty).
utt_0035 utt 127.83 131.54 -X OUR twoND ASSUMPTION IS A MORE GENERAL VARIANT OF THE TEMPORAL COHERENCE.
utt_0036 utt 131.79 139.00 -X WE WANT THE FEATURES EXTRACTED FROM SOME FRAME IN A VIDEO SEQUENCE TO BE CLOSER TO ANY OTHER FEATURE EXTRACTED FROM THE SAME VIDEO,
utt_0038 utt 139.00 139.99 -X FOR EXAMPLE,
utt_0039 utt 139.99 144.60 -X IN TERMS OF THE COSINE DISTANCE, RATHER THAN A FEATURE EXTRACTED FROM ANOTHER VIDEO.
utt_0040 utt 145.17 153.85 -X THIS IS A REASONABLE ASSUMPTION BECAUSE THE CHANGE IN THE SCENE APPEARANCE WITHIN A VIDEO IS SELDOM AS SIGNIFICANT AS BETWEEN DIFFERENT VIDEO SHOTS.
utt_0042 utt 154.23 157.62 -X EVEN THOUGH BOTH EXAMPLES HERE SHOW A RACING CAR SCENARIO,
utt_0043 utt 157.62 160.57 -X THE ROAD SURFACE IS PERCEPTIBLY DISTINGUISHABLE.
utt_0044 utt 160.85 166.17 -X WANG ET AL. (two thousand and twenty-one) RELIED ON THIS ASSUMPTION IN THEIR IMPLEMENTATION OF A CONTRASTIVE LEARNING FRAMEWORK.
utt_0045 utt 166.99 171.09 -X THE threeRD ASSUMPTION IS THE TEMPORAL PERSISTENCE OF SEMANTIC CONTENT.
utt_0046 utt 171.31 180.76 -X THIS MEANS THAT THE SEMANTIC CONTENT OF THE VIDEO CLIPS REMAINS UNCHANGED, AT LEAST WITHIN A SHORT TIME SPAN. TAKING AN EXAMPLE OF THE RACING CAR AGAIN,
utt_0048 utt 180.76 186.14 -X ALTHOUGH THE ORIENTATION AND THE POSITION OF THE CAR WITH RESPECT TO THE CAMERA CHANGES WITH TIME,
utt_0049 utt 186.16 189.59 -X SEMANTICALLY, THESE FRAMES ARE NOT DISSIMILAR.
utt_0050 utt 189.68 197.02 -X THERE IS STILL THE ROAD AND THE RACING CAR. A BIT MORE TECHNICALLY THIS MEANS THAT IF WE EXTRACT A FEATURE FROM ONE GRID CELL,
utt_0052 utt 197.02 201.91 -X WE CAN FIND A SEMANTICALLY CORRESPONDENT FEATURE IN THE OTHER TEMPORALLY CLOSE FRAMES.
utt_0053 utt 202.16 206.46 -X THIS IS IN CONTRAST TO CONTRASTIVE RANDOM WALK APPROACH BY JABRI ET AL. (two thousand and twenty),
utt_0054 utt 206.46 211.77 -X THAT ASSUMES EVERY FRAME TO CONTAIN THE CORRESPONDING FEATURE, WHICH CAN BE PROBLEMATIC DUE TO OCCLUSIONS.
utt_0055 utt 211.89 216.47 -X CONSIDER THE EXAMPLE OF FINDING THE CORRESPONDENCE TO THE ROAD REGION AT TIMESTEP ZERO.
utt_0056 utt 216.95 221.02 -X WHILE WE CAN STILL FIND THE CORRESPONDING ROAD PATCH AT TIMESTEPS ten AND twenty,
utt_0057 utt 221.52 226.17 -X THE AREA BECOMES LARGELY OCCLUDED BY THE CAR AND THE EXHAUST FUMES AT TIME STEP thirty.
utt_0058 utt 227.41 236.89 -X THIS WOULD STILL WORK UNDER OUR ASSUMPTION AS WE MERELY ASSUME AT LEAST ONE OF THE OTHER FRAMES TO CONTAIN THE CORRESPONDING FEATURE, WHICH IS SATISFIED IN THIS EXAMPLE.
utt_0060 utt 236.89 242.62 -X IF WE NOW ATTEMPT TO IMPLEMENT THESE ASSUMPTIONS AND LEARN A FEATURE REPRESENTATION IN A FULLY CONVOLUTIONAL FASHION,
utt_0062 utt 242.62 246.49 -X THIS WILL RESULT IN A DEGENERATE, OR SHORTCUT, SOLUTION.
utt_0063 utt 246.49 253.11 -X IN OUR PRELIMINARY EXPERIMENTS WE OBSERVED THAT THE NETWORK WILL SIMPLY IGNORE THE INPUT AND WILL LEARN, AS WE HYPOTHESISE,
utt_0065 utt 253.11 255.58 -X SOMETHING AKIN TO POSITIONAL ENCODING.
utt_0066 utt 256.05 265.31 -X A SIMILAR OBSERVATION WAS MADE IN PREVIOUS WORKS, AND IS RELATED TO THE FACT THAT THE COMMONLY USED IMPLEMENTATIONS OF THE CONVOLUTIONAL NETWORKS ARE NOT TRULY TRANSLATION INVARIANT,
utt_0068 utt 265.31 273.15 -X FOR INSTANCE, DUE TO THE PADDING. IN THIS WORK, WE MAKE ANOTHER ASSUMPTION THAT ALLOWS TO AVOID SUCH TRIVIAL SOLUTIONS.
utt_0070 utt 273.27 279.16 -X WE ASSUME THAT THE FEATURES SHOULD BE EQUIVARIANT TO SIMILARITY TRANSFORMATIONS, SUCH AS FLIPPING AND SCALING.
utt_0071 utt 279.58 286.75 -X THIS ASSUMPTION IS VALID FOR THE TASK OF VIDEO OBJECT SEGMENTATION, ON WHICH WE FOCUS IN THIS WORK. A BIT MORE TECHNICALLY,
utt_0073 utt 286.75 291.48 -X THIS MEANS THAT THE DISTANCE BETWEEN TWO CORRESPONDING FEATURES IN THE TWO VIEWS SHOULD BE
utt_0074 utt 292.02 295.29 -X CLOSEST W.R.T. ALL OTHER POSSIBLE PAIRS OF THE FEATURES.
utt_0075 utt 295.54 302.91 -X THIS ASSUMPTION HAS NOT YET BEEN EXPLOITED IN PRIOR WORK TO LEARN SEMANTIC FEATURE REPRESENTATIONS FROM VIDEOS IN AN UNSUPERVISED WAY.
utt_0077 utt 303.35 307.39 -X LET US SUMMARISE THE NOVELTY OF OUR APPROACH IN THE CONTEXT OF THESE ASSUMPTIONS.
utt_0078 utt 307.58 315.36 -X WHILE ASSUMPTIONS one AND two ON THE NON-LOCAL FEATURE DIVERSITY AND THE TEMPORAL COHERENCE WERE STUDIED INDEPENDENTLY IN PREVIOUS WORKS,
utt_0080 utt 315.36 324.28 -X WE INVESTIGATE THEIR COMBINATION IN OUR WORK. WE FURTHER INTRODUCE OUR OCCLUSION-AWARE VARIANT OF ASSUMPTION three ON THE TEMPORAL PERSISTENCE OF SEMANTIC CONTENT,
utt_0082 utt 324.41 327.61 -X AND THE YET UNEXPLORED ASSUMPTION ON EQUIVARIANCE.
utt_0083 utt 328.18 330.78 -X UNITING THESE ASSUMPTIONS IN A NOVEL FRAMEWORK,
utt_0084 utt 330.91 340.06 -X WE FIND THAT WE CAN EFFICIENTLY AND EFFECTIVELY LEARN DENSE SEMANTIC FEATURE REPRESENTATIONS IN A FULLY CONVOLUTIONAL REGIME WITHOUT FALLING PREY TO DEGENERATE SOLUTIONS.
utt_0086 utt 340.47 345.28 -X WE IMPLEMENT OUR FIRST THREE ASSUMPTIONS BY PERFORMING UNSUPERVISED CLUSTERING AS FOLLOWS.
utt_0087 utt 345.56 353.98 -X FIRST, WE SAMPLE A VIDEO CLIP TO GENERATE TWO VIEWS WHERE THE twoND VIEW IS GENERATED BY APPLYING A RANDOM SIMILARITY TRANSFORM.
utt_0089 utt 353.98 358.88 -X NEXT, WE SELECT ONE OF THE FRAMES IN A VIDEO AND DESIGNATE IT AS THE REFERENCE FRAME.
utt_0090 utt 359.13 364.13 -X WE THEN SELECT ANCHORS THAT WILL REPRESENT CLUSTER CENTERS FROM THIS REFERENCE FRAME.
utt_0091 utt 364.44 370.75 -X THESE ANCHORS ARE SIMPLY SELECTED FROM THE SET OF THE FEATURE VECTORS PRODUCED BY A FULLY CONVOLUTIONAL NETWORK.
utt_0092 utt 371.39 375.74 -X NOTE THAT THESE STEPS ARE PERFORMED FOR A NUMBER OF VIDEO SEQUENCES IN PARALLEL.
utt_0093 utt 375.77 380.41 -X HENCE, OUR COLLECTION OF THE ANCHORS WILL HAVE ORIGINS IN MULTIPLE VIDEO SEQUENCES.
utt_0094 utt 380.60 387.87 -X WE COMPUTE THE AFFINITY OF ALL FEATURES IN THE FRAMES OF THE OTHER TIMESTEPS IN BOTH VIEWS W.R.T. THESE ANCHORS.
utt_0096 utt 388.06 397.82 -X WE DEFINE THE AFFINITY BETWEEN THE FEATURE AND AN ANCHOR AS THE SOFTMAX OVER COSINE DISTANCES OF THAT FEATURE TO ALL ANCHORS, NORMALISED BY HYPERPARAMETER TAU.
utt_0098 utt 399.04 407.62 -X NOW, EVERY V AND V-HAT AT A GIVEN CELL LOCATION REPRESENTS A DISTRIBUTION OF POSSIBLE ASSIGNMENTS TO THE ANCHORS, AKIN TO CLUSTER ASSIGNMENTS.
utt_0100 utt 407.74 413.31 -X WE THEN SELECT THE DOMINANT ASSIGNMENT OVER THESE DISTRIBUTIONS BY THE ARGMAX OPERATION.
utt_0101 utt 413.31 418.62 -X NOTE THAT THE ARGMAX IS OVER THE INDEX SET OF THE ANCHORS BELONGING TO THE SAME VIDEO SEQUENCE.
utt_0102 utt 419.04 422.85 -X THIS IS TO SATISFY OUR ASSUMPTION two ON THE TEMPORAL COHERENCE.
utt_0103 utt 423.52 428.35 -X THIS WILL GENERATE PSEUDO-LABELS THAT REPRESENT THE CLOSEST ANCHOR TO THE GIVEN FEATURE.
utt_0104 utt 428.96 434.74 -X WE PROVIDE THESE PSEUDO-LABELS TO THE ORIGINAL, FIRST VIEW TRANSFORMED WITH THE SIMILARITY TRANSFORMATION
utt_0105 utt 434.75 437.92 -X THAT WERE USED BEFORE, TO SPATIALLY ALIGN THE FEATURES.
utt_0106 utt 438.30 446.69 -X WE MINIMISE THE DISTANCE OF THE FEATURES TO THE ANCHORS WITH THE CORRESPONDING ASSIGNMENT PROVIDED BY THE PSEUDO-LABELS. AS A CONSEQUENCE OF THE SELF-SUPERVISION,
utt_0108 utt 446.78 452.32 -X FIRST, THE FEATURES FROM THE SAME VIEW WILL GET ATTRACTED TO THE ANCHORS OF THE REFERENCE FRAME,
utt_0109 utt 452.32 456.80 -X SATISFYING OUR ASSUMPTION three ON THE TEMPORAL PERSISTENCE. AND, SECOND,
utt_0110 utt 456.80 458.34 -X IMPLEMENTING ASSUMPTION one,
utt_0111 utt 458.46 463.97 -X THE ANCHORS WILL BE MUTUALLY REPELLED, DUE TO THE SOFTMAX USED IN COMPUTING THE AFFINITIES.
utt_0112 utt 464.45 468.80 -X NOTE THAT BY EXPLOITING THE SECOND VIEW WE SIDESTEP THE DEGENERATE SOLUTIONS,
utt_0113 utt 468.80 477.99 -X SINCE THE TRIVIAL POSITIONAL ENCODING WOULD LEAD TO INCONSISTENT FEATURE REPRESENTATION BETWEEN THE ORIGINAL AND THE TRANSFORMED VIEW. TO FURTHER REGULARISE THE TRAINING,
utt_0115 utt 477.99 481.31 -X WHICH IS ESPECIALLY RELEVANT WHEN TRAINING ON LARGER DATA SETS,
utt_0116 utt 481.44 490.85 -X WE ENSURE THAT THE DISTANCE OF THE CORRESPONDING FEATURES BETWEEN THE TWO VIEWS SATISFIES OUR ASSUMPTION ON EQUIVARIANCE, ASSUMPTION four, THAT WE INTRODUCED BEFORE.
utt_0118 utt 491.14 497.60 -X THAT IS, THE FEATURE IN THE FIRST FEW SHOULD HAVE THE CLOSEST COUNTERPART IN THE CORRESPONDING LOCATION OF THE twoND VIEW.
utt_0120 utt 497.60 503.04 -X WE WILL FORMALISE THIS SHORTLY. WE IMPLEMENT THIS IDEA WITH THE FOLLOWING FRAMEWORK.
utt_0121 utt 503.20 507.46 -X IT CONTAINS TWO COMPUTATIONAL BRANCHES: THE MAIN AND THE REGULARISING BRANCH.
utt_0122 utt 507.74 517.86 -X WE USE MULTIPLE SERIES OF THE ORIGINAL FRAMES CORRESPONDING TO DIFFERENT VIDEO CLIPS, AND DENOTED HERE IN DIFFERENT COLOUR, AS THE INPUT TO THE MAIN BRANCH, AND GENERATE A RANDOM SIMILARITY
utt_0124 utt 517.86 527.94 -X TRANSFORM IN THE REGULARISING BRANCH. WE PASS BOTH INPUTS THROUGH A SHARED FEATURE EXTRACTOR TO PRODUCE A threeD SPACE-TIME FEATURE TENSORS: K AND K-HAT.
utt_0127 utt 528.51 533.77 -X WE THEN SAMPLE THE ANCHORS ON A UNIFORM GRID FROM THE SPATIAL TENSOR IN THE MAIN BRANCH.
utt_0128 utt 534.47 538.31 -X USING A SPARSER SAMPLING GRID FOR THE ANCHORS REDUCES THE MEMORY FOOTPRINT,
utt_0130 utt 538.40 541.06 -X AND, AS WE SHOW EMPIRICALLY IN OUR PAPER,
utt_0131 utt 541.15 543.69 -X ALSO IMPROVES THE SEGMENTATION ACCURACY.
utt_0132 utt 543.97 557.77 -X THE SAMPLING TAKES PLACE ONLY FROM THE FEATURE TENSOR CORRESPONDING TO A SINGLE VIDEO FRAME SELECTED UNIFORMLY AT RANDOM FROM EACH VIDEO CLIP. WE COMPUTE THE COSINE DISTANCE BETWEEN ALL FEATURES IN BOTH BRANCHES WITH RESPECT TO THE ANCHORS.
utt_0135 utt 557.77 565.45 -X NEXT, WE SELECT DOMINANT ASSIGNMENTS TO THESE ANCHORS IN THE REGULARISING BRANCH AND PROVIDE THOSE TO SUPERVISE THE ASSIGNMENT IN THE MAIN BRANCH.
utt_0137 utt 565.79 568.87 -X THIS IS ENSURED BY THE STANDARD CROSS-ENTROPY LOSS.
utt_0138 utt 569.28 570.25 -X SIMILARLY,
utt_0139 utt 570.25 577.06 -X WE COMPUTE THE CROSS-VIEW CONSISTENCY BY ALSO SAMPLING THE FEATURES FROM THE REGULARISING BRANCH ON A UNIFORM GRID.
utt_0141 utt 577.06 587.37 -X NOTE THAT THE FEATURES IN BOTH THE MAIN AND THE REGULARISING BRANCHES ARE SPATIALLY ALIGNED HERE, WITH THE HELP OF THE SIMILARITY TRANSFORMATION THAT WERE USED TO GENERATE THE INPUT TO THE REGULARISING BRANCH.
utt_0144 utt 587.62 591.91 -X WE COMPUTE THE COSINE DISTANCE BETWEEN THE CORRESPONDING FEATURES IN BOTH VIEWS.
utt_0145 utt 592.04 602.47 -X THE SECOND, CROSS-VIEW LOSS, MINIMISES THE RELATIVE DISTANCE BETWEEN THESE CORRESPONDING FEATURES WITH RESPECT TO THE DISTANCES OF THE NON-CORRESPONDING PAIRS, BY MEANS OF THE CROSS-ENTROPY.
utt_0147 utt 602.72 607.72 -X WE ADD THIS LOSS TERM TO THE MAIN LOSS DESCRIBED PREVIOUSLY WITH A SMALL TRADE-OFF FACTOR.
utt_0148 utt 607.97 617.54 -X WE TESTED OUR APPROACH BY TRAINING ON THE NUMBER OF COMMONLY USED VIDEO DATA SETS AND COMPARED TO PREVIOUS WORK THAT USED THE SAME DATASET FOR TRAINING. AS EVALUATION METRICS,
utt_0150 utt 617.54 623.08 -X WE USE THE STANDARD JACCARD'S INDEX FOR THE REGION SIMILARITY AND THE F-MEASURE FOR THE CONTOUR QUALITY.
utt_0152 utt 623.20 627.08 -X WE FIRST TRAIN ON TRACKINGNET AND COMPARE TO THE WORK BY WANG ET AL.,
utt_0153 utt 627.08 634.66 -X WHICH USE PATCH-LEVEL TRACKING TO OBTAIN POSITIVE PAIRS FOR CONTRASTIVE LEARNING. WE SIGNIFICANTLY IMPROVE OVER THIS WORK BY six point four% MEAN J&AMPF SCORE.
utt_0155 utt 636.87 644.04 -X WHEN TRAINED ON KINETICS, WE COMPARE TO THE CONTRASTIVE RANDOM WALK APPROACH BY JABRI ET AL., WHICH RELIES ON THE TIME CYCLE CONSISTENCY.
utt_0157 utt 644.13 655.43 -X WE IMPROVE THEIR RESULTS BY one point one%. TRAINED ON THE SMALLER DATA SET, YOUTUBE-VOS, WE COMPARE TO MAST, AN APPROACH THAT USES AN IMAGE RECONSTRUCTION OBJECTIVE.
utt_0160 utt 655.78 659.31 -X OUR APPROACH NOT ONLY SIGNIFICANTLY IMPROVES OVER THIS WORK,
utt_0161 utt 659.31 662.95 -X BUT IS ALSO ON PAR WITH THE RESULTS OBTAINED FROM MUCH LARGER DATA SETS.
utt_0162 utt 663.05 666.51 -X THIS SUGGESTS THAT OUR APPROACH IS MORE DATA EFFICIENT.
utt_0166 utt 678.73 688.11 -X THE LEARNED FEATURES EXHIBIT MORE ROBUSTNESS IN ESTABLISHING SEMANTIC RELATIONSHIPS BETWEEN TEMPORALLY DISTANT FRAMES, AS CAN BE VISIBLE FROM THE SEGMENTATION OF THE DANCER'S TORSO.
utt_0171 utt 702.09 707.50 -X IN DETAIL, OUR APPROACH EXHIBITS IMPROVED TRAINING EFFICIENCY COMPARED TO PREVIOUS WORK.
utt_0172 utt 707.62 709.87 -X REGARDLESS OF THE TRAINING DATA WE USE,
utt_0173 utt 709.87 713.07 -X OUR APPROACH REQUIRES FEWER TRAINING ITERATIONS TO CONVERGE,
utt_0174 utt 713.07 717.29 -X SOMETIMES NEEDING JUST A FRACTION OF THE TRAINING TIME OF THE PREVIOUS WORKS.
utt_0175 utt 717.38 718.64 -X FOR EXAMPLE,
utt_0176 utt 718.64 723.28 -X OUR APPROACH TRAINS MORE THAN THREE TIMES FASTER THAN THE CONTRASTIVE RANDOM WALK APPROACH.
utt_0178 utt 723.43 724.43 -X MOREOVER,
utt_0179 utt 724.43 736.08 -X OUR FRAMEWORK HAS A LOWER MEMORY FOOTPRINT AND CAN BE TRAINED EFFICIENTLY ON JUST A SINGLE COMMODITY GPU. TO SUMMARISE, WE PRESENTED AN UNSUPERVISED APPROACH TO LEARNING DENSE FEATURE REPRESENTATIONS.
utt_0182 utt 736.08 744.05 -X WE FORMALISED AND INTEGRATED THE ASSUMPTIONS ON THE FEATURE DIVERSITY AND THE TEMPORAL COHERENCE THAT WERE PREVIOUSLY USED ONLY IN ISOLATION.
utt_0184 utt 744.05 749.58 -X WE ADDED THE ASSUMPTION ON TEMPORAL PERSISTENCE, THAT IS CONCEPTUALLY MORE ROBUST TO OCCLUSIONS THAN IN PREVIOUS WORK.
utt_0185 utt 750.22 757.55 -X FINALLY, WE EXPLOITED MODEL EQUIVARIANCE TO SIMILARITY TRANSFORMATIONS. AS THE OUTCOME OF COMBINING THESE ASSUMPTIONS,
utt_0187 utt 757.55 770.40 -X WE HAVE PROPOSED AN UNSUPERVISED FRAMEWORK THAT ALLOWS TO LEARN DENSE FEATURE REPRESENTATIONS IN A FULLY CONVOLUTIONAL MANNER AND AVOIDS DEGENERATE SOLUTIONS, WHICH HAS NOT BEEN POSSIBLE BEFORE. TRAINED WITHOUT ANY SUPERVISION
utt_0190 utt 770.40 775.82 -X OUR FRAMEWORK ACHIEVES STATE-OF-THE-ART VIDEO OBJECT SEGMENTATION ACCURACY COMPARED TO PREVIOUS WORK.
utt_0191 utt 776.07 782.57 -X THIS HOLDS EVEN IF WE USE SIGNIFICANTLY LESS DATA AND IS CONSISTENT ACROSS TRAINING AND TESTING SCENARIOS.
utt_0192 utt 782.57 789.01 -X MOREOVER, OUR APPROACH IS ALSO MORE COMPUTATIONALLY EFFICIENT, IN SOME SCENARIOS SUBSTANTIALLY SO THAN IN PREVIOUS WORK.
utt_0193 utt 789.26 793.87 -X IT EXHIBITS FAST TRAINING CONVERGENCE AND LOWER COMPUTATIONAL AND MEMORY FOOTPRINT.
utt_0194 utt 793.87 799.31 -X THE CODE AND PRE-TRAINED MODELS ARE PUBLICLY AVAILABLE AND WE HOPE YOU FIND IT USEFUL IN YOUR WORK.
utt_0195 utt 799.31 805.68 -X I WOULD LIKE TO CLOSE BY ACKNOWLEDGING THE SUPPORT OF THIS WORK FROM THE EUROPEAN RESEARCH COUNCIL AND LOEWE INITIATIVE.
utt_0196 utt 805.68 806.93 -2.7318 THANK YOU FOR WATCHING!
