utt_0000 utt 1.93 4.56 -X HI, I’M CHENG WAN FROM RICE UNIVERSITY.
utt_0001 utt 5.29 8.37 -X I’M GOING TO INTRODUCE OUR RECENT WORK PIPEGCN.
utt_0002 utt 10.51 14.10 -X HERE IS THE DEFINITION OF GRAPH CONVOLUTIONAL NETWORK.
utt_0003 utt 14.25 19.99 -X THE INPUT OF A GCN IS A GRAPH, AND EACH NODE IS ASSOCIATED WITH SOME FEATURES.
utt_0004 utt 20.72 32.24 -X TO CALCULATE THE FEATURES OF ALL NODES IN THE NEXT LAYER, A GCN LAYER FIRST AGGREGATES ALL NEIGHBORS’ FEATURES, WHICH USUALLY CALCULATES THE AVERAGE OF NEIGHBORS’ FEATURES.
utt_0006 utt 33.71 42.39 -X THEN, WE PASS THE RESULTS AND THE PREVIOUS FEATURES TO A NEURAL NETWORK TO UPDATE THE NODE FEATURES.
utt_0008 utt 42.39 47.00 -X TRAINING A GCN CAN BE CHALLENGING BECAUSE A REAL-WORLD GRAPH CAN BE GIANT.
utt_0009 utt 48.18 53.56 -X FOR EXAMPLE, AN AMAZON CO-PURCHASE GRAPH MAY CONTAIN AROUND ten MILLION NODES.
utt_0010 utt 55.63 61.53 -X EVEN A three-LAYER GCN REQUIRES one hundredGB FOR TRAINING, WHICH IS OUT OF THE CAPACITY OF MODERN GPUS.
utt_0011 utt 63.60 71.58 -X OUR RESEARCH QUESTION IS HOW TO TRAIN GCN AT SCALE, AND HOW TO TRAIN IT EFFICIENTLY.
utt_0012 utt 71.58 75.54 -X THE SOTA APPROACH IS PARTITION-PARALLEL TRAINING.
utt_0013 utt 75.54 82.17 -X WE FIRST SEPARATE A GIANT GRAPH INTO MULTIPLE PARTITIONS AND ASSIGN EACH PARTITION TO ONE GPU.
utt_0015 utt 83.16 89.43 -X DURING THE TRAINING OR INFERENCE, WE REQUIRE POINT-TO-POINT COMMUNICATION FOR SHARING BOUNDARY NODE FEATURES.
utt_0017 utt 91.00 100.95 -X SOME VERY RECENT WORKS HAVE ADOPTED THIS METHOD FOR GCN TRAINING, BUT ITS DRAWBACK HAS NOT BEEN WELL EXPLORED.
utt_0019 utt 100.95 113.44 -X TO THIS END, WE PROPOSE PIPEGCN, IN WHICH WE IDENTIFY THE DRAWBACKS OF PARTITION-PARALLEL GCN TRAINING, AND FURTHER PROPOSE A SIMPLE SOLUTION.
utt_0021 utt 113.50 120.99 -X TO UNDERSTAND THE DRAWBACKS OF PARTITION-PARALLEL TRAINING OF GCN, WE HAVE TO GET FAMILIAR WITH ITS WORKFLOW.
utt_0023 utt 120.99 129.02 -X FOR EACH GCN LAYER, WE NEED TO FIRST COMMUNICATE NODE FEATURES SO THAT ALL DATA ARE PREPARED FOR CALCULATING NEW FEATURES.
utt_0025 utt 130.81 139.10 -X THEN, BY FOLLOWING THE GCN COMPUTATION, WE CALCULATE THE NODE FEATURES WITHIN THE PARTITION.
utt_0027 utt 139.10 140.70 -X HERE IS THE OVERALL WORKFLOW.
utt_0028 utt 142.01 148.48 -X AT THE BEGINNING OF EACH ITERATION, EACH PARTITION ONLY MAINTAINS THE FEATURES OF INNER NODES.
utt_0029 utt 149.88 153.92 -X THEN, WE TRANSFER THE FEATURES OF BOUNDARY NODES FOR THE FIRST LAYER.
utt_0030 utt 155.87 167.14 -X AFTER PREPARING ALL REQUIRED FEATURES, WE CAN COMPUTE INNER NODES’ FEATURES, AND WE REPEAT THIS PROCESS FOR THE OTHER LAYERS DURING THE FORWARD PASS.
utt_0032 utt 167.14 174.53 -X THE BACKWARD PROCESS FOLLOWS A SIMILAR COMMUNICATE-COMPUTE WORKFLOW.
utt_0033 utt 174.81 179.23 -X FINALLY, WE CALCULATE THE WEIGHT GRADIENTS AND SYNCHRONIZE MODEL WEIGHTS.
utt_0034 utt 181.12 191.17 -X HERE WE CAN FIGURE OUT THAT EACH LAYER REQUIRES two SYNCHRONIZATIONS FOR FORWARD PASS AND BACKWARD PASS, THUS, WE CAN CONCLUDE THE FIRST DRAWBACK.
utt_0036 utt 192.10 197.25 -X PARTITION-PARALLEL TRAINING REQUIRES FREQUENT SYNCHRONIZATION.
utt_0037 utt 197.25 202.95 -X ADDITIONALLY, WE OBSERVE THAT THE COMMUNICATION VOLUME OF PARTITION-PARALLEL TRAINING IS SUBSTANTIAL.
utt_0038 utt 203.62 210.15 -X COMMUNICATION MAY TAKE MORE THAN eighty% OF TOTAL TRAINING TIME, WHICH LEADS TO THE SECOND DRAWBACK:
utt_0039 utt 210.15 213.80 -X SIGNIFICANT COMMUNICATION OVERHEAD.
utt_0040 utt 213.80 217.03 -X NOW WE HAVE DISCOVERED TWO DRAWBACKS ABOUT COMMUNICATION.
utt_0041 utt 217.51 220.84 -X OUR SOLUTION IS STRAIGHTFORWARD.
utt_0042 utt 220.84 227.46 -X GIVEN THE TIMELINE OF ALL TRAINING ITERATIONS, WE PIPELINE THE COMPUTATION AND COMMUNICATION.
utt_0043 utt 228.48 230.66 -X THIS IS OUR PROPOSED PIPEGCN.
utt_0044 utt 233.06 234.79 -X HERE IS THE DETAILS OF PIPEGCN.
utt_0045 utt 236.29 247.91 -X FOR EACH ITERATION, INSTEAD OF WAITING FOR THE FEATURES OR GRADIENTS FROM THE CURRENT ITERATION, WE USE THE TRANSFERRED FEATURES OR GRADIENTS FROM THE PREVIOUS ITERATION.
utt_0047 utt 247.91 251.72 -X THIS ALLOWS US TO PERFORM COMPUTATION AND COMMUNICATION IN PARALLEL.
utt_0048 utt 253.35 259.40 -X NOW, IDEALLY, WE NO LONGER NEED SYNCHRONIZATION OR COMMUNICATION FOR FEATURES AND THEIR GRADIENTS.
utt_0049 utt 259.40 262.06 -X BUT WE HAVE ONE CONCERN HERE.
utt_0050 utt 263.69 267.08 -X SOME FEATURES AND THEIR GRADIENTS IN THIS WORKFLOW ARE STALE.
utt_0051 utt 267.49 269.48 -X THIS STALENESS IS NOT TRIVIAL.
utt_0052 utt 270.66 275.50 -X BECAUSE PRIOR DISTRIBUTED TRAINING WORKS ONLY APPLY STALE WEIGHTS AND WEIGHT GRADIENTS.
utt_0053 utt 276.39 280.97 -X ALTHOUGH MANY RECENT WORKS HAVE VERIFIED THE CONVERGENCE WITH STALENESS IN WEIGHTS.
utt_0054 utt 281.93 286.44 -X THE CONVERGENCE OF TRAINING WITH STALE FEATURES AND FEATURE GRADIENTS IS NOT CLEAR.
utt_0055 utt 288.46 293.13 -X WE HAVE TO ASK WHETHER PIPEGCN CONVERGES, AND HOW ABOUT ITS CONVERGENCE RATE.
utt_0056 utt 294.41 304.37 -X IN THIS WORK, WE PROVE THAT PIPEGCN CONVERGES, AND THE CONVERGENCE RATE IS T TO THE POWER OF MINUS TWO OVER THREE.
utt_0058 utt 304.37 314.96 -X COMPARED WITH THE EXISTING WORKS THAT ONLY LEVERAGE STALE FEATURES, PIPEGCN FURTHER ADOPTS STALE FEATURE GRADIENTS BUT IT ACHIEVES THE BEST CONVERGENCE RATE.
utt_0060 utt 316.75 322.77 -X FOR OUR EXPERIMENTS, WE EVALUATE PIPEGCN USING FOUR LARGE-SCALE DATASETS AND TWO SERVERS.
utt_0061 utt 323.69 328.94 -X WE FIRST COMPARE PIPEGCN WITH TWO OPEN-SOURCED BASELINES ROC AND CAGNET.
utt_0062 utt 330.32 333.39 -X PIPEGCN ACHIEVES SIGNIFICANTLY HIGHER THROUGHPUT.
utt_0063 utt 334.51 337.39 -X THE RESULTS FOR THE OTHER DATASETS ARE SIMILAR.
utt_0069 utt 354.74 355.67 -X WE HAVE RELEASED OUR CODE.
utt_0070 utt 355.67 356.79 -4.5056 WELCOME TO EXPLORE IT.
