utt_0000 utt 0.06 9.84 -X HELLO EVERYONE, MY NAME IS VILÉM ZOUHAR AND ON BEHALF OF MY COLLABORATORS I’LL BE PRESENTING OUR WORK ON REDUCING THE SIZE OF KNOWLEDGE BASE INDICES USING DIMENSIONALITY AND PRECISION REDUCTION.
utt_0003 utt 9.84 12.82 -X HISTORICALLY THERE HAS BEEN A LARGE FOCUS ON NON-PARAMETRIC MODELS.
utt_0004 utt 12.82 17.62 -X THOSE ARE NOT MODELS WHICH DON’T HAVE ANY PARAMETERS BUT RATHER THEY DON’T HAVE FIXED AMOUNT OF PARAMETERS.
utt_0006 utt 17.84 25.07 -X THIS APPROACH IS PARTICULARLY SUITED FOR TASKS SUCH AS QUESTION ANSWERING BECAUSE THE KNOWLEDGE IS RETRIEVED FROM A DATABASE OR KNOWLEDGE BASE IN GENERAL.
utt_0008 utt 25.45 30.03 -X OPPOSED TO THAT ARE MODELS WHICH STORE THE KNOWLEDGE IN PARAMETERS, SUCH AS PRE-TRAINED LANGUAGE MODELS.
utt_0010 utt 30.25 36.02 -X THIS IS SOMETHING NEW FROM THE LAST five YEARS BUT AS YOU ALL KNOW, THEY BECAME LARGELY ADOPTED IN THE NLP COMMUNITY.
utt_0012 utt 36.02 41.27 -X THESE MODELS HOWEVER HAVE SOME SERIOUS FLAWS, SUCH AS THEY BECOME OUTDATED VERY QUICKLY,
utt_0013 utt 41.27 45.33 -X THEY PROVIDE VERY LITTLE EXPLAINABILITY AND THEY ARE DO POORLY ON UNSEED PHENOMENA.
utt_0014 utt 45.33 50.58 -X SO THERE ARE THESE TWO GROUPS OF APPROACHES, UTILIZING NON-PARAMETRIC OR PARAMETRIC MODELS.
utt_0015 utt 50.58 54.74 -X BUT RECENTLY THERE HAS BEEN AN EMERGENCE OF THE COMBINATION OF THESE APPROACHES.
utt_0016 utt 54.74 61.27 -X THE WAY THEY WORK IS THAT THEY RETRIEVE SOMETHING FROM A KNOWLEDGE BASE, WHICH I USE AS AN UMBRELLA TERM FOR ANY PERSISTENT STORAGE.
utt_0018 utt 61.27 69.04 -X THIS RETRIEVED ITEM, WHICH I CALL THE ARTEFACT IS THEN MERGED IN THE COMPUTATION OF A PARAMETRIC MODEL.
utt_0020 utt 69.04 79.80 -X FOR EXAMPLE, IN QUESTION ANSWERING, YOU GET THE QUERY, YOU USE THE QUERY TO GET RELEVANT DOCUMENTS, YOU PROCESS THEM SOMEHOW AND THEN YOU FEED THEM TOGETHER WITH THE QUERY TO A FINE-TUNED ROBERTA OR OTHER PARAMETRIC MODEL.
utt_0023 utt 79.92 84.21 -X AND FINALLY GETTING TO THIS GRAPH, WE CAN RECREATE WHAT’S HAPPENING IN THIS MODEL.
utt_0024 utt 84.21 88.60 -X WE START WITH THE QUERY, THEN FOR EXAMPLE, COMPUTE AN EMBEDDING OF IT OR USE TF-IDF,
utt_0025 utt 89.01 99.48 -X THEN WE GIVE THIS TO THE RETRIEVER, WHICH DOES MAXIMUM SIMILARITY SEARCH OVER A KNOWLEDGE BASE, THEN THE AGGREGATOR CAN RERANK THEM AND CONCATENATE THEM AND WE GET SOMETHING CALLED AN ARTEFACT.
utt_0028 utt 99.76 103.09 -X THIS IS THEN FUSED INTO THE MODEL, FOR EXAMPLE, IN THE FORM OF PRIMING.
utt_0029 utt 103.09 104.60 -X AND FINALLY WE GET THE OUTPUT.
utt_0030 utt 104.60 113.46 -X WHAT’S INTERESTING IS THAT THIS ALSO WORKS FOR OTHER TASKS, SUCH AS FACT CHECKING, KNOWLEDGE-BASE-ENHANCED LANGUAGE MODELLING, KNOWLEDGEABLE DIALOGUE AND SO ON.
utt_0032 utt 113.46 117.92 -X THERE ARE SOME DIFFERENCES, FOR EXAMPLE THE SOURCE OF THE KNOWLEDGE-BASE, THE KEY-VALUE
utt_0033 utt 117.94 124.60 -X TYPES, THE SPECIFICITY OF THE ARTEFACT OR, MOST IMPORTANTLY, THE WAY THE ARTEFACTS ARE FUSED INTO THE MODEL.
utt_0035 utt 124.60 126.52 -X FOR EXAMPLE PRIMING WAS EARLY FUSION.
utt_0036 utt 126.52 134.04 -X BUT ESSENTIALLY THEY SHARE THE SAME ARCHITECTURE AND IT’S GOOD TO KEEP IT IN MIND BECAUSE IMPROVEMENTS FOR MODELS FOR ONE TASK CAN ALSO TRANSFER TO OTHERS.
utt_0038 utt 134.48 139.13 -X THIS FORMALISM ALSO ALLOWS US TO ASK QUESTION SUCH AS HOW DO DIFFERENT FUSION MECHANISMS
utt_0039 utt 139.35 141.72 -X AFFECT THE MODEL COMPUTATION.
utt_0040 utt 142.07 146.58 -X THE INDEX OF TEXT WIKIPEDIA two thousand and nineteen IS ABOUT fiftyM VECTORS WHICH IS ~one hundred and sixtyGB IF WE’RE USING
utt_0041 utt 149.36 150.87 -X seven hundred and sixty-eight-DIMENSIONAL VECTORS.
utt_0042 utt 151.41 155.29 -X THIS WON’T FIT ONTO YOUR LOCAL GPU AND PROBABLY ALSO YOUR CLUSTER GPU.
utt_0043 utt 155.35 163.57 -X IF YOU WANT TO USE THE WHOLE INDEX YOU HAVE TO RUN IT ON CPU RAM MEMORY AND NEED ABOUT one TB OF IT BECAUSE YOU MAY BE DUPLICATING IT A FEW TIMES AND DOING STUFF WITH IT.
utt_0045 utt 163.73 173.43 -X THIS CREATES A LARGE BARRIER AND MORE IMPORTANTLY, MAKES EXPERIMENTING WITH KNOWLEDGE BASES UNPLEASANT BECAUSE YOUR EXPERIMENT TURNOVER TIME IS NOT A FEW HOURS BUT A FEW DAYS.
utt_0047 utt 174.64 182.74 -X BECAUSE THE MOST COMMON OPERATION TO DO ON A KNOWLEDGE BASE IS RETRIEVAL, WE FOCUS ON MINIMIZING ITS SIZE WHILE TRYING TO PRESERVE THE RETRIEVAL PERFORMANCE.
utt_0049 utt 182.99 188.50 -X IN RETRIEVAL WHAT WE’RE USUALLY TRYING TO DO IS TO FIND THE MOST COMMON K DOCUMENTS RELATED TO A SPECIFIC QUERY.
utt_0051 utt 188.82 193.82 -X THE RELEVANCY NEEDS TO BE A FUNCTION THAT TAKES A QUERY AND A DOCUMENT AND PRODUCES A NUMBER.
utt_0053 utt 193.82 196.98 -X THE HIGHER THE NUMBER, THE MORE RELEVANT THE DOCUMENT IS TO THE QUERY.
utt_0054 utt 196.98 206.14 -X WHAT HAS BEEN SHOWN TO WORK WELL WAS TO COMPUTE EMBEDDINGS OF THE QUERY AND THE DOCUMENT AND THEN TO USE SOME MATHEMATICAL FUNCTIONS FOR VECTOR SIMILARITY, SUCH AS THE INNER PRODUCT.
utt_0056 utt 206.42 213.08 -X THIS IS SHOWN ON THE SECOND LINE AND EMPIRICALLY WORKS WELL BECAUSE MANY VECTOR SIMILARITY FUNCTIONS ARE DECOMPOSABLE AND RUN VERY FAST.
utt_0058 utt 213.08 218.10 -X THE KNOWLEDGE BASE INDEX IS THESE PRECOMPUTED DOCUMENT VECTORS AND THE TYPICAL DIMENSIONALITY
utt_0059 utt 218.84 220.44 -X IS seven hundred and sixty-eight.
utt_0060 utt 220.44 225.66 -X THE NEXT STEP IS TO USE DIMENSION REDUCTION FUNCTIONS R AND AGAIN THE SAME VECTOR SIMILARITY.
utt_0061 utt 225.66 230.62 -X THIS WOULD MAKE THE COMPUTATION OF RELEVANCIES FASTER AND ALSO REDUCE THE KNOWLEDGE BASE INDEX SIZE.
utt_0063 utt 231.03 233.05 -X HOW WELL THIS WORKS IS THE TOPIC OF THIS PAPER.
utt_0064 utt 233.05 242.01 -X THERE ARE MULTIPLE MODELS THAT REPRESENT FUNCTION F AND SPECIFICALLY WHAT WE USED IS EITHER THE CLS TOKEN REPRESENTATION OR THE AVERAGED TOKEN REPRESENTATION OF THE LAST LAYER.
utt_0066 utt 242.01 246.81 -X THE SIMILARITY FUNCTION IS COMMONLY EITHER AN INNER PRODUCT OR THE Ltwo DISTANCE.
utt_0067 utt 246.81 250.81 -X AND DEPENDING ON WHAT WE USE TOGETHER WITH THE MODEL WE GET DIFFERENT RESULTS.
utt_0068 utt 251.29 256.15 -X WE CAN SEE IN THE SOLID BARS, FOR EXAMPLE, THAT VANILLA BERT IS NOT VERY USEFUL FOR SIMILARITY,
utt_0069 utt 256.15 258.71 -X WHICH IS ONE OF THE CONCLUSION OF THE SENTENCEBERT PAPER.
utt_0070 utt 258.71 266.04 -X THE BEST IS DPR WITH CLS REPRESENTATION WHICH ISN’T REALLY SURPRISING BECAUSE THAT’S WHAT IT WAS TRAINED TO BE - TO PROVIDE A GOOD SIMILARITY FUNCTION.
utt_0072 utt 266.13 271.42 -X AN IMPORTANT ASPECT IS PRE-PROCESSING, SPECIFICALLY CENTERING AND NORMALIZING THE VECTORS.
utt_0073 utt 271.42 278.94 -X IF WE DO THAT, NOT ONLY DO WE MOST OFTEN THAN NOT GET A BETTER PERFORMANCE BUT ALSO THE ORDERING GIVEN BY THE INNER PRODUCT AND THE Ltwo DISTANCE BECOMES THE SAME.
utt_0075 utt 278.97 282.27 -X THE FIRST DIMENSION REDUCTION METHOD IS RANDOM DIMENSION DROPPING.
utt_0076 utt 282.27 285.72 -X HERE WE SIMPLY RANDOMLY CHOOSE DIMENSIONS TO DROP AND WE KEEP THE REST.
utt_0077 utt 285.72 291.36 -X THERE ARE STANDARD METHODS FOR RANDOM PROJECTIONS SUCH AS SPARSE AND GAUSSIAN RANDOM PROJECTIONS BUT THEY WORK LESS WELL.
utt_0079 utt 291.36 300.64 -X A MORE SOPHISTICATED APPROACH IS TO SELECT DIMENSIONS THAT CONTRIBUTE THE LEAST TO THE RETRIEVAL AND THIS WE CALL THE GREEDY DIMENSION DROPPING WHICH TOPS ALL OTHER CURVES.
utt_0081 utt 300.64 304.35 -X FOR THIS METHOD WE CENTER &AMP NORMALIZE BEFORE AND AFTER THE DIMENSION REDUCTION.
utt_0082 utt 304.35 312.35 -X WHILE GREEDY DIMENSION DROPPING IS THE BEST, MOST OF THESE METHODS SATURATE VERY QUICKLY AND REACH CLOSELY TO THE UNCOMPRESSED PERFORMANCE, WHICH IS THE GREY HORIZONTAL LINE.
utt_0084 utt 313.24 320.96 -X WHILE THE PERFORMANCE OF THE SPARSE PROJECTIONS SERVES AS A BASELINE, WE NATURALLY EXAMINED ALSO OTHER METHODS, NAMELY PCA AND GRADIENT-OPTIMIZED AUTOENCODER.
utt_0086 utt 321.21 325.98 -X THIS GRAPH SHOWS THE PCA AND A SIMPLE TWO LAYER AUTOENCODER WITH VARIOUS PRE-PROCESSING.
utt_0087 utt 326.62 331.48 -X THE RED LINES ARE RETRIEVAL USING Ltwo DISTANCE AND BLUE LINES RETRIEVAL USING THE INNER PRODUCT.
utt_0088 utt 331.77 334.78 -X BOTH OF THESE METHODS ARE VERY SENSITIVE TO PRE-PROCESSING.
utt_0089 utt 334.78 338.43 -X FOR PCA THAT’S CLEAR - THE PREPROCESSING IS A VITAL PART OF THE ALGORITHM.
utt_0090 utt 338.46 343.07 -X FOR THE AUTOENCODER IT BECOMES MUCH LESS STABLE AND WHILE IT’S A BIT BETTER THAN THE PCA,
utt_0091 utt 343.07 345.89 -X BOTH SATURATE VERY QUICKLY AT AROUND one hundred and twenty-eight DIMENSIONS.
utt_0092 utt 346.49 353.34 -X WE TRAINED THE MODELS EITHER ON JUST THE DOCUMENT EMBEDDINGS, QUERY EMBEDDINGS OR BOTH AND SEE THAT THEY DIFFER QUITE A BIT.
utt_0094 utt 353.34 359.42 -X THEY ARE REPRESENTED AS DIFFERENT LINE STYLE: SOLID IS FOR DOCUMENTS, DOTTED FOR QUERIES AND DASHED FOR BOTH.
utt_0096 utt 359.64 363.58 -X WE CAN OBSERVE THAT TRAINING ON QUERIES IS BETTER WHEN WE’RE NOT DOING ANY PREPROCESSING.
utt_0097 utt 363.58 366.91 -X THAT REASON IS THAT QUERIES ARE ALREADY MORE CENTERED.
utt_0098 utt 366.91 370.14 -X THIS STOPS BEING AN ISSUE IF WE DO THE PREPROCESSING.
utt_0099 utt 370.36 374.91 -X WE CAN ALSO SEE THAT THE RECONSTRUCTION LOSS IS NOT THE BEST INDICATOR OF THE RETRIEVAL PERFORMANCE.
utt_0101 utt 375.48 379.68 -X THAT’S SOMETHING WHICH I CONSIDER TO BE A MAJOR FLAW OF MOST OF THESE APPROACHES.
utt_0102 utt 379.68 386.56 -X IT IS POSSIBLE TO CONSTRUCT AN EXAMPLE OF A PROJECTION MATRIX THAT HAS zero RECONSTRUCTION LOSS BUT DOES NOT WORK FOR RETRIEVAL AT ALL.
utt_0104 utt 386.56 400.83 -X THERE ARE METHODS FOR MANIFOLD/DISTANCE LEARNING BUT THEY JUST DIDN’T WORK MOSTLY BECAUSE THE COMMON METHODS SUCH AS T-SNE OR UMAP ARE USUALLY USED FOR VISUALIZATION SO THEY WORK WELL WITH twoD OR threeD BUT WE CAN’T ASK THEM TO CONSTRUCT A NEW one hundred DIMENSIONAL SPACE.
utt_0107 utt 401.76 409.25 -X WE TRIED PROPOSING OUR OWN METHOD FOR MANIFOLD OR DISTANCE LEARNING THAT WAS BASED ON CONTRASTIVE LEARNING AND LEARNING THE DISTANCES OF THE ORIGINAL SPACE.
utt_0109 utt 409.25 412.93 -X THIS IS A LOSS MORE CLOSELY RELATED TO THE RETRIEVAL TASK.
utt_0110 utt 412.96 417.09 -X UNFORTUNATELY THE INTERMEDIATE RESULTS WERE AT BEST COMPARABLE TO THE BASELINES.
utt_0111 utt 417.60 421.28 -X THAT SAID, I BELIEVE THAT THIS IS STILL A PROMINENT AREA FOR FUTURE RESEARCH.
utt_0112 utt 422.97 427.49 -X ANOTHER STANDARD WAY TO REDUCE THE SIZE OF THE DATA IS TO SIMPLY USE LOWER NUMBER PRECISION.
utt_0113 utt 427.49 430.62 -X THIS CAN ALSO BE COMBINED WITH THE DIMENSION REDUCTION METHODS, SUCH AS PCA.
utt_0114 utt 430.81 434.53 -X THEN IT BECOMES A TRADEOFF BETWEEN THE COMPRESSION RATIO AND THE PERFORMANCE LOSS.
utt_0115 utt 434.68 446.88 -X WE FIND THAT eight-BIT PRECISION IS ALMOST AS GOOD AS thirty-two-BIT PRECISION AND WE CAN COMBINE THAT WITH PCA TO one hundred and twenty-eight DIMENSIONS, WHICH IS HALF THE SIZE OF THE ORIGINAL VECTOR AND GET twenty-four FOLD COMPRESSION WITH ninety-two% OF THE ORIGINAL PERFORMANCE.
utt_0118 utt 447.58 449.99 -X WE ALSO USE JUST one BIT PER DIMENSION.
utt_0119 utt 449.99 454.66 -X SIMPLY IF THE NUMBER IS LARGER THAN zero THEN SET IT TO one AND IF IT’S LOWER THAN zero THEN SET IT TO zero.
utt_0121 utt 454.66 461.06 -X THIS HAS MUCH WORSE PERFORMANCE BUT ALSO GIVES FOR EXAMPLE one hundred FOLD COMPRESSION WHEN COMBINED WITH PCA.
utt_0123 utt 462.01 464.32 -X HERE WE GET seventy-five% OF THE ORIGINAL PERFORMANCE.
utt_0124 utt 464.51 471.71 -X HERE WE HAVE A BIG SUMMARY OF THE METHODS JUST TO SHOW THAT IT REALLY IS A TRADEOFF BETWEEN THE COMPRESSION RATIO AND THE ORIGINAL UNCOMPRESSED PERFORMANCE.
utt_0126 utt 472.13 475.78 -X AS MENTIONED BEFORE, PCA ARE SLIGHTLY WORSE THAN THE AUTOENCODERS.
utt_0127 utt 475.81 479.62 -X NOTABLE ARE THE COMBINATIONS OF PCA AND THE PRECISION REDUCTION.
utt_0128 utt 479.62 484.13 -X IMPORTANT IS ALSO THE EFFECT OF POST-PROCESSING, THAT IS AFTER THE DIMENSION REDUCTION.
utt_0129 utt 484.13 487.65 -X IN MOST CASES CENTERING AND NORMALIZING THE DATA AGAIN ONLY HELPS.
utt_0130 utt 488.13 494.21 -X FOR THE PCA WE ALSO USED A FEW TRICKS SUCH AS RESCALING DOWN THE TOP five DIMENSIONS SO THAT THEY DON’T DOMINATE THE VECTORS SPACE.
utt_0132 utt 494.72 497.83 -X ALSO FOR THE AUTOENCODER WE CONSIDERED SEVERAL MODELS.
utt_0133 utt 497.83 507.20 -X THE FIRST ONE IS JUST ONE LINEAR ENCODER AND DECODER LAYER, THEN THREE ENCODER AND THREE DECODER LAYER, THAT’S THE FULL MODEL AND FINALLY THREE ENCODER AND ONE DECODER LAYER,
utt_0135 utt 507.20 508.39 -X CALLED THE SHALLOW DECODER.
utt_0136 utt 508.48 519.17 -X THE IDEA BEHIND THIS IS THAT WE WANT THE LATENT SPACE GIVEN BY THE BOTTLENECK LAYER TO BE AS CLOSE TO THE ORIGINAL AS POSSIBLE AND WE ACHIEVE THIS BY HAVING JUST ONE “DECODING/UNWRAPPING” OPERATION.
utt_0139 utt 519.17 521.96 -X ANOTHER WAY TO ACHIEVE THIS IS TO REGULARIZE THE AUTOENCODER.
utt_0140 utt 521.96 523.40 -X AND INDEED THIS HELPS A BIT.
utt_0141 utt 524.10 528.23 -X FOR ALL METHODS WE CAN ALSO SEE THAT PRE-PROCESSING AND POST-PROCESSING IS VITAL.
utt_0142 utt 529.31 531.88 -X WE ALSO EXPLORED SOME OTHER PROPERTIES OF THE METHODS.
utt_0143 utt 531.88 535.68 -X FOR EXAMPLE, HOW MUCH DATA DO WE NEED TO FIT THE PCA AND THE AUTOENCODER.
utt_0144 utt 535.68 538.47 -X THAT’S SHOWN ON THE LEFT FIGURE WITH THE SOLID LINES.
utt_0145 utt 538.72 544.20 -X QUITE UNSURPRISINGLY, PCA NEEDS JUST ABOUT one thousand DOCUMENTS BECAUSE IT ONLY ESTIMATES THE COVARIANCE MATRIX.
utt_0147 utt 544.61 547.46 -X THE AUTOENCODER IS MUCH LESS STABLE AND NEEDS A LOT MORE DATA.
utt_0148 utt 547.46 552.93 -X I WAS ALSO CURIOUS WHAT HAPPENS IF WE ADD SOME RANDOM DOCUMENTS WHICH ARE NOT RELEVANT TO ANY QUERY.
utt_0150 utt 552.93 555.56 -X THAT MEANS THAT THEY CAN ONLY WORSEN THE PERFORMANCE.
utt_0151 utt 555.56 558.02 -X THAT’S SHOWN WITH THE DASHED LINES IN THE FIGURE.
utt_0152 utt 558.02 564.49 -X THE GRAY DASHED LINE IS THE UNCOMPRESSED PERFORMANCE AND IF WE ADD MORE DOCUMENTS THEN IT OBVIOUSLY GOES DOWN WITH A CERTAIN SLOPE.
utt_0154 utt 564.49 567.27 -X PLEASE NOTE THAT THE X-AXIS IS LOG-SCALED.
utt_0155 utt 567.27 576.23 -X IF WE ADD MORE IRRELEVANT DOCUMENTS AND THEN RUN THE DIMENSION REDUCTION THEN THE PERFORMANCE DETERIORATES MORE QUICKLY WHICH IS A CAVEAT OF THESE UNSUPERVISED DIMENSION REDUCTION APPROACHES.
utt_0158 utt 577.51 580.39 -X NEXT IS JUST A SMALL SANITY CHECK OF THE PERFORMANCE.
utt_0159 utt 580.42 584.39 -X EVERY HOTPOTQA QUESTION REQUIRES EXACTLY TWO RELEVANT DOCUMENTS TO BE RETRIEVED.
utt_0160 utt 584.39 598.92 -X FOR EXAMPLE A QUESTION MAY BE “IS PERSON A OLDER THAN PERSON B?” AND FOR THIS WE NEED TO RETRIEVE SPANS THAT TALK ABOUT THE PERSON A AND PERSON B. WHAT I WAS WORRIED ABOUT WAS THAT EVEN THOUGH THE NUMERIC PERFORMANCE WAS OK, WHAT COULD BE HAPPENING IS THAT WE
utt_0163 utt 598.92 602.02 -X STOP RETRIEVING BOTH THE DOCUMENTS AND JUST RETRIEVE ONE DOCUMENT.
utt_0164 utt 602.02 606.89 -X BUT AS THE FIGURE ON THE RIGHT SHOWS, THAT’S NOT THE CASE BECAUSE THE DIAGONAL IS STILL DOMINATING THE FIRST ROW.
utt_0166 utt 607.04 616.58 -X A SEMI-INTERESTING FINDING IS THAT THE SUBPLOTS LOOK SIMILAR EVEN THOUGH IN THE FIRST CASE WE USE PCA FOR DIMENSION REDUCTION AND IN THE OTHER WE USE one BIT PER DIMENSION.
utt_0168 utt 616.58 621.29 -X THAT MEANS THAT THE INTRODUCED ERROR BY THE DIMENSION REDUCTION IS RANDOM AND NOT SYSTEMATIC.
utt_0169 utt 622.24 636.78 -X FINALLY I ALSO EXPLORED SOME IMPLEMENTATIONS OF PCA AND AUTOENCODER AND HOW FAST THEY TRAIN AND RUN THE INFERENCE AND THE RECOMMENDATION IS TO USE PCA WITH PYTORCH AND IF YOU WANT TO RUN THE AUTOENCODER THEN ONLY IF YOU HAVE A GPU.
utt_0172 utt 636.78 642.70 -X THE MAIN TAKE-AWAYS IS TO DO PRE- AND POST-PROCESSING PROPERLY, THAT IS CENTERING AND NORMALIZATION.
utt_0173 utt 642.70 655.11 -X FOR THE METHODS, PCA IS A QUICK AND GOOD-ENOUGH SOLUTION THAT REQUIRES VERY LITTLE DATA, IS STABLE AND CAN EASILY BE COMBINED WITH PRECISION REDUCTION FOR THE FUTURE WORK I WOULD LIKE TO SEE MORE PROGRESS IN FAST DISTANCE OR MANIFOLD LEARNING
utt_0176 utt 655.24 659.34 -X FOR HIGH DIMENSIONAL DATA BECAUSE AS WE SAW, THE METHODS RELYING ON RECONSTRUCTION LOSS,
utt_0177 utt 659.34 663.18 -X SUCH AS THE PCA AND THE AUTOENCODER, HAVE NO THEORETICAL GUARANTEES.
utt_0178 utt 663.53 669.77 -X THANK YOU FOR LISTENING TO MY PRESENTATION AND PLEASE REACH OUT TO ME IF YOU’D LIKE TO ASK ANY QUESTIONS OR NEED SOME CLARIFICATIONS.
utt_0180 utt 669.77 670.67 -1.2140 BYE!
