utt_0000 utt 0.97 13.68 -X HELLO MY NAME IS MARIAH SCHRUM. ERIN HEDLUND AND I WILL BE PRESENTING ON OUR WORK IMPROVING ROBOT-CENTRIC LEARNING FROM DEMONSTRATION VIA PERSONALIZED EMBEDDINGS WITH CO-AUTHOR MATTHEW GOMBOLAY. WE WOULD LIKE TO ACKNOWLEDGE OUR SPONSORS:
utt_0003 utt 13.68 20.69 -X THE NATIONAL SCIENCE FOUNDATION, NASA, MIT LINCOLN LABORATORY, AND GEORGIA TECH.
utt_0004 utt 20.69 24.50 -X FOR ROBOTS TO OPERATE IN THE REAL WORLD AND LEARN FROM NON-EXPERT HUMANS,
utt_0005 utt 24.50 28.24 -X THEY MUST BE CAPABLE OF EFFECTIVELY LEARNING FROM HUMAN DEMONSTRATIONS.
utt_0006 utt 28.59 37.04 -X HOWEVER, PRIOR WORK HAS SHOWN THAT HUMANS ARE SUBOPTIMAL WHEN PROVIDING CORRECTIVE DEMONSTRATIONS TO ROBOTS WHICH MAKES IT DIFFICULT FOR ROBOTS TO LEARN FROM HUMANS.
utt_0008 utt 37.71 42.58 -X TO OVERCOME THIS LIMITATION, ROBOTS MUST TAKE INTO ACCOUNT HUMAN SUBOPTIMALITY.
utt_0009 utt 44.17 47.76 -X LEARNING FROM DEMONSTRATION CAN BE BROKEN DOWN INTO TWO MAIN TYPES:
utt_0010 utt 47.82 58.96 -X HUMAN-CENTRIC LFD AND ROBOT-CENTRIC LFD. IN HUMAN-CENTRIC LFD, THE HUMAN IS IN CONTROL AND DEMONSTRATES THE TASK TO THE ROBOT. AN EXAMPLE OF HUMAN-CENTRIC LFD
utt_0012 utt 58.96 69.07 -X IS BEHAVIORAL CLONING IN WHICH THE DEMONSTRATOR PROVIDES A SERIES OF DEMONSTRATIONS AND THE AGENT LEARNS VIA SUPERVISED LEARNING. WHILE HUMAN-CENTRIC LFD IS INTUITIVE TO THE USER,
utt_0014 utt 69.07 74.48 -X IT SUFFERS FROM COVARIATE SHIFT PROBLEMS AND PERFORMS POORLY IN STOCHASTIC ENVIRONMENTS.
utt_0015 utt 74.67 86.07 -X ROBOT-CENTRIC LFD ATTEMPTS TO OVERCOME THE COVARIATE SHIFT PROBLEM. IN ROBOT-CENTRIC LFD, THE ROBOT ROLLS OUT A POLICY AND THE HUMAN PROVIDES CORRECTIVE FEEDBACK WHILE OBSERVING THE ROBOT.
utt_0017 utt 86.07 97.94 -X ONE EXAMPLE OF ROBOT-CENTRIC LFD IS DATASET AGGREGATION OR DAGGER. IN DAGGER, THE HUMAN’S CORRECTIVE FEEDBACK IS AGGREGATED EACH ITERATION AND THE AGENT IS RETRAINED ON THE NEW DATASET.
utt_0019 utt 98.35 103.28 -X WHILE THIS ALGORITHM WORKS WELL IF THE LABELS PROVIDED BY THE HUMAN ARE HIGH QUALITY,
utt_0020 utt 103.28 113.11 -X PRIOR WORK HAS SHOWN THAT HUMANS OFTEN PROVIDE POOR LABELS. FOR EXAMPLE, SEVERAL HUMAN SUBJECT STUDIES HAVE BEEN CONDUCTED THAT DEMONSTRATE THIS PROBLEM AND SHOW THAT IN REAL TIME,
utt_0022 utt 113.11 126.19 -X WITH AN INEXPERIENCED OR EVEN AVERAGE USER, DAGGER DOES NOT PERFORM WELL. WE PROPOSE TO LEARN TO MAP POOR HUMAN LABELS TO BETTER LABELS, THEREBY IMPROVING ON THE PERFORMANCE OF DAGGER.
utt_0024 utt 126.19 137.65 -X TO IMPROVE UPON THE DEFICIENCIES IN ROBOT-CENTRIC LFD, WE INTRODUCED MUTUAL INFORMATION DRIVEN META-LEARNING FROM DEMONSTRATION OR MIND MELD. TO BETTER LEARN FROM SUBOPTIMAL CORRECTIVE LABELS,
utt_0026 utt 137.65 146.68 -X MIND MELD META-LEARNS AN INDIVIDUAL-SPECIFIC MAPPING FROM HUMAN LABELS TO PREDICTED GROUND TRUTH LABELS VIA A LONG SHORT-TERM MEMORY (LSTM) BASED NEURAL NETWORK ARCHITECTURE.
utt_0028 utt 146.96 151.89 -X WE DEMONSTRATE OUR ARCHITECTURE IN A DRIVING SIMULATOR DOMAIN. IN THIS DOMAIN,
utt_0029 utt 151.89 157.30 -X THE ROBOT ROLLS OUT ITS CURRENT LEARNED POLICY AND THE HUMAN PROVIDES CORRECTIVE FEEDBACK.
utt_0030 utt 157.30 168.56 -X BECAUSE THE CAR IS NOT ACTUALLY RESPONDING TO THE HUMAN, IT IS UNINTUITIVE FOR THE HUMAN TO KNOW EXACTLY HOW MUCH TO TURN THE WHEEL RESULTING IN SUBOPTIMAL BEHAVIOR. BASED UPON THE FEEDBACK THE
utt_0032 utt 168.56 178.77 -X HUMAN PROVIDES WE LEARN A PERSONALIZED EMBEDDING IN N-DIMENSIONAL SPACE THAT DESCRIBES THEIR STYLE OF CORRECTIVE FEEDBACK. IN THIS EXAMPLE, WE SEE THAT THIS INDIVIDUAL IS AN OVER-CORRECTOR,
utt_0034 utt 178.77 183.12 -X MEANING THAT THEY TEND TO TURN THE WHEEL TOO FAR WHEN PROVIDING CORRECTIVE FEEDBACK.
utt_0035 utt 183.25 192.28 -X BASED ON THEIR LEARNED STYLE, WE THEN MAP THEIR CORRECTIVE LABELS TO BETTER LABELS SO THAT THE LFD AGENT CAN MORE EFFECTIVELY LEARN FROM THE HUMAN.
utt_0037 utt 193.58 204.15 -X TO LEARN THE INDIVIDUAL’S STYLE OF PROVIDING CORRECTIVE FEEDBACK, WE DESIGN CALIBRATION TASKS WITH KNOWN, OPTIMAL SOLUTIONS. THE CALIBRATION TASKS ARE REPRESENTATIVE OF THE OVERALL TASK
utt_0039 utt 204.15 215.61 -X DOMAIN. FOR EACH CALIBRATION TASK, THE AGENT ROLLS OUT A WIZARD-OF-OZ POLICY AS SHOWN ON THE RIGHT AND THE DEMONSTRATOR MUST PROVIDE CORRECTIVE FEEDBACK. FOR EXAMPLE, THE CAR TURNED TO THE
utt_0041 utt 215.61 220.79 -X LEFT AND THE DEMONSTRATOR SHOULD TURN THE WHEEL TO THE RIGHT TO AVOID COLLIDING WITH THE POLE.
utt_0042 utt 220.91 225.27 -X THE TASKS ARE ALL PRE-RECORDED TO PROVIDE CONSISTENCY ACROSS PARTICIPANTS.
utt_0043 utt 226.64 236.50 -X WE TRAIN OUR MIND MELD ARCHITECTURE ON THE GROUND TRUTHS OPTIMAL LABELS, AND CORRECTIVE ACTIONS PROVIDED DURING THE CALIBRATION TASKS TO LEARN THE PERSONALIZED EMBEDDING AND MAPPING
utt_0045 utt 236.50 247.69 -X FROM SUBOPTIMAL LABELS TO BETTER LABELS. HERE WE SHOW OUR ARCHITECTURE WHICH IS TRAINED ON THE CALIBRATION TASKS. TO ACCURATELY MAP A DEMONSTRATORS LABEL TO A LABEL CLOSER TO
utt_0047 utt 247.69 256.98 -X GROUND TRUTH, WE MUST FIRST LEARN THE STYLE OF THE DEMONSTRATOR. OUR NETWORK ARCHITECTURE SIMULTANEOUSLY LEARNS THE STYLE OF THE DEMONSTRATOR VIA A PERSONALIZED EMBEDDING, W,
utt_0049 utt 257.39 269.65 -X AND LEARNS THE MAPPING FROM DEMONSTRATOR LABEL, A_T’, TO THE DIFFERENCE BETWEEN THE DEMONSTRATOR LABEL AND THE GROUND TRUTH. OUR BI-DIRECTIONAL LSTM LEARNS THE EMBEDDING Z WHICH ENCODES
utt_0051 utt 269.65 275.45 -X TEMPORAL INFORMATION ABOUT THE DEMONSTRATOR’S SUBOPTIMALITY. THE SUBNETWORK, F SUB THETA,
utt_0052 utt 275.45 286.10 -X MAPS THIS ENCODING TO THE DIFFERENCE BETWEEN THE HUMAN PROVIDED LABEL AND THE GROUND TRUTH. WE INCLUDE THE SUBNETWORK, G_PHI, WHICH APPROXIMATES THE PROBABILITY DISTRIBUTION FROM WHICH W
utt_0054 utt 286.10 298.74 -X IS DRAWN AND IS NECESSARY TO MAXIMIZE MUTUAL INFORMATION. DURING TRAINING, WE SIMULTANEOUSLY LEARN THE PERSONALIZED EMBEDDING, W, AND LEARN THE NETWORK PARAMETERS, THETA, PHI AND PHI PRIME.
utt_0056 utt 298.74 310.14 -X WE ILLUSTRATE HOW OUR NETWORK OPERATES AT TEST TIME WITH OUR DRIVING SIMULATOR EXAMPLE. WE FEED A SEQUENCE OF DEMONSTRATOR LABELS INTO OUR LSTM, CONDITIONED ON THE PERSONALIZED EMBEDDING, W.
utt_0058 utt 310.87 317.37 -X WE THEN LEARN TO MAP THE LABEL AT TIME, T’, TO A LABEL CLOSER TO THE OPTIMAL LABEL.
utt_0059 utt 318.39 321.66 -X WE DO THIS FOR THE ENTIRE SEQUENCE OF CORRECTIVE LABELS.
utt_0060 utt 324.53 328.41 -X WE LEARN THE PERSONALIZED EMBEDDINGS VIA VARIATIONAL INFERENCE.
utt_0061 utt 328.41 332.06 -X TO DO SO, WE MAXIMIZE THE MUTUAL INFORMATION BETWEEN W AND THE DIFFERENCE, D.
utt_0062 utt 332.47 339.29 -X THIS MEANS THAT BEHAVIORS THAT DIFFER IN THE WAY THEY ARE SUBOPTIMAL WILL RESULT IN DIFFERENT LEARNED EMBEDDINGS.
utt_0064 utt 339.29 346.65 -X EXPANDING THE EQUATION FOR MUTUAL INFORMATION GIVES US AN EQUATION WITH THE ENTROPY OF W AND THE EXPECTATION OF THE PROBABILITY DISTRIBUTION OF W.
utt_0066 utt 347.22 350.36 -X HOWEVER, THIS PROBABILITY DISTRIBUTION IS UNKNOWN.
utt_0067 utt 350.36 353.98 -X THEREFORE, WE APPROXIMATE IT WITH THE DISTRIBUTION G_PHI.
utt_0068 utt 354.16 363.93 -X THROUGH A FEW TRICKS USING THE KL DIVERGENCE AND A LOWER BOUND WE ARRIVE AT AN EQUATION THAT, THROUGH GRADIENT DESCENT,
utt_0070 utt 363.99 369.37 -X CAN BE USED TO LEARN THE PARAMETERS WHICH MAXIMIZE THE MUTUAL INFORMATION.
utt_0071 utt 369.37 377.85 -X BEFORE MOVING ON TO OUR EXPERIMENTS, WE WANT TO NOTE THAT OUR MIND MELD ALGORITHM MAKES SEVERAL ASSUMPTIONS. FIRST, WE ASSUME THAT HUMANS PROVIDE SUBOPTIMAL CORRECTIVE FEEDBACK.
utt_0073 utt 378.07 390.33 -X SECOND, WE ASSUME THAT MIND MELD CAN LEARN AN EMBEDDING THAT CAPTURES A HUMAN’S STYLE OF PROVIDING FEEDBACK. THIRD, WE ASSUME THAT HUMANS ARE CONSISTENT AND PROVIDE FEEDBACK IN A PREDICTABLE WAY ACROSS TASKS. OUR POSITIVE RESULTS,
utt_0076 utt 390.33 395.90 -X WHICH WE WILL DISCUSS NEXT, SUGGEST THAT OUR ASSUMPTIONS HOLD TRUE IN OUR STUDY.
utt_0077 utt 395.90 405.13 -X ADDITIONALLY, MIND MELD REQUIRES ACCESS TO TRAINING PARTICIPANTS AND REQUIRES A SET OF CALIBRATION TASKS WHERE GROUND TRUTH CAN BE CALCULATED. WE HAVE FOUND THAT THE EXTRA EFFORT
utt_0079 utt 405.13 412.92 -X TO TRAIN MIND MELD RESULTS IN A LARGE IMPROVEMENT ON PARTICIPANT PROVIDED FEEDBACK AND LABELS.
utt_0080 utt 413.69 424.35 -X TO TEST THE ABILITY OF OUR ARCHITECTURE TO LEARN SUBOPTIMAL DEMONSTRATOR STYLES, WE CREATE A SYNTHETIC EXPERIMENT. WE SIMULATE A DRIVING TASK IN WHICH THE OBJECTIVE OF THE DEMONSTRATOR IS TO
utt_0082 utt 424.35 429.95 -X TEACH THE AGENT TO DRIVE TO A GOAL LOCATION IN THE ENVIRONMENT. TO CREATE SYNTHETIC TRAINING DATA,
utt_0083 utt 429.95 441.08 -X WE FIRST CREATE A SET OF ARTIFICIAL DAGGER-LIKE ROLL-OUTS. WE CALCULATE THE GROUND TRUTH LABELS AS THE DIFFERENCE IN THE HEADING OF THE AGENT AND THE ANGLE TO THE GOAL. WE NEXT CREATE A
utt_0085 utt 441.08 446.17 -X SET OF ARTIFICIAL, SUBOPTIMAL DEMONSTRATORS BY RANDOMLY ASSIGNING EACH DEMONSTRATOR A STYLE.
utt_0086 utt 446.17 451.35 -X SUCH AS DELAYED (MEANING ACTIONS ARE EXECUTED LATER IN TIME COMPARED TO THE GROUND TRUTH),
utt_0087 utt 452.02 457.18 -X ANTICIPATORY (ACTIONS ARE EXECUTED SOONER IN TIME COMPARED TO THE GROUND TRUTH), OR NEITHER
utt_0088 utt 457.81 468.54 -X AND TO BE EITHER AN OVER-CORRECTOR (ACTIONS ARE GREATER IN MAGNITUDE COMPARED TO THE GROUND TRUTH) OR UNDER-CORRECTOR (ACTIONS ARE SMALLER IN MAGNITUDE COMPARED TO THE GROUND TRUTH).
utt_0090 utt 468.54 472.48 -X THIS “STYLE” IS THEN UTILIZED TO MAP THE GROUND TRUTH LABELS TO SUBOPTIMAL,
utt_0091 utt 472.48 482.78 -X ARTIFICIAL HUMAN LABELS. WE THEN TRAIN OUR ARCHITECTURE ON THIS DATA AND DEMONSTRATE THAT OUR NETWORK CAN IMPROVE SUBOPTIMAL HUMAN LABELS AND LEARN MEANINGFUL PERSONALIZED EMBEDDINGS.
utt_0093 utt 485.24 496.28 -X WE TRAIN OUR MODEL ON four BASELINE TASKS WITH five DAGGER ROLLOUTS PER TASK AND four0 ARTIFICIAL DEMONSTRATORS. WE FIND THAT WE ARE ABLE TO IMPROVE UPON THE CORRECTIVE LABELS BY sixty-one%
utt_0095 utt 496.76 499.42 -X ON THE CALIBRATION TRAINING TASKS AND fifty-five%
utt_0096 utt 500.02 506.30 -X ON HOLDOUT SET OF TEST TASKS. ON THE LEFT IS A PLOT OF THE LEARNED EMBEDDINGS IN LATENT SPACE.
utt_0097 utt 507.29 518.30 -X BECAUSE SIMILAR DEMONSTRATORS TENDED TO CLUSTER TOGETHER, WE CAN CONCLUDE THAT OUR EMBEDDINGS LEARN A MEANINGFUL REPRESENTATION OF AN INDIVIDUAL’S DEMONSTRATION STYLE. DEMONSTRATORS
utt_0099 utt 518.30 523.55 -X SHOWN IN RED PROVIDED DELAYED FEEDBACK, THOSE IN GREEN PROVIDED ANTICIPATORY FEEDBACK, AND BLUE,
utt_0100 utt 523.55 533.79 -X NEITHER. DEMONSTRATORS TOWARDS THE LEFT OF THE PLOT WHERE THE CIRCLES ARE SMALLER TENDED TO UNDER-CORRECT AND THOSE ON THE RIGHT WHERE THE CIRCLES ARE LARGER TENDED TO OVER-CORRECT.
utt_0102 utt 537.50 547.96 -X NEXT WE CONDUCT A HUMAN-SUBJECTS STUDY USING THE AIRSIM DRIVING SIMULATOR DOMAIN AND AN XBOX STEERING WHEEL. THE OBJECTIVE OF THE TASK IS TO TEACH THE CAR TO DRIVE TO THE ORANGE
utt_0104 utt 547.96 559.29 -X BALL WHILE AVOIDING ALL OBSTACLES. WE FIRST RECRUITED A SET OF thirty-four TRAINING PARTICIPANTS WHO PERFORMED THE SET OF ALL CALIBRATION TASKS. WE USED THIS DATA TO TRAIN OUR MODEL.
utt_0106 utt 562.55 576.73 -X THIS SLIDE SHOWS THE twelve CALIBRATION TASKS THAT WE USED IN OUR STUDY. EACH CALIBRATION TASK IS A PRE-RECORDED TRAJECTORY OF THE CAR AND THE PARTICIPANT MOVES THE STEERING WHEEL TO PROVIDE CORRECTIVE FEEDBACK AT EACH TIMESTEP TO TEACH THE CAR TO GO TO THE ORANGE BALL.
utt_0109 utt 578.11 584.35 -X WE USE THE DATA COLLECTED FROM THE PARTICIPANTS IN THE CALIBRATION TASKS TO LEARN THE NETWORK PARAMETERS FOR MIND MELD.
utt_0111 utt 587.74 591.17 -X FOR EACH TASK, WE CALCULATE THE OPTIMAL CORRECTIVE LABEL AT EACH
utt_0112 utt 591.32 596.48 -X TIMESTEP ALONG THE TRAJECTORY TO FIND THE SHORTEST-COLLISION FREE PATH TO THE GOAL.
utt_0113 utt 596.48 609.18 -X IN THIS EXAMPLE, THE TRAJECTORY IS SHOWN IN BLUE. THE OPTIMAL PATH FROM EACH TIMESTEP TO THE GOAL IS CALCULATED USING RRT* AND THIS PATH IS SHOWN IN RED. TO ACCOUNT FOR THE PHYSICS OF THE CAR,
utt_0115 utt 609.37 614.17 -X WE USE AN MPC CONTROLLER TO FIND THE BEST ACTION THAT THE CAR SHOULD TAKE TO FOLLOW THE RRT* PATH.
utt_0116 utt 614.39 620.96 -X THIS BEST ACTION IS THE GROUND TRUTH LABEL FOR THE OPTIMAL STEERING ANGLE AT THAT TIMESTEP.
utt_0117 utt 624.09 629.47 -X OUR FIRST OBJECTIVE METRIC IS THE AMOUNT THAT MIND MELD IMPROVED PARTICIPANT LABELS.
utt_0118 utt 629.47 639.26 -X WE MEASURED PERCENT IMPROVEMENT BY COMPARING THE DIFFERENCE BETWEEN LABELS ALTERED BY MIND MELD AND GROUND TRUTH (D_IMPROVED) AND THE DIFFERENCE BETWEEN THE ORIGINAL PARTICIPANT
utt_0120 utt 639.26 646.02 -X LABELS AND THE GROUND TRUTH (D_INITIAL). TO MEASURE A PARTICIPANT’S STYLISTIC TENDENCIES,
utt_0121 utt 646.02 651.58 -X WE USED DYNAMIC TIME WARPING TO TEMPORALLY MATCH THE PARTICIPANT’S FEEDBACK SIGNAL AND THE GROUND TRUTH SIGNAL.
utt_0123 utt 652.09 661.06 -X THE CHANGE IN AMPLITUDE BETWEEN THE SIGNALS INDICATED WHETHER SOMEONE OVER OR UNDER-CORRECTED AND THE DIFFERENCE IN TIMING INDICATED WHETHER SOMEONE WAS DELAYED OR ANTICIPATORY.
utt_0125 utt 662.36 671.30 -X DURING THE STUDY, PARTICIPANTS FIRST COMPLETED SURVEYS TO COLLECT DEMOGRAPHIC INFORMATION SUCH AS AGE AND GENDER AND TO DETERMINE THEIR BIG FIVE PERSONALITY TRAITS,
utt_0127 utt 671.30 677.12 -X THEIR PRIOR EXPERIENCE WITH DRIVING, VIDEO GAMES, AND RACING GAMES, AND THEIR TRUST IN AUTOMATION.
utt_0128 utt 677.47 686.69 -X WE WANTED TO DETERMINE IF ANY OF THESE SUBJECTIVE METRICS IMPACTED A PERSON’S STYLE OF PROVIDING FEEDBACK AND IF THEY CORRELATED WITH THE LEARNED PERSONALIZED EMBEDDINGS.
utt_0130 utt 689.18 695.07 -X THIS TABLE SHOWS OUR RESULTS. OVERALL, WE FOUND THAT MIND MELD IMPROVED CORRECTIVE LABELS BY sixty-three%.
utt_0131 utt 695.52 702.11 -X THIS SHOWS THAT MIND MELD CAN EFFECTIVELY MAP SUBOPTIMAL HUMAN LABELS TO LABELS CLOSER TO OPTIMAL.
utt_0133 utt 703.16 706.91 -X ALSO, WE HYPOTHESIZED THAT MIND MELD WOULD CAPTURE A PARTICIPANT’S
utt_0134 utt 707.36 718.53 -X STYLISTIC TENDENCY THROUGH THE PERSONALIZED EMBEDDINGS. TO INVESTIGATE WHICH INFORMATION IS ENCAPSULATED BY THE EMBEDDINGS IN LATENT SPACE, WE CONDUCTED A CORRELATION ANALYSIS BETWEEN THE
utt_0136 utt 718.53 733.28 -X EMBEDDINGS AND THE STYLISTIC TENDENCIES DETERMINED BY DYNAMIC TIME WARPING. WE FOUND THAT A PERSON’S STYLISTIC TENDENCY SIGNIFICANTLY CORRELATED WITH THE LEARNED EMBEDDING, WHICH SHOWS THAT MIND MELD IS LEARNING THE SUBOPTIMAL WAYS IN WHICH A PERSON PROVIDES FEEDBACK.
utt_0139 utt 733.69 737.28 -X LASTLY, WE FOUND THAT THE LEARNED EMBEDDINGS WERE CORRELATED WITH GENDER.
utt_0140 utt 740.32 750.98 -X IN THE FUTURE, WE PLAN TO CONDUCT AN EXPERIMENT WHERE WE COMPARE MIND MELD TO A ROBOT-CENTRIC BASELINE, DAGGER, AND A HUMAN-CENTRIC BASELINE, BEHAVIORAL CLONING. PARTICIPANTS WILL TRAIN ALL
utt_0142 utt 750.98 762.75 -X THREE AGENTS TO DRIVE FROM A START LOCATION TO A GOAL LOCATION IN REAL-TIME. WE WILL COMPARE ALGORITHM PERFORMANCE AND HUMAN SUBJECTIVE METRICS SUCH AS TRUST, WORKLOAD, AND LIKEABILITY.
utt_0144 utt 762.85 766.56 -X WE ALSO PLAN TO IMPLEMENT MIND MELD ON A ROBOTIC ARM DOMAIN
utt_0145 utt 766.59 770.31 -X TO DEMONSTRATE HOW MIND MELD CAN GENERALIZE ACROSS DIFFERENT DOMAINS.
utt_0146 utt 773.44 784.19 -X TO SUMMARIZE, WE INTRODUCED A NOVEL FRAMEWORK, MIND MELD THAT LEARNS AND ACCOUNTS FOR DEMONSTRATOR STYLE AND IMPROVES UPON SUBOPTIMAL DEMONSTRATIONS FOR ROBOT-CENTRIC LFD.
utt_0148 utt 785.31 796.64 -X WE CONDUCTED A HUMAN-SUBJECTS EXPERIMENT THAT SHOWED THAT MIND MELD COULD IMPROVE SUBOPTIMAL HUMAN LABELS BY sixty-three% AND THAT MIND MELD EFFECTIVELY LEARNS DEMONSTRATOR STYLE.
utt_0150 utt 796.64 801.31 -3.9083 THANK YOU FOR LISTENING AND WE WOULD BE HAPPY TO ANSWER ANY QUESTIONS.
