utt_0000 utt 0.26 12.37 -X HELLO EVERYONE MY NAME IS SHAI AHARON, I'LL BE PRESENTING MY PAPER HYPER NETWORK BASED ADAPTIVE IMAGE RESTORATION. DR. GIL BEN-ARTZI WAS MY ADVISOR WHO WORKED WITH ME ON THE PAPER.
utt_0002 utt 13.32 24.43 -X A NEURAL NETWORK THAT RESTORES AN IMAGE WITH A FIXED NOISE IS QUITE TRIVIAL THE PROBLEM ARISES WHEN THE NOISE LEVEL IS NOT KNOWN DURING THE TRAINING TIME THE NAIVE SOLUTION IS TO
utt_0004 utt 24.43 36.24 -X TRAIN MULTIPLE NETWORKS EACH ONE IS TRAINED ON A DIFFERENT NOISE LEVEL. CLEARLY, IT IS NOT A VERY ELEGANT SOLUTION TO TACKLE THIS PROBLEM. ADAPTIVE NETWORKS HAVE BEEN PROPOSED. THE IDEA IS THAT AT
utt_0006 utt 36.24 42.16 -X INFERENCE TIME THE NETWORK WILL RECEIVE THE NOISY IMAGE AND THE NOISE LEVEL AS A PARAMETER. THE
utt_0007 utt 42.89 54.61 -X CURRENT STATE OF THE ART ADAPTIVE NETWORKS ALL SHARE A SIMILAR BASE METHOD. THEY TRAIN TWO SETS OF WEIGHTS FOR THE NETWORK ONE FOR THE LOW NOISE IMAGES AND ONE FOR THE HIGH NOISE. IMAGES
utt_0009 utt 54.90 63.44 -X DURING INFERENCE THEY INTERPOLATE THE WEIGHTS OF THE SET OF THE TWO SETS USING AN ALPHA BETWEEN zero AND one IN ORDER TO GET THE RIGHT WEIGHTS FOR THE GIVEN NOISE.
utt_0011 utt 64.46 75.88 -X THE PREVIOUS METHODS ASSUME THAT THE WEIGHTS BETWEEN THE HIGH AND THE LOW SETS ALL LIE ON A SINGLE LINEAR LINE, UNFORTUNATELY IT IS REALLY THE CASE AND THOSE METHODS TEND TO
utt_0013 utt 75.88 86.77 -X LOSE ACCURACY WHEN THE NOISE LEVEL IS IN THE MIDDLE RANGE. THE QUICK SOLUTION IS DIVIDING THE RANGE INTO SMALLER INTERVALS BUT IT COMES WITH THE COST OF DOUBLING THE PARAMETERS
utt_0015 utt 91.06 97.56 -X OUR METHOD WAS ABLE TO ACHIEVE THE SAME ACCURACY WHILE REDUCING THE PARAMETER AMOUNT BY UP TO seventy-five
utt_0016 utt 97.75 108.69 -X PERCENT. ALSO, OUR APPROACH IS NOT CONSTRAINED TO ONLY TWO LEVELS OF DEGRADATIONS DURING THE TRAINING PHASE AND CAN EFFECTIVELY LEARN FROM ANY NUMBER OF DEGRADATIONS.
utt_0018 utt 109.49 122.12 -X THIS FLEXIBILITY ENABLES OUR MODEL TO INCREASE ACCURACY WITHOUT INCREASING THE SIZE OR COMPLEXITY, AND FINALLY, OUR METHOD CAN BE APPLIED TO VARIOUS STANDARD IMAGE RESTORATION
utt_0020 utt 122.12 128.79 -X ARCHITECTURES WITHOUT CHANGING THE NETWORK STRUCTURE, TRAINING PROCEDURE, OR HYPERPARAMETERS.
utt_0021 utt 128.98 134.14 -X ALL IT TAKES IS REPLACING THE CONVOLUTIONAL LAYERS WITH OUR HYPER CONVOLUTIONAL LAYER.
utt_0022 utt 135.03 146.23 -X THE PROPOSED NETWORK USES A HYPER NETWORK THAT RECEIVES AS INPUT THE DEGRADATION LEVEL AND GENERATES WEIGHTS FOR THE RESTORATION NETWORK THAT SUITS THE DEGRADATION LEVEL.
utt_0024 utt 146.23 158.31 -X THE RESIDUAL BLOCKS ARE PLACEHOLDERS WITH NO WEIGHTS THE WEIGHTS ARE INJECTED INTO THEM FROM THE HYPER NETWORK DURING THE FORWARD PASS. THIS HAPPENS IN REAL TIME WITHOUT THE NEED TO RETRAIN
utt_0026 utt 158.31 164.19 -X THE NETWORK. THE HYPER NETWORK IS VERY SHALLOW THUS MAKING THE GENERATION PART NEGLIGIBLE.
utt_0027 utt 165.17 177.61 -X WE TESTED OUR METHOD ON VARIOUS DATA SETS AND NOISES ON EACH TASK WE AVERAGE THE PSNR SCORE OVER ALL THE DIFFERENT DEGRADATION LEVELS. THE PLOTS BELOW SHOW THE MEAN ACCURACY AND
utt_0029 utt 177.61 190.27 -X THE NUMBER OF PARAMETERS FOR EACH MODEL. AS YOU CAN SEE OUR METHOD WAS ABLE TO ACHIEVE THE SAME OR BETTER ACCURACY WHILE USING BETWEEN twenty-five% TO fifty% OF THE PARAMETERS OF THE PREVIOUS
utt_0031 utt 190.27 196.17 -X STATE-OF-THE-ART SOLUTIONS. THE PRUNING WAS DONE BY REDUCING THE NUMBER OF RESIDUAL
utt_0042 utt 255.99 267.07 -X THE THEORY BEHIND ALL THIS IS THE OBSERVATION THAT FOR A SINGLE NETWORK ARCHITECTURE THERE ARE INFINITE SETS OF WEIGHTS THAT PRODUCE THE SAME OR SIMILAR ACCURACY FOR THE SAME NOISE LEVEL.
utt_0044 utt 267.07 271.17 -X PREVIOUS METHODS FOUND TWO SETS OF WEIGHTS, HIGH AND LOW,
utt_0045 utt 271.17 277.12 -X AND ASSUMED THAT THE REST CAN BE INTERPOLATED FROM THEM. OUR METHOD, ON THE CONTRARY,
utt_0046 utt 277.12 288.18 -X CONSTRAINS THE NETWORK TO FIND WEIGHTS THAT CAN BE LINEARLY INTERPOLATED TO THE WHOLE RANGE. WE FORCE THE NETWORK TO FIND A BASE SET OF WEIGHTS AND A VECTOR SUCH THAT ONE
utt_0048 utt 288.18 293.70 -X CAN TRAVEL LINEARLY FROM THE LOW SET THROUGH THE MIDDLE SETS ALL THE WAY TO THE HIGH SET.
utt_0049 utt 294.75 306.16 -X DURING THE TRAINING PHASE, WE CREATE MULTIPLE VIRTUAL NETWORKS EACH DEDICATED TO HANDLING ONE DEGRADATION LEVEL. THE WEIGHTS ARE INJECTED FROM THE HYPER NETWORK TO THOSE VIRTUAL
utt_0051 utt 306.16 311.87 -X NETWORKS BY INSERTING MULTIPLE DEGRADATION LEVELS SCALARS INTO THE HYPER NETWORK.
utt_0052 utt 311.87 317.79 -X WE TAKE THE SAME IMAGE WITH DIFFERENT DEGRADATION LEVELS AND FEED EACH INTO ITS DEDICATED NETWORK.
utt_0053 utt 318.05 326.63 -X THE LOSS IS THEN AVERAGE AND BACK PROPPED STRAIGHT INTO THE HYBRID NETWORK. THIS FORM OF TRAINING CONSTRAINS THE NETWORK TO FIT MULTIPLE DEGRADATIONS AT ONCE.
utt_0055 utt 327.87 333.38 -4.0594 THANK YOU FOR LISTENING BE SURE TO CHECK OUT OUR PAPER AT ARXIV AND OUR GIT REPOSITORY
