utt_0000 utt 1.29 6.00 -X HI THERE I'M PANKY HA A GRADUATE STUDENT AT THE CHINESE UNIVERSITY OF HONG KONG
utt_0001 utt 6.09 17.17 -X I'M VERY HAPPY TO SHARE OUR RECENT WORK USING MULTI-ENGINEER REINFORCEMENT LEARNING TO SIMULATE SELF-DRIVEN PARTICLE SYSTEM ESPECIALLY THE TRAFFIC FLOWS THIS WORK IS SUPERVISED BY PROFESSOR BOLEJO
utt_0003 utt 18.13 30.48 -X IN THE VERY BEGINNING LET'S BRIEFLY REVIEW WHAT IS SELF-DRIVEN PARTICLE SYSTEM SDP CAN MODELS A WIDE RANGE OF MULTI-AGENT SYSTEMS IN NATURE AND HUMAN SOCIETY SUCH AS THE BIRD FLUX FISH SCHOOL HUMAN
utt_0005 utt 30.48 43.89 -X CROWD AND TRAFFIC SYSTEMS IN STP SYSTEM EACH INDIVIDUAL PURSUE ITS OWN GOALS AND INTERACTS WITH EACH OTHER DURING SUCH INTERACTION VERY COMPLEX SOCIAL BEHAVIOR CAN EMERGE SO THAT'S WHY
utt_0007 utt 43.89 57.81 -X WE ARE SO INTERESTED IN SIMULATING STP SYSTEM WE FOCUS ON SIMULATING A TYPICAL TYPE OF SDB SYSTEM THE TRAFFIC SYSTEM IN THIS VIDEO WE SHOW THE FOOTAGE ON THE ROUNDABOUT ENVIRONMENT IF WE CAN
utt_0009 utt 57.81 69.97 -X USE VIRTUAL AGENTS TO SIMULATE THE HUMAN DRIVER BEHAVIOR WE CAN EASILY TO FIGURE OUT THE REASONS AND MOTIVATION OF THOSE COMPLEX BEHAVIOR THERE WILL THIS WILL BENEFIT A DOWNSTREAM APPLICATION
utt_0011 utt 69.97 75.65 -X SUCH AS DEVELOPING INTELLIGENT TRANSPORTATION SYSTEM AND ALSO DEVELOPING SELF-DRIVING CARS
utt_0012 utt 77.46 89.40 -X THERE IS THESE MANY WAYS TO SIMULATE SUCH TRAFFIC FLOWS MULTI-REINFORCEMENT LEARNING IS A PROMISING APPROACH HOWEVER PREVIOUS WORK MIGHT NOT BE SUITABLE TO THIS NEW SETTING FOR
utt_0014 utt 89.40 94.45 -X EXAMPLE IN A COOPERATIVE SETTING ALL THE AGENTS NEED TO COOPERATE TO MAXIMIZE A JOINT REWARD
utt_0015 utt 94.93 100.18 -X WHICH IS NOT GOING TO LEAD TO THE REALISTIC BEHAVIORS FOR EXAMPLE THE AGENT MIGHT LEARN
utt_0016 utt 100.21 105.75 -X FREE WRITING AND ALSO LEARN TO SACRIFICING THEMSELVES TO MAXIMIZE THE JOINT REWARD
utt_0017 utt 106.51 111.77 -X IN THE COMPETITIVE SETTINGS AGENTS WILL FORM THE ANNIVERSARY RELATIONSHIP
utt_0018 utt 111.89 115.22 -X AND THIS IS NOT ALSO EXPECTED IN THE REAL WORLD TRAFFIC SCENARIO
utt_0019 utt 116.31 122.38 -X THE MISS MOTIVE RL IS A SIMILAR SETTING TO US WHERE THE AGENT PURSUED THEIR EACH ON GOALS
utt_0020 utt 122.87 136.44 -X BY THIS WORK WE FOCUS ON SIMULATING THE HUMAN DRIVER BEHAVIORS INSTEAD OF TRYING TO FINDING OPTIMAL SOLUTIONS TO THE GAMES OF MISS MOTIF RL BESIDES WE ALSO TEST OUR METHOD IN A VERY
utt_0022 utt 136.44 141.69 -X COMPLEX DRIVING SCENARIO INSTEAD OF THE RELATIVELY SIMPLE GREEN WORLDS ENVIRONMENT
utt_0023 utt 143.83 155.51 -X THERE IS SOME RELEVANT WORKS FOCUSED ON SIMULATING TRAFFIC SYSTEM FOR EXAMPLE IN THE FLOW SIMULATOR THE RESEARCHER TRYING TO LEARN A HIGH-LEVEL CONTROLLER TO THE TRAFFIC FLOWS HOWEVER IN THIS
utt_0025 utt 155.51 168.99 -X SIMULATOR THE DETAILED BEHAVIOR MIGHT NOT EXHIBIT IN THE SMART SIMULATOR MRMS ARE USED TO TRAIN AGENTS IN DIFFERENT TRAFFIC CONDITIONS WE PROPOSE A NORMAL MRR METHOD TO TREND THE TRAFFIC FLOW
utt_0027 utt 168.99 174.52 -X IN A MORE CHALLENGING ENVIRONMENT WITH A LARGE SCALE OF OUR AGENTS AND A COMPLEX ROAD NETWORK
utt_0028 utt 177.01 184.35 -X WE SUMMARIZE THE KEY FEATURE OF FDP SYSTEM HERE THE FIRST EACH INDIVIDUAL IS SELF-INTERESTED
utt_0029 utt 184.95 189.40 -X AND BECAUSE OF THIS FEATURE THE RELATIONSHIP BETWEEN THE AGENT IS TIME WIRING
utt_0030 utt 189.69 193.31 -X FOR EXAMPLE THE AGENT CAN FORM COOPERATION RELATIONSHIP
utt_0031 utt 194.30 205.52 -X LIKE IN THIS ENVIRONMENT THE AGENT LEARN TO YIELD OTHERS BUT AGENTS CAN ALSO BE COMPETITIVE SUCH AS IN THIS ENVIRONMENT THEY USUALLY LEARN TO CUT IN OTHERS SO THEY CAN DRIVE FASTER
utt_0033 utt 207.00 218.46 -X SO TO FACE THIS NEW KIND OF FEATURES THERE IS SOME CHALLENGE TO THE CURRENT METHODS FIRST IS THAT HOW CAN WE ADOPT THIS THING METHOD TO THIS NEW SETTING AND THE SECOND
utt_0035 utt 218.46 223.61 -X IS THAT HOW CAN WE LEARN A SOCIALLY COMPLIANT BEHAVIOR IN A POPULATION USING CURRENT METHODS
utt_0036 utt 224.35 230.85 -X TO TACKLE THIS CHALLENGE IN THIS WORK WE PROPOSE COORDINATED POLICY OPTIMIZATION
utt_0037 utt 231.03 237.79 -X A NOVEL MRI METHOD INSTEAD OF COPPO COBO CAN FACILITATE BY LEVEL OPTIMIZATION
utt_0038 utt 237.95 243.90 -X TO OPTIMIZE THE POPULATION TO ACHIEVE THE SOCIAL BEHAVIOR AND VERY EFFICIENT POPULATIONS
utt_0039 utt 245.59 260.06 -X KOBO AGENTS CAN LEARN REALISTIC CROWD ACTIONS IN THIS ROUNDABOUT ENVIRONMENT WE CAN SHOW WE CAN FIND A LARGE SCALE OF OUR AGENTS IS A VERY COORDINATED BEHAVIORS AND ALSO COBOL
utt_0041 utt 260.06 264.90 -X CAN LEARN SAFE DRIVING SUCH AS IN THIS BOTTLENECK ENVIRONMENT THEY JUST LEARN TO YIELD TO OTHERS
utt_0042 utt 266.75 281.44 -X AND ALSO COBO CAN LEARN SOCIAL BEHAVIORS SUCH AS IN THIS VERY CHALLENGING UNPROTECTED INTERSECTION ENVIRONMENT AGENT LEARN NEGOTIATION AND OBSERVING OTHER OTHERS BEHAVIOR SO THAT YOU CAN GO THROUGH THIS INTERSECTION SAFELY
utt_0045 utt 284.06 296.48 -X NOW WE'LL BRIEFLY DISCUSS THE METHODOLOGY OF COPPO COPO FIRST OF ALL LET'S CONSIDER AGENT THAT WE'RE GOING TO OPTIMIZE WE CALL THIS TARGET AGENT WE CAN EASILY TO DRAW A NEIGHBORHOOD
utt_0047 utt 296.51 305.47 -X OF THIS AGENT AND WE COLLECT THE INDIVIDUAL REWARD FROM THE ENVIRONMENT OF THE CURRENT AGENT WE CAN
utt_0048 utt 305.47 311.62 -X ALSO COMPUTE A NEIGHBORHOOD REWARD WHICH IS THE AVERAGE REWARD OF ALL NEIGHBORS OF A GIVEN AGENT
utt_0049 utt 313.54 326.92 -X THE FIRST LEVEL OPTIMIZATION IS CALLED A LOCAL COORDINATION IN LOCAL COORDINATION WE TRY TO MAXIMIZE BOTH OF THESE TWO REWARDS AND THE SECOND LEVEL ORGANIZATION IS CALLED THE GLOBAL
utt_0051 utt 326.92 340.39 -X COORDINATION WHERE ARE WE TRYING TO MAXIMIZE THE GLOBAL REWARD WHICH IS THE SUMMATION OF REWARD OF ALL AGES THIS IS CALLED A GLOBAL COORDINATION NOW WE FIRST DISCUSS THE LOCAL COORDINATION WE PROPOSE
utt_0053 utt 340.39 346.40 -X AN IDEA CALLED A LOCAL COORDINATION FACTOR LCF HERE IT IS AN ANGLE RANGE FROM NEGATIVE ninety DEGREE
utt_0054 utt 346.40 355.59 -X TO THE POSITIVE ninety DEGREE WE USE LCF TO WEIGHT SUM TWO REWARDS AND FROM THE CORRELATED REWARD HERE
utt_0055 utt 357.15 363.65 -X THE COORDINATED REWARD IS USED TO COMPUTE A COORDINATED OBJECTIVE JC HERE FOLLOWING THE
utt_0056 utt 363.71 371.86 -X PEOPLE LOSS FOR EXAMPLE AND WE USE THIS OBJECTIVE TO UPDATE EACH POLICY INDEPENDENTLY THIS IS
utt_0057 utt 371.86 379.62 -X LOCAL COORDINATION WHICH REMAIN A VERY MAJOR CHALLENGE IS THAT HOW CAN WE DETERMINE LCFfive HERE
utt_0058 utt 380.45 393.13 -X SUPPOSE WE MANUALLY ASSIGN THE LCAFfive EQUAL TO ZERO DEGREE THEN THE COORDINATED REWARD WILL REDUCE TO THE INDIVIDUAL REWARD AND THE CURRENT LOCAL COORDINATION WILL REDUCE TO INDEPENDENT LEARNING
utt_0060 utt 394.08 398.79 -X THIS LEARNING WILL LEAD TO AGGRESSIVE AGENT AS SHOWN IN THIS VIDEO BECAUSE
utt_0061 utt 398.92 403.62 -X AGENTS ONLY LEARN TO MAXIMIZE THEIR OWN REWARD WITHOUT AWARE OF OTHERS
utt_0062 utt 405.60 416.68 -X ON THE CONTRARY IF WE MANUALLY ASSIGN THE FINE TO ninety DEGREE THEN THE COORDINATED REWARD WILL REDUCE TO THE NEIGHBORHOOD REWARD AND WE ARE GOING TO LEARN CONSERVATIVE AGENT HERE
utt_0064 utt 416.84 423.37 -X BECAUSE AGENT ONLY LEARNED TO YIELD TO OTHERS SO THEY CAN LET ITS NEIGHBORS TO EARN REWARD FOR IT
utt_0065 utt 425.09 429.16 -X HOW CAN WE DETERMINE THE LCF MORE INTELLIGENTLY
utt_0066 utt 431.11 442.84 -X WE USE GLOBAL COORDINATION TO ACHIEVE THIS IN A GLOBAL COORDINATION WE FIRST GET A CONCEPT CALLED A GLOBAL OBJECTIVE THE GLOBAL OBJECTIVE IS DEFINED AS THE REWARD OF ALL ACTIVE AGENTS
utt_0068 utt 442.84 451.95 -X AT ALL TIME STEP AND WE ARE GOING TO OPTIMIZE THE LCFfive TO MAXIMIZE THE GLOBAL OBJECTIVE
utt_0069 utt 452.68 464.84 -X HOW CAN WE ACHIEVE THIS WE USE THE IDEA OF META GRADIENT HERE WE WRITE DOWN THE GLOBAL OBJECTIVE AND WE CAN EASILY COMPUTE THE META GRADIENT OF THE GLOBAL OBJECTIVE WITH RESPECT TO THE
utt_0071 utt 465.22 478.99 -X LCF PHI AND THIS GLOBAL AND THIS META GRADIENT CAN BE EASILY DECOMPOSED INTO TWO PARTS THE FIRST PART IS THE GRADIENT OF THE GLOBAL OBJECTIVE WITH RESPECT TO THE UPDATED POLICY PARAMETER SYSTEM
utt_0073 utt 478.99 484.69 -X NEW AND THIS IS OBVIOUSLY THE POLICY GRADIENT OKAY EASY TO WRITE DOWN OUR EQUATIONS HERE
utt_0074 utt 485.29 492.21 -X AND THE SECOND TERM CAN YOU WE CAN USE THE TITLE EXPANSIONS ON THE PARAMETERS OF THE POLICY AND
utt_0075 utt 492.21 500.72 -X BUILD A CONNECTION BETWEEN THE LCF WITH THE COORDINATED OBJECTIVE JC HERE AND AFTER THESE
utt_0076 utt 500.72 508.01 -X TWO TURN WE CAN WRITE DOWN A COMPLETE LOSS CORRESPONDING TO THIS META GRADIENT IN THE LOSS
utt_0077 utt 508.20 521.26 -X WE USE THE RED BOX HERE TO MARK THE RELATIONSHIP BETWEEN THE GLOBAL OBJECTIVE AND ILCF THIS LOSS HERE IS MUCH MORE SIMPLIFIED PLEASE REFER TO THE PAPER FOR DETAILED DEDUCTION
utt_0079 utt 523.02 533.87 -X NOW WE MAKE A BRIEF SUMMARY OF THE COPA METHOD FIRST OF ALL WE HAVE A TARGET AGENT AND WE CAN COLLECT THE INDIVIDUAL REWARD FOR THE ENVIRONMENT OF THE TARGET AGENT AND THEN WE CAN COMPUTE
utt_0081 utt 533.87 538.59 -X A NEIGHBORHOOD OF THIS AGENT AND COMPUTE THE NEIGHBORHOOD REWARD BY AVERAGING ALL THE REWARDS
utt_0082 utt 538.99 552.72 -X IN THE NEIGHBORHOOD AND THIS TWO REWARD IS MISSED BY THE LCF USING THE EQUATION OF COORDINATED REWARD LEADING TO THE COORDINATED OBJECTIVE WHICH IS USED TO UPDATE EACH INDIVIDUAL POLICIES
utt_0084 utt 552.72 566.16 -X THIS IS CALLED A LOCAL COORDINATION AND THEN WE COMPUTE THE GLOBAL REWARD BY SUMMING UP ALL THE REWARDS OF THE ACTIVE AGENT IN THE ENVIRONMENT WE USE THIS TO COMPUTE THE META GRADIENT WITH RESPECT
utt_0086 utt 566.16 572.21 -X TO THE LCF THIS IS CALLED GLOBAL COORDINATION WE THEN UPDATE THE LCF ACCORDING TO THIS OBJECTIVE
utt_0087 utt 572.78 583.89 -X SO THIS IS THE WHOLE PROCESS OF COPPER METHOD TO EVALUATE AND TEST DIFFERENT METHODS WE PROPOSE FIVE MULTI-AGENT TRAFFIC SIMULATION ENVIRONMENT
utt_0089 utt 583.89 596.20 -X THEY ARE THE ROUND VALVE TAILGATE BOTTLENECK INTERSECTION AND PARKING LOT ENVIRONMENTS THESE FIRE ENVIRONMENTS ARE POWERED BY THE METADRIVE A LIGHTWEIGHT AND EFFICIENT DRIVING SIMULATOR
utt_0091 utt 597.65 607.99 -X WE FIRST COMPARE THE SUCCESS RATE OF DIFFERENT METHODS THE SUCCESS RATE HERE IS DEFINES THE RATIO OF ASIANS ACHIEVING THEIR DESTINATIONS WE COMPARE THE BASELINE INDEPENDENT PPO AND THE
utt_0093 utt 608.37 620.73 -X CENTRAL ACCREDITED METHOD MEAN FAIL PPO ALSO THE CURRICULUM LEARNING METHOD WITH THE COPPO WE CAN FIND THAT COBO OUTPERFORMED ALL BASELINES IN ALL ENVIRONMENT WITH A LARGE MARGIN IN A SUCCESS RATE
utt_0095 utt 622.03 633.69 -X BESIDES WE ALSO COMPUTE THE EFFICIENCY AND SAFETY AS TWO EXTRA MATRIX TO EVALUATE THE TREND POPULATIONS THE EFFICIENCY HERE IS DEFINED AS THE TOTAL NUMBER OF SUCCESS IN A GIVEN
utt_0097 utt 633.69 637.56 -X TIME INTERVAL AND THE SAME THING IS THE TOTAL NUMBER OF CRASHES HAPPENING IN THE ENVIRONMENT
utt_0098 utt 637.62 644.06 -X THE LOWER THE BETTER WE CAN FIND OUT THE PROPOSED COUPON METHOD CAN LEARN POPULATION THAT ACHIEVE
utt_0099 utt 644.08 650.62 -X BEST RESULT IN THE INTERSECTION ENVIRONMENT IN THIS VIDEO IN THIS SLIDES WE'RE GOING TO SHOW
utt_0100 utt 650.74 655.19 -X FOUR VIDEOS IN THE TOP DOWN VIEW OF THE TREND POPULATIONS OF DIFFERENT METHODS
utt_0101 utt 656.15 668.41 -X WE CAN FIND THAT THE PRO COCOA AGENTS CAN LEARN MUCH MORE COORDINATED BEHAVIOR COMPARED TO INDEPENDENT PPO WE USE THE RED DOTS TO INDICATE THE CRASHES HELPING THE ENVIRONMENT
utt_0103 utt 669.27 674.36 -X THE RED DOTS IN THE INDEPENDENT PPO ARE MUCH MORE THAN THE COPPER POPULATION
utt_0104 utt 683.25 697.07 -X IN THIS SLIDE WE SHOW THE DETAILED BEHAVIOR OF COPPOLA AGENTS IN THE INTERSECTION ENVIRONMENT KOBO CAN LEARN QUEUING YIELDING RUSHING NEGOTIATION AND SO ON AND OTHER DETAILED BEHAVIORS
utt_0106 utt 707.67 712.36 -X IN A ROUNDABOUT ENVIRONMENT THE KOBO AGENT CAN ALSO LEARN CUT IN AND BYPASSING
utt_0107 utt 723.16 734.01 -X THAT'S ALL FOR THIS SHARING FOR MORE INFORMATION WE WELCOME YOU TO VISIT OUR WEBPAGE AND READ THE PAPER OF THIS WORK I'M PANCHO HA HOPE YOU ENJOY THIS WORK THANKS
utt_0109 utt 740.76 745.28 -0.4106 YOU
