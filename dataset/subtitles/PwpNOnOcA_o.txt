utt_0001 utt 0.66 7.50 -X IN THIS PAPER, WE PRESENT A WEAKLY SUPERVISED METHOD FOR TRAINING THE REAL-WORLD IMAGE STITCHING.
utt_0002 utt 7.50 13.49 -X THE IMAGE STITCHING AIMS TO GENERATE A SINGLE IMAGE WITH A LARGER FIELD OF VIEW USING MULTIPLE IMAGES.
utt_0004 utt 13.61 18.48 -X WITH THIS ADVANTAGE, THE IMAGE STITCHING CAN BE USED IN VARIOUS APPLICATIONS.
utt_0005 utt 19.15 23.38 -X HOWEVER, THERE ARE TWO MAJOR DIFFICULTIES IN THE REAL-WORLD TASK.
utt_0006 utt 23.57 26.58 -X THE FIRST ONE IS PARALLAX BETWEEN CAMERAS.
utt_0007 utt 26.77 32.66 -X EVEN THOUGH YOU CAN CAPTURE THE SAME CONTENT USING MULTIPLE CAMERAS ENSURING TEMPORAL SYNCHRONIZATION,
utt_0008 utt 32.69 36.37 -X PHYSICAL SHIFT OCCURS IF THE CAMERA CENTERS ARE DIFFERENT.
utt_0009 utt 36.59 43.73 -X THE RIGHT FIGURE SHOWS THE CHANGE IN THE RELATIVE POSITION OF THE OBJECT ACCORDING TO THE MOVEMENT OF THE CENTER.
utt_0011 utt 44.69 52.12 -X FROM THE SCENARIO IN THE PREVIOUS SLIDE, STITCHING OUTPUTS MAY BE DIFFERENT DEPENDING ON WHICH DEPTH IS TAKEN AS A REFERENCE.
utt_0013 utt 52.47 56.69 -X YOU CAN SEE THE GHOSTING EFFECT DUE TO DEPTH DIFFERENCE IN EACH FIGURE.
utt_0014 utt 58.48 63.45 -X FROM THE PHOTOSHOP EXAMPLE, GEOMETRIC DISTORTION CAN BE FOUND INSIDE THE RED BOX.
utt_0015 utt 66.04 70.42 -X AND THE SECOND ONE IS THE DIFFERENCE IN COLOR TONES BETWEEN INPUT IMAGES.
utt_0016 utt 70.58 78.97 -X SINCE THE INPUT IMAGES WERE TAKEN FROM DIFFERENT VIEWPOINTS, THE COLOR TONE MAY NOT BE CONSISTENT DEPENDING ON THE LOCATION OF THE LIGHT SOURCE.
utt_0018 utt 79.64 83.35 -X VARIOUS METHODS FOR STITCHING DATASETS HAVE BEEN PROPOSED.
utt_0019 utt 83.57 88.31 -X FIRST, HAND-CRAFTED METHODS CAN BE USED TO GENERATE PSEUDO-LABELS.
utt_0020 utt 88.34 92.44 -X BUT, THIS METHOD COULD BE SENSITIVE ON THE EXISTING METHOD.
utt_0021 utt 93.30 99.90 -X SECOND, A CROPPING METHOD IS PROPOSED USING LARGE DATASETS TO SUPERVISE ESTIMATING SINGLE HOMOGRAPHY.
utt_0023 utt 100.15 108.16 -X THIS SCHEME CONSIDERS ONLY SINGLE DEPTH LAYER, SO THE MODEL CANNOT COVER CONTINUOUS DEPTH LAYERS IN THE REAL-WORLD SCENE.
utt_0025 utt 109.34 113.31 -X THIRD, VIRTUAL SIMULATOR CAN BE USED TO GENERATE GROUND-TRUTH.
utt_0026 utt 113.37 118.85 -X HOWEVER, THIS METHOD REQUIRES ADDITIONAL TRAINING SUCH AS DOMAIN ADAPTATION.
utt_0027 utt 119.13 125.98 -X UNLIKE PREVIOUS PROPOSALS, OUR DATASET CONSISTS OF six FISHEYE IMAGES CAPTURED FROM VR CAMERA.
utt_0028 utt 126.23 135.52 -X FROM THESE IMAGES, HALF OF IMAGES ARE USED AS INPUTS AND THE REMAINING IMAGES IN ERP FORMAT ARE USED FOR WEAK-SUPERVISION.
utt_0030 utt 136.09 144.42 -X BASED ON THIS IDEA, OUR METHOD IS ROBUST TO VARIOUS ARTIFACTS AND ENABLES END-TO-END TRAINING WITHOUT GENUINE GROUND-TRUTH.
utt_0032 utt 144.70 150.08 -X THE ARCHITECTURE IS COMPOSED OF ENCODER, DECODER, AND REGRESSOR MODULES.
utt_0033 utt 151.23 159.49 -X THE ENCODER EXTRACTS VISUAL FEATURES FROM EACH INPUT IMAGE, AND THESE FEATURES ARE CONCATENATED ALONG THE CHANNEL AXIS.
utt_0035 utt 159.77 162.91 -X THIS FEATURE MAP IS CALLED THE ENCODING MAP.
utt_0036 utt 164.03 168.64 -X THE DECODER MODULE CONSISTS OF A SHARED DECODER AND PRIVATE DECODERS.
utt_0037 utt 168.67 173.00 -X A SHARED DECODER EXTRACTS A DECODING MAP USING THE ENCODING OUTPUT.
utt_0038 utt 173.38 179.62 -X THE PRIVATE DECODERS GENERATE four COMPONENTS NEEDED TO CREATE THE FINAL OUTPUT FROM THE DECODING MAP.
utt_0040 utt 180.35 184.23 -X HOW THESE COMPONENTS ARE USED WILL BE DISCUSSED LATER.
utt_0041 utt 185.34 191.14 -X THE REGRESSOR MODULE ESTIMATES GLOBAL AFFINE MATRICES FOR GUIDANCE TO FIND OPTIMAL FLOW MAPS.
utt_0043 utt 191.33 195.75 -X FROM THESE MATRICES, THE GLOBAL WARPING MAP G_N IS CREATED.
utt_0044 utt 196.58 204.30 -X THE FINAL OUTPUT IS GENERATED IN SEVERAL STEPS USING FIVE COMPONENTS FROM THE DECODER AND THE REGRESSOR.
utt_0046 utt 204.87 208.68 -X NOW, WE CAN DISCUSS HOW A PANORAMA IS MADE.
utt_0047 utt 209.76 216.09 -X FIRST, THE COLOR TONE OF EACH FISHEYE INPUT IS CORRECTED THROUGH THIS QUADRATIC EQUATION
utt_0048 utt 216.16 219.75 -X USING THE PRE-COLOR CORRECTION MAP C_PRE.
utt_0049 utt 219.75 224.71 -X THE COLOR-CORRECTED IMAGES ARE WARPED USING THE ADJUSTED WARPING MAP U_N.
utt_0050 utt 225.25 232.41 -X U_N IS CREATED BY ADDING A GLOBAL WARPING MAP G_N AND A LOCAL ADJUSTMENT MAP FROM THE
utt_0052 utt 233.67 241.23 -X THE FIGURE ON THE LEFT SHOWS HOW THE FISHEYE IMAGES ARE WARPED INTO ERP FORMAT AS THE TRAINING PROGRESSES.
utt_0054 utt 242.22 249.42 -X THE IMAGES WARPED IN THIS WAY ARE WEIGHTED BY THE FOLLOWING EQUATION USING A PER-PIXEL WEIGHT MAP W_N.
utt_0056 utt 251.66 262.73 -X FINALLY, WE USE THE POST-COLOR CORRECTION MAP C_POST TO CORRECT THE COLOR OF THE WEIGHTED IMAGES ONCE AGAIN ACCORDING TO THIS EQUATION.
utt_0058 utt 262.73 272.33 -X FOR THE TRAINING, WE USED PERCEPTUAL LOSS AND SSIM LOSS TOGETHER, AND THE APPLICATION AREA OF EACH LOSS FUNCTION IS SHOWN IN THE LOWER ROW OF THIS PICTURE.
utt_0060 utt 272.65 277.49 -X NOTE THAT THE SSIM IS APPLIED ON WHITE AREA ONLY.
utt_0061 utt 277.49 279.09 -X THIS IS A VIDEO RESULT.
utt_0062 utt 280.59 285.84 -3.2708 OUR METHOD CAN PRODUCE VISUALLY PLEASANT RESULTS USING INPUT FISHEYE IMAGES.
