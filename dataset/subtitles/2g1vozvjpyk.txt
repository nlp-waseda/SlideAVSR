utt_0000 utt 1.07 13.90 -X HI EVERYONE, I‚ÄôM CHANG LU FROM STEVENS INSTITUTE OF TECHNOLOGY. TODAY I‚ÄôM GOING TO PRESENT OUR WORK ‚ÄúCOLLABORATIVE GRAPH LEARNING WITH AUXILIARY TEXT FOR TEMPORAL EVENT PREDICTION IN HEALTHCARE‚Äù
utt_0003 utt 14.76 26.06 -X MY COLLABORATORS INCLUDE PROFESSOR REDDY FROM VIRGINIA TECH, DOCTOR CHAKRABORTY FROM IBM RESEARCH, PROFESSOR KLEINBERG AND NING FROM STEVENS INSTITUTE OF TECHNOLOGY.
utt_0005 utt 27.28 36.82 -X HERE IS THE OUTLINE FOR THIS REPORT I WILL FIRST INTRODUCE SOME PRELIMINARY ABOUT HEALTH EVENT PREDICTIONS. THEN I‚ÄôM GOING TO TALK ABOUT OUR WORK, NAMED
utt_0007 utt 37.10 45.71 -X CGL. FINALLY, I WILL PRESENT EXPERIMENTS TO DEMONSTRATE THE PREDICTION PERFORMANCE AND ANALYSIS TO THE RESULTS OF OUR METHOD.
utt_0009 utt 46.99 59.60 -X IN MODERN HEALTHCARE SYSTEMS, ELECTRONIC HEALTH RECORDS DATA ARE WIDELY USED FOR RECORDING PATIENTS‚Äô VISIT INFORMATION TO HOSPITALS, SUCH AS DIAGNOSES, PROCEDURES, MEDICATIONS,
utt_0011 utt 59.60 60.75 -X AND CLINICAL NOTES.
utt_0012 utt 62.13 73.43 -X EHR DATA ARE ABLE TO AUTOMATE ACCESS TO INFORMATION AND HAVE THE POTENTIAL TO STREAMLINE THE CLINICIAN'S WORKFLOW. IN ADDITION, THEY ALSO PROVIDE VALUABLE RESOURCES FOR RESEARCHERS.
utt_0014 utt 73.43 84.47 -X HERE ARE SOME EXAMPLES OF EHR DATA. PATIENTS AND VISITS ARE ENCODED INTO ID AND PATIENTS CAN HAVE MULTIPLE VISITS TO HOSPITALS.
utt_0016 utt 85.77 93.17 -X TO BETTER CLASSIFY DISEASES, THE ICD SYSTEM IS DESIGNED BY THE WORLD HEALTH ORGANIZATION.
utt_0017 utt 93.17 101.46 -X IN THE ICD SYSTEM, DIAGNOSES AND PROCEDURES ARE ASSIGNED WITH DIGITAL CODES. THEY NORMALLY FORM A HIERARCHICAL STRUCTURE.
utt_0019 utt 101.46 106.48 -X THIS FIGURE SHOWS AN EXAMPLE OF THIS HIERARCHICAL STRUCTURE
utt_0020 utt 107.34 112.72 -X THE HIGHER-LEVEL DISEASES PROVIDE A GENERAL DESCRIPTION TO A CATEGORY OF DISEASES.
utt_0021 utt 113.81 118.67 -X THE LOWER LEVEL DISEASES ARE MORE PRECISE TO DESCRIBE A DISEASE WITH DETAILS.
utt_0022 utt 120.14 125.59 -X TRADITIONAL PREDICTION MODELS IN HEALTHCARE, SUCH AS GRAM AND G-BERT EXPLORE THE DISEASE
utt_0023 utt 125.65 132.56 -X HIERARCHICAL STRUCTURE WITH ATTENTION METHODS. IT IS ABLE TO DETECT THE ANCESTOR-CHILDREN RELATIONSHIP.
utt_0025 utt 133.52 144.85 -X HOWEVER, EXCEPT THE ANCESTOR-CHILDREN RELATIONSHIP, THERE ARE ANOTHER RELATIONSHIP AMONG DISEASES REPRESENTED BY LEAF NODES. ALTHOUGH TWO DISEASES DO NOT BELONG TO THE
utt_0027 utt 144.85 156.05 -X SAME PARENT, THEY MAY FREQUENTLY CO-OCCUR IN A NUMBER OF PATIENTS‚Äô DIAGNOSIS. THIS RELATIONSHIP CAN REFLECT DISEASE COMPLICATIONS AND HELP TO PREDICT FUTURE DIAGNOSES.
utt_0029 utt 157.58 162.20 -X THEREFORE, WE WANT TO DETECT THIS KIND OF HORIZONTAL RELATIONSHIP FOR DISEASES.
utt_0030 utt 163.38 167.80 -X ANOTHER CHALLENGE IS HOW TO COLLABORATIVELY LEARN PATIENT-DISEASE INTERACTIONS.
utt_0031 utt 168.75 182.48 -X IN EXISTING WORK, PATIENTS ARE REGARDED AS INDEPENDENT SAMPLES. IF WE WANT TO PREDICT FUTURE DIAGNOSES FOR A PATIENT, EXISTING WORK ONLY USES PREVIOUS DIAGNOSES OF THIS PATIENT TO MAKE PREDICTIONS.
utt_0034 utt 183.54 189.62 -X HOWEVER, THIS KIND OF METHOD IGNORE THE PATIENT-DISEASE INTERACTIONS AND PATIENT SIMILARITY.
utt_0035 utt 191.28 203.29 -X FOR EXAMPLE, IN THE RIGHT FIGURE, PATIENT ONE AND PATIENT TWO ARE SIMILAR PATIENTS BECAUSE THEY HAVE MANY COMMON PREVIOUS DIAGNOSIS. IT IS HIGHLY POSSIBLE THAT PATIENT ONE WILL
utt_0037 utt 203.29 206.93 -X BE DIAGNOSED AN UNSEEN DISEASE BUT OCCURRED IN PATIENT TWO.
utt_0038 utt 208.59 216.44 -X IN ADDITION, UNSTRUCTURED TEXT LIKE CLINICAL NOTES IN EHR DATA ALSO CONTAIN VALUABLE INFORMATION.
utt_0039 utt 216.44 218.97 -X PLEASE SEE THE FOLLOWING SNIPPET OF ONE CLINICAL NOTE:
utt_0040 utt 220.18 226.82 -X IN THIS NOTE, WE CAN INFER THAT THIS PATIENT MAY HAVE A HISTORY OF RESPIRATORY PROBLEMS AND HYPERTENSION
utt_0042 utt 228.21 235.00 -X THEREFORE, WE ALSO WANT TO INCORPORATE CLINICAL NOTES TO IMPROVE THE PREDICTION ACCURACY AND INTERPRETABILITY.
utt_0044 utt 235.00 248.37 -X TO ADDRESS THESE CHALLENGES, WE PROPOSED CGL, A COLLABORATIVE GRAPH LEARNING MODEL WITH DOMAIN KNOWLEDGE AND AUXILIARY TEXT HERE IS THE MODEL OVERVIEW OF CGL.
utt_0046 utt 248.37 253.75 -X THE FIRST IS LEARNING DOMAIN KNOWLEDGE WITH HIERARCHICAL EMBEDDING.
utt_0047 utt 254.55 260.21 -X THEN WE CREATE A COLLABORATIVE GRAPH FOR PATIENTS AND DISEASES TO LEARN THE HIDDEN RELATIONSHIP.
utt_0048 utt 262.19 266.77 -X NEXT, WE USE AN RNN WITH ATTENTION METHOD TO LEARN TEMPORAL FEATURES FOR DIAGNOSES.
utt_0049 utt 267.63 272.85 -X FINALLY, WE PROPOSE A TF-IDF-RECTIFIED ATTENTION METHOD FOR CLINICAL NOTES.
utt_0050 utt 274.19 284.50 -X HERE ARE THE NOTATIONS USED IN THE CGL MODEL, INCLUDING THE PATIENT SET U, MEDICAL CODES C, WORDS IN CLINICAL NOTES OMEGA, AND N.
utt_0052 utt 285.27 288.95 -X FOR THE T-TH VISIT, IT INCLUDES A SUBSET OF C AND N.
utt_0053 utt 290.61 301.50 -X FOR THE HIERARCHICAL EMBEDDING METHOD, THE OUTPUT IS AN EMBEDDING MATRIX FOR ALL MEDICAL CODES E, HERE, K IS THE LEVEL NUMBER, AND D_C IS THE EMBEDDING SIZE.
utt_0055 utt 302.39 307.90 -X FOR A LEAF NODE, WE CONCATENATE EMBEDDINGS OF EACH LEVEL.
utt_0056 utt 307.90 317.11 -X HERE, WE DON‚ÄôT USE ATTENTION-BASED METHOD BECAUSE WE DON‚ÄôT FOCUS ON THE ANCESTOR-CHILDREN RELATIONSHIP. IF A PATIENT IS DIAGNOSED WITH A HIGHER LEVEL
utt_0058 utt 317.11 327.58 -X DISEASE, THE LEVEL NUMBER IS DIFFERENT FROM OTHER LEAF NODES. IN THIS CASE, WE CREATE VIRTUAL LEAF NODES FOR THESE HIGHER-LEVEL NODES TO MAKE SURE THEY ARE IN THE SAME LEVEL.
utt_0060 utt 329.46 336.57 -X THEN, WE CREATE A COLLABORATIVE GRAPH TO EXPLORE THE PATIENT SIMILARITY AND DISEASE HIDDEN RELATIONSHIP AS MENTIONED BEFORE.
utt_0062 utt 337.52 342.30 -X THE GRAPH CONTAINS A PATIENT DISEASE OBSERVATION GRAPH AND A DISEASE ONTOLOGY GRAPH.
utt_0063 utt 342.30 347.70 -X FOR THE PATIENT DISEASE OBSERVATION GRAPH, WE CREATE IT BASED ON EHR DATA.
utt_0064 utt 348.76 357.08 -X THE NODES ARE PATIENTS OR MEDICAL CODES. IF A PATIENT HAS BEEN DIAGNOSED WITH A MEDICAL CODE, WE CREATE AN EDGE FOR THIS PATIENT AND MEDICAL CODE.
utt_0066 utt 357.53 360.95 -X THIS GRAPH IS REPRESENTED BY AN ADJACENCY MATRIX ùíú_ùí∞ùíû
utt_0067 utt 362.93 368.03 -X IN THIS WAY, PATIENTS WITH SIMILAR DIAGNOSES WILL HAVE SIMILAR CONNECTIONS.
utt_0068 utt 369.46 374.52 -X FOR THE DISEASE ONTOLOGY GRAPH, WE CREATE IT BASED ON THE ICD nine STRUCTURE
utt_0069 utt 375.13 377.63 -X THE NODES OF THIS GRAPH ARE MEDICAL CODES.
utt_0070 utt 378.58 386.81 -X TO MODEL THE HORIZONTAL RELATIONSHIP, WE CREATE WEIGHTED EDGES IN THIS GRAPH BASED ON THE LOWEST COMMON ANCESTOR OF TWO MEDICAL CODES.
utt_0072 utt 387.96 393.63 -X FOR EXAMPLE, THE RIGHT TWO NODES ARE CONNECTED IN A NODE IN LEVEL two, AND THE WEIGHT IS two.
utt_0073 utt 394.77 399.74 -X AND THE LEFT TWO NODES ARE CONNECTED IN A NODE IN LEVEL one. THEREFORE, THE WEIGHT IS one.
utt_0075 utt 400.82 404.31 -X THIS GRAPH CAN BE REPRESENTED BY AN ADJACENCY MATRIX ùíú_ùíûùíû.
utt_0076 utt 404.44 411.64 -X THEN WE USE A GRAPH NEURAL NETWORK TO LEARN THE HIDDEN RELATIONSHIP AMONG PATIENTS AND MEDICAL CODES.
utt_0078 utt 412.47 425.02 -X THE INPUT OF THIS GRAPH NEURAL NETWORK IS A RANDOM INITIALIZED PATIENT EMBEDDING MATRIX P, THE MEDICAL CODE EMBEDDING MATRIX E WHICH IS CALCULATED BY THE HIERARCHICAL EMBEDDING, THE TWO GRAPHS A_UC AND A_CC.
utt_0081 utt 425.02 431.58 -X AND THE GRAPH LAYER NUMBER L IN ADDITION, IN THE FIRST LAYER OF GNN, WE
utt_0082 utt 433.49 436.06 -X USE H_P_zero AND H_C_zero TO DENOTE P AND E.
utt_0083 utt 437.43 444.38 -X THE OUTPUT OF THIS GNN IS THE FINAL PATIENT EMBEDDING MATRIX H_P AND THE FINAL MEDICAL CODE EMBEDDING MATRIX H_C
utt_0085 utt 445.91 452.64 -X IN EACH GRAPH LAYER, WE FIRST AGGREGATE NEIGHBORS OF PATIENTS IN THE OBSERVATION GRAPH, WE USE
utt_0086 utt 452.92 456.86 -X THE ADJACENCY MATRIX A_UC TO MULTIPLY MEDICAL CODE EMBEDDING H_C.
utt_0087 utt 458.01 461.40 -X AND PROJECT TO THE DIMENSION OF PATIENT WITH A WEIGHT W_CU.
utt_0088 utt 463.00 468.86 -X THEN WE AGGREGATE THE NEIGHBORS OF PATIENTS, WHICH ARE MEDICAL CODES AND GET AN INTERMEDIATE VARIABLE Z_P
utt_0090 utt 470.26 481.92 -X THEN WE AGGREGATE NEIGHBORS OF MEDICAL CODES IN THE ONTOLOGY GRAPH, WE FIRST DEFINE AN ONTOLOGY WEIGHT PHI_K FOR A MEDICAL CODE C_J TO NORMALIZE THE INFLUENCE OF C_J ON OTHER
utt_0092 utt 481.92 492.00 -X MEDICAL CODES CONNECTED IN LEVEL K, HERE, MIU_J AND THETA_J ARE TWO TRAINABLE VARIABLES FOR C_J, AND SIGMA IS THE SIGMOID FUNCTION.
utt_0094 utt 493.05 498.17 -X THE SECOND EQUATION IS FOR THE MATRIX CALCULATION USING THE ADJACENCY MATRIX A_CC AND WE GET
utt_0095 utt 499.77 502.49 -X AN ONTOLOGY WEIGHT MATRIX FOR ALL MEDICAL CODES.
utt_0096 utt 503.64 510.19 -X AND THEN WE AGGREGATE THE NEIGHBORS OF MEDICAL CODES WHICH ARE ALSO MEDICAL CODES, BY MULTIPLYING PHI AND H_C
utt_0098 utt 511.67 520.73 -X FINALLY, WE USE THE TRANSPOSE OF THE ADJACENCY MATRIX A_UC TO MULTIPLY PATIENT EMBEDDING H_P AND PROJECT TO THE DIMENSION OF MEDICAL CODES
utt_0100 utt 520.92 530.38 -X WITH A WEIGHT W_UC. THEN WE AGGREGATE THE NEIGHBORS OF MEDICAL CODES WHICH ARE PATIENTS, AND GET AN INTERMEDIATE VARIABLE Z_C FOR MEDICAL CODES
utt_0102 utt 531.64 541.28 -X IN THE NEXT STEP, WE USE A DENSE LAYER WITH RELU FUNCTION AND APPLY A BATCH NORMALIZATION TO CALCULATE H_P AND H_C IN THE NEXT LAYER.
utt_0104 utt 542.07 546.85 -X IN THE LAST LAYER, WE USE H_C_L AS THE FINAL MEDICAL CODE EMBEDDING H_C
utt_0105 utt 548.47 557.25 -X AND USE H_P_Lminus one AS THE FINAL PATIENT EMBEDDING. BECAUSE IN THE FOLLOWING CALCULATIONS FOR VISITS, WE ONLY USE H_C AND H_P_L IS NOT USED.
utt_0107 utt 559.67 564.03 -X THEN, WE USE AN RNN WITH ATTENTION METHOD FOR TEMPORAL LEARNING FOR VISITS.
utt_0108 utt 564.44 569.92 -X THE INPUTS ARE VISIT SEQUENCES. THEY INCLUDE THE MEDICAL CODES RECORD OF A PATIENT.
utt_0109 utt 570.59 576.57 -X ANOTHER INPUT IS THE MEDICAL CODE EMBEDDING MATRIX H_C CALCULATED BY GRAPH LEARNING.
utt_0110 utt 576.92 580.32 -X THE OUTPUT IS AN EMBEDDING VECTOR O_V FOR THIS PATIENT.
utt_0111 utt 581.18 587.07 -X FIRSTLY, WE CALCULATE AN AVERAGE OF THE EMBEDDING VECTORS OF MEDICAL CODES IN EACH VISIT.
utt_0112 utt 588.09 599.04 -X THEN WE APPLY GRU TO THE VISIT EMBEDDING AND USE AN ATTENTION METHOD TO CALCULATE THE WEIGHT OF EACH VISIT. FINALLY, THE EMBEDDING O_V IS THE WEIGHTED SUM OF ALL VISITS.
utt_0115 utt 600.22 605.47 -X AS WE MENTIONED BEFORE, CLINICAL NOTES ALSO CONTAIN VALUABLE INFORMATION FOR PREDICTIONS.
utt_0116 utt 606.04 610.88 -X THEREFORE, WE WANT TO HIGHLIGHT IMPORTANT WORDS RELATED TO DIAGNOSES IN A CLINICAL NOTE.
utt_0117 utt 611.96 617.02 -X BY INTRODUCING TF-IDF WEIGHT, WE CAN PROVIDE BETTER INTERPRETABILITY FOR ATTENTION WEIGHT.
utt_0118 utt 618.24 625.73 -X THE INPUT SAMPLE IS A NOTE IN THE LAST VISIT. WE THINK THAT IT CONTAINS THE MEDICAL HISTORY AND FUTURE PLAN FOR PATIENTS.
utt_0120 utt 626.49 630.11 -X FIRSTLY, WE CALCULATE TF-IDF WEIGHT ùõΩ_ùëñ FOR EACH WORD IN A NOTE.
utt_0121 utt 632.06 645.70 -X THEN WE CALCULATE ATTENTION WEIGHT ùõº_ùëñ FOR EACH WORD USING WORD EMBEDDINGS AND VISIT OUTPUT ùê®_ùë£ AS CONTEXT. FINALLY, WE DESIGNED THE FOLLOWING PENALTY FUNCTION TO LET TF-IDF WEIGHTS GUIDE THE ATTENTION WEIGHT
utt_0124 utt 645.70 651.71 -X SO THAT IMPORTANT WORDS CAN BE SIMULTANEOUSLY DETECTED BY TF-IDF WEIGHT AND ATTENTION METHOD.
utt_0125 utt 652.70 658.08 -X AFTER CALCULATING THE OUTPUT O_V FOR MEDICAL CODES AND THE OUTPUT O_N FOR CLINICAL NOTES,
utt_0126 utt 659.04 661.60 -X THE MODEL OUTPUT O IS THE CONCATENATION OF THEM
utt_0127 utt 663.10 673.70 -X FINALLY, WE USE THE COMBINATION OF TF-IDF-RECTIFIED ATTENTION LOSS AND CROSS-ENTROPY LOSS TO TRAIN THE MODEL. HERE LAMBDA IS A COEFFICIENT TO ADJUST THE WEIGHT OF ATTENTION LOSS.
utt_0129 utt 675.04 678.53 -X HERE ARE THE DATASET AND TASKS USED IN THIS WORK.
utt_0130 utt 679.13 689.31 -X WE USE A COMMON PUBLIC EHR DATASET NAMED MIMIC-III AND VALIDATE OUR MODEL ON TWO TASKS, DIAGNOSIS PREDICTION AND HEART FAILURE PREDICTION.
utt_0132 utt 689.31 695.01 -X WE COMPARE OUR CGL MODEL WITH VARIOUS STATE-OF-THE-ART BASELINES AND A MODEL ONLY USE CLINICAL NOTES.
utt_0133 utt 695.68 699.14 -X HERE ARE THE RESULTS OF DIAGNOSIS AND HEART FAILURE PREDICTION.
utt_0134 utt 699.87 705.38 -X THE EVALUATION METRICS FOR DIAGNOSIS PREDICTION ARE WEIGHTED F_one SCORE AND TOP K RECALL.
utt_0135 utt 706.01 709.89 -X THE EVALUATION METRICS FOR HEART FAILURE PREDICTION ARE AUC AND F_one.
utt_0136 utt 710.65 714.08 -X WE CAN SEE THAT OUR CGL MODEL HAS THE BEST PERFORMANCE.
utt_0137 utt 715.65 722.31 -X WE ALSO DEMONSTRATE AN ABLATION STUDY BY REMOVING HIERARCHICAL EMBEDDING, CLINICAL NOTES, AND ONTOLOGY WEIGHTS.
utt_0139 utt 722.31 726.75 -X WE SEE THAT ALL THE REMOVED PARTS CONTRIBUTE TO THE PREDICTION PERFORMANCE.
utt_0140 utt 727.97 732.16 -X WE THEN GIVE AN ANALYSIS FOR THE PREDICTION OF NEW-ONSET DISEASES.
utt_0141 utt 733.21 745.22 -X WE DIVIDE THE GROUND TRUTH OF DIAGNOSIS PREDICTION INTO TWO PARTS, OCCURRED AND NEW-ONSET. OCCURRED MEANS FUTURE DIAGNOSES HAVE OCCURRED IN PREVIOUS VISITS, WHILE NEW-ONSET MEANS NOT.
utt_0143 utt 746.27 752.87 -X WE USE CGL WITHOUT NOTES IN THIS EXPERIMENT FOR A FAIR COMPARISON WITH OTHER BASELINES.
utt_0144 utt 753.47 765.19 -X WE CAN SEE THAT OUR MODEL IS BETTER IN BOTH OCCURRED AND NEW-ONSET. IT VERIFIES THAT OUR PROPOSED COLLABORATIVE GRAPH LEARNING IS ABLE TO LEARN FROM SIMILAR PATIENTS AND PREDICT
utt_0146 utt 765.25 767.11 -X NEW-ONSET DISEASES IN THE FUTURE.
utt_0147 utt 768.51 774.15 -X HERE ARE THE LEARNED REPRESENTATIONS OF MEDICAL CODES FROM GRAM, TIMELINE, AND CGL. THE COLORS
utt_0148 utt 775.49 784.29 -X REPRESENT DISEASE CATEGORIES IN EACH LEVEL. WE CAN SEE FROM THE FIGURE THAT THE REPRESENTATION OF TIMELINE IS LIKE A RANDOM DISTRIBUTION.
utt_0150 utt 784.90 795.28 -X THE REPRESENTATION LEARNED BY GRAM HAVE LARGE INTER-CLUSTER DISTANCE AND SMALL INTRA-CLUSTER DISTANCE COMPARED TO CGL. WE MAY INFER THAT ALTHOUGH HIGHER-LEVEL DISEASES
utt_0152 utt 795.28 801.67 -X OF GRAM DISTRIBUTE INTO MULTIPLE CLUSTERS, THE LOWER-LEVEL DISEASES CANNOT BE WELL DISTINGUISHED BY GRAM.
utt_0154 utt 801.67 805.80 -X THIS FIGURE SHOWS A CASE STUDY OF CLINICAL NOTES.
utt_0155 utt 806.21 819.19 -X WE TRY TO DETECT IMPORTANT WORDS RELATED TO PREDICTIONS AND UNIMPORTANT WORDS THAT HAS NO CONTRIBUTIONS BASED ON ATTENTION WEIGHT. THE PINK COLOR DENOTES IMPORTANT WORDS AND GRAY COLOR MEANS UNIMPORTANT WORDS
utt_0158 utt 819.78 833.54 -X WE CAN SEE THAT SOME IMPORTANT WORDS SUCH AS ACUTE AND RESPIRATORY ARE NOT DETECTED WITHOUT PENALTY. IN ADDITION, WITHOUT PENALTY SOME UNIMPORTANT WORDS LIKE PATIENT AND DIAGNOSIS HAVE HIGH ATTENTION WEIGHT.
utt_0161 utt 835.52 845.54 -X THEREFORE, WE CAN CONCLUDE THAT OUR DESIGNED TF-IDF-RECTIFIED ATTENTION CAN HELP TO PROVIDE INTERPRETABILITY BY DETECTING IMPORTANT WORDS RELATED TO PREDICTIONS.
utt_0163 utt 846.18 851.91 -X FINALLY, IN THIS PAPER, WE PROPOSE TO COLLABORATIVELY LEARN REPRESENTATIONS FOR PATIENTS AND DISEASES
utt_0164 utt 852.03 856.42 -X BY CONSTRUCTING AN OBSERVATION GRAPH AND ONTOLOGY GRAPH.
utt_0165 utt 856.42 864.20 -X WE ALSO DESIGN A TF-IDF-RECTIFIED ATTENTION METHOD FOR CLINICAL NOTES. DETAILED ANALYSES ARE ALSO PROVIDED FOR MODEL PREDICTIONS.
utt_0167 utt 864.71 869.09 -4.0721 THAT‚ÄôS FOR TODAY‚ÄôS REPORT OF OUR WORK. THANK YOU VERY MUCH FOR YOUR LISTENING.
