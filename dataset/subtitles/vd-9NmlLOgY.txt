utt_0000 utt 0.30 6.22 -X HEY, THIS IS XIUMING ZHANG FROM MIT CSAIL. TODAY I'M EXCITED TO PRESENT NEURAL LIGHT TRANSPORT
utt_0001 utt 6.32 13.50 -X FOR RELIGHTING AND VIEW SYNTHESIS. RELIGHTING HAS A LOT OF APPLICATIONS, FOR INSTANCE, IN VR AND AR
utt_0002 utt 13.58 21.36 -X OR POST-CAPTURE PHOTO ENHANCEMENT. HOWEVER, RELIGHTING IS HARD. IT'S HARD IN TWO ASPECTS:
utt_0003 utt 21.36 26.86 -X WHAT I CALL THE LOCAL ASPECT AND THE NON-LOCAL ASPECT. LOCALLY,
utt_0004 utt 26.86 33.04 -X THE GEOMETRY HAS TO BE GOOD BECAUSE OTHERWISE BUMPY NORMALS WOULD LEAD TO NOISY RENDERING.
utt_0005 utt 33.36 44.53 -X BESIDES GEOMETRY, THE REFLECTANCE IS OFTEN SPATIALLY VARYING, SO WE NEED DIFFERENT BRDFS AT DIFFERENT POINTS IN THE SCENE. FOR INSTANCE, THE BLUE AND YELLOW DOTS SHOWN HERE
utt_0007 utt 44.68 50.54 -X ARE SPATIALLY VERY CLOSE, BUT THE HOTDOG BUN REFLECTANCE IS VERY DIFFERENT FROM THE KETCHUP
utt_0008 utt 50.99 60.82 -X REFLECTANCE. ON THE NON-LOCAL ASPECT, SHADOWS ARE HARD TO MODEL AS A GLOBAL LIGHT TRANSPORT EFFECT.
utt_0009 utt 60.82 73.78 -X FOR INSTANCE, THE PIXEL POINTED BY THE BLUE ARROW THERE IS IN SHADOW BECAUSE THE FARAWAY HOTDOG BUN IS CASTING SHADOWS THERE. AND OF COURSE, WE ALSO NEED TO CONSIDER GLOBAL ILLUMINATION.
utt_0011 utt 73.93 81.71 -X IN THIS EXAMPLE, THE LAMBERTIAN GREEN WALL IS CASTING GREEN TINT ONTO THE WHITE DRAGON.
utt_0012 utt 82.57 86.90 -X IN THIS WORK, WE PRESENT NEURAL LIGHT TRANSPORT FOR RELIGHTING AND VIEW SYNTHESIS
utt_0013 utt 86.93 93.46 -X OR NLT FOR SHORT. IT'S A UNIFIED FRAMEWORK FOR DIRECTIONAL OR IMAGE-BASED RELIGHTING,
utt_0014 utt 93.46 98.45 -X VIEW SYNTHESIS, OR BOTH TASKS AT THE SAME TIME.
utt_0015 utt 98.45 111.06 -X LET'S FRAME THE PROBLEM OF FIGURING OUT THE RADIANS AT LOCATION X AS THAT OF INTERPOLATING A LIGHT TRANSPORT FUNCTION, WHERE X IS THE LOCATION ON THE SURFACE (TWO DEGREES OF FREEDOM),
utt_0017 utt 111.06 115.25 -X OMEGA_I THE INCOMING LIGHT DIRECTION (ANOTHER TWO DEGREES OF FREEDOM),
utt_0018 utt 115.25 129.49 -X AND OMEGA_O THE VIEWING DIRECTION (THE FINAL TWO DEGREES OF FREEDOM). SO THIS IS A sixD FUNCTION THAT RETURNS RGB COLORS. IF WE HAVE ACCESS TO F AND QUERY IT AT NOVEL OMEGA_O,
utt_0020 utt 129.61 135.41 -X WE CAN SYNTHESIZE NOVEL VIEWS OF THE SCENE. NEURAL VOLUMES IS ONE PAPER DOING THAT,
utt_0021 utt 135.41 142.99 -X AND IT DOESN'T CHANGE LIGHTING. IF WE QUERY F AND NOVEL OMEGA_I, WE CAN RELIGHT THE SCENE.
utt_0022 utt 143.28 156.31 -X LIGHT STAGE SUPER-RESOLUTION TACKLES THIS PROBLEM, AND THE VIEWPOINT IS FIXED. IF WE WANT TO CHANGE BOTH VIEWS AND LIGHTING, WE CAN FIRST SYNTHESIZE ONE NOVEL VIEW OF THE SCENE AND
utt_0024 utt 156.31 163.73 -X THEN RELIGHT THAT VIEW. DEEP VIEW SYNTHESIS IS ONE SUCH APPROACH THAT ADOPTS THIS TWO-STEP PROCEDURE.
utt_0025 utt 165.49 171.36 -X DUAL PHOTOGRAPHY DEMONSTRATES HOW TO SYNTHESIZE THE SCENE'S APPEARANCE FROM THE PROJECTOR'S VIEW
utt_0026 utt 171.79 175.89 -X POINTING OUT THAT CAMERAS THE LIGHTS ARE REALLY NO DIFFERENT.
utt_0027 utt 175.89 181.08 -X PARTIALLY INSPIRED BY IT, WE TREAT CAMERAS AND LIGHTS EQUALLY IN THE WORK
utt_0028 utt 181.42 189.56 -X AND BUILD A UNIFIED FRAMEWORK FOR VIEW SYNTHESIS AND RELIGHTING.
utt_0029 utt 190.29 204.63 -X SO WE CAPTURED OUR DATA USING A LIGHT STAGE WITH ABOUT three hundred and thirty LIGHTS AND fifty-five CAMERAS. THE LIGHTS ARE TURNED ON ONE-LIGHT-AT-A-TIME, OR OLAT, WITH THE PERSON SITTING STILL IN THE LIGHT STAGE.
utt_0031 utt 205.30 211.35 -X SO THESE CAPTURED IMAGES ARE ESSENTIALLY SLICES OR SAMPLES OF THE sixD LIGHT TRANSPORT FUNCTION F.
utt_0032 utt 213.10 216.28 -X NOW IF WE INTERPOLATE BETWEEN THE PHYSICAL LIGHTS,
utt_0033 utt 216.78 220.56 -X WE ARE RELIGHTING THE SCENE WITH NON-EXISTENT VIRTUAL LIGHTS.
utt_0034 utt 222.67 224.88 -X OR IF BETWEEN PHYSICAL CAMERAS,
utt_0035 utt 224.88 230.10 -X WE ARE SYNTHESIZING NOVEL VIEWS OF THE SCENE FROM NON-EXISTENT VIRTUAL CAMERAS.
utt_0036 utt 231.60 243.96 -X DOING BOTH AT THE SAME TIME ALLOWS US TO SUPER-RESOLVE THE LIGHT STAGE IN TERMS OF BOTH CAMERAS AND LIGHTS. THE RENDER EQUATION RESPONSIBLE FOR WHAT THE PHYSICAL CAMERAS
utt_0038 utt 243.96 251.03 -X SEE IS THIS, WHERE F_S IS THE sixD LIGHT TRANSPORT FUNCTION THAT WE HAVE ALREADY DISCUSSED, L_I
utt_0039 utt 251.66 258.20 -X IS THE INCOMING LIGHT DIRECTION MASKED BY LIGHT VISIBILITY, AND THE REST IS JUST THE COSINE TERM.
utt_0040 utt 261.04 273.94 -X FROM THE LIGHT STAGE DATA, WE CAN COMPUTE THE DIFFUSE RENDER ASSUMING LAMBERTIAN. IT IS THE PRODUCT OF ALBEDO RHO, INCOMING LIGHT L_I, AND THE COSINE TERM. HOW DO WE COMPUTE
utt_0042 utt 273.94 286.97 -X ALBEDO FOR A REAL CAPTURE? WE'LL COME BACK TO THAT LATER. NOTE THAT THE INCOMING LIGHT L_I HERE HAS BEEN MASKED BY LIGHT VISIBILITY, AND THIS ESSENTIALLY ADDS HARD SHADOWS TO OUR DIFFUSE BASE.
utt_0044 utt 287.60 294.36 -X TO COMPUTE THE LIGHT VISIBILITY, WE SIMPLY CAST RAYS FROM A LIGHT TO THE GEOMETRY PROXY,
utt_0045 utt 294.51 300.41 -X AND THE GEOMETRY IS GIVEN BY THE LIGHT STAGE'S ACTIVE SCANNING AND MAY BE NOISY.
utt_0046 utt 301.75 315.72 -X THE NLT NETWORK AIMS TO BRIDGE THE GAP BETWEEN THE FULL RENDER AND THE LAMBERTIAN BASE BY PREDICTING NON-DIFFUSE RESIDUALS. THESE NON-DIFFUSE RESIDUALS INCLUDE SPECULARITIES AND GLOBAL ILLUMINATION AND
utt_0048 utt 315.72 329.01 -X MANY MORE. THE BENEFITS OF PREDICTING RESIDUALS RATHER THAN DIRECTLY PREDICTING THE FINAL RGB COLOR IS THAT THE DIFFUSE RENDER ALREADY PROVIDES BASE COLORS AND PHYSICALLY-CORRECT
utt_0050 utt 329.33 340.85 -X HARD SHADOWS SO THAT THE NETWORK CAN FOCUS ON LEARNING MORE COMPLICATED HIGHER-ORDER LIGHT TRANSPORT EFFECTS, SUCH AS SPECULARITIES AND GLOBAL ILLUMINATION.
utt_0052 utt 342.74 349.88 -X WE EMBED THE FUNCTION INTO THE UV SPACE COMPUTED FROM THE GEOMETRY PROXY WE PREVIOUSLY MENTIONED.
utt_0053 utt 349.88 362.55 -X USING THE UV SPACE IS KEY TO HANDLING VIEWPOINT CHANGES. WE PROVIDE LIGHT AND VIEWING DIRECTIONS IN THE FORM OF COSINE MAPS, WHERE WE DOT-PRODUCT THE DIRECTIONS WITH THE SURFACE NORMALS.
utt_0055 utt 362.99 369.85 -X AGAIN, THE GEOMETRY PROXY IS FROM THE LIGHT STAGE'S ACTIVE SCANNING AND CAN BE VERY NOISY.
utt_0056 utt 372.15 386.94 -X WE THEN ASK THE NETWORK TO PREDICT THE RGB VALUES FOR EACH TEXEL. THE PREDICTED TEXTURE MAP GETS THEN RESAMPLED BACK TO THE CAMERA'S PERSPECTIVE AND COMPARED AGAINST THE GROUND
utt_0058 utt 386.94 393.30 -X TRUTH CAPTURED IMAGE. WE ALSO FEED IN A PHYSICALLY-CORRECT DIFFUSE BASE RENDERING
utt_0059 utt 393.59 400.06 -X SO THAT THE NETWORK DOES NOT NEED TO LEARN THE ALBEDO AND HARD SHADOWS FROM SCRATCH.
utt_0060 utt 400.50 413.13 -X TO COMPUTE THE DIFFUSE BASE, WE FIRST ADD UP ALL OF THE OLAT IMAGES TO SIMULATE TURNING ALL OF THE LIGHTS ON ON THE LIGHT STAGE, WHICH GIVES US APPROXIMATED ALBEDO. WE THEN MODULATE
utt_0062 utt 413.24 420.15 -X THE FULLY LIT IMAGE WITH THE LIGHT COSINES AND MASK IT WITH BOTH VIEW AND LIGHT VISIBILITIES.
utt_0063 utt 421.56 425.40 -X VISIBILITIES ARE COMPUTED USING THE GEOMETRY PROXY,
utt_0064 utt 425.91 432.09 -X AND THE LIGHT VISIBILITY ESSENTIALLY ADDS HARD SHADOWS TO OUR DIFFUSE BASE RENDERING.
utt_0065 utt 432.56 444.60 -X THERE IS ALSO A SKIP LINK FROM THE DIFFUSE BASE TO THE NETWORK'S PREDICTION TO EXPLICITLY TELL THE NETWORK TO FOCUS ON LEARNING HIGHER-ORDER LIGHT TRANSPORT EFFECTS, SUCH AS SPECULARITIES
utt_0067 utt 444.60 456.54 -X AND GLOBAL ILLUMINATION. FINALLY, WE HAVE AN OBSERVATION PATH TO ADDITIONALLY ENCODE WHAT THE NON-DIFFUSE RESIDUALS LOOK LIKE FOR NEIGHBORING OBSERVED LIGHTS AND VIEWS.
utt_0069 utt 459.09 464.28 -X SUCH OBSERVATION FEATURES GET AVERAGED AND CONCATENATED BACK TO THE QUERY PATH
utt_0070 utt 464.31 471.67 -X FOR IT TO DECODE THE QUERY-VIEW RESIDUALS. BECAUSE THE OBSERVATION FEATURE MAPS GET AVERAGED,
utt_0071 utt 471.80 476.18 -X THE NETWORK CAN HANDLE AN ARBITRARY NUMBER OF OBSERVATIONS.
utt_0072 utt 478.87 484.47 -X NOW LET'S LOOK AT SOME RESULTS WHERE WE RELIGHT THE PERSON WITH A DIRECTIONAL LIGHT,
utt_0073 utt 484.47 490.87 -X AND THE VIEWPOINT REMAINS UNCHANGED IN THIS CASE. THESE ARE THE LIGHT DIRECTIONS
utt_0074 utt 490.97 497.05 -X AND THE CORRESPONDING IMAGES USED FOR TRAINING. NOTICE THAT HERE THE VIEWPOINT NEVER CHANGES.
utt_0075 utt 499.99 503.79 -X OUR PREDICTION ADDS SPECULARITIES AND GLOBAL ILLUMINATIONS
utt_0076 utt 503.96 511.77 -X TO THE DIFFUSE BASE, AS INDICATED BY THE BLUE AND YELLOW ARROWS, RESPECTIVELY.
utt_0077 utt 512.53 519.23 -X IT ALSO COMPENSATES FOR GEOMETRIC ACCURACY VISIBLE IN THE DIFFUSE BASE, INDICATED BY THE GREEN ARROW.
utt_0078 utt 519.32 527.58 -X THIS IS WHY I SAID WE NEED ONLY A GEOMETRY PROXY RATHER THAN THE ACTUAL PERFECT GEOMETRY.
utt_0079 utt 527.77 531.13 -X IT IS ALSO MORE THAN JUST COPYING FROM THE NEAREST NEIGHBOR,
utt_0080 utt 531.13 539.00 -X AS SEEN FROM DIFFERENT SHADOW BOUNDARIES INDICATED BY THE YELLOW ARROWS HERE.
utt_0081 utt 539.00 543.07 -X HERE ARE MORE DIRECTIONAL RELIGHTING RESULTS. NOTICE THE LIGHT-DEPENDENT EFFECTS
utt_0082 utt 543.07 554.78 -X SUCH AS FACIAL SPECULARITIES AND THE SPECULARITIES ON THE LADY'S JACKET.
utt_0083 utt 555.29 557.24 -X WE ALSO HAVE CAST SHADOWS.
utt_0084 utt 572.63 578.17 -X BY LINEARLY COMBINING THE DIRECTIONAL RELIGHTING RESULTS, WE CAN RELIGHT THE PERSON ACCORDING TO
utt_0085 utt 578.17 613.31 -X ANY ARBITRARY LIGHT PROBE. HERE ARE SOME RESULTS FOR RELIGHTING THE PERSON WITH A LIGHT PROBE.
utt_0086 utt 613.72 626.78 -X WE CAN ALSO SYNTHESIZE NOVEL VIEWS OF THE SCENE WITH VIEW-DEPENDENT EFFECTS. HERE ARE THE CAMERA LOCATIONS AND THE CORRESPONDING IMAGES USED FOR TRAINING. NOTICE THAT HERE THE LIGHTING DOESN'T CHANGE.
utt_0089 utt 629.02 633.40 -X NOW WE ARE RENDERING THE PERSON WITH AN ARBITRARY SMOOTH CAMERA PATH.
utt_0090 utt 633.69 645.60 -X AS SEEN, IT DOESN'T WORK TO JUST RENDER THE CAPTURED GEOMETRY USING A LAMBERTIAN BRDF OR JUST COPY FROM THE NEAREST NEIGHBOR.
utt_0092 utt 645.60 651.96 -X HERE ARE MORE VIEW SYNTHESIS RESULTS. NOTICE THE VIEW-DEPENDENT EFFECTS SUCH AS FACIAL SPECULARITIES.
utt_0094 utt 662.20 664.60 -X SOME MORE RESULTS ON VIEW SYNTHESIS.
utt_0095 utt 678.71 683.93 -X WE COMPARE OUR METHOD WITH NERF, A STATE-OF-THE-ART VIEW SYNTHESIS MODEL.
utt_0096 utt 684.09 694.59 -X IT HAS THE BENEFIT OF NOT REQUIRING ANY GEOMETRY, BUT IT DOESN'T SUPPORT RELIGHTING. NEXT, I'M GOING TO FLIP BACK AND FORTH BETWEEN OUR RESULTS AND NERF'S RESULTS.
utt_0098 utt 717.15 721.98 -X WE ALSO COMPARE OUR METHOD WITH DEFERRED NEURAL RENDERING IN VIEW SYNTHESIS.
utt_0099 utt 732.83 752.64 -X FLIPPING BACK AND FORTH.
utt_0100 utt 769.24 774.56 -X WE SHOW HOW OUR MODEL SIMULTANEOUSLY RELIGHTS AND SYNTHESIZES NOVEL VIEWS OF THE SCENE.
utt_0101 utt 775.64 785.21 -X HERE ARE THE CAMERAS AND LIGHTS, AND THEIR CORRESPONDING IMAGES USED FOR TRAINING. THESE DATA HAVE BOTH VIEWPOINT AND LIGHTING VARIATIONS.
utt_0103 utt 788.44 795.42 -X AT TEST TIME, THE LIGHT FIRST FOLLOWS THE CAMERA EXACTLY IN DOING A three hundred and sixty AND THEN DIVERGES INTO A
utt_0104 utt 795.42 805.44 -X SPIRAL PATH. HERE ARE SOME MORE RESULTS ON SIMULTANEOUS RELIGHTING AND VIEW SYNTHESIS.
utt_0105 utt 823.90 828.99 -X FINALLY, WE EXTEND NERF TO SUPPORT SIMULTANEOUS RELIGHTING AND VIEW SYNTHESIS,
utt_0106 utt 829.66 833.89 -X BY CONDITIONING ITS OUTPUT RADIANCE ALSO ON THE LIGHT DIRECTION,
utt_0107 utt 833.89 843.14 -X IN ADDITION TO THE ORIGINAL VIEWING DIRECTION. WE TRAINED THIS EXTENSION WITH THE SAME DATA THAT WE USED TO TRAIN NLT AND COMPARE THIS EXTENSION AGAINST NLT.
utt_0109 utt 846.08 857.76 -X BECAUSE, UNLIKE NLT, NERF + LIGHT LACKS PHYSICAL MODELING OF SHADOWS AND IS HAVING A HARD TIME SYNTHESIZING THE HARD SHADOWS IN THIS CASE, AS INDICATED BY THE
utt_0111 utt 857.76 863.74 -X RED ARROW. NEXT, I'LL FLIP BACK AND FORTH BETWEEN NERF + LIGHT AND NLT.
utt_0112 utt 872.25 884.54 -X TO SUMMARIZE, WE HAVE THREE CONTRIBUTIONS. FIRSTLY, WE'VE PRESENTED A UNIFIED FRAMEWORK FOR SIMULTANEOUS VIEW SYNTHESIS AND RELIGHTING, BY EMBEDDING NETWORKS INTO THE TEXTURE ATLAS.
utt_0114 utt 885.47 890.94 -X SECOND, THIS IS A RESIDUAL LEARNING SCHEME ON TOP OF A PHYSICALLY-CORRECT DIFFUSE BASE,
utt_0115 utt 891.16 897.06 -X WHICH TOGETHER ALLOW THE NETWORK TO EASILY LEARN NON-DIFFUSE HIGHER-ORDER LIGHT TRANSPORT EFFECTS.
utt_0116 utt 897.98 905.03 -X THIRD, THIS IS A SEMI-PARAMETRIC NEURAL METHOD FOR INTERPOLATING THE sixD LIGHT TRANSPORT FUNCTION.
utt_0117 utt 905.03 912.42 -4.9868 PLEASE CHECK OUT THE PROJECT PAGE FOR THE PAPER, TECHNICAL VIDEO, AND CODE. THANKS FOR WATCHING.
