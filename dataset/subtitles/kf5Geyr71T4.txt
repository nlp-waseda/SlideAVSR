utt_0000 utt 0.97 8.91 -X HELLO! MY NAME IS ANNA MEYER AND I WILL BE PRESENTING OUR PAPER CERTIFYING ROBUSTNESS TO PROGRAMMABLE DATA BIAS IN DECISION TREES,
utt_0002 utt 8.91 12.21 -X WHICH IS JOINT WORK WITH AWS ALBARGHOUTHI AND LORIS D'ANTONI.
utt_0003 utt 13.96 17.14 -X TO BEGIN, LET'S THINK ABOUT A MACHINE LEARNING PIPELINE.
utt_0004 utt 17.87 23.79 -X WE MIGHT BE INTERESTED IN QUESTIONS ABOUT THE MODEL SUCH AS IS IT FAIR?, IS IT ACCURATE?,
utt_0005 utt 23.79 29.30 -X IS IT TRUSTWORTHY? THESE ARE VALUABLE QUESTIONS AND HAVE BEEN STUDIED FROM A VARIETY OF ANGLES,
utt_0006 utt 29.48 34.19 -X BUT IN THIS PAPER WE TAKE A STEP BACK AND CONSIDER THE IMPACT THAT THE DATA SET HAS.
utt_0007 utt 34.73 41.23 -X WE KNOW THAT IF THE DATA SET HAS VARIOUS FLAWS (IF IT'S BIASED, INCOMPLETE OR NON-REPRESENTATIVE),
utt_0008 utt 41.23 45.78 -X WE WILL END UP WITH A MODEL THAT'S NOT AS ACCURATE OR FAIR AS IT MIGHT OTHERWISE BE.
utt_0009 utt 46.83 59.63 -X MOST OF THE TIME OUR DATA SET, IN FACT, WILL NOT BE PERFECT, SO IN THIS WORK WE AIM TO ANSWER THE QUESTION OF WHAT IS THE IMPACT OF BIAS TRAINING DATA ON THE MODEL'S PREDICTIONS?
utt_0011 utt 59.63 65.81 -X MORE SPECIFICALLY, WE WANT TO CERTIFY POINT-WISE ROBUSTNESS TO DATA BIAS IN THE TRAINING DATA. I'LL
utt_0012 utt 66.96 79.06 -X GET INTO WHAT THAT MEANS IN JUST A MOMENT, BUT FIRST I WANT TO HIGHLIGHT A FEW FORMS OF TRAINING-DATA BIAS TO HELP ILLUSTRATE WHAT I MEAN BY THAT AND ALSO THAT THIS IS A COMMON PROBLEM.
utt_0014 utt 80.37 86.10 -X WE FOCUS ON THREE MAIN TYPES OF DATA BIAS IN THIS PAPER. INCORRECT LABELS, WHICH MAY OCCUR,
utt_0015 utt 86.10 97.43 -X FOR EXAMPLE, DUE TO HISTORICAL BIASES LIKE WOMEN BEING MARKED AS NOT HIRED FOR A JOB EVEN THOUGH THEY WERE QUALIFIED. ALTERNATIVELY, DATA BIAS CAN BE IN THE FORM OF MISSING DATA, LIKE IF THE DATA
utt_0017 utt 97.43 110.10 -X CURATOR NEGLECTED TO COLLECT DATA FROM A MINORITY NEIGHBORHOOD. AND FINALLY, DATA BIAS CAN BE IN THE FORM OF FAKE DATA, WHICH COULD COME, FOR EXAMPLE, FROM FAKE ANSWERS SUBMITTED VIA CROWDSOURCING.
utt_0019 utt 110.13 121.40 -X OUR CONCEPTUAL MODEL ALSO ALLOWS FOR STACKING TYPES OF BIAS. FOR EXAMPLE, MAYBE WE THINK THAT ten LABELS IN A DATA SET ARE INCORRECT AND THAT WE ARE ALSO MISSING twenty DATA POINTS.
utt_0021 utt 122.38 126.32 -X FOR THE REST OF THIS TALK, WE'RE GOING TO FOCUS ON THE INCORRECT LABEL CASE,
utt_0022 utt 126.32 131.60 -X WHICH I MIGHT ALSO REFER TO AS LABEL FLIPPING.
utt_0023 utt 131.60 135.96 -X NOW, RETURNING TO OUR MACHINE LEARNING PIPELINE, WE'RE GOING TO END UP WITH SOME MODEL
utt_0024 utt 136.11 140.28 -X AND THEN WE CAN USE THAT MODEL TO GET A PREDICTION ON A NEW INPUT X.
utt_0025 utt 140.75 143.57 -X WE FRAME THE BIAS ROBUSTNESS PROBLEM AS FOLLOWS:
utt_0026 utt 144.43 156.92 -X FOR ALL DATASETS D' THAT ARE SIMILAR TO THE ORIGINAL DATASET D, WHERE SIMILARITY IS DEFINED AS, FOR EXAMPLE, DISAGREEING WITH D ON AT MOST N LABELS, WE WANT TO SHOW THAT THE MODEL TRAINED
utt_0028 utt 157.49 163.99 -X ON D' (F SUB D') OUTPUTS THE SAME PREDICTION FOR X AS THE MODEL TRADE ON TRAINED ON D (F SUB D).
utt_0029 utt 167.25 179.09 -X LET'S LOOK AT A SPECIFIC EXAMPLE. IN THIS DATA SET THERE ARE TWO FEATURES: CIRCLE COLOR AND CIRCLE VALUE, AND THEN THERE ARE TWO CLASSES THAT WE'RE TRYING TO PREDICT, EITHER RED X
utt_0031 utt 179.19 191.96 -X OR A GREEN CHECK MARK. THE NAÏVE BASELINE APPROACH IS TO ENUMERATE ALL THE DATA SETS THAT DIFFER FROM THE SOURCE DATASET BY UP TO THE NUMBER OF LABELS SPECIFIED. HERE WE'RE ASSUMING
utt_0033 utt 191.96 198.71 -X THAT WE CAN ONLY FLIP ONE LABEL. SO THE TOP ROW IS THE ORIGINAL DATASET, IN THE SECOND ROW WE
utt_0034 utt 198.71 206.97 -X FLIP CIRCLE zero, IN THE THIRD ROW WE FLIP THE LABEL OF CIRCLE one, AND SO ON. THEN, GIVEN OUR LIST OF
utt_0035 utt 206.97 214.07 -X ALL THE PERTURBED DATA SETS, WE CAN TRAIN A MODEL ON EACH ONE AND GET A SET OF PREDICTIONS FOR X,
utt_0036 utt 214.93 220.98 -X AS PREDICTED BY EACH INDIVIDUAL MODEL. IF ALL OF THESE PREDICTIONS AGREE, THEN X IS ROBUST,
utt_0037 utt 220.98 233.65 -X AND IF NOT THEN WE HAVE A COUNTER-EXAMPLE TO ROBUSTNESS. THE PROBLEM WITH THIS APPROACH IS THAT IT'S NOT SCALABLE. FOR EXAMPLE, IF WE HAVE A DATA SET WITH one,zero ELEMENTS AND one0 INCORRECT
utt_0039 utt 233.65 240.76 -X LABELS WE WOULD HAVE TO TRAIN A MODEL AND CLASSIFY THE INDIVIDUAL TEST SAMPLE ABOUT ten^twenty-three TIMES.
utt_0040 utt 244.08 258.01 -X SO TO RECAP, WE WANT TO PROVE BIAS-ROBUSTNESS FOR X, BUT IT'S NOT POSSIBLE TO DO THIS DIRECTLY OR NAÏVELY. INSTEAD, WE USE ABSTRACT INTERPRETATION, WHICH IS A TECHNIQUE FROM PROGRAM VERIFICATION.
utt_0042 utt 258.23 263.42 -X OUR GOAL HERE IS TO REPRESENT THE SOURCE DATA SET AND ALL OF ITS PERTURBATIONS COMPACTLY
utt_0043 utt 263.60 267.51 -X AND THEN RUN THAT INPUT THROUGH AN ABSTRACT VERSION OF OUR LEARNING ALGORITHM,
utt_0044 utt 268.40 282.68 -X WHERE IN RETURN WE'LL RECEIVE A LARGE SET OF MODELS, AGAIN REPRESENTED COMPACTLY. IN THIS PAPER WE ARE WORKING WITH DECISION TREES. SO TO RECAP, WE WANT TO PROVE BIAS ROBUSTNESS
utt_0046 utt 282.68 287.42 -X AND WE'RE GOING TO DO IT WITH ABSTRACT INTERPRETATION. TO MOTIVATE HOW WE DO THAT,
utt_0047 utt 287.42 291.90 -X I'M FIRST GOING TO TALK ABOUT BUILDING A DECISION TREE IN THE NORMAL NON-ABSTRACT CASE
utt_0048 utt 294.00 299.00 -X WE HAVE OUR SAMPLE DATASET. SINCE THIS DATASET IS SO SMALL AND WELL-SEPARATED,
utt_0049 utt 299.00 304.28 -X IT'S EASY TO SEE THAT THE OPTIMAL DEPTH-ONE TREE SPLITS ON THE PREDICATE VALUE &LT= three.
utt_0050 utt 304.69 310.47 -X (AS A SIDE NOTE TO AVOID ANY CONFUSION, WE COULD ALSO SPLIT ON THE PREDICATE
utt_0051 utt 310.87 316.63 -X VALUE &LT= four FOR THE SAME RESULTS, BUT I'M GOING TO FOCUS ON THIS SINGLE PREDICATE, VALUE &LT= three.)
utt_0052 utt 318.23 323.48 -X AFTER SPLITTING ON THE PREDICATE VALUE &LT= three, WE END UP WITH THE FOLLOWING TWO LEAVES.
utt_0053 utt 324.63 328.35 -X BUT LET'S SUPPOSE WE DIDN'T KNOW THIS SPLIT WAS OPTIMAL. WHAT WOULD WE DO?
utt_0054 utt 329.40 332.54 -X WELL, FOR EACH POTENTIAL SPLIT WE COMPUTE THE ENTROPY.
utt_0055 utt 333.72 345.21 -X FOCUSING IN ON THE RIGHT CHILD, TO COMPUTE THE ENTROPY WE FIRST COUNT UP THE NUMBER OF SAMPLES WITH EACH LABEL AND THEN USE THAT TO CALCULATE GINI IMPURITY, WHICH IS A FORM OF ENTROPY. (AGAIN,
utt_0057 utt 345.49 350.62 -X TO AVOID ANY CONFUSION, I DO WANT TO CLARIFY THAT I'M SIMPLIFYING THE TRUE GINI IMPURITY FORMULA:
utt_0058 utt 351.80 364.35 -X IN PRACTICE WE SUM OVER BOTH BRANCHES, BUT HERE I'M JUST FOCUSING ON THE SINGLE NODE TO BE ABLE TO ILLUSTRATE THE ABSTRACT VERSION OF THIS MORE SIMPLY.) SO WE'VE CALCULATED GINI IMPURITY.
utt_0060 utt 364.47 369.05 -X AND THEN IF WE'RE TRYING TO FIGURE OUT WHAT IS THE OPTIMAL PREDICATE TO SPLIT ON,
utt_0061 utt 369.05 379.52 -X WE WOULD REPEAT THIS PROCESS FOR EACH POSSIBLE PREDICATE AND THEN CHOOSE THE PREDICATE WHOSE GINI IMPURITY IS SMALLEST. I'M NOT GOING TO REPEAT THIS FOR EVERY PREDICATE BUT IF YOU DID,
utt_0063 utt 379.52 388.22 -X YOU WOULD FIND THAT VALUE &LT= three IS INDEED OPTIMAL. NOW, LET'S THINK ABOUT THE ABSTRACT CASE.
utt_0064 utt 388.22 395.32 -X I AM GOING TO STICK WITH THE PREDICATE VALUE &LT= three FOR ILLUSTRATIVE PURPOSES.
utt_0065 utt 396.22 402.33 -X AND SO AGAIN SPLITTING THE ORIGINAL DATA SET, WE END UP WITH THESE TWO LEAVES.
utt_0066 utt 402.42 408.16 -X FOCUSING IN ON THE RIGHT CHILD, IF WE TRY TO COUNT UP THE NUMBER OF ELEMENTS WITH EACH LABEL,
utt_0067 utt 408.41 412.80 -X WE HAVE TO DO THINGS A LITTLE DIFFERENTLY. BECAUSE WE CAN FLIP one LABEL,
utt_0068 utt 412.80 423.87 -X WE CAN NO LONGER ASSUME THAT THERE ARE four GREEN CHECK MARKS. INSTEAD, AFTER FLIPPING A LABEL POTENTIALLY THEY'RE GOING TO BE BETWEEN three AND five GREEN CHECK MARKS AND BETWEEN zero AND two RED X'S.
utt_0070 utt 426.52 440.64 -X FROM THERE, WE COMPUTE GINI IMPURITY AGAIN, BUT NOW WE LIFT THE ALGEBRAIC OPERATORS INTO INTERVAL ARITHMETIC. AND SO WE END UP WITH AN INTERVAL OF VALUES FOR THE GINI IMPURITY SCORE.
utt_0072 utt 441.72 452.73 -X WE WOULD REPEAT THIS PROCESS FOR EACH POSSIBLE PREDICATE SPLIT AND THEN FIGURE OUT WHICH SPLITS ARE OPTIMAL. SINCE THE IMPURITY IS AN INTERVAL AND THE INTERVALS CORRESPONDING TO DIFFERENT
utt_0074 utt 452.73 466.01 -X PREDICATES MAY OVERLAP, WE MIGHT END UP WITH MULTIPLE OVERLAPPING BEST-FIT BEST SPLITS. THAT IS FINE: WE JUST TAKE THE ENTIRE SET AND CONTINUE WORKING WITH THAT AS OUR SET OF OPTIMAL SPLITS.
utt_0076 utt 466.39 478.51 -X WE ALSO HANDLE TREES WITH LARGER DEPTHS, BUT I'M NOT GOING TO GO INTO THE DETAILS OF HOW THAT WORKS HERE. TO SUMMARIZE THE PROCESS, WE BUILD AN ABSTRACT DECISION TREE WHERE ALL THE OPERATORS
utt_0078 utt 478.51 483.10 -X ARE LIFTED INTO INTERVAL ARITHMETIC, LIKE THE EXAMPLE FOR GINI IMPURITY THAT YOU JUST SAW.
utt_0079 utt 483.93 493.50 -X NEXT WE FIND THE PREDICTION FOR X UNDER EACH OF THE TREES RETURNED BY THE ABSTRACT DECISION TREE ALGORITHM, AND FINALLY WE EVALUATE WHETHER ALL THE PREDICTIONS AGREE.
utt_0081 utt 493.82 500.13 -X IF SO THEN WE SAY THAT X IS CERTIFIABLY-ROBUST. THAT MEANS THAT UNDER THE SPECIFIED BIAS MODEL,
utt_0082 utt 500.13 512.26 -X NO DATASET PERTURBATION CAN RESULT IN A DIFFERENT PREDICTION FOR X. IN THIS CASE, WE CAN TRUST THE OUTPUT OF THE MODEL ON X BECAUSE IT'S NOT ARBITRARILY ALTERED BASED ON POOR DATA CURATION.
utt_0084 utt 513.69 524.54 -X ON THE OTHER HAND, IF SOME OF THE PREDICTIONS DIFFER FROM EACH OTHER THEN THE ALGORITHM IS INCONCLUSIVE. IT'S POSSIBLE THAT X IS NOT ROBUST TO DATA BIAS, BUT IT'S ALSO POSSIBLE
utt_0086 utt 524.54 532.16 -X THAT THE LACK OF CERTIFICATION STEMS FROM THE OVER APPROXIMATIONS MADE THROUGH INTERVAL ARITHMETIC.
utt_0087 utt 532.86 541.76 -X SO AT THIS POINT, WE HAVE SHOWN HOW TO APPLY ABSTRACT INTERPRETATION TO SOLVE THE BIAS ROBUSTNESS PROBLEM, BUT HOW WELL DOES THIS WORK ON REAL DATA SETS?
utt_0089 utt 542.75 552.13 -X WE RAN EXPERIMENTS ON FOUR DATA SETS, WHICH WERE THE ADULT INCOME DATASET FROM UCI'S MACHINE LEARNING REPOSITORY, THE COMPAS DATASET AS RELEASED BY PROPUBLICA,
utt_0091 utt 552.48 556.00 -X THE DRUG CONSUMPTION DATASET, WHICH IS ALSO AVAILABLE FROM UCI,
utt_0092 utt 556.80 562.56 -X AND MNISTminus oneminus seven, WHICH IS THE HANDWRITING DIGIT RECOGNITION TASK BUT LIMITED TO JUST one'S AND seven'S.
utt_0093 utt 565.21 576.99 -X THE FIRST RESULT I WANT TO TALK ABOUT IS CERTIFICATION RATE. THAT IS, FOR DIFFERENT AMOUNTS AND TYPES OF BIAS, WERE WE SUCCESSFULLY ABLE TO CERTIFY ROBUSTNESS FOR TEST SAMPLES?
utt_0095 utt 576.99 586.72 -X IT TURNS OUT, WE WERE. THIS TABLE SHOWS CERTIFICATION RATES FOR MISSING-DATA BIAS ACROSS THREE OF THE DATASETS I MENTIONED AND AT DIFFERENT LEVELS OF BIAS. THE FINAL TWO
utt_0097 utt 586.72 592.90 -X ROWS WHERE IT SAYS TARGETED LIMIT THE BIAS TO A PARTICULAR DEMOGRAPHIC SUBGROUP. FOR EXAMPLE,
utt_0098 utt 592.90 598.08 -X FOR THE ADULT INCOME DATASET, WE ASSUMED ALL OF THE MISSING DATA CORRESPONDED TO WOMEN.
utt_0099 utt 598.72 604.74 -X THE COLUMNS IN THIS TABLE ARE THE BIAS AMOUNT AS A PERCENTAGE OF THE TRAINING SET SIZE, VARYING FROM
utt_0100 utt 606.17 614.02 -X zero point zero five% OF THE TRAINING SET SIZE TO one%. RETURNING TO THE TOP three ROWS (SO THIS IS BIAS APPLIED
utt_0101 utt 614.02 621.54 -X INDISCRIMINATELY, WITH NO RESTRICTIONS), WE SEE THAT WITH zero point zero five% BIAS WE WERE ABLE TO CERTIFY
utt_0102 utt 622.11 630.05 -X eighty-nine% OF COMPAS TEST SAMPLES AS ROBUST AND ABOUT ninety-five% FOR BOTH DRUG CONSUMPTION AND ADULT INCOME.
utt_0103 utt 630.30 634.64 -X INCREASING TO zero point four% BIAS, WHICH IS ONE OF THE MIDDLE COLUMNS, THE CERTIFICATION RATE
utt_0104 utt 634.85 639.91 -X HAD DROPPED TO forty-five% FOR COMPAS AND ABOUT sixty% FOR ADULT INCOME, BUT REMAINED AT ABOUT ninety-five%
utt_0105 utt 640.29 646.98 -X FOR DRUG CONSUMPTION. AND THEN AT one% BIAS, WHICH IS THE LAST COLUMN ON THE RIGHT, WE WERE STILL
utt_0106 utt 647.94 653.44 -X ABLE TO VERIFY eighty-five% OF THE DRUG CONSUMPTION SAMPLES AS ROBUST, BUT ONLY nine% FOR COMPAS.
utt_0107 utt 655.71 665.92 -X AT THIS LEVEL OF BIAS, WE WERE NOT ABLE TO COMPLETE EXPERIMENTS FOR THE ADULT INCOME DATASET BECAUSE THE MEMORY REQUIREMENTS WERE TOO HIGH, SO THAT'S WHY THOSE CELLS ARE EMPTY.
utt_0109 utt 667.81 672.61 -X FINALLY, THE COLOR CODING ON THIS TABLE CORRESPONDS TO THE SIZE OF THE PERTURBATION SET,
utt_0110 utt 672.74 686.53 -X THAT IS THE NUMBER OF DATASETS THAT CAN BE CREATED BY ADDING WHATEVER PERCENT MISSING DATA THAT THE BIAS MODEL SPECIFIES. WE SEE THAT ALL OF THESE BIAS SETS CONTAIN OVER ten^fifty ELEMENTS AND THAT THE
utt_0112 utt 686.53 691.84 -X ADULT INCOME BIAS SET SIZES ARE INFINITE, WHICH IS DUE TO THE REAL VALUED FEATURES IN THAT DATASET.
utt_0113 utt 693.09 705.16 -X VERIFYING ROBUSTNESS OF A SINGLE DATA POINT TAKES UNDER zero point one SECONDS FOR BOTH COMPAS AND DRUG CONSUMPTION AND TYPICALLY JUST A FEW SECONDS FOR ADULT INCOME. CLEARLY THIS IS FAR,
utt_0115 utt 705.28 709.28 -X FAR BETTER THAN THE NAÏVE BASELINE WE TALKED ABOUT NEAR THE BEGINNING OF THIS TALK.
utt_0116 utt 711.49 722.12 -X WE ALSO LOOKED AT DISCREPANCIES IN CERTIFICATION RATES BETWEEN DIFFERENT DEMOGRAPHIC GROUPS. FOR THE COMPAS DATASET WE FOUND THAT WHITE INDIVIDUALS WERE CERTIFIABLY
utt_0118 utt 722.12 727.56 -X ROBUST AT A HIGHER RATE THAN BLACK INDIVIDUALS, ESPECIALLY AS THE AMOUNT OF DATA BIAS INCREASES.
utt_0119 utt 728.03 736.29 -X MORE EXPERIMENTATION IS NEEDED TO UNDERSTAND WHY THIS TREND OCCURS AND HOW CONSISTENT IT IS WHILE VARYING OTHER FACTORS SUCH AS THE TRAIN-TEST SPLIT,
utt_0121 utt 736.29 741.51 -X BUT THIS IS POTENTIALLY PROBLEMATIC FOR THE DEPLOYMENT OF RECIDIVISM MODELS IN THE REAL WORLD.
utt_0122 utt 743.97 754.37 -X TO CONCLUDE, I'VE WALKED YOU THROUGH OUR PAPER ON CERTIFYING ROBUSTNESS TO PROGRAMMABLE DATA BIAS IN DECISION TREES. WE SEE A NUMBER OF WAYS TO CONTINUE THIS LINE OF RESEARCH,
utt_0124 utt 754.37 759.17 -X INCLUDING EXTENDING THE IDEAS TO OTHER MACHINE LEARNING ALGORITHMS. ADDITIONALLY,
utt_0125 utt 759.17 769.38 -X WE'RE INTERESTED IN ESTABLISHING TECHNIQUES TO FIND COUNTER-EXAMPLES TO ROBUSTNESS, WHICH ARE MINIMAL TRAINING DATASET PERTURBATIONS THAT RESULT IN A DIFFERENT CLASSIFICATION FOR A FIXED INPUT
utt_0127 utt 770.79 775.43 -X WHEN THE MACHINE LEARNING MODEL IS TRAINED ON THE PERTURBED DATASET RATHER THAN THE ORIGINAL ONE.
utt_0128 utt 776.10 776.90 -2.6455 THANK YOU.
