utt_0000 utt 0.21 3.68 -X HI I'M ANTOINE YANG AND I'M GOING TO PRESENT YOU OUR WORK
utt_0001 utt 3.82 7.92 -X JUST ASK LEARNING TO ANSWER QUESTIONS FROM MILLIONS OF NARRATED VIDEOS
utt_0002 utt 8.52 13.92 -X OUR CODE DATASETS AND TRAINED MODELS ARE AVAILABLE ON OUR PROJECT WEBPAGE
utt_0003 utt 15.56 21.36 -X VIDEO QUESTION ANSWERING OR VIDEOQA IS A PROMISING TASK TO EVALUATE VIDEO UNDERSTANDING
utt_0004 utt 22.06 27.44 -X ANSWERING A QUESTION ABOUT A VIDEO REQUIRES A DETAILED UNDERSTANDING OF THE VISUAL CONTENT
utt_0005 utt 27.82 39.99 -X AND ITS ASSOCIATION WITH NATURAL LANGUAGE FOR INSTANCE THE EXAMPLE ON THE LEFT REQUIRES TEMPORARY REASONING AS MULTIPLE FRUITS ARE DISPLAYED DURING THE VIDEO THE EXAMPLE ON THE
utt_0007 utt 39.99 45.19 -X RIGHT REQUIRES SPATIAL REASONING TO IDENTIFY AN OBJECT LOCALIZED RELATIVELY TO THE MAN
utt_0008 utt 46.99 51.09 -X VIDEOQA IS DIFFICULT BECAUSE OF THE DIVERSITY OF QUESTIONS THAT ONE MAY ASK
utt_0009 utt 51.25 58.58 -X REQUIRING THE ABILITY TO RECOGNIZE ACTIONS OBJECTS COLORS AT DIFFERENT SPATIO TEMPORAL GRANULARITIES
utt_0010 utt 59.79 64.15 -X CURRENT APPROACHES ADDRESS THIS TASK USING FULLY SUPERVISED MODELS
utt_0011 utt 64.24 75.57 -X TRAINED ON MANUALLY ANNOTATED DATA SETS HOWEVER COLLECTION OF QUESTIONS AND ANSWERS FOR VIDEO DATA IS BURDENSOME EXPENSIVE AND THEREFORE NOT SCALABLE
utt_0013 utt 76.43 82.45 -X IN THIS WORK WE WISH TO TACKLE VIDEOQA USING THE LEAST AMOUNT OF MANUAL SUPERVISION POSSIBLE
utt_0014 utt 84.02 89.14 -X FOR THIS PURPOSE WE PROPOSE A NOVEL AND SCALABLE APPROACH TO GENERATE VIDEOQA DATA
utt_0015 utt 89.65 102.58 -X FROM NARRATED VIDEOS RELYING ON LANGUAGE MODELS TRAINED ON TEXT ONLY ANNOTATIONS WE WILL THEN SHOW HOW VIDEOQA MODELS CAN BENEFIT FROM SUCH DATA BY TACKLING VIDEOQA WITHOUT
utt_0017 utt 102.58 107.40 -X ANY MANUAL SUPERVISION OF VISUAL DATA OR BY FINE-TUNING OUR PRETRAINED MODEL
utt_0018 utt 109.46 115.91 -X TO GENERATE VIDEOQA DATA WE START FROM NARRATED VIDEOS WITH READILY AVAILABLE SPEECH TRANSCRIPTS
utt_0019 utt 116.66 120.82 -X THESE ARE EASY TO OBTAIN AT SCALE FOR INSTANCE BY BROWSING YOUTUBE
utt_0020 utt 120.85 123.86 -X AND USING THEIR AUTOMATIC SPEECH RECOGNITION SERVICE
utt_0021 utt 125.88 136.18 -X THE WEAK CORRELATION BETWEEN SPEECH AND VISUAL CONTENT IN NARRATED VIDEOS HAS HELPED IMPROVED ON OTHER TASKS SUCH AS TEXT-TO-VIDEO RETRIEVAL OR ACTION RECOGNITION
utt_0023 utt 137.94 150.46 -X OUR GENERATION PIPELINE NOTABLY RELIES ON AN ANSWER EXTRACTOR AND A QUESTION GENERATOR TRAINED ON SQUAD Vone A TEXT ONLY QUESTION ANSWERING DATA SET WE ARGUE THAT TEXT ONLY
utt_0025 utt 150.46 155.26 -X QUESTION ANSWERING ANNOTATIONS ARE SIGNIFICANTLY EASIER TO OBTAIN THAN VIDEOQA ANNOTATIONS
utt_0026 utt 157.53 166.89 -X IN DETAIL OUR PROPOSED PIPELINE TAKES US INPUT NARRATED VIDEOS WITH ASSOCIATED SPEECH TRANSCRIPTS AND AUTOMATICALLY OUTPUTS VIDEO CLIP QUESTION ANSWER TRIPLETS
utt_0028 utt 168.25 173.29 -X FIRST WE USE A PUNCTUATION MODEL TO INFER PUNCTUATION ON RAW SPEECH TRANSCRIPTS
utt_0029 utt 173.40 177.39 -X AND WE EXTRACTED THE VIDEO CLIPS TEMPORALLY ALIGNED WITH THE INFERRED SENTENCES
utt_0030 utt 178.45 182.30 -X FOR REFERENCE THIS PUNCTUATION MODEL WAS TRAINED ON IWSLT two thousand and eleven
utt_0031 utt 184.28 188.68 -X A TEXT ONLY DATA SET CONSISTING OF TED TALKS ANNOTATED WITH PUNCTUATION
utt_0032 utt 190.55 196.44 -X IN THIS EXAMPLE THE INFERRED SENTENCE IS I PUT UP SOME PICTURES OF HIM WITH ANOTHER MONKEY
utt_0033 utt 197.88 211.20 -X SECOND AN ANSWER EXTRACTOR TRANSFORMER IS USED TO EXTRACT POTENTIAL ANSWERS IN THE INFERRED SENTENCES THIRD A QUESTION GENERATION TRANSFORMER IS USED TO GENERATE A QUESTION GIVEN EACH PAIR OF
utt_0035 utt 211.20 222.56 -X INFERRED EXTRACTED ANSWER AND SENTENCE IN THIS EXAMPLE THE EXTRACTED ANSWER IS MONKEY AND THE ASSOCIATED GENERATED QUESTION IS WHAT ANIMAL DID I PUT UP PICTURES OF HIM WITH
utt_0037 utt 223.99 229.44 -X APPLYING OUR PIPELINE TO THE VIDEOS FROM THE HOWTOone hundredM DATA SET WE GENERATE A NEW
utt_0038 utt 230.84 235.74 -X DATA SET HOWTOVQAsixty-nineM CONSISTING OF sixty-nine MILLION VIDEO CLIP QUESTION ANSWER TRIPLETS
utt_0039 utt 236.79 242.65 -X THIS IS TWO ORDERS OF MAGNITUDE LARGER THAN PREVIOUS VIDEOQA DATASETS AS SHOWN ON THIS PLOT
utt_0040 utt 244.73 250.88 -X THE GENERATED ANNOTATIONS ARE INHERENTLY NOISY WE MANUALLY EVALUATE THE QUALITY OF one hundred RANDOMLY
utt_0041 utt 251.10 258.83 -X GENERATED SAMPLES WE FIND THAT thirty SAMPLES ARE RELEVANT MEANING THE QUESTION ANSWER MAKES SENSE
utt_0042 utt 258.84 271.15 -X AND THE VIDEO HELPS ANSWERING THE QUESTION LIKE THE EXAMPLE ON THE LEFT WHERE SOMEONE IS ACTUALLY CUTTING SOMETHING OFF IN THE VIDEO thirty-one SAMPLES HAVE AN ERROR IN THE QUESTION ANSWER GENERATION
utt_0044 utt 271.39 276.77 -X LIKE THE EXAMPLE IN THE MIDDLE WHERE THE GENERATION MODELS ASSIMILATE ORANGE
utt_0045 utt 276.99 283.62 -X AS A COLOR INSTEAD OF A FRUIT AND thirty-nine SAMPLES HAVE A QUESTION ANSWER CORRECTLY GENERATED
utt_0046 utt 284.12 293.14 -X BUT UNRELATED TO THE VIDEO CONTENT LIKE THE EXAMPLE ON THE RIGHT WHERE SEEING A PUPPET TALKING DOESN'T REALLY HELP ANSWERING THE GENERATED QUESTION
utt_0048 utt 294.91 298.42 -X YOU CAN FIND FURTHER EXAMPLES OF GENERATED SAMPLES IN OUR PAPER
utt_0049 utt 300.16 313.83 -X WE NOW EXPLAIN HOW WE MAKE USE OF OUR GENERATED DATA SET TO TRAIN A VIDEOQA MODEL GIVEN THE LIMITED DIVERSITY OF EXISTING DATA SETS CURRENT METHODS TYPICALLY REDUCE VIDEOQA TO A CLASSIFICATION
utt_0051 utt 313.83 327.97 -X PROBLEM WHERE FREQUENT ANSWERS ARE ASSIGNED TO UNIQUE CLASSES TYPICALLY UP TO fiveK UNIQUE ANSWERS SUCH AN APPROACH HOWEVER DOES NOT SCALE TO THE LARGE-SCALE VOCABULARY OF sixty MILLION DIFFERENT
utt_0053 utt 327.97 342.02 -X ANSWERS IN HOWTOVQAsixty-nineM THEREFORE WE PROPOSE A TRAINING PROCEDURE BASED ON THE CONTRASTIVE LOSS BETWEEN A VIDEO QUESTION TRANSFORMER AND AN ANSWER TRANSFORMER IN DETAIL FOR EACH
utt_0055 utt 342.02 348.82 -X POSITIVE VIDEO CLIP QUESTION ANSWER TRIPLET V Q A WE CONSTRUCT A NEGATIVE TRIPLETS V
utt_0056 utt 349.38 356.24 -X Q AJ BY CONSIDERING OTHER ANSWERS AJ WITHIN THE TRAINING BATCH WHERE AJ IS DIFFERENT TO A
utt_0057 utt 358.82 361.96 -X FOR EVALUATION WE MANUALLY COLLECT IVQA
utt_0058 utt 362.31 368.23 -X A NEW OPEN-ENDED VIDEOQA DATA SET MADE WITH ten zero VIDEO CLIPS FROM HOWTOten0M
utt_0059 utt 368.99 382.33 -X EACH ANNOTATED WITH ONE QUESTION FIRST SIMILARLY TO THE VQAVtwo DATASET IN THE IMAGE DOMAIN IVQA CONTAINS MULTIPLE POSSIBLE ANSWERS FOR EACH QUESTION TO PROVIDE A MORE DETAILED EVALUATION
utt_0061 utt 383.33 387.72 -X SECOND WE MANUALLY EXCLUDE NON-VISUAL QUESTIONS TO REDUCE LANGUAGE BIAS
utt_0062 utt 391.39 397.64 -X WE FIRST EVALUATE OUR MODEL TRAINED ON HOWTOVQAsixty-nineM ON A NEW SETTING ZERO SHOT VIDEOQA
utt_0063 utt 398.24 402.57 -X WHERE WE PROHIBIT ANY MANUAL SUPERVISION OF VISUAL DATA DURING TRAINING
utt_0064 utt 403.68 417.58 -X OUR MODEL VQA-T OUTPERFORMS ITS LANGUAGE ONLY VARIANT QA-T WITH LARGE MARGINS WHEN BOTH ARE TRAINED ON HOWTOVQAsixty-nineM SHOWING THE IMPORTANCE OF MULTIMODALITY IN OUR GENERATED DATASET
utt_0066 utt 418.82 421.77 -X IT ALSO SIGNIFICANTLY OUTPERFORMS ITS VARIANT VQA-T
utt_0067 utt 422.34 433.42 -X TRAINED ON HOWTOone hundredM WITH STANDARD LOSSES USED TO PRETRAIN MULTI-MODAL TRANSFORMERS THIS SHOWS THE BENEFITS OF TRAINING VIDEOQA MODELS ON OUR GENERATED DATA SET
utt_0069 utt 435.43 447.79 -X QUALITATIVELY WE OBSERVE THAT THE LANGUAGE ONLY VARIANT QA-T PROVIDES PLAUSIBLE BUT VIDEO UNRELATED ANSWERS TO THE QUESTIONS FOR INSTANCE IN THE EXAMPLE ON THE LEFT ONION IS A PLAUSIBLE
utt_0071 utt 447.79 453.55 -X ANSWER TO A QUESTION ASKING TO IDENTIFY WHAT IS BEING CUT MOREOVER THE VARIANT VQA-T
utt_0072 utt 453.61 458.92 -X TRAINED ON HOWTOone hundredM IS ABLE TO ASSOCIATE VISUAL CONTENT WITH RELATED ANSWERS
utt_0073 utt 459.14 464.08 -X FOR INSTANCE IN THE VIDEO ON THE MIDDLE THERE ARE ACTUALLY OBJECTS THAT LOOK LIKE TROWELS
utt_0074 utt 464.65 468.84 -X IN THE VIDEO ON THE RIGHT THERE IS ACTUALLY A SLOTTED SPOON APPEARING AT SOME POINT
utt_0075 utt 469.67 472.76 -X BUT IT FAILS TO HAVE A COMPLEX MULTIMODAL UNDERSTANDING
utt_0076 utt 473.54 483.87 -X OUR MODEL TRAINED ON HOWTOVQAsixty-nineM ON THE OTHER HAND CORRECTLY UNDERSTANDS THESE QUESTIONS AND USES VISUAL CUES IN THE VIDEOS TO PROVIDE CORRECT ANSWERS
utt_0078 utt 485.19 491.31 -X YOU CAN FIND MORE QUALITATIVE EXAMPLES IN AN ADDITIONAL VIDEO PROVIDED ON OUR WEBPAGE
utt_0079 utt 492.36 496.72 -X YOU CAN ALSO PLAY WITH OUR MODELS ON AN ONLINE VIDEOQA DEMO WE HOST
utt_0080 utt 496.87 501.49 -X WHERE YOU CAN ASK ANY QUESTION YOU WANT TO OUR MODELS ABOUT A LARGE SET OF VIDEOS
utt_0081 utt 503.69 508.62 -X WE NOW EVALUATE OUR PRETRAINED MODEL AFTER FINE TUNING ON A DOWNSTREAM VIDEOQA DATA SET
utt_0082 utt 509.93 514.67 -X WE FIND THAT OUR MODEL SIGNIFICANTLY IMPROVES OVER THE SAME ARCHITECTURE TRAINED FROM SCRATCH
utt_0083 utt 515.53 520.53 -X OR PRETRAINED ON HOWTOone hundredM IT ALSO OUTPERFORMS THE STATE OF THE ART ON MSRVTT-QA
utt_0084 utt 522.38 524.37 -X MSVD-QA ACTIVITYNET-QA AND HOWtwoQA
utt_0085 utt 527.08 531.86 -X WE NOW ANALYZE VIDEOQA RESULTS BY SEGMENTING DATASETS PER ANSWER FREQUENCY
utt_0086 utt 532.62 544.66 -X TRAINING ON DOWNSTREAM VIDEOQA DATA SETS TYPICALLY LEADS TO PARTICULARLY LARGE IMPROVEMENTS FOR QUESTIONS WITH MOST FREQUENCIES ANSWERS OUR APPROACH BRINGS SIGNIFICANT IMPROVEMENTS
utt_0088 utt 544.66 549.14 -X BOTH FOR COMMON AND RARE ANSWERS COMPARED TO THE MODEL TRAINED FROM SCRATCH
utt_0089 utt 550.32 562.51 -X OR THE MODEL PRETRAINED ON HOWTOone hundredM WE CONCLUDE THAT VIDEOQA SPECIFIC PRETRAINING ON ADDITIONAL LARGE-SCALE DIVERSE DATA HELPS IMPROVE THE GENERALIZATION OF VIDEOQA MODELS
utt_0091 utt 564.33 574.58 -X WE FURTHER COMPARE OUR QUESTION ANSWER GENERATION APPROACH TO HEILMAN AND OTHERS THAT WAS NOTABLY USED TO GENERATE VIDEOQA DATA FROM MANUALLY ANNOTATED VIDEO DESCRIPTIONS IN PREVIOUS WORK
utt_0093 utt 575.53 580.59 -X WE RUN THE METHODS FROM HEILMAN AND OTHERS ON SENTENCES WE EXTRACTED FROM HOWTOone hundredM
utt_0094 utt 581.23 593.83 -X AND APPLY OUR PRETRAINING METHOD ON THE GENERATED DATA WE FIND THAT OUR GENERATION METHOD LEADS TO SIGNIFICANTLY BETTER DOWNSTREAM PERFORMANCE BOTH IN THE ZERO SHOT AND FINE TUNING SETTINGS
utt_0096 utt 594.61 606.47 -X QUALITATIVELY WE FIND THAT OUR NEURAL GENERATION METHOD PROVIDES A HIGHER QUALITY AS WELL A HIGHER DIVERSITY OF QUESTION ANSWER PAIRS COMPARED TO THE RULE-BASED APPROACH FROM HEILMAN AND OTHERS
utt_0098 utt 606.70 611.67 -X WHEN APPLIED TO THE UNCURATED SENTENCES WE EXTRACTED FROM SPEECH IN NARRATED VIDEOS
utt_0099 utt 612.82 623.54 -X FOR INSTANCE WE CAN SEE AN EXAMPLE BELOW WHERE THE METHOD FROM HEILMAN AND OTHERS DOES NOT GENERATE A MEANINGFUL QUESTION ANSWER WHILE OURS IS ABLE TO GENERATE A RELEVANT ONE
utt_0101 utt 625.55 631.35 -X WE FINALLY DISCUSS SOME ABLATION STUDIES TO MOTIVATE OUR PROPOSED PRETRAINING STRATEGY
utt_0102 utt 631.38 641.29 -X WE SHOW THAT BEST RESULTS ARE OBTAINED WHEN ADDING A MASKED LANGUAGE MODELING OBJECTIVE AND REMOVING DUPLICATE ANSWERS IN THE SAMPLING PROCESS OF OUR CONTRASTIVE LOSS
utt_0104 utt 641.84 651.16 -X WE BELIEVE THAT SAMPLING MULTIPLE TIMES THE SAME ANSWER LEADS TO WORSE RESULTS BECAUSE OF DIFFERENT DISTRIBUTIONS OF ANSWERS IN THE PRE-TRAINING AND DOWNSTREAM DATA SETS
utt_0106 utt 652.37 657.16 -X WE ALSO FIND THAT SCALE AS MEASURED BY THE NUMBER OF VIDEOS USED FOR PRE-TRAINING
utt_0107 utt 657.20 662.36 -X IS AN IMPORTANT FACTOR OF PERFORMANCE BOTH IN THE ZERO SHOT AND FINE-TUNING SETTINGS
utt_0108 utt 663.19 668.18 -X THESE NUMBERS SHOW THAT WE CAN EXPECT FURTHER IMPROVEMENTS WITH ADDITIONAL PRE-TRAINING DATA
utt_0109 utt 668.40 671.03 -X DEMONSTRATING THE SCALABILITY OF OUR APPROACH
utt_0110 utt 673.40 681.66 -X TO SUM UP WE AUTOMATICALLY GENERATE HOWTOVQAsixty-nineM A LARGE SCALE VIDEOQA DATA SET USING LANGUAGE
utt_0111 utt 681.66 688.44 -X MODELS TRAINED ON TEXT ONLY ANNOTATIONS AND NARRATED VIDEOS WE SHOWED THAT OUR VIDEOQA MODEL
utt_0112 utt 689.08 701.54 -X TRAINED ON HOWTOVQAsixty-nineM CAN EFFECTIVELY TACKLE VIDEOQA WITHOUT USING ANY MANUAL SUPERVISION OF VISUAL DATA ADDITIONALLY AFTER FINE TUNING OUR MODEL IMPROVES THE STATE OF THE ART ON
utt_0114 utt 701.54 707.39 -X FOUR VIDEOQA DATA SETS WE FURTHER VALIDATE OUR APPROACH ON A NEW BENCHMARK WE MANUALLY COLLATE
utt_0115 utt 707.96 719.58 -X IVQA WHICH HAS REDUNDANT ANNOTATIONS AND REDUCED LANGUAGE BIAS THANK YOU FOR WATCHING AND I INVITE YOU TO READ OUR PAPER AND CHECK OUR WEBPAGE FOR FURTHER DETAILS AND INSIGHTS
