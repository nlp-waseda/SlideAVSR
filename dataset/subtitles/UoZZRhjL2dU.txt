utt_0000 utt 3.18 8.11 -X SO GOOD MORNING EVERYONE! I'M ANNA FARIHA FROM UNIVERSITY OF MASSACHUSETTS AMHERST,
utt_0001 utt 8.11 12.53 -X AND THIS WORK IS DONE BY ME AND MY ADVISER PROFESSOR ALEXANDRE MELIOU.
utt_0002 utt 12.53 22.48 -X SO TODAY I'M GOING TO TALK TO YOU ABOUT A SYSTEM THAT LETS YOU QUERY RELATIONAL DATABASES WITHOUT WRITING ANY SQL. SO WE LIVE IN A TIME WHERE THE
utt_0004 utt 22.51 28.05 -X DATA IS HIGHLY AVAILABLE, ALL SORTS OF PEOPLE WANT TO USE THIS DATA. HOWEVER,
utt_0005 utt 28.05 32.50 -X THE PROBLEM IS PEOPLE WHO DON'T HAVE EXPERTISE IN DATA SYSTEMS,
utt_0006 utt 32.50 44.79 -X DON'T HAVE A SIMPLE WAY TO USE IT, AND ABOUT sixty-five% OF THE STRUCTURED DATA RESIDES IN RELATIONAL DATABASES. TO ACCESS THIS DATA, YOU NEED SQL EXPERTISE, BUT A LOT
utt_0008 utt 44.79 48.53 -X OF THESE PEOPLE SIMPLY DON'T HAVE THIS SQL EXPERTISE.
utt_0009 utt 48.53 53.39 -X SO LET'S TAKE FOR EXAMPLE THE IMDB DATABASE. SO IT'S ABOUT THE MOVIES, ACTORS,
utt_0010 utt 53.39 66.71 -X THEIR RELATIONSHIPS, LIKE WHO WAS CAST IN WHICH MOVIE AND SO ON. SUPPOSE THAT WE HAVE A NAIVE USER ALICE WHO WANTS TO FIND ALL FUNNY ACTORS FROM THE IMDB DATABASE. ALICE IS LOOKING FOR FUNNY ACTORS, AND NOW LET'S SEE THE STEPS
utt_0013 utt 66.71 81.59 -X ALICE WILL HAVE TO GO THROUGH FOR THAT. FIRST, ALICE HAS TO UNDERSTAND THE COMPLEX SCHEMA OF THE IMDB DATABASE, WHICH IS VERY HARD FOR ALICE. AND SECOND, ALICE HAS TO LEARN SQL, EXPRESS HER INTENT IN A SQL QUERY LIKE THIS.
utt_0016 utt 81.61 93.11 -X AGAIN, THIS IS VERY CHALLENGING FOR ALICE, WHO HAS NO PROGRAMMING BACKGROUND. THERE ARE ALTERNATIVE MECHANISMS LIKE QUERY BY EXAMPLE. IN THESE SYSTEMS... THESE
utt_0018 utt 93.11 97.30 -X SYSTEMS BYPASS THE COMPLEXITY OF WRITING SQL AND WORK WITH EXAMPLES.
utt_0019 utt 97.30 107.51 -X SO FOR FUNNY ACTORS, ALICE MIGHT GIVE THESE EXAMPLES OF EDDIE MURPHY, ROBIN WILLIAMS, AND JIM CARREY, AND THE SYSTEM IS SUPPOSED TO IDEALLY DETECT THE INTENT,
utt_0021 utt 107.57 118.10 -X AND RETURN THE LIST OF ALL FUNNY ACTORS IN THE DATA SET. BUT THIS DOESN'T HAPPEN IN REALITY. IF YOU GIVE THESE EXAMPLES TO A TRADITIONAL QUERY BY EXAMPLE SYSTEM,
utt_0023 utt 118.10 130.96 -X YOU GET THIS: A LIST OF ALL ACTORS IN THE DATASET. BUT HOW CAN WE TELL THAT ALICE IS LIKELY LOOKING FOR FUNNY ACTORS? AS HUMAN, WE ACTUALLY SEE SOMETHING IN THE
utt_0025 utt 130.96 142.23 -X EXAMPLES. AS HUMAN, WE SEE THE CONTEXT OF THE EXAMPLE. WHAT'S THE SIMILARITY IN THE EXAMPLES. SO HOW I CAN WE HELP A QUERY INTENT DISCOVERY SYSTEM TO UNDERSTAND
utt_0027 utt 142.23 152.60 -X THE CONTEXT? SO UNFORTUNATELY, THERE IS NO FUNNY ATTRIBUTE IN THE DATA, BUT THERE ARE OTHER WAYS TO FIND THAT. IF WE LOOK... IF WE HAVE A CLOSER LOOK AT THE MOVIES THESE
utt_0029 utt 152.60 158.10 -X ACTORS APPEARING, WE WILL FIND THIS: A LARGE NUMBER OF THESE MOVIES ARE COMEDY MOVIES.
utt_0030 utt 158.10 170.58 -X SO JIM CARREY APPEARED IN MORE THAN forty COMEDY MOVIES AND SO IS TRUE FOR THE OTHER TWO ACTORS. THIS IS A SIMILARITY. IT'S THE SEMANTIC SIMILARITY IN THE EXAMPLES. SO BEFORE GOING TO OUR WORK, I WILL JUST
utt_0033 utt 170.58 183.25 -X QUICKLY GO OVER THE LIMITATIONS OF THE PRIOR ART. THE TRADITIONAL FOOD BY EXAMPLE SYSTEMS DOES NOT MODEL THIS CONTEXT AND THERE ARE METHODS IN KNOWLEDGE GRAPHS THAT DOES NOT MODEL SOME IMPLICIT PROPERTIES WHICH I WILL BE
utt_0036 utt 183.25 196.69 -X TALKING IN SHORTLY THERE ARE METHODS CALLED QUERY REVERSE ENGINEERING BUT IT REQUIRES THE ENTIRE OUTPUT THAT'S THAT FAILS AT THE FIRST PLACE BECAUSE IF I KNEW THE ENTIRE OUTPUT I DON'T LOOK FOR EQUITY INTENT DISCOVERY SYSTEM AND THERE
utt_0039 utt 196.69 208.47 -X ARE MACHINE LEARNING APPROACHES AND ACTIVE LEARNING BASED APPROACHES BUT THAT REQUIRE EITHER TOO MANY EXAMPLES OR TOO MANY USER INTERACTIONS SO TODAY I AM GOING TO TALK UP TO YOU ABOUT SQUID WHICH IS AN AWESOME ACRONYM FOR SEMANTIC
utt_0042 utt 208.47 217.89 -X SIMILARITY OUR QUERY INTENT DISCOVERY SO IN THE NEXT FEW SLIDES I WILL FIRST TALK TO YOU ABOUT HOW SQUID MODELS THE SEMANTIC CONTEXT HOW IT DISCOVERS THE
utt_0044 utt 217.94 228.53 -X QUERY HOW IT DOES SO EFFICIENTLY AND FINALLY I WILL PRESENT SOME EVOLUTION RESULTS SO THE FIRST THING IS CONTEXT HOW HOW SQUID REPRESENT CONTEXT SO ONE
utt_0046 utt 228.53 241.62 -X THE FIRST KIND OF CONTEXT WE CONSIDER IS THE BASIC PROPERTIES SO LIKE BIRTHDAY OR GENDER OF A PERSON IS THE BASIC PROPERTY IT'S DIRECTLY AFFILIATED WITH THAT PERSON SAY ME IS A DIRECT PROPERTY OF JIM CARREY AND THE SECOND TYPE OF
utt_0049 utt 241.62 247.64 -X CONTEXT IS FOUND BY AGGREGATING OVER THE BASIC PROPERTIES OF SOME AFFILIATED
utt_0050 utt 248.91 259.99 -X ENTITY SO JIM CARREY APPEARED IN forty MOVIES THAT HAD THE GENRE COMEDY SO forty COMEDY MOVIES IS A DERIVED PROPERTY FOR JIM CARREY SO HOW DO WE ENCODE THIS
utt_0052 utt 259.99 273.81 -X CONTEXT IN SEQUEL WE CAN USE SELECTION PREDICATE SO IN OUR WORK WE USE A TERM CALLED FILTERS WHICH IS ACTUALLY A COMBINATION OF SELECTION PREDICATES BUT FOR THE PURPOSE OF THIS TALK YOU CAN SIMPLY ASSUME THAT FILTERS ARE SELECTION
utt_0055 utt 273.81 286.95 -X PREDICATES SO SO FAR I HAVE HIGHLIGHTED TO YOU THAT ALICE IS LOOKING FOR FUNNY ACTORS AND I GUESSED IT FROM THE FACT THAT THESE PEOPLE APPEARS IN A LARGE NUMBER OF COMEDY MOVIES BUT THESE PEOPLE ARE
utt_0058 utt 286.95 299.03 -X SIMILAR IN MANY OTHER WAYS THEY ARE ALL MALE THEY ARE ALL BORN IN NORTH AMERICA AND SO ON AND SO FORTH ARE ALL THESE SIMILARITIES INTENDED BY ALICE HOW WE CAN FIGURE THIS OUT THE DIRTY TRUTH IS THERE IS NO WAY TO BE
utt_0061 utt 299.03 306.52 -X SURE SO FOR DUE TO THIS UNCERTAINTY WE APPLY AN INFERENCE MECHANISM CALLED ABDUCTION SO WHAT IS ABDUCTION
utt_0063 utt 306.74 317.21 -X SO ABDUCTION IS FINDING THE MOST LIKELY EXPLANATION OF SOME OBSERVATION SO IN OUR CASE OUR OBSERVATION AT THE EXAMPLES AND OUR GOAL IS TO FIND THE MOST LIKELY
utt_0065 utt 317.21 326.55 -X QUERY GIVEN THESE EXAMPLES SO SO THIS IS IN CONTRAST WITH OUR MORE WELL-KNOWN TECHNICAL DEDUCTION WHICH WOULD GUARANTEE YOU THAT THE INFER TREE IS THE
utt_0067 utt 326.55 341.46 -X ONE THAT IN USERS MIND BUT THIS DOESN'T APPLY IN IN THESE SETTINGS SO HERE IS OUR PROBLEM DEFINITION SO GIVEN THE DATABASE AND EXAMPLES WE ARE GOING TO YOU WANT TO FIND A QUERY SUBSTITUE CRITERIA HOOD FIRST THE EXAMPLE SHOULD
utt_0070 utt 341.46 353.05 -X BE A PART OF THE QUERY OUTPUT AND THE QUERY SHOULD BE THE ONE THAT MAXIMIZES THE PROBABILITY OF BEING THE INDENTED QUERY GIVEN THE EXAMPLES SO HERE IS OUR
utt_0072 utt 353.05 363.74 -X ABDUCTION MODEL AND WE WANT TO FIND THE QUERY POSTERIOR THAT IS THE PROBABILITY THAT THE QUERY IS THE INTENDED ONE GIVEN THE EXAMPLES SO FROM THE EXAMPLES WE SEE
utt_0074 utt 363.74 378.10 -X THE CONTEXT WE CAN GET THE CONTEXT AND JUST BY APPLYING SOME BASE BAYES RULE AND SOME SIMPLE DEDUCTIONS WE COME UP WITH THIS EQUATION SO WE HAVE THREE TERMS HERE THAT QUERY PRIOR WHICH IS THE LIKELIHOOD OF A QUERY BEING THE INTENDED
utt_0077 utt 378.10 391.67 -X ONE WITHOUT SEEING ANY EXAMPLES THE SEMANTIC CONTEXT PRIOR WHICH IS THE PROBABILITY OF SEEING A CONTEXT IN ANY RANDOM SAMPLING OF EXAMPLES WITHOUT SEEING ANY QUERY AND THE SEMANTIC CONTEXT POSTERIOR THAT IS THE
utt_0080 utt 391.67 401.78 -X PROBABILITY OF SEEING A CONTEXT GIVEN A PARTICULAR TREE SO IN THE NEXT FEW SLIDES I WILL GO OVER THESE THREE COMPONENTS FIRST HOW A MODEL QUERY PRIOR
utt_0082 utt 401.84 412.47 -X SO IF WORRY IS ALL ABOUT THE FILTERS IT APPLY AND IF WE CAN GET THE FILTER PRIORS WE CAN COMBINE THEM TO COMPUTE QUERY PRIOR SO WE USE THREE INTUITIONS
utt_0084 utt 412.47 423.42 -X ARE TO DETERMINE FILTER PRIORS SO OUR FIRST INTUITION IS DOMAIN SELECTIVITY SO IT BASICALLY SAYS IF THE FILTER IS MORE SPECIFIC LIKE IT'S GETTING SMALL PORTION
utt_0086 utt 423.42 437.27 -X FROM THE ENTIRE DOMAIN THEN IT'S MORE LIKELY AND THE BROADER FILTERS ARE LESS LIKELY SO WE HAVE TWO OTHER OCEANS ARE THAT MOSTLY APPLY FOR DERIVED FILTERS BUT I DON'T HAVE TIME IN THIS TALK TO GO OVER THIS PATH PLEASE REFER
utt_0089 utt 437.27 452.23 -X TO OUR PAPER FOR MORE DETAILS ABOUT THESE TWO OTHER INCLUSIONS SO THE SECOND ROUND IS THE SEMANTIC CONTEXT PRIOR SO HOW WE CAN GET THE SEMANTIC CONTEXT PRIOR WITHOUT SEEING ANY ANY OF THE UP QUERY SO IT COMES FROM THE DATA SO IN
utt_0092 utt 453.75 462.94 -X THIS DATA eighty% OF THE PEOPLE ARE FROM US AND twenty% FROM CANADA SO THAT'S OUR PRIOR OF SEMANTIC CONTEXT THAT WHAT'S THE LIKELIHOOD THAT WE'LL OBSERVE
utt_0094 utt 462.97 473.08 -X THE CONTEXT USA IN ANY RANDOM SAMPLE IT COMES FROM THE RADAR AND THE THIRD COMPONENT IS THE SEMANTIC CONTEXT POSTERIOR SO IF THE QUERY INCLUDES THE
utt_0096 utt 474.39 487.71 -X FILTER THAT YOU SPECIFY THAT COUNTRY SHOULD BE USA THEN IT'S TRIVIAL THAT YOU'LL OBSERVE THAT CONTEXT IN THE IN THE EXAMPLES BECAUSE YOU ARE EXPLICITLY SPECIFYING IT THE MORE INTERESTING SCENARIO IS WHEN THE QUERY DOES NOT
utt_0099 utt 487.71 491.51 -X CONTAIN THE FILTER THERE IS NO FILTER BUT WE'RE STILL SEEING THIS CONTEXT
utt_0100 utt 491.61 506.19 -X SO FOR ONLY ONE EXAMPLE THE PROBABILITY IS SIMPLY THE PROPORTION OF THE DATA THAT HAS EVER SET FOR TWO EXAMPLES ASSUMING INDEPENDENCE WE CAN JUST MULTIPLY IT AND GET THE PROBABILITY AND SO ON SO FORTH A KEY INTUITION HERE IS
utt_0103 utt 506.19 512.12 -X THAT AS WE SEE MORE EXAMPLES IT'S MORE LIKELY THAT THE FILTER WAS INDEED INDEED
utt_0104 utt 512.54 524.41 -X IT WAS IN THE USERS MIND BECAUSE THIS CANNOT BE JUST A SIMPLE COINCIDENCE SO FINALLY I COME TO THE ALGORITHM WE APPLY AS WE SHOW IN THE PAPER IT BOILS DOWN TO
utt_0106 utt 524.41 537.50 -X A SIMPLE PRINCIPLE SO FOR EACH FILTER WE CHECK IF THIS TERM IS HIGHER WITH THE FILTER OR WITHOUT THE FILTER IF THIS TERM IS HIGHER WITH THE FILTER THE ALGORITHM PICKS THE FILTER OTHERWISE IT DROPS IT AND WITH JUST AN
utt_0109 utt 537.50 552.16 -X ITERATION OVER ALL THE FILTERS THE ALGORITHM IS ABLE TO CONSTRUCT THE MOST LIKELY QUERY THAT MAXIMIZES THE LIKELIHOOD OF BEING THE INTENDED ONE GIVEN THE EXAMPLES SO SO FAR I HAVE TALKED TO YOU ABOUT HOW SQUID AS USES
utt_0112 utt 552.16 566.33 -X CONTEXT TO INFER ARE PRETTY BUT FOR A REAL-TIME QUERY BY EXAMPLE SYSTEM IT HAS TO BE ON THE FLY IT SHOULD HAPPEN ON THE FLY TO MAKE IT HAPPEN SQUID PRE COMPUTES AND ABDUCTION READY DATABASE WHICH IS AN AUGMENTATION OF THE ORIGINAL
utt_0115 utt 566.33 581.05 -X DATABASE WITH SOME DIRECT RELATIONS AND SEMANTIC PROPERTY STATISTICS AND SQUID PRECOMPUTE SIT IN THE OFFLINE MODULE AND THE ONLINE MODULE CONSOLES THIS DATABASE TO PROVIDE THE USER A REAL-TIME EXPERIENCE AND COMPUTES CONTEXT ON THE
utt_0118 utt 581.05 592.54 -X FILE SO I'M DONE TALKING ABOUT HOW SQUID IN FIRST A QUERY AND NOW WE HAVE TO SEE HOW SQUID DOES IN PRACTICE IN I WILL SEE SOME EVOLUTION RESULTS SO WE TRY TO
utt_0120 utt 592.67 606.05 -X ANSWER THREE QUESTIONS HERE SO THE FIRST IS HOW EFFICIENT SQUID IS AND IF IT DISCOVERS THEIR CORRECT QUERY INTENT AND HOW IT COMPARES TO ALTERNATIVE TECHNIQUES LIKE QUERY REVERSE ENGINEERING AND MACHINE LEARNING HERE
utt_0123 utt 606.05 616.21 -X ARE THE DATA STEPS WE WORKED ON AND IN THIS TALK I'LL BE PRESENTING THE RESULTS FROM THE IMDB DATA SET WHICH HAS HAS A COMPLEX SCHEMA OF fifteen RELATIONS AND WE
utt_0125 utt 616.31 630.67 -X DESIGNED sixteen BENCHMARK QUERIES I WILL TALK ABOUT THE BENCHMARK QUERIES IN A MOMENT AND THIS BENCHMARK QUERIES HAD LIKE UP TO eight JOINTS AND seven SELECTIONS AND VARYING RESULT SIZE UP TO two thousand TUPLES SO HERE IS THE EXPERIMENTAL SETTINGS SO WE
utt_0128 utt 630.67 644.46 -X TAKE A BENCHMARK WAY WE EXECUTE IT AND FIND SOME RESULTS WE MARK IS AT THE GROUND TRUTH LIKE AS IF THIS IS THE TUPLES THE USER IS LOOKING FOR IT AT TRUE INTENTION WE SAMPLE SOME SOME TUPLES FROM THIS RESULT AND FORM OUR
utt_0131 utt 644.46 648.86 -X EXAMPLE TUPLES WE GIVE IT TO SQUID SQUID IN FIRST QUERY
utt_0132 utt 648.99 657.92 -X WE EXECUTE IT AND WE FIND THE RESERVED SQUID PRODUCERS NOW WE COMPARE THIS RESULT WITH THE GROUND TRUTH IN TERMS OF PRECISION RECALL AND F SCORE TO SEE HOW
utt_0134 utt 657.92 669.34 -X SQUID PERFORMED SO THIS IS THE RESULTS WITH VARYING EXAMPLES AND VARYING DATABASE SIZE AND WE FOUND THAT SQUIDS INFERENCE IS LINEAR IN EXAMPLE SIZE AND
utt_0136 utt 669.34 679.49 -X LOGARITHMIC IN DATABASE SIZE SO IT'S SCALABLE AND ALSO THE INFERENCE TIME WAS PRETTY REASONABLE SO ABOUT five TO ten EXAMPLES QUIT REQUIRES ONLY FEW SECONDS
utt_0138 utt 679.49 684.48 -X TO INFER THE RIGHT QUAY AND HERE IS OUR OVERALL RESULT FOR sixteen BENCHMARK QUERIES
utt_0139 utt 684.60 697.39 -X AS YOU CAN SEE FOR MOST OF THE BENCHMARKS THE IF SCORE RISES PRETTY QUICKLY NOW LET'S FOCUS ON ONE PARTICULAR SO IT WAS A COMPLEX INTENT ABOUT FINDING ALL INDIAN ACTOR SO ACTED IN AT LEAST fifteen HOLLYWOOD MOVIES
utt_0142 utt 697.47 711.87 -X AND JUST WITH TEN EXAMPLES SQUID WAS ABLE TO QUICKLY DETECT THAT THEM TO MATCH THE USERS EXPECTATION SO I SHOULD MENTION THAT THERE ARE SOME CASES WHERE SQUID DOESN'T DO AS WELL BUT WE FOUND IT VERY RARE AND THIS IS DUE TO THE FACT
utt_0145 utt 711.87 725.28 -X THAT THE QUERY IN QUERY WAS TOO COMPLEX FOR SQUID START SPACE SO NOW I'LL COMPARES I WILL PRESENT RESULTS COMPARING SQUID WITH QUERY REVERSE ENGINEERING SYSTEM BUT I WOULD LIKE TO SAY UP WHAT THE SYSTEM IS SO IF WE
utt_0148 utt 725.28 739.49 -X REALLY MUST ENGINEERING SYSTEM TAKES THE INPUT AS THE ENTIRE QUERY OUTPUT NOT BY EXAMPLES AND IT IT REVERSE ENGINEERS THE QUERY AND THE OUTPUT SHOULD EXACTLY MATCH THE INPUT SO AN EXAMPLE AND INSTANCE OF THE QUERY REVERSE
utt_0151 utt 739.49 748.80 -X ENGINEERING SYSTEM IS TALOS AND THIS IS WHAT WE USE FOR OUR COMPARISON SO WE FOUND THAT WHEN COMPARED WITH TALOS QUERIES BASICALLY MOSTLY BETTER IN TERMS
utt_0153 utt 748.80 763.23 -X OF F SCORE AND IT'S INFERENCE TIME IS USUALLY FASTER THAN CELLO'S HOWEVER I WANT TO FOCUS ON THIS PART SO THESE TALKS THE QUALITY OF THE QUERY THE SYSTEMS ARE PRODUCING AND WE MEASURED THE QUALITY OF THE QUERY IN NUMBER OF
utt_0156 utt 763.23 767.70 -X SELECTION PREDICATES AND NOTE THAT THIS IS IN LOG SCALE SO TALOS IS PRODUCING
utt_0157 utt 768.00 782.66 -X WAY TO LONGER QUERIES THAN SQUID SO LET'S HAVE A LOOK AT ONE PARTICULAR EXAMPLE SO THIS WAS ABOUT FINDING ALL ANIMATION MOVIES BY PICTURE SO THIS WAS THE ORIGINAL QUERY THIS IS WHAT SQUID PRODUCED WITH FEW MORE EXTRA PREDICATES
utt_0160 utt 782.66 797.41 -X BECAUSE IT WASN'T ABLE TO IDENTIFY THAT THESE YEARS WERE NOT TRULY INTENDED JUST A COINCIDENT OF SIMILARITY BUT THIS IS WHAT IS PRODUCED SO IT PRODUCED HUNDREDS OF EXTRA PREDICATES BECAUSE IT WANTED TO GENERATE THE EXACT QUERY TO MATCH THE
utt_0163 utt 797.41 812.26 -X OUTPUT AND IT DIDN'T GENERALIZE AND IT OVERFEED THE DATA SO THAT'S THE PROBLEM WITH QUERY REVERSE ENGINEERING SYSTEMS AND FINALLY SOME PEOPLE MIGHT SEE THESE QUERY INTENT DISCOVERY SYSTEM AS A BINARY CLASSIFICATION PROBLEM SO QUERY
utt_0166 utt 812.26 822.02 -X IS A CLASSIFIER WHICH CLASSIFIES EACH TUPLE AS INTERNET OR NOT SO IN THIS CASE WE DON'T HAVE ANY NEGATIVE EXAMPLES WE JUST HAVE POSITIVE EXAMPLES SO WITH THAT
utt_0168 utt 822.21 836.10 -X A CLASS OF MACHINE LEARNING THAT ADDRESSES THIS IS CALLED POSITIVE AND UNLEVEL LEARNING OR PU LEARNING IN SHA SO WE APPLIED THIS AND WHAT WE FOUND IS THAT THE PEOPLE EARNING APPROACHES REQUIRE ABOUT seventy% OF THE DATA TO MATCH
utt_0171 utt 837.25 841.31 -X ITS ACCURACY WE'RE SQUEE DOES WELL FROM PRETTY WELL FROM THE VERY BEGINNING
utt_0172 utt 841.41 855.49 -X MOREOVER THESE SYSTEMS DO NOT SCALE AS THE DATA SIZE KEEPS GOING SO THIS IS DUE TO THE FACT THAT THESE GENETIC MACHINE LEARNING APPROACHES CANNOT MODEL RELATIONAL DATABASE MANAGEMENT SYSTEMS SPECIFIC ASSUMPTIONS LIKE QUERY IS THE
utt_0175 utt 855.49 869.32 -X MECHANISM THAT IS GENERATING THE DATA SO THAT CONCLUDES MY TALK SO HERE ARE A FEW KEY TAKEAWAYS SO SQUID IS AN EXAMPLE DRIVEN QUERY INTENT DISCOVERY SYSTEM WHICH UNDERSTANDS CONTEXT IT EXPLORES SEMANTIC
utt_0178 utt 869.32 882.37 -X SIMILARITY OF THE EXAMPLE IT WORKS WITH VERY FEW EXAMPLES IT PERFORMS WELL IN QUERY REVERSE ENGINEERING SYSTEM ALTHOUGH IT WASN'T DESIGNED FOR THAT AND IT OUTPERFORMS LEARNING METHODS SO THANK YOU FOR LISTENING AND I WILL TAKE QUESTIONS
