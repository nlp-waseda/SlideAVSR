utt_0000 utt 0.59 5.23 -X HI I'M IASON SARANTOPOULOS AND I WILL PRESENT YOU OUR WORK WITH MARIOS KIATOS,
utt_0001 utt 5.23 10.38 -X ZOE DOULGERI AND SOTIRIS MALASSIOTIS TITLED TOTAL SINGULATION WITH MODULAR REINFORCEMENT LEARNING.
utt_0002 utt 12.04 21.86 -X WHEN DEALING WITH CLUTTER SCENES LIKE THE ONE ON THE RIGHT, THE ROBOT IS NOT ABLE TO GRASP THE RED TARGET OBJECT. SO, THE ROBOT NEEDS TO REARRANGE THE OBSTACLES AROUND THE TARGET
utt_0004 utt 21.86 31.38 -X IN ORDER TO CREATE THE NECESSARY SPACE FOR THE FINGERS TO PERFORM THE GRASP. THE ROBOT SHOULD TOTALLY SINGULATE THE TARGET WHICH MEANS THAT THE CLOSEST OBSTACLE TO THE TARGET SHOULD BE FURTHER
utt_0006 utt 31.38 41.65 -X THAN A DISTANCE THRESHOLD. IN CONTRAST TO THE PARTIAL SINGULATION, TOTAL SINGULATION ALLOWS THE ROBOT TO USE EXISTING GRASPING METHODS AND STRATEGIES, WHICH REQUIRE FREE SPACE
utt_0008 utt 41.65 55.63 -X ALL AROUND THE TARGET. OUR PROPOSED SOLUTION IS BASED ON A MODULAR REINFORCEMENT LEARNING APPROACH THAT USES DIFFERENT PUSHING PRIMITIVE POLICIES AND SELECTS BETWEEN THEM WITH A HIGH LEVEL POLICY.
utt_0010 utt 56.27 67.28 -X THE EXISTING LITERATURE FOCUS MOSTLY ON PARTIAL SINGULATION, WHILE USING DISCRETE ACTIONS AND DOES NOT INCORPORATE PRIOR KNOWLEDGE EFFECTIVELY. IN THIS PAPER, WE USE CONTINUOUS ACTIONS,
utt_0012 utt 67.28 77.11 -X SOMETHING WHICH INCREASES THE AVAILABLE OPTIONS FOR THE ROBOTS. WE ALSO COMBINE DIFFERENT POLICIES THAT CAN BE PRODUCED INDEPENDENTLY. WE CAN USE REINFORCEMENT LEARNING,
utt_0014 utt 77.11 81.11 -X SUPERVISED LEARNING OR EVEN DESIGN THE POLICIES AND THEN COMBINE THEM TOGETHER.
utt_0015 utt 81.30 86.28 -X WE ALSO INTEGRATE EFFECTIVELY PRIOR KNOWLEDGE VIA THE PRIMITIVES AND THE STATE REPRESENTATIONS
utt_0016 utt 86.54 91.57 -X AND FINALLY THE METHOD ACHIEVES SINGULATION IN DIFFERENT ENVIRONMENTS,
utt_0017 utt 91.63 97.14 -X SO WE CAN MANAGE EQUALLY A TABLE WITH OPEN LIMITS AND A BIN WITH CONSTRAINING WALLS ON ITS LIMITS
utt_0018 utt 97.97 108.98 -X AND THIS IS SHOWN DURING THE EXPERIMENTS AT THE END OF THE PRESENTATION. IN THIS WORK WE DEFINED TWO PUSHING PRIMITIVES. THE PUSH-TARGET AND THE PUSH-OBSTACLE PRIMITIVE AND THEY DEFINE THE EXACT
utt_0020 utt 108.98 114.61 -X WAY THAT A PUSH WILL BE EXECUTED. WE DEFINE EACH PUSH AS A LINE SEGMENT WITH INITIAL POSITION Pone
utt_0021 utt 114.80 127.03 -X AND A FINAL POSITION Ptwo. FIRSTLY, WE HAVE THE PUSH-TARGET PRIMITIVE. TAKE FOR EXAMPLE THIS SCENE WITH THE RED OBJECT BEING OUR TARGET OBJECT. PO IS THE POSITION OF THE TARGET OBJECT
utt_0023 utt 127.54 133.33 -X AND THE DIRECTION OF THE PUSH IS DEFINED BY AN ANGLE THETA WITH THE X-AXIS OF THE INERTIA FRAME.
utt_0024 utt 133.59 146.52 -X THE INITIAL POSITION Pone IS PLACED IN THE OPPOSITE DIRECTION IN A DISTANCE D_I FROM THE TARGET AND THE FINAL POSITION Ptwo IS PLACED IN DISTANCE D_P WHICH IS THE PUSHING DISTANCE. THE INITIAL
utt_0026 utt 146.99 152.02 -X POSITION Pone IS ALWAYS SELECTED AS THE INITIAL POSITION THAT IS NOT OCCUPIED BY AN OBSTACLE.
utt_0027 utt 152.15 157.59 -X THIS MEANS THAT IT'S NOT NECESSARY TO PLACE THE FINGER EXACTLY BESIDE THE TARGET OBJECT.
utt_0028 utt 157.68 170.71 -X TO FIND THE COLLISION FREE Pone WE USE THE VISUAL DATA AS WE WILL SEE IN THE NEXT SLIDE. FOR THIS PRIMITIVE WE LEARN THE ANGLE THETA AND THE PUSHING DISTANCE D_P AND WE DESIGN D_I AS DESCRIBED
utt_0030 utt 171.51 185.59 -X FOR AVOIDING OBSTACLES. THE SECOND PRIMITIVE IS THE PUSH-OBSTACLE PRIMITIVE. HERE WE PLACE THE FINGER RIGHT ABOVE THE TARGET OBJECT AND WE PUSH IN THE DIRECTION THETA FOR A DISTANCE
utt_0032 utt 186.52 193.88 -X D_P AND WITH THIS PRIMITIVE WE CAN MOVE OBSTACLES AWAY FROM THE TARGET WITHOUT MOVING THE TARGET, AS
utt_0033 utt 193.88 204.19 -X YOU WILL SEE IN THIS ANIMATION. HERE WE LEARN THE ANGLE THETA AND WE HAVE A FIXED PUSHING DISTANCE.
utt_0034 utt 204.92 210.78 -X ALL THE PARAMETERS THAT WE LEARNED ARE CONTINUOUS WE DO NOT DISCRETIZE THE ACTIONS IN THIS WORK.
utt_0035 utt 212.66 226.62 -X FOR THE STATE REPRESENTATION WE USE BOTH A FULL STATE REPRESENTATION AND A VISUAL STATE REPRESENTATION. THE FULL STATE REPRESENTATION IS TAKEN FROM THE SIMULATION AND CONTAINS THE BOUNDING BOX OF EACH OBJECT AS WELL AS THEIR EXACT POSES. THE VISUAL STATE REPRESENTATION IS
utt_0038 utt 226.62 232.54 -X TAKEN BY RENDERING THE three-D SCENE WITH A CAMERA AND ACQUIRING RGB AND DEPTH INFORMATION. FROM THE RGB,
utt_0039 utt 232.92 237.50 -X WE EXTRACT THE MASK OF THE TARGET AND FROM THE DEPTH WE EXTRACT THE HEIGHTMAP OF THE SCENE.
utt_0040 utt 237.50 251.92 -X THE MASK IS PRODUCED BY A COLOR DETECTOR OR A MORE ADVANCED OBJECT DETECTION ALGORITHM AND THE HEIGHT MAP IS GENERATED USING THE DEPTH INFORMATION. BOTH IMAGES ARE TRANSLATED WITH RESPECT TO THE CENTROID OF THE TARGET TO PRODUCE TRANSLATION INVARIANT FEATURES. WE FURTHER CROP
utt_0043 utt 251.92 256.44 -X THE MASK AND HEIGHT MAP TO KEEP ONLY RELEVANT INFORMATION FOR THE PREDEFINED WORKSPACE.
utt_0044 utt 256.44 267.12 -X TO GENERATE THE VISUAL REPRESENTATION WE FUSE THE MASK AND THE HEIGHT MAP BY ASSIGNING THE VALUE one TO EACH PIXEL THAT BELONGS TO AN OBSTACLE, THE VALUE zero point five TO EACH PIXEL THAT BELONGS TO THE TARGET
utt_0046 utt 267.13 271.45 -X AND THE VALUE zero IN ANY OTHER CASE. AND THIS IS THE VISUAL PRESENTATION FOR THE PUSH TARGET.
utt_0047 utt 271.80 281.47 -X REMEMBER FROM THE PREVIOUS SLIDE THAT THE INITIAL POSITION FOR THE PUSH-TARGET IS PROPERLY SELECTED TO AVOID OBSTACLES. THIS IS PERFORMED WITH THIS STATE REPRESENTATION. GIVEN THE DIRECTION OF THE
utt_0049 utt 281.47 287.17 -X PUSH, WE MOVE A PATCH OF PIXELS IN THE OPPOSITE DIRECTION AND THEN WE FIND AN OBSTACLE-FREE SPACE.
utt_0050 utt 287.17 295.49 -X FOR THE PUSH-OBSTACLE, WE TAKE THE SAME REPRESENTATION AND WE DISCARD PIXELS THAT BELONG TO OBSTACLES SHORTER THAN THE TARGET AND WITH A DISTANCE LARGER THAN THE PUSHING DISTANCE.
utt_0052 utt 295.49 307.01 -X THIS REDUCES VISUAL NOISE IN THE TRAINING AS IT REMOVES THE OBSTACLES THAT CANNOT BE AFFECTED BY THE PRIMITIVE. THE FUSED VISUAL STATE REPRESENTATIONS ARE THEN FED TO AN AUTOENCODER FOR
utt_0054 utt 307.01 312.22 -X DIMENSIONALITY REDUCTION AND THE RESULTED LATENT VECTORS ARE THE INPUTS TO THE HIGH LEVEL POLICY.
utt_0055 utt 312.22 323.61 -X NOTE THAT IN PUSH-TARGET LATENT VECTOR WE APPEND THE DISTANCES OF THE TARGET FROM THE TABLE LIMITS IN ORDER TO LEARN HOW TO AVOID THROWING THE TARGET OFF OF THE TABLE LIMITS. THEN THE HIGH
utt_0057 utt 323.61 333.06 -X LEVEL POLICY SELECTS BETWEEN THE PRIMITIVE POLICIES AND THEN WE FEED THE CORRESPONDING LATENT VECTOR TO THE SELECTED PRIMITIVE POLICY WHICH PRODUCES A PUSH FOR THE ROBOT.
utt_0059 utt 336.16 347.55 -X SO FOR THE TRAINING OF THE POLICIES... FIRSTLY, WE TRAIN THE PUSH-TARGET POLICY WITH REINFORCEMENT LEARNING. SINCE WE USE CONTINUOUS ACTIONS, WE UTILIZE THE DDPG ALGORITHM FOR TRAINING.
utt_0061 utt 347.90 359.22 -X WE ACTUALLY EXPLOIT THE FACT THAT WE TRAIN IN SIMULATION TO USE AN ASYMMETRIC ACTOR CRITIC WHICH MEANS THAT WE USE THE FULL STATE REPRESENTATION AS INPUT TO THE CRITIC AND THE
utt_0063 utt 359.22 365.86 -X VISUAL STATE REPRESENTATION AS INPUT TO THE ACTOR. THE REWARD IN CASE OF SINGULATION IS one WHICH IS
utt_0064 utt 366.24 371.62 -X A SUCCESSFUL TERMINAL STATE. IF TARGET FALLS OFF THE TABLE, WE PENALIZE THE AGENT WITH minus one, WHICH
utt_0065 utt 371.74 383.14 -X IS A FAILED TERMINAL STATE AND IF THE PUSH ACTION INCREASES THE AVERAGE DISTANCE OF THE OBSTACLES FROM THE TARGET WE ASSIGNED zero point one AND WE ASSIGNED A zero point two five IN ANY OTHER CASE INSTEAD OF ZERO,
utt_0067 utt 386.59 390.85 -X IN ORDER TO MOTIVATE THE AGENT TO SIMULATE THE TARGET WITH A MINIMUM NUMBER OF PUSHES
utt_0068 utt 392.93 396.67 -X THE PUSH-OBSTACLE POLICY IS TRAINED WITH SUPERVISED LEARNING. SO,
utt_0069 utt 396.67 401.44 -X WE CREATE A DATASET UTILIZING THE FULL STATE REPRESENTATION FROM THE SIMULATOR.
utt_0070 utt 401.89 408.74 -X SO THE GROUND POLICY PUSHES AT THE CENTER OF MASS OF EACH OBSTACLE AND AS WE
utt_0071 utt 408.96 415.27 -X REGRESS ON ANGLES WE USE FOR TRAINING LOSS THE COSINE OF THE ABSOLUTE ERROR.
utt_0072 utt 416.86 421.59 -X SO THE HIGH LEVEL POLICY... THE GOAL OF THE HIGH LEVEL POLICY IS TO COMBINE
utt_0073 utt 421.82 428.31 -X THE TRAINED PRIMITIVE POLICIES. FOR THAT REASON WE USE THE SPLIT-DQN ALGORITHM, WHICH SPLITS THE
utt_0074 utt 428.51 442.66 -X VANILLA DQN (DEEP Q-NETWORK) TO TWO SEPARATE NETWORKS ONE NETWORK FOR EACH PRIMITIVE. EACH NETWORK TAKES AS INPUT THE CORRESPONDING LATENT VECTOR FOR THE CORRESPONDING PRIMITIVE AND OUTPUTS
utt_0076 utt 442.66 448.64 -X THE Q-VALUE FOR THIS PRIMITIVE. SO THE PRIMITIVE WITH THE HIGHEST Q-VALUE IS THEN EXECUTED.
utt_0077 utt 454.95 466.73 -X SO WE TRAIN EACH PRIMITIVE POLICY SEPARATELY IN SIMULATION AND THEN WE TRAIN THE HIGH LEVEL POLICY IN ORDER TO COMBINE THEM. FIRST, WE COMPARE AND SIMULATION OUR PROPOSED METHOD TSO
utt_0079 utt 468.16 473.64 -X WITH TWO BASELINES: SDQN (THE SPLIT-DQN) AND GTI.
utt_0080 utt 475.23 485.93 -X SO IN THE TABLE YOU CAN SEE THE SUCCESS RATE, THE MEAN NUMBER OF ACTIONS UNTIL SINGULATION AND THE STANDARD DEVIATION OF THE NUMBER OF ACTIONS. AS YOU CAN SEE THE POLICY DERIVED BY THE PROPOSED
utt_0082 utt 485.93 499.54 -X METHOD OUTPERFORMS THESE BASELINES IN TERMS OF SUCCESS RATE. WE ALSO PERFORMED A COMPARISON BETWEEN USING THE ASYMMETRIC ACTOR-CRITIC AND NOT USING IT. HERE, YOU CAN SEE THE SUCCESS
utt_0084 utt 499.54 509.99 -X RATE BY USING AND NOT USING IT. THE RESULTS SHOW THAT USING THE ASYMMETRIC ACTOR-CRITIC RESULTS TO INCREASED SUCCESS RATE, WHICH MEANS THAT THIS CHOICE TRULY CONTRIBUTED TO THE RESULTS.
utt_0086 utt 514.53 525.19 -X THEN, WE EVALUATED THE POLICY IN A URfive ROBOT WITH REALISTIC OBJECTS THAT CAN BE FOUND IN AN OFFICE. THE SUCCESS RATE IN THE REAL ROBOT WAS eighty-two point five%, A SMALL DROP FROM
utt_0088 utt 527.33 535.14 -X SIMULATION, WHICH MEANS THAT THE POLICY IS TRANSFERRED ROBUSTLY. SO IN THIS VIDEO YOU CAN SEE
utt_0089 utt 536.26 546.79 -X THE SINGULATION OF A TARGET OBJECT WHICH IS THE SMALL YELLOW OBJECT AT THE CENTER OF THIS CLUSTER OF OBJECTS. ON THE LEFT YOU CAN SEE THE RGB AND DEPTH INFORMATION, THE OUTPUT
utt_0091 utt 546.79 555.34 -X OF THE AUTOCODER, THE PRIMITIVE THAT ROBOT SELECTS EACH TIME AND ALSO YOU CAN SEE THE
utt_0092 utt 556.81 563.34 -X PUSH THAT THE SELECTED PRIMITIVE PRODUCE EACH TIME UNTIL THE SINGULATION OF THE TARGET.
utt_0093 utt 567.37 579.85 -X FINALLY, WE MADE THE ENVIRONMENT HARDER BY ADDING WALLS ON THE LIMITS OF THE SUPPORT SURFACE. THIS MEANS THAT THERE ARE STATES WHICH BOTH PRIMITIVES ARE USELESS AND CANNOT CHANGE THE SCENE,
utt_0095 utt 579.85 586.54 -X AS YOU WILL SEE IN THE VIDEO. FOR THIS REASON WE ADDED A NEW PRIMITIVE, A SLIDING PRIMITIVE.
utt_0096 utt 586.70 599.53 -X WITH THIS PRIMITIVE WE CAN MOVE THE TARGET OBJECT AWAY FROM THE WALLS AND CREATE THE NECESSARY SPACE FOR PLACING THE FINGERS FOR A PUSH-TARGET, AS YOU CAN SEE ON THIS VIDEO. SO THIS LIGHTING PRIMITIVE
utt_0098 utt 599.53 607.69 -X EFFECTIVELY CREATES THE NECESSARY AFFORDANCES, IN ORDER TO USE THE OTHER TWO PRIMITIVES.
utt_0099 utt 607.69 621.10 -X SO THE MODULARITY OF THE APPROACH ALLOWS THIS ADDITION OF THE PRIMITIVE WITHOUT RETAINING THE OTHER TWO PRIMITIVES AGAIN. WE ONLY TRAIN THE HIGH LEVEL POLICY. IN THE TABLE YOU CAN SEE
utt_0101 utt 621.10 633.52 -X THE SUCCESS RATE IN THIS ENVIRONMENT WHEN WE USE THE PRIMITIVE, HERE, AND WHEN WE DON'T USE THE PRIMITIVE AND YOU CAN SEE THAT USING THIS SLIDING PRIMITIVE MAKES A LOT OF DIFFERENCE.
utt_0103 utt 635.40 641.49 -2.0510 THANK YOU. VIDEOS, SOURCE CODE AND MORE MATERIAL WILL BE AVAILABLE IN THIS LINK.
