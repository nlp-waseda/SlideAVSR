utt_0000 utt 0.10 14.06 -X HI, EVERYONE I'M RICARDO SALAZAR AND I'M HAPPY TO SHARE OUR WORK ON AUTOMATED FEATURE ENGINEERING FOR ALGORITHMIC FAIRNESS. THIS IS A JOIN WORK WITH FELIX NEUTATZ AND ZIAWASCH ABEDJAN. TWO PEOPLE GO TO A CREDIT INSTITUTION ASKING
utt_0003 utt 14.06 27.28 -X FOR CREDIT. THESE TWO PEOPLE SHARE SIMILAR CHARACTERISTICS WITH RESPECT TO FEATURES ARE IMPORTANT TO GET A CREDIT APPROVED, SUCH AS INCOME YEARS WORKING OR TYPE OF CONTRACT. NONETHELESS, ONE OF THESE PEOPLE BELONGS TO A MINORITY WITH RESPECT
utt_0006 utt 27.28 40.91 -X TO SENSITIVE ATTRIBUTES SUCH AS NATIONALITY, GENDER OR SEX. THE CREDIT INSTITUTION TRAINED A MACHINE LEARNING APPLICATION WITHOUT INCLUDING ANY SENSITIVE INFORMATION TO GET CREDITS GRANTED OR APPROVED. THE CREDIT
utt_0009 utt 40.91 45.27 -X INSTITUTION, HAS GRANTED THE CREDIT OF THE PERSON THAT BELONGS TO THE MAJORITY,
utt_0010 utt 45.27 49.23 -X WHILE HAS REJECTED THE CREDIT OF THE PERSON THAT BELONGS TO THE MINORITY.
utt_0011 utt 49.23 62.75 -X SO NOW THE QUESTION IS, HOW CAN WE ASSESS IF THE MACHINE LEARNING APPLICATION IS BEING FAIR OR NOT? TO SOLVE THIS PROBLEM CAPUCHIN, THE STATE OF THE ART APPROACH, PROPOSES A HORIZONTAL APPROACH. THE IDEA IS TO ADD AND REMOVE TUPLES
utt_0014 utt 62.75 75.54 -X TO REMOVE THE BIAS OF THE TRAINING SET BASED ON A FEATURE CATEGORIZATION BY THE USER. SO THE USER CATEGORIZES FEATURES INTO ADMISSIBLE, INADMISSIBLE AND SENSITIVE. ADMISSIBLE FEATURES REPRESENT FEATURES THAT THE USER CONSIDER
utt_0017 utt 75.54 79.06 -X COULD BE INFLUENCED BY THE SENSITIVE FEATURE. ON THE OTHER HAND,
utt_0018 utt 79.06 89.01 -X INADMISSIBLE FEATURES REPRESENT FEATURES THAT COULD BE A PROXY TO THE SENSITIVE FEATURE. INSTEAD, WE PROPOSE A VERTICAL APPROACH THAT USES FEATURE CONSTRUCTION
utt_0020 utt 89.01 99.38 -X TO ENRICH THE FEATURE VECTOR. OUR SYSTEM FAIREXP, SHORT FOR FAIRNESS EXPLORER RECEIVES A BIASED DATA SET FROM THE USER AND ALSO THE FEATURE CATEGORIZATION.
utt_0022 utt 99.38 111.12 -X OPTIONALLY, THE USER CAN CHOOSE A MACHINE LEARNING CLASSIFIER AND CAN DECIDE THE WEIGHTS FOR THE OBJECTIVES. THEN, WE CONSTRUCT NEW FEATURES BASED ON THE ORIGINAL ONES WITHOUT EXCLUDING THE SENSITIVE AND INADMISSIBLE FEATURES.
utt_0025 utt 111.15 120.47 -X IN A SECOND, I WILL SHOW YOU HOW WE CONSTRUCT NEW FEATURES WITH DIFFERENT OPERATORS. THEN, WE EXPLORE INTERESTING FEATURE SETS WITH RESPECT TO ACCURACY
utt_0027 utt 120.47 131.09 -X AND FAIRNESS. LAST, WE COMPUTE THE PARETO FRONT OF OPTIMAL SOLUTIONS BASED ON ALL THE FEATURE SETS THAT WE VISITED DURING THE FEATURE SET EXPIRATION, AND WE RETURN
utt_0029 utt 131.44 136.95 -X ONE UNBIASED DATASET TO THE USER. OUR APPROACH HAS THREE MAIN CHALLENGES. FIRST,
utt_0030 utt 136.95 151.54 -X ACCURACY AND FAIRNESS ARE COMPETING OBJECTIVES. USUALLY ACCURACY BENEFITS FROM LEARNING MORE INFORMATION, WHILE FAIRNESS BENEFITS FROM A SMALL SUBSET OF FEATURES. SECOND, HOW CAN WE GENERATE THESE NEW FEATURES THAT CAN REPLACE
utt_0033 utt 151.54 160.34 -X EXISTING INADMISSABLE FEATURES IN A FEATURE REPRESENTATION? THIRD, HOW CAN WE EFFICIENTLY TRAVERSE THE EXPONENTIAL SPACE OF THE FEATURE TRANSFORMATIONS?
utt_0035 utt 160.34 173.30 -X TO CONSTRUCT NEW FEATURES, WE APPLY FEATURE CONSTRUCTION OPERATORS PROPOSED ORIGINALLY BY EXPLOREKIT. THE IDEA IS TO RECURSIVELY APPLY UNARY AND BINARY OPERATORS TO GET NEW CONSTRUCTED FEATURES. IN THE UNARY OPERATORS WE HAVE NORMALIZERS
utt_0038 utt 173.30 178.55 -X SUCH AS MIN-MAX SCALING OR LOG-SCALING. WE ALSO DESCRETIZE NUMERICAL FEATURES,
utt_0039 utt 178.61 192.44 -X AND APPLY ONE-HOT-ENCODING TRANSFORMATION FOR CATEGORICAL FEATURES. IN THE BINARY OPERATORS, WE HAVE THE BASIC ARITHMETIC OPERATORS, AND WE ALSO HAVE HIGH ORDER AGGREGATORS. NOW, I WOULD LIKE TO SHOW YOU AN EXAMPLE OF HOW CAN A CONSTRUCTED
utt_0042 utt 192.44 197.98 -X FEATURE REDUCE BIAS AND ENHANCE THE CLASSIFICATION TASK. IN THE ADULT DATASET,
utt_0043 utt 197.98 208.02 -X THE CLASSIFICATION TASK IS TO PREDICT IF SOMEONE WILL MAKE MORE THAN FIFTY THOUSAND DOLLARS PER YEAR. THE REPORTED INCOME FOR MARRIED INDIVIDUALS IS THE INCOME
utt_0045 utt 208.02 214.01 -X OF THE HOUSEHOLD. MOREOVER, THERE ARE MORE MARRIED MALES IN THE SAMPLE THAN FEMALES,
utt_0046 utt 214.01 224.54 -X THEREFORE, INCLUDING MARITAL STATUS IN THE REPRESENTATION WILL RESULT IN BIAS AGAINST THE SENSITIVE GROUP, WHICH IN THIS CASE IS THE FEMALE CATEGORY. WHEN WE HAVE A LOOK
utt_0048 utt 224.54 236.12 -X AT OTHER ADMISSABLE FEATURES SUCH AS WORKCLASS, WE SEE THAT THERE ARE CATEGORIES WITHIN THIS CATEGORICAL FEATURE THAT SHOW A MORE BALANCED
utt_0050 utt 236.12 250.01 -X DISTRIBUTION BETWEEN FEMALES AND MALES. MOREOVER, WE OBSERVE DIFFERENCES IN THE MEAN AND STANDARD DEVIATION BETWEEN THE CLASSES WITH RESPECT TO THE PROPORTION OF INSTANCES THAT ARE LABELED AS WORLKCLASS EQUALS FEDERAL GOVERNMENT.
utt_0053 utt 250.01 259.67 -X SO THE QUESTION IS, CAN WE REDUCE BIAS COMBINING INADMISSABLE AND ADMISIBLE FEATURES WITH A BALANCED DISTRIBUTION BETWEEN SENSITIVE AND NONSENSITIVE GROUP?
utt_0055 utt 259.67 272.70 -X IN THIS FIGURE, WE CAN OBSERVE THE VALUES OF THE NEW CONSTRUCTED FEATURE FOR THE DIFFERENT CATEGORIES OF MARITAL STATUS. THE NEW CONSTRUCTED FEATURE COMBINES THE STANDARD DEVIATION OF THE PROPORTION OF INSTANCES THAT ARE LABELED
utt_0058 utt 273.11 282.91 -X AS WORKCLASS EQUALS FEDERAL-GOVERNMENT GROUP BY MARITAL STATUS. AS YOU CAN OBSERVE, THE DISTRIBUTION OF THE VALUES OF THE NEW CONSTRUCTED FEATURE IS BALANCED
utt_0060 utt 282.91 295.35 -X ACROSS THE DIFFERENT CATEGORIES OF MARITAL STATUS. MOREOVER, WHEN WE HAVE A LOOK AT THE PROBABILITY OF THESE VALUES FOR BEING CLASSIFIED AS HAVING MORE THAN FIFTY
utt_0062 utt 295.35 306.94 -X THOUSAND DOLLARS PER YEAR, WE SEE THAT THIS CONSTRUCTED FEATURE ALSO ENABLES CLASS SEPARABILITY. AS YOU CAN SEE HERE, LOWER VALUES HAVE A LARGER PROBABILITY
utt_0064 utt 306.94 311.45 -X OF BEING CLASSIFIED AS HAVING LESS THAN FIFTY THOUSAND DOLLARS PER YEAR,
utt_0065 utt 311.45 324.57 -X WHILE LARGER VALUES HAVE MORE PROBABILITY OF BEING CLASSIFIED AS HAVING MORE THAN FIFTY THOUSAND DOLLARS PER YEAR. DURING OUR RESEARCH, WE FOUND THAT MULTI-OBJECTIVE FEATURE SELECTION APPROACHES, SUCH AS THE GENETIC ALGORITHM,
utt_0068 utt 324.57 339.55 -X NSGAII, GOT STUCK IN LOCAL OPTIMA AND USUALLY ONE OF THE OBJECTIVES TEND TO DOMINATE OVER THE OTHER, GIVEN THE LARGE FEATURE SEARCH SPACE. THUS, WE HAD THE IDEA OF EXPLORING INTERESTING FEATURE SETS IN A TWO-PHRASE APPROACH.
utt_0071 utt 339.55 353.21 -X OUR APPROACH STARTS WITH CONSTRUCTED FEATURE SET RANKED INCREASINGLY BY DESCRIPTION LENGTH. DESCRIPTION LENGTH, REFERS TO THE COMPLEXITY OF THE CONSTRUCTED FEATURES. IT SUMMARISES THE NUMBER OF RAW FEATURES
utt_0074 utt 353.21 357.05 -X AND TRANSFORMATIONS INVOLVED IN A CONSTRUCTED FEATURE. BY DOING THIS,
utt_0075 utt 357.05 362.17 -X WE WEIGH MORE FEATURES THAT ARE SIMPLER OVER FEATURES THAT ARE MORE COMPLEX.
utt_0076 utt 362.17 373.69 -X THIS WILL HELP EXPLAINABILITY. SO WE START PERFORMING FORWARD SELECTION TO ADD FEATURES THAT INCREASE ACCURACY. AFTER THE FIRST PHASE HAS FINISHED, WE HAVE A SUBSET
utt_0078 utt 373.69 379.39 -X A THAT OPTIMIZES ACCURACY. WITH THE SUBSET A, WE START THE SECOND PHASE
utt_0079 utt 379.42 385.09 -X WHICH OPTIMIZES FOR FAIRNESS, AND FOR THAT WE PERFORM BACKWARD SELECTION,
utt_0080 utt 385.09 394.59 -X REMOVING FEATURES IF BY DOING THIS WE INCREASE FAIRNESS. WE ALSO PERFORM HERE AT FLOATING, BUT FLOATING HERE REFERS TO ADDING A FEATURE THAT WAS REMOVED,
utt_0082 utt 394.59 409.31 -X IF BY DOING THIS WE WILL INCREASE FAIRNESS. AT THE END, WE CAN COMPUTE THE PARETO FRONT OF OPTIMAL SOLUTIONS SINCE WE EVALUATE BOTH OBJECTIVES IN ALL THE FEATURES THAT WE VISITED DURING BOTH PHASES. TO GUARANTEE THAT THE FAIRNESS
utt_0085 utt 409.31 420.22 -X CONSTRAINTS ARE MET, AND INSPIRED BY SEQSEL, RECENTLY PROPOSED BY GHALOTRA AND OTHERS, WE PROPOSE A VARIANT OF OUR TWO-PHASE APPROACH. THE IDEA IS TO REMOVE
utt_0087 utt 420.22 428.90 -X THE CONSTRUCTED FEATURES THAT COULD POTENTIALLY LEAK INFORMATION FROM SENSITIVE AND INADMISSIBLE FEATURES INTO THE OUTCOMES. FOR THAT,
utt_0089 utt 429.18 435.58 -X AFTER THE FIRST PHASE, WE FILTER THE SUBSET A USING SEQSEL, SUCH THAT WE REMOVE
utt_0090 utt 435.74 449.87 -X THOSE CONSTRUCTED FEATURES THAT POTENTIALLY LEAK INFORMATION FROM THE SENSITIVE AND THE INADMISSIBLE, AND THEN WE CONTINUE WITH THE SECOND PHASE AS PROPOSED. ONE OF THE MAIN CHALLENGES OF OUR APPROACH IS SCALING FEATURE
utt_0093 utt 449.87 463.36 -X ENGINEERING. FOR SCALING FEATURE ENGINEERING, ONE HAS TO SCALE FEATURE CONSTRUCTION AND FEATURE SELECTION. TO SCALE FUTURE CONSTRUCTION, WE PRUNE ALGEBRAIC EQUIVALENT FEATURES. WE ALSO PARALLELIZE THE CONSTRUCTION OF FEATURES,
utt_0096 utt 463.36 473.63 -X AND FOR SCALING FEATURE SELECTION, WE PERFORM TRAINING PARALLELIZATION AND WE ALSO PARALLELIZE CROSS-VALIDATION. WE ALSO APPLY A SPECULATIVE EVALUATION
utt_0098 utt 473.63 478.43 -X TO PARALLELIZE PARTS OF THE ALGORITHM THAT COULD BE POTENTIALLY PARALLELIZABLE.
utt_0099 utt 478.43 491.76 -X TO TEST OUR SYSTEM, WE USE FOUR DATA SETS THAT ARE KNOWN FOR SHOWING BIAS AGAINST CERTAIN GROUPS OF PEOPLE. MOREOVER, WE USE THE TRAFFIC DATA SET TO OBSERVE HOW OUR SYSTEMS SCALE WITH A LARGE NUMBER OF ROWS, AND A LARGE NUMBER OF ORIGINAL
utt_0102 utt 491.76 502.88 -X FEATURES. WE COMPARE OUR SYSTEM AGAINST FOR FAIRNESS-AWARE METHODS. WE ALSO USE THE STATE-OF-THE-ART MULTI-OBJECTIVE FEATURE SELECTION APPROACH NSGAII
utt_0104 utt 502.88 509.35 -X TO COMPARE OUR TWO-PHASE APPROACH. FURTHERMORE, WE EVALUATE NINE MEASURES,
utt_0105 utt 509.35 524.18 -X ONE MEASURE FOR ACCURACY, THE Fone-SCORE. WE USE SEVEN MEASURES FOR FAIRNESS AND WE ALSO COMPARE RUNTIME. WE PERFORM SEVEN EXPERIMENTS. BECAUSE OF TIME CONSTRAINTS, I WILL FOCUS TODAY ON THE EFFECTIVENESS RESULTS. THIS FIGURE
utt_0108 utt 524.18 538.82 -X SUMMARIZES ACCURACY RESULTS FOR THE ADULT DATASET. WE COMPARE OUR SYSTEM AGAINST THE STATE OF THE ART CAPUCHIN, AND WE ALSO USE THE DROP APPROACH, WHICH EXCLUDES FROM THE FEATURE REPRESENTATION THE SENSITIVE AND INADMISSIBLE FEATURES.
utt_0111 utt 538.82 550.10 -X WE ALSO COMPARE OUR SYSTEM AGAINST THE STATE-OF-THE-ART MULTI-OBJECTIVE FEATURE SELECTION APPROACH NSGAII. WE ALSO COMPARE AGAINST THE ORIGINAL FEATURE
utt_0113 utt 550.10 562.60 -X REPRESENTATION, WHICH DOESN'T EXCLUDE ANY FEATURE FROM THE REPRESENTATION. AS YOU CAN OBSERVE, BOTH OF OUR PROPOSED METHODS ACHIEVE HIGHER ACCURACY
utt_0115 utt 562.60 573.67 -X THAN THE STATE OF THE ART AND THE DROPPED APPROACH. MOREOVER, OUR PROPOSED APPROACHES, ACHIEVE SIMILAR OR HIGHER FAIRNESS COMPARED TO THE STATE OF THE ART
utt_0117 utt 573.73 578.31 -X AND THE DROPPED APPROACH. NONETHELESS, OUR SYSTEM HAS A LARGER RUNTIME. TO CONCLUDE,
utt_0118 utt 578.31 587.49 -X I WOULD LIKE TO UNDERLINE THREE THINGS. FIRST, FEATURE ENGINEERING IS A VIABLE ALTERNATIVE TO HORIZONTAL PREPROCESSING TO ENSURE FAIRNESS AND ACCURACY. SECOND,
utt_0120 utt 587.49 596.71 -X WE CAN GENERATE NEW ADMISSABLE FEATURES BY GENERATING NONLINEAR COMBINATIONS OF FEATURES. AND THIRD, WE CAN SCALE OUR APPROACH TO AROUND seven point fiveK NEW CONSTRUCTED
utt_0122 utt 597.83 600.84 -4.3271 FEATURES BY PRUNING AND PARALLELIZATION. THANK YOU.
