utt_0000 utt 2.70 5.74 -X HI, EVERYONE. WELCOME TO THE PRESENTATION OF OUR WORK:
utt_0001 utt 5.74 11.66 -X IMPROVING FEATURE GENERALIZABILITY WITH MULTITASK LEARNING IN CLASS INCREMENTAL LEARNING.
utt_0002 utt 11.66 15.38 -X MY NAME IS IAN, AND I AM A PHD STUDENT AT THE UNIVERSITY OF CAMBRIDGE.
utt_0003 utt 15.44 18.22 -X DONG MA, WHO SHARES THE JOINT FIRST AUTHORSHIP,
utt_0004 utt 18.22 22.22 -X IS NOW AN ASSISTANT PROFESSOR AT THE SINGAPORE MANAGEMENT UNIVERSITY.
utt_0005 utt 24.40 28.91 -X TO BEGIN, I WOULD LIKE TO GIVE AN OVERVIEW OF THE BACKGROUND AND MOTIVATIONS FOR THIS WORK.
utt_0006 utt 31.08 35.47 -X CONVENTIONAL DEEP LEARNING MODELS ARE USUALLY BUILT UPON A FIXED DATASET.
utt_0007 utt 35.47 38.00 -X THIS MAKES THE MODELS THAT WERE TRAINED,
utt_0008 utt 38.00 44.56 -X LACK THE ABILITY AND FLEXIBILITY OF ADAPTING TO NEW DATA, SUCH AS THOSE OF NEW CLASSES.
utt_0009 utt 44.75 48.69 -X EVEN IF WE TRY TO DO RETRAINING WITH THE NEW DATA,
utt_0010 utt 48.69 54.45 -X STORING THE ENTIRE DATASET THAT WAS PREVIOUSLY SEEN MIGHT HAVE A SUBSTANTIAL MEMORY REQUIREMENT.
utt_0011 utt 54.61 59.19 -X FURTHERMORE, TRAINING A NEW MODEL FROM SCRATCH CAN TAKE SIGNIFICANT TIME.
utt_0012 utt 59.19 65.20 -X THIS BRINGS US TO THE RESEARCH AREA OF CLASS INCREMENTAL LEARNING,
utt_0013 utt 65.20 70.83 -X WHICH AIMS TO MAKE MODELS RETAIN THE ACQUIRED KNOWLEDGE WHILE LEARNING NEW CONCEPTS.
utt_0014 utt 70.93 75.67 -X TYPICALLY, CLASS INCREMENTAL LEARNING CONSISTS OF TWO STAGES, AS SHOWN IN THE FIGURE.
utt_0015 utt 75.98 88.43 -X THE BASE MODEL LEARNING STAGE UTILIZES THE FULL BASE TRAINING DATASET AT HAND TO TRAIN A BASE MODEL. DURING THE INCREMENTAL LEARNING STAGE, THE BASE MODEL WOULD BE FINE-TUNED WITH
utt_0017 utt 88.43 93.87 -X NEW DATA RECEIVED OVER TIME, TOGETHER WITH A SMALL PORTION OF THE OLD DATA.
utt_0018 utt 96.46 99.79 -X HOWEVER, ONE OF THE MAIN CHALLENGES THAT CLASS INCREMENTAL
utt_0019 utt 100.21 106.36 -X LEARNING METHODS FACE, IS THAT MODELS TEND TO OVERFIT ON NEW CLASS DATA,
utt_0020 utt 106.54 119.16 -X WHILE FORGETTING PREVIOUSLY ACQUIRED KNOWLEDGE. THIS RESULTS IN DEGRADED INFERENCE PERFORMANCE OVERALL. ONE OF THE CAUSES IS THE IMBALANCE BETWEEN THE OLD AND NEW CLASS DATA,
utt_0022 utt 119.41 126.90 -X BECAUSE ONLY PART OF THE OLD DATA IS KEPT OR NONE OF IT IS KEPT TO SAVE STORAGE DURING FINE-TUNING.
utt_0023 utt 128.50 133.52 -X TO TACKLE THIS PROBLEM, CLASS INCREMENTAL LEARNING HAS BEEN QUITE AN ACTIVE AREA OF RESEARCH.
utt_0024 utt 133.84 139.22 -X MANY EXISTING APPROACHES MAINLY FOCUS ON THE CLASS INCREMENTAL LEARNING STAGE,
utt_0025 utt 139.22 143.83 -X WITH NEW EXEMPLAR SELECTION STRATEGY, NEW LOSS FUNCTIONS, ETC.
utt_0026 utt 146.06 148.60 -X HOWEVER, WE OBSERVE THAT, IN GENERAL,
utt_0027 utt 148.60 152.66 -X WHEN DEEP LEARNING MODELS ARE TRAINED WITH DIFFERENT INITIAL SEATS,
utt_0028 utt 152.98 166.58 -X RESULT IN DIFFERENT SETS OF WEIGHTS WITH SIMILAR PERFORMANCE. THIS MEANS THAT THE SOLUTION IS NOT QUITE UNIQUE. SO, WE ARE NOW WONDERING, AMONG ALL THESE POSSIBLE SOLUTIONS, WHICH SET OF WEIGHTS IS
utt_0030 utt 166.58 171.70 -X BETTER FOR CLASS INCREMENTAL LEARNING, AND IN PARTICULAR, IN THE BASE MODEL LEARNING STAGE?
utt_0031 utt 174.03 185.01 -X WE HYPOTHESIZED THAT IT IS POSSIBLE TO REDUCE CATASTROPHIC FORGETTING BY TRAINING A MORE TRANSFERABLE BASE MODEL, BECAUSE LESS TRANSFERABLE BASE MODEL
utt_0033 utt 185.04 191.16 -X WILL TEND TO REQUIRE MORE SIGNIFICANT CHANGES TO THE WEIGHTS DURING INCREMENTAL LEARNING,
utt_0034 utt 191.76 196.05 -X AND THIS CAN CAUSE EVEN MORE SEVERE CATASTROPHIC FORGETTING AS A RESULT.
utt_0035 utt 198.26 200.53 -X AND NOW I WILL DESCRIBE OUR APPROACH.
utt_0036 utt 203.12 214.52 -X THE INTUITION BEHIND OUR APPROACH IS THAT, IN CLASS INCREMENTAL LEARNING, THE MODEL IS REQUIRED TO RETAIN PREVIOUS KNOWLEDGE WHILE LEARNING NEW CLASSES, SO, WHAT WE ARE TRYING TO DO
utt_0038 utt 214.52 219.83 -X IS TO SIMULATE CLASS INCREMENTAL LEARNING DURING BASE MODEL LEARNING STAGE,
utt_0039 utt 222.00 228.79 -X AND THE WAY WE DO THIS IS TO CREATE A MULTITASK LEARNING SETUP BY DECOMPOSING THE BASE TASK
utt_0040 utt 229.46 235.29 -X INTO SEVERAL SUB-TASKS, AND TRAIN THE MODEL WITH A SHARED BACKBONE.
utt_0041 utt 235.41 249.69 -X WE TRAIN THE MODEL CONCURRENTLY ON ALL THESE SUB-TASKS AND WE WANT TO FIND WEIGHTS WHICH CAN SOLVE ALL THESE TASKS AT ONCE, WITH THE AIM TO IMPROVE THE GENERALIZABILITY OF THE MODEL, AND
utt_0043 utt 249.69 262.78 -X HENCE MAKE IT MORE TRANSFERABLE. THIS PARTICULAR SETUP IS ANALOGOUS TO THE CLASS INCREMENTAL LEARNING STAGE, WHERE THE MODEL IS REQUIRED TO PERFORM WELL ON DIFFERENT SETS OF CLASSES.
utt_0045 utt 264.92 279.80 -X HOWEVER, ONE DESIGN CHOICE THAT WE HAVE TO MAKE IS THE SELECTION OF THE SUB-TASKS, BECAUSE THE NUMBER OF DISTINCT, VALID SUB-TASKS GROWS EXPONENTIALLY. THERE IS GENERALLY A TRADE-OFF BETWEEN DIFFICULTY
utt_0047 utt 279.92 291.74 -X AND DIVERSITY BETWEEN DIFFERENT SUB-TASKS, WHERE SUB-TASKS WITH MORE CLASSES ARE GENERALLY MORE DIFFICULT, WHILE THOSE WITH MUTUALLY EXCLUSIVE CLASSES ARE MORE DIVERSE.
utt_0049 utt 292.40 297.30 -X AS THERE ARE TOO MANY POSSIBLE COMBINATIONS OF THESE SUB-TASKS TO BE EXPLORED,
utt_0050 utt 297.30 303.99 -X WE EXPLORED THEM ALONG TWO DIRECTIONS: DIFFERENT NUMBER OF CLASSES IN EACH SUB-TASK,
utt_0051 utt 304.85 312.47 -X AND DIFFERENT SUBSET OF CLASSES BUT WITH THE SAME NUMBER OF CLASSES IN WITHIN EACH TASK.
utt_0052 utt 315.41 320.75 -X ANOTHER OBSERVATION THAT WE HAVE IS THAT HIGH LEARNING RATE SIGNIFICANTLY CHANGES
utt_0053 utt 321.27 332.70 -X THE WEIGHTS OF THE NEURAL NETWORK DURING INCREMENTAL LEARNING. THIS COULD MAKE KNOWLEDGE RETENTION VERY DIFFICULT, SO, WE PROPOSE A TWO-STEP FINE-TUNING STRATEGY:
utt_0055 utt 332.79 343.79 -X IN STEP ONE, WE FREEZE MOST OF THE UPSTREAM LAYERS AND THEN WE FINE-TUNE THE DOWNSTREAM LAYERS WITH A RELATIVELY HIGH LEARNING RATE. AFTER THESE NEW LAYERS
utt_0057 utt 343.86 352.28 -X CONVERGE TO A REASONABLE SOLUTION TO BOTH THE NEW AND OLD CLASSES, THE UPSTREAM LAYERS ARE UNFROZEN
utt_0058 utt 352.31 364.63 -X AND THE ENTIRE NETWORK IS FINE-TUNED WITH A LOW LEARNING RATE. THIS SETUP IS TO AVOID TOO MUCH CHANGES BEING MADE TO THE WEIGHTS IN THE FEATURE EXTRACTOR TO ALLOW FOR BETTER KNOWLEDGE RETENTION.
utt_0060 utt 365.69 371.71 -X AN EARLY STOPPING MECHANISM IS ALSO ADOPTED IN OUR SCHEME.
utt_0061 utt 371.71 376.19 -X NOW I AM GOING TO DESCRIBE OUR EVALUATION PROTOCOL AND PRESENT THE RESULTS.
utt_0062 utt 378.26 384.57 -X WE EVALUATED OUR METHODS AND OTHER BASELINES USING TWO DATASETS: THE URBANSOUNDeightK
utt_0063 utt 385.24 391.48 -X AND THE GOOGLE SPEECH COMMANDS. THEY HAVE ten AND twenty CLASSES RESPECTIVELY,
utt_0064 utt 391.48 397.47 -X AND WE SPLIT THE DATA BY THE CLASS LABEL INTO A MULTI-STEP INCREMENTAL LEARNING TASK.
utt_0065 utt 399.67 404.76 -X WE MAKE COMPARISONS BETWEEN OUR PROPOSAL AND THAT PROPOSED IN THE PREVIOUS WORK,
utt_0066 utt 405.05 419.20 -X ESSENTIALS FOR CLASS INCREMENTAL LEARNING, WHICH REPORTED STATE-OF-THE-ART RESULTS FOR CLASS INCREMENTAL LEARNING. SPECIFICALLY, THEY UTILIZED CROSS-ENTROPY LOSS TOGETHER
utt_0068 utt 419.20 426.56 -X WITH KNOWLEDGE DISTILLATION LOSS, ON BOTH NEW AND OLD DATA CLASSES DURING INCREMENTAL LEARNING,
utt_0069 utt 426.56 440.52 -X WITH A BALANCED EXEMPLAR SET. SINCE OUR METHOD FOCUSES ON BASE MODEL LEARNING INSTEAD OF THE CLASS INCREMENTAL LEARNING STAGE, OUR METHOD CAN BE COMBINED WITH THIS WORK, AND THE COMPARISON
utt_0071 utt 440.52 446.68 -X IS MADE BETWEEN WHETHER WE ADOPTED MULTITASK LEARNING OR NOT DURING BASE MODEL TRAINING.
utt_0072 utt 448.89 460.93 -X THE FIRST SET OF RESULTS WE WANT TO PRESENT IS THAT ON TASK SELECTION. AS WE HAVE MENTIONED BEFORE, WE EXPLORED ALONG TWO DIRECTIONS: DIFFERENT SUBSETS OF CLASSES, AND DIFFERENT
utt_0074 utt 460.93 473.20 -X NUMBER OF CLASSES. WE HAVE FOUND THAT THERE IS NO CONSISTENT IMPROVEMENT IN ACCURACY WHEN WE INCREASES THE SELECTION OF SUBSETS OF CLASSES, WHILE THERE IS CONSISTENT ACCURACY IMPROVEMENT
utt_0076 utt 473.20 484.99 -X WHEN WE INCREASES THE DIVERSITY OF TASKS WITH DIFFERENT NUMBER OF CLASSES. WE HYPOTHESIZE THAT HAVING MANY SIMILAR TASKS MAY NOT OFFER MUCH HELP IN IMPROVING THE MODEL'S GENERALIZABILITY,
utt_0078 utt 485.24 499.23 -X WHILE HAVING MORE DISTINCT TASKS ARE BETTER AT DOING THAT. THIS IS BECAUSE OUR MULTITASK TRAINING OBJECTIVE IS TO ALLOW THE MODEL TO SEE WIDER AND MORE DIVERSE COMBINATIONS OF THE BASE CLASSES,
utt_0080 utt 499.23 504.64 -X SO THAT IT CAN BE EFFECTIVELY EXTENDED TO NEW CLASSES DURING INCREMENTAL LEARNING.
utt_0081 utt 504.64 510.46 -X AS A RESULT, IF THE TASKS ARE TOO SIMILAR, THAT IS, WITH THOSE WITH A HIGH LEVEL OF OVERLAPPING,
utt_0082 utt 510.84 514.53 -X THE MODEL MIGHT NOT BE ABLE TO LEARN IN A MORE GENERALIZABLE WAY.
utt_0083 utt 516.95 522.78 -X WE FURTHER CONDUCTED A SET OF EXPERIMENTS WITH VARYING QUANTITY OF EXEMPLARS,
utt_0084 utt 524.92 529.50 -X FROM twenty TO four hundred FOR GOOGLE SPEECH COMMANDS, AND FROM ten TO twenty0 FOR URBANSOUNDeightK.
utt_0085 utt 531.13 536.93 -X WE OBSERVED A CONSISTENT PERFORMANCE GAIN WHEN WE INCREASES THE NUMBER OF EXEMPLARS.
utt_0086 utt 537.50 543.52 -X IN THIS FIGURE WE ALSO PLOTTED THE PERFORMANCE OF OUR PROPOSAL WITH THE TAG MULTITASK,
utt_0087 utt 545.47 547.52 -X AND THAT OF THE PREVIOUS STATE OF THE ART.
utt_0088 utt 549.76 553.58 -X WE FOUND THAT, IN THE VAST MAJORITY OF CASES, OUR METHODS
utt_0089 utt 553.72 558.43 -X OUTPERFORMED THE BASELINE, ESPECIALLY ON THE GOOGLE SPEECH COMMANDS.
utt_0090 utt 558.52 564.86 -X THE AVERAGE INCREMENTAL LEARNING ACCURACY IS IMPROVED BY UP TO five point five PERCENT.
utt_0091 utt 567.64 575.23 -X WE ALSO INVESTIGATED THE IMPACT ON PERFORMANCE WHEN WE INCLUDE DIFFERENT LOSSES DURING TRAINING.
utt_0092 utt 575.23 580.93 -X WE FOUND THAT THE MOST SIGNIFICANT PERFORMANCE IMPROVEMENT COMES FROM THE USE OF EXEMPLAR,
utt_0093 utt 582.08 586.90 -X BY ABOUT thirty% IN ABSOLUTE ACCURACY. ON THE OTHER HAND, KNOWLEDGE DISTILLATION
utt_0094 utt 586.91 590.11 -X PROVIDES LIMITED ACCURACY IMPROVEMENT.
utt_0095 utt 592.51 605.19 -X SO, TO CONCLUDE, OUR WORK IS BASED ON THE HYPOTHESIS THAT MORE TRANSFERABLE FEATURE REPRESENTATION MIGHT BE BENEFICIAL TO CLASS INCREMENTAL LEARNING, BECAUSE THIS SHOULD MITIGATE
utt_0097 utt 605.19 612.16 -X THE IMPACT OF CATASTROPHIC FORGETTING. TO ACHIEVE THAT, WE INTRODUCED A MULTITASK LEARNING SCHEME,
utt_0098 utt 613.09 618.18 -X WHICH IS SET UP TO SIMULATE CLASS INCREMENTAL LEARNING DURING THE BASE MODEL LEARNING PHASE.
utt_0099 utt 618.69 625.22 -X WE HAVE SHOWN THAT OUR MODEL IMPROVES THE AVERAGE INCREMENTAL LEARNING ACCURACY BY UP TO
utt_0100 utt 625.82 638.47 -X five point five PERCENT, COMPARED TO THE PREVIOUS STATE OF THE ART, AND WE BELIEVE THAT THIS OPENS THE DOOR TO RESEARCH IN IMPROVING THE QUALITY OF THE BASE MODEL IN INCREMENTAL LEARNING.
utt_0102 utt 638.47 642.72 -X THESE ARE THE REFERENCES TO THE MATERIALS THAT WERE USED IN THIS PRESENTATION.
utt_0103 utt 645.41 652.58 -X FOR MORE INFORMATION, PLEASE GET IN TOUCH USING THE EMAIL ADDRESSES OR VISIT OUR PERSONAL PAGES. THANK YOU VERY MUCH.
