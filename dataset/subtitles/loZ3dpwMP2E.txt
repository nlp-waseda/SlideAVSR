utt_0000 utt 0.49 7.50 -X HELLO, MY NAME IS YASMIN SARCHESHMEHPOUR AND I AM A RESEARCHER AT AALTO UNIVERSITY IN FINLAND.
utt_0001 utt 7.53 22.29 -X IN THIS VIDEO I WILL PRESENT OUR ACCEPTED ICASSP PAPER WITH THE TITLE FEDERATED LEARNING FROM BIG DATA OVER NETWORKS. OUR PAPER CONSIDERS MACHINE LEARNING PROBLEMS WITH LARGE
utt_0004 utt 26.90 39.17 -X STRUCTURE. AS AN EXAMPLE, CONSIDER LOCAL DATASETS GENERATED BY THE SMARTPHONES OF HUMANS. THESE LOCAL DATASETS ARE RELATED VIA
utt_0006 utt 39.17 45.55 -X DIFFERENT NETWORK STRUCTURES BASED ON PHYSICAL OR SOCIAL PROXIMITY OF THE SMARTPHONE USERS.
utt_0007 utt 47.12 59.22 -X WE PROPOSE A FEDERATED LEARNING ALGORITHM THAT LEARNS PERSONALIZED MODELS FOR EACH NODE IN THE NETWORKS. OUR ALGORITHM USES A KNOWN NETWORK
utt_0009 utt 59.22 67.36 -X STRUCTURE OF DATASETS TO GROUP LOCAL DATASETS INTO WELL-CONNECTED CLUSTERS AND AT THE SAME
utt_0010 utt 69.20 77.27 -X TIME LEARN INDIVIDUAL MODELS FOR EACH CLUSTER. THIS ALLOWS US TO COPE WITH HETEROGENEOUS
utt_0012 utt 81.91 94.58 -X IS GIVEN. AND METHODS FOR LEARNING THE NETWORK STRUCTURE ARE LEFT TO FUTURE WORK. LET ME EMPHASIZE THAT OUR ALGORITHM DOES NOT REQUIRE THE
utt_0014 utt 94.58 102.20 -X EXCHANGE OF RAW DATA AND IS THEREFORE APPEALING FOR APPLICATIONS THAT REQUIRE HIGH LEVELS
utt_0015 utt 102.20 113.59 -X OF PRIVACY PROTECTION. THE LOCAL DATASETS CONSIST OF A VARYING NUMBER OF DATAPOINTS.
utt_0016 utt 113.71 121.36 -X EACH DATAPOINT IS CHARACTERIZED BY A FEATURE VECTOR X AND A LABEL VECTOR Y.
utt_0017 utt 125.71 131.67 -X WE ASSUME THAT THE LABELS ARE KNOWN ONLY FOR A
utt_0018 utt 132.40 140.88 -X (TYPICALLY VERY SMALL) SUBSET OF NODES WHICH SERVE AS A TRAINING SET DENOTED AS CALLIGRAPHY
utt_0019 utt 141.10 150.92 -X M. GIVEN THE NETWORK STRUCTURE AND THE TRAINING SET, OUR ALGORITHM LEARNS
utt_0020 utt 150.92 158.10 -X A LOCAL MODEL FOR EACH NODE IN A FULLY DECENTRALIZED FASHION AND
utt_0021 utt 158.10 168.41 -X WITHOUT DIRECTLY SHARING ANY PARTS OF THE LOCAL DATASETS. THE LOCAL MODELS ARE THEN
utt_0022 utt 168.41 174.26 -X USED TO PREDICT THE LABEL OF A DATAPOINT BASED ON ITS FEATURES. IN THIS
utt_0024 utt 185.01 192.57 -X PRINCIPLE OUR ALGORITHM MIGHT LEARN A DIFFERENT MODEL FOR EACH NODE. HOWEVER, FOR CLUSTERED
utt_0025 utt 194.90 203.51 -X NETWORKS OUR ALGORITHM WILL DELIVER THE SAME LOCAL MODEL FOR EACH NODE BELONGING TO THE SAME CLUSTER. ONE KEY PRINCIPLE USED IN
utt_0027 utt 207.96 213.13 -X MANY MACHINE LEARNING METHODS IS THE BASIC IDEA IS THAT SIMILAR DATA POINTS
utt_0028 utt 216.24 222.84 -X HAVE SIMILAR LABELS. WE USE A SLIGHT VARIATION OF THIS CLUSTERING HYPOTHESIS AND REQUIRE
utt_0029 utt 223.48 230.42 -X SIMILAR DATASETS, THOSE BELONGING TO WELL-CONNECTED CLUSTERS,
utt_0030 utt 230.58 243.22 -X TO HAVE SIMILAR PREDICTORS. MORE PRECISELY, WE REQUIRE THE WEIGHT VECTORS FOR THE LOCAL MODELS TO NOT VARY TOO MUCH OVER WELL-CONNECTED CLUSTERS
utt_0032 utt 243.22 248.44 -X OF LOCAL DATASETS. TO ENFORCE THE WEIGHT VECTOR TO BE CONSTANT OVER CLUSTERS
utt_0033 utt 248.85 253.98 -X WE REQUIRE THEM TO HAVE A SMALL TOTAL VARIATION. THUS,
utt_0034 utt 253.98 259.61 -X TO LEARN THE WEIGHT VECTORS OF LOCAL MODELS WE BALANCE BETWEEN TWO OBJECTIVE COMPONENTS.
utt_0035 utt 259.64 272.57 -X THE FIRST COMPONENT IS THE TRAINING ERROR WHICH MEASURES THE DIFFERENCE BETWEEN THE PREDICTIONS AND ACTUAL LABELS FOR THE LOCAL DATASETS
utt_0037 utt 277.40 291.72 -X WITH KNOWN LABELS, I..E, THE TRAINING SET M. THE SECOND COMPONENT OF THE OBJECTIVE FUNCTION IS THE TOTAL VARIATION WHICH SHOULD BE SMALL IN ORDER TO FORCE THE
utt_0039 utt 291.72 296.51 -X WEIGHT VECTORS TO BE CONSTANT OVER THE WELL CONNECTED NODES OR CLUSTERS.
utt_0040 utt 296.57 309.93 -X THIS REGULARIZED EMPIRICAL RISK MINIMIZATION PROBLEM IS AN INSTANCE OF THE NETWORK LASSO.NOTE THAT THIS NETWORK LASSO PROBLEM INVOLVES A TUNING PARAMETER LAMBDA
utt_0042 utt 311.09 318.91 -X WHICH ALLOWS TO TRADE BETWEEN A SMALL TRAINING ERROR (FOR SMALL LAMBDA)
utt_0043 utt 319.13 327.04 -X OR ENFORCING LARGER CLUSTERS OF NODES WHICH SHARE THE SAME PREDICTOR (FOR LARGE LAMBDA).
utt_0046 utt 333.98 340.83 -X A SINGLE GLOBAL PREDICTOR FOR ALL NODES WHICH IS THE TRADITIONAL FEDERATED LEARNING SETTING.
utt_0047 utt 343.13 347.84 -X IN OUR EXPERIMENTS WE CHOOSE THE VALUE OF LAMBDA USING VALIDATION.
utt_0048 utt 347.84 357.80 -X WE ARE CURRENTLY DEVELOPING A THEORETICAL ANALYSIS OF THE PRECISE CLUSTER
utt_0049 utt 358.39 371.13 -X STRUCTURE DELIVERED BY NETWORK LASSO. THIS ANALYSIS IS EXPECTED TO BE USEFUL FOR GUIDING THE CHOICE OF LAMBDA BASED ON A FEW PARAMETERS OF THE NETWORKED DATASETS.
utt_0051 utt 371.13 382.27 -X OUR FEDERATED LEARNING ALGORITHM IS OBTAINED BY A PRIMAL-DUAL METHOD FOR SOLVING CONVEX OPTIMIZATION PROBLEMS. THIS METHOD IS BASED ON JOINTLY SOLVING THE NETWORK LASSO
utt_0053 utt 382.27 394.49 -X PROBLEM AND ITS DUAL PROBLEM WHICH IS OBTAINED FROM THE CONVEX CONJUGATES OF THE NETWORK LASSO COMPONENTS. NOTE THAT THE PRIMAL NETWORK LASSO PROBLEM IS ABOUT
utt_0056 utt 398.91 411.58 -X DUAL PROBLEM OPTIMIZES WEIGHT VECTORS ASSOCIATED WITH THE EDGES OF THE NETWORK. ELEMENTARY THEORY OF CONVEX DUALITY PROVIDES US WITH AN ELEGANT CHARACTERIZATION OF THE SOLUTIONS
utt_0058 utt 411.58 421.98 -X TO THE PRIMAL AND DUAL PROBLEMS. ANY PAIR CONSISTING OF A PRIMAL SOLUTION TO THE NETWORK LASSO AND A DUAL SOLUTION TO THE DUAL PROBLEM
utt_0061 utt 426.17 431.39 -X EQUATION. INDEED, THESE CONDITIONS CAN BE RE-WRITTEN
utt_0063 utt 434.85 447.87 -X IS OBTAINED BY ITERATING THIS FIXED-POINT EQUATION. NOTE THAT EACH ITERATION OF THIS PRIMAL-DUAL METHOD UPDATES A PAIR OF PRIMAL VARIABLES, WHICH ARE THE WEIGHT VECTORS FOR THE LOCAL (NODE-WISE) MODELS AS WELL AS THE DUAL WEIGHT
utt_0066 utt 449.56 460.83 -X VECTORS ASSIGNED TO THE EDGES IN THE DATA NETWORK. THE RESULTING PRIMAL-DUAL METHOD IS GUARANTEED TO CONVERGE TO A SOLUTION OF THE NETWORK LASSO AND ITS DUAL.
utt_0068 utt 460.83 468.99 -X THE UPDATE EQUATIONS INVOLVE THE BLOCK-DIAGONAL MATRICES T AND SIGMA.
utt_0069 utt 468.99 478.43 -X EACH MAIN DIAGONAL BLOCK OF THE MATRIX T CORRESPONDS TO A NODE IN THE DATA NETWORK. THIS BLOCK IS SET TO A SCALED IDENTITY MATRIX WITH THE SCALING FACTOR BEING
utt_0071 utt 478.43 484.13 -X THE INVERSE OF THE NODE DEGREE, I..E., THE INVERSE OF THE NUMBER OF ITS NEIGHBORS.
utt_0072 utt 484.13 488.90 -X THE MAIN DIAGONAL BLOCK OF THE MATRIX SIGMA CORRESPONDS TO AN EDGE IN THE DATA NETWORK.
utt_0073 utt 488.90 502.47 -X THIS BLOCK IS SET TO A SCALED IDENTITY MATRIX WITH SCALING FACTOR ONE-HALF. LET US HAVE A CLOSER LOOK AT THE UPDATE OF THE PRIMAL (NODE-WISE) WEIGHT VECTORS. THE PRIMAL UPDATE AMOUNTS TO A
utt_0076 utt 502.47 510.08 -X REGULARIZED EMPIRICAL RISK MINIMIZATION PROBLEM. THE EMPIRICAL RISK INVOLVES THE MODEL USED
utt_0078 utt 516.32 529.31 -X AND THE LOSS FUNCTION. THE REGULARIZATION TERM IS THE SQUARED EUCLIDEAN DISTANCE TO THE PREVIOUS PRIMAL ITERATE. NOTE THAT THE REGULARIZATION PARAMETER DEPENDS
utt_0080 utt 529.82 531.30 -X ON THE NODE DEGREE.
utt_0081 utt 531.30 545.44 -X THE DUAL UPDATE STEP OPERATES ON THE EDGES OF THE DATA NETWORK. THE DUAL VARIABLE AT EACH EDGE IS UPDATED BASED ON THE DIFFERENCE BETWEEN PRIMAL WEIGHT VECTORS
utt_0083 utt 545.44 558.69 -X OF THE NODES ADJACENT TO THE EDGE. AFTER THE UPDATE, WE APPLY A CLIPPING OPERATOR TO THE DUAL VARIABLES. THIS CLIPPING OPERATION CAN BE INTERPRETED AS ENFORCING
utt_0085 utt 558.69 567.20 -X A CAPACITY CONSTRAINT FOR A NETWORK FLOW THAT IS REPRESENTED BY THE DUAL VARIABLES. THE CLIPPING THRESHOLD OR CAPACITY
utt_0087 utt 567.20 577.73 -X CONSTRAINT IS PARAMETERIZED BY THE NETWORK LASSO PARAMETER. IN PARTICULAR, THE CAPACITY IS INCREASED BY INCREASING THE NLASSO PARAMETER LAMBDA.
utt_0089 utt 580.00 586.72 -X ALGORITHM one SUMMARIZES OUR PRIMAL-DUAL METHOD FOR SOLVING NETWORK LASSO. THIS ALGORITHM
utt_0090 utt 588.35 593.80 -X CAN BE IMPLEMENTED AS A MESSAGE PASSING PROTOCOL. EACH ITERATION OF
utt_0092 utt 597.57 611.56 -X NETWORK. SUCH A MESSAGE PASSING IMPLEMENTATION IS HIGHLY SCALABLE FOR SPARSE NETWORKS WHERE EACH NODE HAS A SMALL NUMBER OF NEIGHBOURS. NOTE THAT ALGORITHM
utt_0094 utt 611.56 621.77 -X one CAN BE USED WITH ARBITRARY COMBINATIONS OF PREDICTOR MODELS AND LOSS FUNCTIONS. THESE DIFFERENT CHOICES RESULT IN DIFFERENT PRIMAL UPDATE OPERATORS.
utt_0096 utt 621.92 624.17 -X LET US SHOW THREE PARTICULAR EXAMPLES.
utt_0097 utt 624.17 635.91 -X NETWORKED LINEAR REGRESSION IS OBTAINED FOR USING LINEAR PREDICTORS AND THE SQUARED
utt_0098 utt 636.03 644.93 -X ERROR LOSS. NOTE THAT THIS NETWORKED LINEAR REGRESSION METHOD WILL TYPICALLY ONLY BE USEFUL WHEN THE SIZE OF LABELED DATASETS IS LARGER
utt_0100 utt 644.93 656.17 -X THAN THE NUMBER OF FEATURES. FOR A HIGH-DIMENSIONAL SETTING WHERE DATAPOINTS HAVE FAR MORE FEATURES THAN THE SIZE OF LOCAL DATASETS, WE CAN
utt_0102 utt 656.17 662.79 -X USE ALGORITHM one TO LEARN LOCAL LASSO MODELS. IN PARTICULAR, WE AGAIN USE LINEAR PREDICTORS
utt_0103 utt 662.88 673.64 -X FOR EACH NODE BUT AUGMENT THE SQUARED ERROR LOSS WITH A REGULARIZATION TERM GIVEN BY THE ELLone NORM OF THE PREDICTOR WEIGHTS.
utt_0105 utt 674.15 682.09 -X FOR CLASSIFICATION PROBLEMS, WE USE ALGORITHM one TO LEARN LOCALIZED LINEAR CLASSIFIERS. HERE WE USE LINEAR PREDICTOR MAPS BUT THE LOGISTIC LOSS INSTEAD OF THE
utt_0108 utt 689.10 699.40 -X IN CLOSE-FORM. HOWEVER, SINCE IT AMOUNTS TO AN UNCONSTRAINED SMOOTH CONVEX OPTIMIZATION PROBLEM WE CAN SOLVE IT NUMERICALLY. WE VERIFY THE USEFULNESS OF OUR ALGORITHM
utt_0110 utt 699.40 713.93 -X one BY APPLYING IT TO A SYNTHETIC NETWORKED DATASET. THIS DATASET IS GENERATED USING A COMPARE OUR ALGORITHM WITH SIMPLE LINEAR REGRESSION AND DECISION TREE REGRESSION FOR A STOCHASTIC BLOCK MODEL IN WHICH THE PROBABILITY OF THE EDGES
utt_0113 utt 713.93 718.41 -X INSIDE EACH CLUSTERS WAS zero point five AND THE PROBABILITY OF THE EDGES BETWEEN
utt_0114 utt 718.41 730.12 -X THE CLUSTERS WAS zero point zero zero one, AS YOU CAN SEE THE MSE OF OUR METHOD FOR BOTH THE TRAINING SET(M),
utt_0115 utt 730.28 734.92 -X WHICH IS twenty% OF THE NODES, AND THE REMAINING eighty PERCENT OF THE NODES
utt_0116 utt 737.93 743.16 -X OF THE GRAPH) IS REALLY SMALLER IN COMPARE TO SIMPLE LINEAR REGRESSION OR
utt_0117 utt 743.85 752.78 -X DECISION TREE REGRESSION AND THE REASON IS THAT OUR METHOD USES THE NETWORK STRUCTURE AND THE MODEL PARAMETERS FOR EACH NODE ARE DIFFERENT.
utt_0120 utt 758.86 768.65 -X LASSO TO LEARN LOCALIZED OR PERSONALIZED MODELS FROM NETWORKED COLLECTIONS OF LOCAL DATASETS. A WE HAVE IMPLEMENTED THIS NETWORK LASSO USING
utt_0122 utt 768.65 775.21 -X A HIGHLY SCALABLE MESSAGE PASSING ALGORITHM WHICH IS OBTAINED FROM A PRIMAL DUAL METHOD.
utt_0124 utt 778.79 787.34 -X AND LOSS FUNCTIONS. YOU CAN FIND THE PREPRINT OF OUR PAPER ON ARXIV AND THE SOURCE CODE FOR NUMERICAL EXPERIMENTS ON MY GITHUB .
utt_0126 utt 788.17 790.99 -1.9709 THANKS A LOT FOR YOUR ATTENTION.
