utt_0000 utt 0.30 4.72 -X HI EVERYONE, I AM BAIHAN LIN, FROM COLUMBIA UNIVERSITY.
utt_0001 utt 5.04 13.36 -X I AM PRESENTING OUR WORK: A STORY OF TWO STREAMS: REINFORCEMENT LEARNING MODELS FROM HUMAN BEHAVIOR AND NEUROPSYCHIATRY.
utt_0003 utt 14.06 22.26 -X THIS IS A WORK DONE AT IBM RESEARCH WITH GUILLERMO CECCHI, DJALLEL BOUNEFFOUF, JENNA REINEN AND IRINA RISH.
utt_0005 utt 23.82 29.01 -X HUMAN DECISION MAKING IS DIFFERENT FROM TRADITIONAL REINFORCEMENT LEARNING.
utt_0006 utt 29.01 35.31 -X AT EACH STATE, THE AVAILABLE ACTIONS ALL HAVE THEIR CORRESPONDING REWARD AND COST.
utt_0007 utt 35.66 43.57 -X AN RL AGENT AIMS TO FIND THE OPTIMAL POLICY TO MAXIMIZE THE MARGINAL REWARDS IN ALL FUTURE SCENARIOS.
utt_0009 utt 44.72 49.04 -X WHILE THE HUMAN MINDS UNDERGO MUCH MORE COMPLICATED PROCESSES.
utt_0010 utt 50.35 57.43 -X THE COST AND REWARDS CAN COME IN DIFFERENT ASPECTS AND THUS, YIELDING DIFFERENT WEIGHTS TO FIND THE OPTIMAL POLICY.
utt_0012 utt 57.81 62.10 -X FOR INSTANCE, ONE MIGHT NOT WORRY TOO MUCH OF CERTAIN COSTS.
utt_0013 utt 62.35 66.04 -X OR ONE MIGHT IGNORE THE REWARDS OF CERTAIN TYPES.
utt_0014 utt 67.25 72.08 -X THE TIMESCALE OF HUMAN DECISION MAKING PROCESS IS ALSO DIFFERENT.
utt_0015 utt 72.62 78.68 -X SOME MIGHT FOCUS ON THE PRESENT DAY PLEASURE, WITHOUT CARING ABOUT THE LONG TERM BENEFITS.
utt_0016 utt 79.92 85.72 -X THESE ARE ALL REWARD BIAS HIDDEN IN EACH HUMAN'S CHARACTERS AND IDIOCRASIES.
utt_0017 utt 86.35 91.16 -X IN ANOTHER WORD, IT IS NOT THAT SIMPLE.
utt_0018 utt 91.16 95.09 -X INSIGHTS FROM NEUROSCIENCE AND PSYCHIATRY CONCUR WITH THIS INTUITION.
utt_0019 utt 95.51 106.01 -X THE REWARD SYSTEM OF THE BRAIN OFTEN INVOLVES MULTIPLE BRAIN REGIONS THAT REPRESENTS PREDICTION ERROR AND VALUES IN VARIOUS CIRCUITRIES AND MECHANISMS.
utt_0021 utt 106.64 115.58 -X THESE INTERACTING SYSTEMS DRIVE DOWNSTREAM BEHAVIORS LIKE MOTIVATION, APPROACH BEHAVIOR AND ACTION SELECTION.
utt_0023 utt 115.76 124.41 -X IN CLINICAL LITERATURE, MANY PSYCHIATRIC DISORDERS ARE KNOWN TO BE INVOLVED WITH DIFFERENT TYPES OF REWARD PROCESSING.
utt_0025 utt 124.47 131.54 -X PRESENTED HERE ARE A SUBSET OF THESE NEUROPSYCHIATRIC CONDITIONS, INCLUDING PARKINSON'S, ADDICTION,
utt_0026 utt 133.72 137.75 -X ALZHEIMER, CHRONIC PAIN, DEMENTIA, AND ADHD.
utt_0027 utt 140.09 153.51 -X FROM THE EVOLUTIONARY PSYCHIATRY POINT OF VIEW, WE CAN ALSO VIEW MENTAL DISORDERS AS EXTREME POINTS IN A CONTINUOUS SPECTRUM OF BEHAVIORS AND TRAITS DEVELOPED FOR VARIOUS
utt_0029 utt 153.51 155.64 -X PURPOSES DURING EVOLUTION.
utt_0030 utt 156.76 163.36 -X SOMEWHAT LESS EXTREME VERSIONS OF THOSE TRAITS CAN BE ACTUALLY BENEFICIAL IN SPECIFIC ENVIRONMENTS.
utt_0031 utt 163.54 170.56 -X FOR INSTANCE, ADHD-LIKE FAST-SWITCHING ATTENTION CAN BE LIFE-SAVING IN CERTAIN ENVIRONMENTS
utt_0032 utt 170.68 176.67 -X THEREFORE, MODELING THESE DISORDER-RELATED REWARD BIAS CAN HELP US BUILD BETTER AI.
utt_0033 utt 178.20 185.37 -X LET'S GET BACK TO THE TRADITIONAL REINFORCEMENT LEARNING PROBLEM, WHERE WE HAVE AN AGENT IN A CERTAIN STATE.
utt_0035 utt 185.37 190.57 -X IT MAKES AN ACTION GIVEN ITS CURRENT POLICY AND THE ENVIRONMENT REVEALS THE NEXT STATE
utt_0036 utt 190.84 192.09 -X AND A REWARD.
utt_0037 utt 192.44 198.72 -X WE IN THIS PAPER, ARE MORE INTERESTED IN THE REWARD PROCESSING MECHANISM OF THE AGENTS,
utt_0038 utt 198.72 203.13 -X WHICH DRIVES ITS LEARNING AND ACTION SELECTION.
utt_0039 utt 203.93 210.05 -X IN THE REINFORCEMENT LEARNING SETTING, WE ARE GIVEN THE REWARD PARAMETERS AND THE ENVIRONMENT,
utt_0040 utt 210.05 212.61 -X AND WE WISH TO LEARN AN OPTIMAL POLICY.
utt_0041 utt 215.07 228.42 -X IN THE INVERSE REINFORCEMENT LEARNING SETTING, WE ARE GIVEN THE REWARD PARAMETERS AND THE POLICY, OR THE BEHAVIORAL TRAJECTORIES, AND WE AIM TO DISCOVER THE REWARD FUNCTIONS OF THE ENVIRONMENT.
utt_0044 utt 229.18 243.10 -X IN THE BEHAVIORAL MODELING SETTING ON THE OTHER HAND, WE ARE USUALLY GIVEN A BEHAVIORAL TRAJECTORIES AND THE ENVIRONMENT PAYOFFS, AND WE WISH TO INFER THE REWARD PARAMETERS THAT DRIVES THE AGENT TO ITS CURRENT STATE.
utt_0047 utt 246.14 251.62 -X TO ACCOMPLISH ALL THREE PROBLEM SETTINGS, WE PROPOSED SPLIT Q LEARNING.
utt_0048 utt 251.71 258.82 -X SPLIT Q LEARNING EXTENDED UPON THE Q LEARNING BY INTRODUCING A TWO-STREAM REWARD PROCESSING MECHANISM.
utt_0050 utt 259.10 269.28 -X IT ASSUMES THAT THE REWARD COMES IN TWO STREAMS, AND EACH STREAM OF REWARD WENT THROUGH THE LEARNING PROCESSES INDEPENDENTLY OF THE OTHER STREAM.
utt_0052 utt 269.28 272.83 -X WE INTRODUCED FOUR TUNABLE PARAMETERS FOR THE TWO STREAMS.
utt_0053 utt 272.89 277.38 -X FIRST, WE HAVE TWO PARAMETERS FOR THE WEIGHTS ON THE REWARD HISTORIES.
utt_0054 utt 277.38 281.32 -X A SMALLER PARAMETER WOULD MEAN WE DISCOUNT THE PAST MORE.
utt_0055 utt 281.82 285.67 -X AND WE HAVE TWO PARAMETERS TO ACCOUNT FOR THE REWARD PERCEPTION.
utt_0056 utt 285.67 292.07 -X A BIGGER PARAMETER WOULD MEAN THAT THE REWARD OF THIS STREAM IS AMPLIFIED IN CERTAIN DEGREE.
utt_0057 utt 292.35 299.59 -X GIVEN THIS SETUP, WE CAN SIMULATE DIFFERENT NEUROPSYCHIATRIC CONDITIONS WITH OUR MODEL BASED ON THE CLINICAL LITERATURE.
utt_0059 utt 299.59 305.70 -X FOR INSTANCE, THE CHRONIC PAIN HAS A LARGER WEIGHT ON THE ENTIRE NEGATIVE STREAM THAN THE POSITIVE STREAM.
utt_0061 utt 305.70 309.57 -X THE ADDICTION FORGETS ABOUT THE NEGATIVE HISTORY FASTER THAN OTHER AGENTS.
utt_0062 utt 309.86 318.31 -X THE PARKINSON'S' PATIENTS' BEHAVIORS ARE MORE ASSOCIATED WITH A MAGNIFIED PERCEPTION OF THE NEGATIVE REWARD OR PAIN.
utt_0064 utt 318.31 319.94 -X OTHER THAN THESE MENTAL VARIANTS.
utt_0065 utt 319.94 330.66 -X WE ALSO HAVE A MODERATE VERSION THAT WE CALLED M, A STANDARD VERSION, AND TWO SINGLE-STREAMED VARIANTS CALLED POSITIVE SPLIT QL AND NEGATIVE SPLIT QL.
utt_0067 utt 331.01 339.05 -X FIRST, WE TESTED OUR APPROACH ON A SIMULATED MARKOV DECISION PROCESS WITH NOT GAUSSIAN REWARD DISTRIBUTIONS.
utt_0069 utt 339.17 342.06 -X FOR INSTANCE, WE HAVE FIVE STATES.
utt_0070 utt 342.06 351.08 -X FROM THE INITIAL STATE A WE CAN EITHER CHOOSE TO GO LEFT OR RIGHT, AND FROM THERE, THE NEXT ACTION CAN SAMPLE THE REWARD FROM CERTAIN DISTRIBUTIONS.
utt_0072 utt 351.52 358.47 -X PRESENTED HERE IS AN EXAMPLE SCENARIO, WHERE THE REWARD DISTRIBUTIONS ARE TWO BIMODAL DISTRIBUTIONS.
utt_0073 utt 358.69 364.30 -X AS SHOWN, THE ORANGE ARM, OR THE RIGHT ARM HAVE A BETTER EXPECTED REWARD.
utt_0074 utt 365.03 370.76 -X OVER five hundred ITERATIONS, WE OBSERVE THAT, OUR SPLIT Q LEARNING OUTPERFORMS QL AND DOUBLE
utt_0075 utt 371.05 379.56 -X Q LEARNING IN THE SPEED TO CONVERGE TO THE RIGHT ARM, AS WELL AS ITS ACCUMULATED FINAL REWARD OVER five hundred ITERATIONS.
utt_0077 utt 380.01 392.91 -X THE SECOND ROW GAVE US A MORE INSIDE LOOK OF WHAT'S GOING ON IN THE Q TABLES OF BOTH STREAMS, WHERE WE CAN SEE THAT THE SPLIT MECHANISM OFFERED A ROBUST REGULARIZATION TO THE Q VALUE
utt_0079 utt 392.91 395.15 -X ESTIMATION.
utt_0080 utt 395.88 403.79 -X WE ALSO TESTED THE IOWA GAMBLING TASK, A VERY POPULAR PSYCHOLOGY EXPERIMENTS TO MODEL HUMAN DECISION MAKING.
utt_0082 utt 403.79 410.44 -X THE AGENT GETS TO PICK FROM THESE FOUR DECKS OF CARDS, TRYING TO MAXIMIZE ITS FINAL REWARD.
utt_0083 utt 410.82 419.24 -X COMPUTATIONALLY, IT IS A MULTI-ARMED BANDIT TASK WITH FOUR ARMS, WITH TWO ARMS AS BETTER ARMS FOR GIVING A BETTER EXPECTED PAYOFF.
utt_0085 utt 419.98 428.91 -X AS IN THE REWARD DISTRIBUTION PLOT SHOWN HERE, WE SEE THAT THE REWARD DISTRIBUTION IS HIGHLY NON-GAUSSIAN, WHERE THE Q LEARNING WOULD USUALLY UNDERPERFORM.
utt_0087 utt 429.32 435.50 -X IN THIS PAPER, WE COMPARED THE TWO MOST COMMON PAYOFF SCHEMES FOR THE TASK, CALLED IGT SCHEME
utt_0088 utt 437.03 437.84 -X one AND two.
utt_0089 utt 438.35 446.86 -X WE SOON SEE THAT, THE SPLIT QL BACKBONE DOES A GREAT JOB DISTINGUISHING THE MENTAL AGENTS FROM ONE ANOTHER.
utt_0091 utt 447.11 455.60 -X THE SHORT-TERM DYNAMICS HERE, SHOWN AS THE PERCENTAGE OF ACTIONS CHOOSING THE BETTER DECKS, DEMONSTRATED INTERESTING TRAJECTORIES.
utt_0093 utt 458.06 467.31 -X FOR INSTANCE, WE SEE THAT THE ADDICTION PICKED UP THE BETTER DECKS MOST RAPIDLY, BUT AS TIME GOES ON, IT WAS CAUGHT UP BY OTHERS.
utt_0095 utt 467.72 476.72 -X THIS IS DUE TO THE FACT THAT IT DOESN'T CARRY MUCH MEMORY FOR THE NEGATIVE REWARDS AND THUS TENDING TO STICK TO THE SHORT-TERM REWARD PAYOFF.
utt_0097 utt 477.26 488.50 -X THE T-SNE PLOTS OF THE BEHAVIORAL TRAJECTORIES ALSO DEMONSTRATED THAT THESE AGENTS FORM LITTLE CLUSTERS LABELED BY THEIR REWARD BIAS.
utt_0099 utt 490.16 494.71 -X WE ALSO TESTED OUR APPROACH ON THE PACMAN GAME IN THE NONSTATIONARY SETTING.
utt_0100 utt 494.71 500.88 -X FOR THE POSITIVE AND NEGATIVE STREAMS, WE INSTALLED THREE INDEPENDENT NON-STATIONARITIES.
utt_0101 utt 501.36 507.44 -X IN REWARD MUTING, WE RANDOMLY MUTE ONE, BOTH OR NEITHER STREAMS.
utt_0102 utt 507.44 514.45 -X FOR INSTANCE, IF THE POSITIVE STREAM IS MUTED, EATING PACDOT RECEIVES A REWARD OF ZERO.
utt_0103 utt 515.28 522.26 -X IN REWARD SCALING, WE RANDOMLY AMPLIFY ONE, BOTH OR NEITHER STREAMS OF REWARDS.
utt_0104 utt 522.26 529.56 -X FOR INSTANCE, IF THE POSITIVE STREAM IS SCALED, EACH REWARD IS one hundred TIMES LARGER THAN BEFORE.
utt_0105 utt 529.56 535.73 -X IN REWARD FLIPPING, WE RANDOMLY FLIP ONE, BOTH OR NEITHER STREAMS OF REWARDS.
utt_0106 utt 535.73 542.16 -X FOR INSTANCE, IF THE POSITIVE STREAM IS FLIPPED, EACH REWARD IS NOW THE NEGATIVE AMOUNT OF THAT REWARD.
utt_0108 utt 542.32 547.48 -X TO SIMULATE A LIFELONG LEARNING SETTING, WE HAVE THIS STOCHASTIC CHANGE HAPPEN EVERY N ROUNDS.
utt_0110 utt 548.40 552.34 -X HERE WE SEE THAT THE ORANGE CURVE IS OUR SPLIT QL AGENT.
utt_0111 utt 552.37 564.41 -X THE AVERAGED SCORE OVER one thousand ITERATIONS DEMONSTRATED THAT SPLIT QL IS VERY COMPETITIVE IN THE STATIONARY SETTING, AND BEATS BASELINES IN ALMOST ALL NONSTATIONARY SETTING.
utt_0113 utt 564.41 578.17 -X IF WE LOOK CLOSER IN THE REWARD FLIPPING SCENARIO, WE ALSO VALIDATED OUR EVOLUTIONARY PSYCHIATRY UNDERSTANDING, THAT SEVERAL MENTAL VARIANTS OF SPLIT QL PERFORMS EVEN BETTER THAN SPLIT
utt_0115 utt 578.45 580.41 -X QL AND ALL THE BASELINES.
utt_0116 utt 580.60 587.06 -X FOR INSTANCE, THE ADHD AGENT, DUE TO ITS FAST SWITCHING BIAS, ADAPTS QUITE WELL IN THESE
utt_0117 utt 587.41 588.89 -X CONDITIONS.
utt_0118 utt 588.89 593.88 -X HERE ARE SOME MENTAL AGENTS IN ACTION AFTER one thousand ITERATIONS OF TRAINING.
utt_0119 utt 593.88 596.57 -X WE CAN SEE MORE INTERESTING BEHAVIORS GOING ON.
utt_0120 utt 596.72 607.48 -X FOR INSTANCE, THE CHRONIC PAIN AGENT DOESN'T SEEM TO CARE MUCH ABOUT THE REWARD, SO IT ONLY MOVES WHENEVER AN IMMINENT THREAT IS PRESENT, TO AVOID BEING EATEN BY THE GHOST.
utt_0122 utt 607.73 613.79 -X THE ADDICTION AGENT ON THE OTHER HAND, ONLY CARES OF EATING AS MUCH DOTS AS POSSIBLE,
utt_0123 utt 613.79 618.65 -X EVEN IF THAT MEANS GOING BEHIND A GHOST IN A DANGEROUSLY CLOSE DISTANCE.
utt_0124 utt 619.00 621.66 -X THERE ARE MANY INTERESTING ROUTES FROM HERE.
utt_0125 utt 621.97 627.67 -X WE AIM TO FURTHER INVESTIGATE THE OPTIMAL REWARD BIAS IN DIFFERENT CRITERIA, SUCH AS
utt_0126 utt 629.53 633.66 -X SURVIVING THE LONGEST OR HAVING THE HIGHEST SCORE.
utt_0127 utt 633.78 639.23 -X WE ARE ALSO INTERESTED TO SEE MULTIPLE REWARD-BIASED AGENT INTERACTING IN THE SAME ARENA.
utt_0128 utt 639.70 647.52 -X WE ARE ALSO LEARNING THE REWARD BIAS PARAMETERS FROM REAL PATIENT DATA, IN ORDER TO BETTER TUNE OUR MODELS FOR CLINICAL APPLICATIONS.
utt_0130 utt 648.47 654.56 -X LAST BUT NOT LEAST, WE ARE GOING DEEP, TO TEST THE TWO-STREAM MECHANISM IN DEEP Q NETWORKS.
utt_0131 utt 655.35 656.92 -X THANK YOU FOR YOUR INTEREST.
utt_0132 utt 657.02 660.48 -X IF YOU HAVE ANY QUESTION, FEEL FREE TO REACH OUT TO US.
utt_0133 utt 660.76 668.35 -3.4518 THE FULL PAPER CAN BE ACCESSED ON ARXIV AND THE CODES ARE AVAILABLE AT OUR GITHUB REPOSITORY.
