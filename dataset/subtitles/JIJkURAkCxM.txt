utt_0001 utt 7.33 17.27 -X INSTITUTE FOR INTELLIGENT SYSTEMS AND THE UNIVERSITY OF TUBINGEN FOR THEIR PAPER CALLED GIRAFFE, WHICH LOOKS AT THE TASK OF CONTROLLABLE IMAGE SYNTHESIS.
utt_0004 utt 17.27 26.23 -X IN OTHER WORDS, THEY LOOK AT GENERATING NEW IMAGES AND CONTROLLING WHAT WILL APPEAR, THE OBJECTS AND THEIR POSITIONS AND ORIENTATIONS, THE BACKGROUND, ETC.
utt_0006 utt 26.41 34.03 -X USING A MODIFIED GAN ARCHITECTURE, THEY CAN EVEN MOVE OBJECTS IN THE IMAGE WITHOUT AFFECTING THE BACKGROUND OR THE OTHER OBJECTS!
utt_0010 utt 41.97 50.58 -X AS YOU ALREADY KNOW, IF YOU REGULARLY WATCH MY VIDEOS, CONVENTIONAL GAN ARCHITECTURES WORK WITH AN ENCODER AND A DECODER SETUP, JUST LIKE THIS.
utt_0012 utt 50.58 56.28 -X DURING TRAINING, THE ENCODER RECEIVES AN IMAGE, ENCODES IT INTO A CONDENSED REPRESENTATION,
utt_0013 utt 56.28 60.85 -X AND THE DECODER TAKES THIS REPRESENTATION TO CREATE A NEW IMAGE CHANGING THE STYLE.
utt_0014 utt 60.85 72.40 -X THIS IS REPEATED NUMEROUS TIMES WITH ALL THE IMAGES WE HAVE IN OUR TRAINING DATASET SO THAT THE ENCODER AND DECODER LEARN HOW TO MAXIMIZE THE RESULTS OF THE TASK WE WANT TO ACHIEVE DURING TRAINING.
utt_0017 utt 72.40 80.63 -X ONCE THE TRAINING IS DONE, YOU CAN SEND AN IMAGE TO THE ENCODER, AND IT WILL DO THE SAME PROCESS, GENERATING A NEW AND UNSEEN IMAGE FOLLOWING YOUR NEEDS.
utt_0019 utt 80.63 91.67 -X IT WILL WORK VERY SIMILARLY WHATEVER THE TASK, WHETHER IT IS TO TRANSLATE AN IMAGE OF A FACE INTO ANOTHER STYLE LIKE A CARTOONIFIER OR CREATE A BEAUTIFUL LANDSCAPE OUT OF A QUICK DRAFT.
utt_0022 utt 91.73 101.88 -X USING ONLY THE DECODER, WHICH WE ALSO CALL THE GENERATOR SINCE IT IS THE MODEL RESPONSIBLE FOR CREATING THE NEW IMAGE, WE CAN WALK IN THIS ENCODED INFORMATION SPACE
utt_0024 utt 101.88 108.12 -X AND SAMPLE INFORMATION THAT WE SEND THE GENERATOR TO GENERATE AN INFINITE AMOUNT OF NEW IMAGES.
utt_0025 utt 108.12 116.63 -X THIS ENCODED INFORMATION SPACE IS OFTEN REFERRED TO AS THE LATENT SPACE, AND THE INFORMATION WE USE TO GENERATE THE NEW IMAGE THE LATENT CODE.
utt_0027 utt 116.63 128.70 -X WE BASICALLY SELECT SOME LATENT CODE RANDOMLY WITHIN THIS OPTIMAL SPACE, AND IT GENERATES A NEW RANDOM IMAGE FOLLOWING THE TASK WE WANT TO ACHIEVE, FOLLOWING A TRAINING PROCESS OF THIS GENERATOR, OF COURSE.
utt_0030 utt 128.70 140.60 -X THIS IS INCREDIBLY COOL, BUT AS I JUST SAID, THE IMAGE IS COMPLETELY RANDOM, AND WE HAVE NO OR FEW IDEAS ON WHAT IT WILL LOOK LIKE, WHICH IS ALREADY A LOT LESS USEFUL FOR CREATORS.
utt_0032 utt 140.60 143.71 -X THIS IS THE PROBLEM THEY ATTACKED WITH THIS PAPER.
utt_0033 utt 143.71 153.82 -X INDEED, BY TAKING LATENT CODES OF THE SHAPE AND APPEARANCES OF OBJECTS AND SENDING IT TO THE DECODER, OR GENERATOR, THEY ARE ABLE TO CONTROL THE POSE OF THE OBJECTS,
utt_0035 utt 153.82 162.33 -X WHICH MEANS THEY CAN MOVE THEM AROUND, CHANGE THEIR APPEARANCES, ADD OTHER OBJECTS, CHANGE THE BACKGROUND AND EVEN CHANGE THE CAMERA POSE.
utt_0037 utt 162.33 170.30 -X ALL THESE TRANSFORMATIONS CAN BE DONE INDEPENDENTLY ON EACH OBJECT OR BACKGROUND, WITHOUT AFFECTING ANYTHING ELSE IN THE IMAGE!
utt_0039 utt 170.30 181.44 -X AS YOU CAN SEE, IT IS MUCH BETTER THAN OTHER GAN-BASED APPROACHES THAT TYPICALLY CANNOT DISENTANGLE THE OBJECTS FROM ONE ANOTHER AND ARE ALL AFFECTED BY THE MODIFICATION OF A SPECIFIC OBJECT.
utt_0042 utt 181.44 193.50 -X THE DIFFERENCE WITH THEIR METHOD IS THAT THEY ATTACK THIS PROBLEM IN A THREE-DIMENSIONAL SCENE REPRESENTATION, JUST LIKE HOW WE SEE THE REAL WORLD, INSTEAD OF STAYING IN THE TWO-DIMENSIONAL IMAGE WORLD AS OTHER GANS DO.
utt_0045 utt 193.50 196.44 -X BUT OTHER THAN THAT, THE PROCESS IS QUITE SIMILAR.
utt_0046 utt 196.44 201.56 -X THEY ENCODE THE INFORMATION, IDENTIFY THE OBJECTS, EDIT THEM INSIDE THE LATENT SPACE,
utt_0047 utt 201.56 203.97 -X AND DECODE IT TO GENERATE THE NEW IMAGE.
utt_0048 utt 203.97 208.64 -X HERE, THERE ARE JUST SOME MORE STEPS TO DO INSIDE THIS LATENT SPACE.
utt_0049 utt 208.64 220.61 -X WE CAN SEE THIS AS A COMBINATION OF THE CLASSICAL GAN IMAGE SYNTHESIS NETWORK WITH A NEURAL RENDERER USED TO GENERATE THE threeD SCENE FROM THE IMAGES SENT TO THE NETWORK, AS WE WILL SEE.
utt_0052 utt 220.61 223.26 -X THERE ARE THREE MAIN STEPS TO ACHIEVE THAT.
utt_0053 utt 223.26 230.65 -X AFTER ENCODING THE INPUT IMAGE, MEANING THAT WE ARE ALREADY IN THE LATENT SPACE, THE FIRST STEP IS TO TRANSFER THE IMAGE INTO A threeD SCENE.
utt_0055 utt 230.75 238.11 -X BUT NOT JUST A SIMPLE threeD SCENE, A threeD SCENE COMPOSED OF threeD ELEMENTS, WHICH ARE THE OBJECTS AND BACKGROUND.
utt_0057 utt 238.11 249.28 -X THIS WAY OF SEEING THE IMAGES AS A SCENE COMPOSED OF GENERATED VOLUME RENDERINGS ALLOWS THEM TO CHANGE THE CAMERA ANGLE IN THE GENERATED IMAGE AND CONTROL THE OBJECTS INDEPENDENTLY.
utt_0059 utt 249.28 254.53 -X THIS IS ACHIEVED USING A SIMILAR MODEL AS THE PAPER I PREVIOUSLY COVERED CALLED NERF,
utt_0060 utt 254.53 260.67 -X BUT INSTEAD OF USING A SINGLE MODEL TO GENERATE THE ENTIRE LOCKED SCENE FROM THE INPUT IMAGE,
utt_0061 utt 260.67 265.47 -X THEY INDEPENDENTLY GENERATE THE OBJECTS AND BACKGROUND USING TWO SEPARATE MODELS.
utt_0062 utt 265.47 268.35 -X HERE CALLED THE SAMPLED FEATURE FIELDS.
utt_0063 utt 268.35 272.29 -X THE PARAMETERS OF THIS NETWORK ARE ALSO LEARNED DURING TRAINING.
utt_0064 utt 272.29 277.99 -X I WON'T ENTER INTO THE DETAILS, BUT IT IS VERY SIMILAR TO NERF, WHICH I COVERED IN ANOTHER VIDEO.
utt_0066 utt 277.99 282.53 -X IF YOU WOULD LIKE TO HAVE MORE DETAILS ON SUCH NETWORKS, IT IS APPEARING ON THE TOP
utt_0067 utt 283.93 286.85 -X RIGHT CORNER RIGHT NOW AND IN THE DESCRIPTION.
utt_0068 utt 287.39 294.98 -X HAVING THIS SCENE WITH DISENTANGLED ELEMENTS, WE CAN EDIT THEM INDIVIDUALLY WITHOUT AFFECTING THE REST OF THE IMAGE.
utt_0070 utt 294.98 296.58 -X THIS IS THE SECOND STEP.
utt_0071 utt 296.58 301.51 -X THEY CAN DO WHATEVER THEY WANT TO THE OBJECT, LIKE CHANGING ITS POSITION AND ORIENTATION.
utt_0072 utt 301.51 305.28 -X IN OTHER WORDS, THEY CHANGE THE POSE OF THE OBJECTS OR BACKGROUND.
utt_0073 utt 305.28 309.83 -X AT THIS POINT, THEY CAN EVEN ADD NEW OBJECTS PLACED WHEREVER THEY WANT.
utt_0076 utt 318.72 323.46 -3.6802 FINALLY, WE HAVE TO COME BACK TO THE twoD WORLD OF NATURAL IMAGES.
