utt_0000 utt 0.32 6.16 -X HELLO! MY NAME IS ALBERT WEBSON. I'M A PHD STUDENT AT BROWN UNIVERSITY ADVISED BY ELLIE PAVLICK.
utt_0001 utt 6.19 17.55 -X IN THIS TALK, I WILL PRESENT OUR EMNLP PAPER: “ARE ‘UNDOCUMENTED WORKERS’ THE SAME AS ‘ILLEGAL ALIENS’? DISENTANGLING DENOTATION AND CONNOTATION IN VECTOR SPACES”. SO WHAT ARE DENOTATION AND
utt_0003 utt 17.55 27.95 -X CONNOTATION? AND WHY SHOULD WE CARE? BEFORE WE GET PHILOSOPHICAL ABOUT DEFINITIONS, I'D LIKE TO START WITH A PRACTICAL EXAMPLE JUST TO ILLUSTRATE THE PROBLEM. SO SUPPOSE WE USE OUR TOP TWO FAVORITE
utt_0005 utt 27.95 42.90 -X SEARCH ENGINES AND ASK THEM “ARE UNDOCUMENTED WORKERS GOOD?” THEN YOU GET RESULTS THAT SHOW OVERWHELMING EMPIRICAL EVIDENCE ON THE ECONOMIC BENEFITS OF IMMIGRANTS. BUT SUPPOSE INSTEAD THAT YOU SEARCH FOR “ARE ILLEGAL ALIENS GOOD?” THEN YOU GET ARTICLES THAT ARE STATISTICAL P-HACKING
utt_0008 utt 42.90 48.15 -X AT BEST AND OUTRIGHT XENOPHOBIA AT WORST. SO I KNOW THIS MIGHT BE UNSURPRISING TO MANY OF YOU,
utt_0009 utt 48.15 58.68 -X BUT LET'S RESET OUR ASSUMPTIONS FOR A SECOND. IN AMERICAN POLITICS, BOTH UNDOCUMENTED WORKERS AND ILLEGAL ALIENS REFER TO ROUGHLY THE SAME GROUP OF PEOPLE, THEN WHY SHOULD WE GET DIFFERENT
utt_0011 utt 58.68 63.41 -X RESULTS WHEN WE ASK FOR A REPRESENTATION OF THE SAME REFERENCE IN THE REAL WORLD?
utt_0012 utt 63.41 68.15 -X SO MAYBE WE SAY THAT “OKAY I SEE THE PROBLEM. THESE TWO PHRASES MEAN THE SAME THING.
utt_0013 utt 68.15 79.25 -X SO WHY DON'T WE JUST USE SOME CO-REFERENCE RESOLUTION OR ENTITY LINKING AND REPRESENT THEM AS THE SAME TOKEN IN OUR VOCABULARY?” BUT ASSERTING THAT THEY HAVE EXACTLY THE SAME SEMANTIC MEANING
utt_0015 utt 79.25 83.92 -X IS NOT RIGHT EITHER. SUPPOSE IN A DIFFERENT NLP SETTING. LET'S SAY NATURAL LANGUAGE INFERENCE.
utt_0016 utt 84.02 88.26 -X SUPPOSE WE ARE GIVEN A PREMISE “MARCO RUBIO BELIEVES THAT UNDOCUMENTED WORKERS ARE BAD”
utt_0017 utt 88.34 99.41 -X AND A HYPOTHESIS “MARCO RUBIO BELIEVES THAT ILLEGAL ALIENS ARE BAD”. THEN SHOULD THIS BE ENTAILMENT OR CONTRADICTION? WELL BERT SAYS IT'S ninety-three% ENTAILMENT, BUT THAT'S JUST HIGH
utt_0019 utt 99.41 111.13 -X LEXICAL OVERLAP. INSTEAD LET'S TRY A SHORT PROOF BY CONTRADICTION HERE: SUPPOSE THE CONTRARY THAT “UNDOCUMENTED WORKERS” AND “ILLEGAL ALIENS” REALLY HAVE EXACTLY THE SAME MEANING, THEN WE SHOULD BE
utt_0021 utt 111.13 120.95 -X ABLE TO SUBSTITUTE THE TWO PHRASES BETWEEN THE TWO SENTENCES, BUT THEN THE TWO SENTENCES WOULD JUST TRIVIALLY ENTAIL EACH OTHER, AND WE KNOW FOR A FACT THAT IT'S NOT A TRIVIAL ENTITLEMENT.
utt_0023 utt 120.95 126.33 -X SO THESE TWO PHRASES CANNOT BE INTERCHANGEABLE, THUS THEY CANNOT HAVE THE SAME SEMANTIC MEANING.
utt_0025 utt 131.25 141.59 -X IN FACT, IT WAS POPULARIZED BY FREGE’S FAMOUS SENSE AND REFERENCE PAPER IN one thousand, eight hundred and ninety-four, IN WHICH HE ARGUED THAT SEMANTIC MEANING MUST HAVE AT LEAST TWO COMPONENTS: ONE BEING WHAT YOU’RE REFERRING
utt_0027 utt 141.59 152.60 -X TO. THE OTHER BEING *HOW* ARE YOU PRESENTING THAT REFERENCE. BUT DESPITE THE PROBLEM WAS KNOWN WAY BACK IN one thousand, eight hundred and ninety-four, MOST SEMANTICISTS DIDN'T REALLY TAKE THE NON-REFERENCE PART TOO SERIOUSLY,
utt_0029 utt 152.60 157.50 -X AND THEY MOSTLY FAVOR THEORIES WHICH REDUCE EVERYTHING TO JUST REFERENCE AND TRUTH CONDITIONS.
utt_0030 utt 158.04 169.30 -X HOWEVER, HALF A CENTURY LATER, FOLKS LIKE NED BLOCK AND GILBERT HARMAN PROPOSED A FRAMEWORK KNOWN AS CONCEPTUAL ROLE SEMANTICS, WHERE THEY ARGUED THAT, IN ADDITION TO REFERENCE, SEMANTICS
utt_0032 utt 169.30 174.33 -X MUST ALSO BE GROUNDED ON MENTAL OR PSYCHOLOGICAL CONCEPTS AND HOW HUMANS USE SUCH CONCEPTS.
utt_0033 utt 174.36 179.90 -X SO FOR THEM, “UNDOCUMENTED WORKERS” AND “ILLEGAL ALIENS” ARE THE SAME THING IN THE EXTERNAL WORLD,
utt_0034 utt 179.90 189.93 -X BUT INTERNAL TO OUR BRAIN, THEY'RE ENCODED AS DIFFERENT MENTAL CONCEPTS, AND WE USE THOSE CONCEPTS DIFFERENTLY. AND THAT MENTAL CONCEPT MUST ALSO BE A VALID PART OF THE MEANING OF WORDS
utt_0036 utt 189.93 202.33 -X IN ADDITION TO REFERENCE. SO IF YOU TAKE THE UNION OF BOTH REFERENCE AND CONCEPT, WHAT YOU GET IS… TWO-FACTOR SEMANTICS. AND VERY EXCITINGLY, IT HAS GRADUATED FROM ARMCHAIR LINGUISTICS, RECEIVING
utt_0038 utt 202.33 213.79 -X ENDORSEMENT FROM PROMINENT COGNITIVE SCIENTISTS SUCH AS SUSAN CAREY. AND THIS FORMS THE BASIS OF OUR WORK TODAY, WHERE WE ARGUE THAT, IN ORDER TO SOLVE THE MOTIVATING EXAMPLES EARLIER, WE SHOULD
utt_0040 utt 213.79 223.42 -X EXPLICITLY MODEL THE TWO FACTORS OF SEMANTICS AND WE PROPOSE A NEURAL MODEL THAT DISENTANGLES A PRETRAINED REPRESENTATION AS INDEPENDENT DENOTATION AND CONNOTATION REPRESENTATIONS.
utt_0042 utt 223.48 232.92 -X AND NOTE REAL QUICK THAT, IN THIS PAPER, WE CALL THEM “DENOTATION” AND “CONNOTATION” IN ORDER TO AVOID CONFUSION WITH THE TECHNICAL DEFINITIONS OF “REFERENCE” AND “CONCEPT” FROM EXISTING
utt_0045 utt 238.76 244.51 -X LABELS, BUT IN THE INTEREST OF TIME, THIS TALK ONLY PRESENTS THE FIRST ONE. IN THIS EXPERIMENT,
utt_0046 utt 244.51 259.33 -X OUR CORPUS IS THE CONGRESSIONAL RECORD, WHICH IS THE OFFICIAL TRANSCRIPT OF THE FLOOR SPEECHES OF THE UNITED STATES CONGRESS. AND HERE OUR DENOTATION IS GROUNDED ON WHICH LEGISLATION EACH SPEECH IS REFERRING TO, AND THE CONNOTATION IS GROUNDED ON THE PARTY AFFILIATION OF THE SPEAKER.
utt_0049 utt 259.58 269.85 -X BECAUSE AMERICAN POLITICS IS SO SADLY POLARIZED THESE DAYS, WE CAN ACTUALLY REASONABLY ASSUME THAT THE CONNOTATION OF EVERY SPEECH IS SIMPLY “TO ADVANCE THE PARTISAN AGENDA OF THE SPEAKER”.
utt_0051 utt 270.27 274.27 -X OVERALL, THE IDEA IS TO EMULATE A HUMAN WATCHING POLITICAL SPEECHES,
utt_0052 utt 274.27 278.08 -X WHERE THEY GET THIS DENOTATION AND CONNOTATION LABEL FROM THE TV SUBTITLES,
utt_0053 utt 278.08 283.42 -X AND THAT ENABLES HUMANS TO GRADUALLY LEARN TO DISENTANGLE DENOTATION [FROM] CONNOTATION.
utt_0054 utt 283.71 287.20 -X AND TO IMPLEMENT THIS, WE PROPOSE AN ADVERSARIAL NEURAL NETWORK
utt_0056 utt 292.22 296.70 -X THE FULL MODEL LOOKS SOMEWHAT COMPLICATED, BUT I PROMISE IT'S ACTUALLY QUITE INTUITIVE,
utt_0057 utt 296.70 310.66 -X ESPECIALLY SINCE THE MODEL IS SYMMETRICAL, AND WE ONLY NEED TO GO OVER HALF OF THIS. SO LET'S FOCUS ON THE DENOTATION SPACE FOR NOW. ITS GOAL IS TO PRESERVE AS MUCH DENOTATION INFORMATION AS POSSIBLE, WHILE REMOVING AS MUCH CONNOTATION INFORMATION AS POSSIBLE FROM THE PRETRAINED.
utt_0060 utt 311.07 316.22 -X SO WE START WITH A SENTENCE ENCODER, WHICH IN PRINCIPLE COULD BE ANY NEURAL MODULE
utt_0061 utt 316.32 321.50 -X BUT FOR REASONS WE EXPLAIN IN OUR PAPER, WE USE A STATIC BAG OF EMBEDDINGS IN THIS EXPERIMENT,
utt_0062 utt 321.50 332.71 -X INITIALIZED WITH WORDtwoVEC PRETRAINED ON THE SAME CORPUS. AND THEN WE FEED THE ENCODED SENTENCE INTO TWO MLP CLASSIFIER PROBES. THE DENOTATION PROBE TRIES TO CLASSIFY THE REFERENCE, THAT IS,
utt_0064 utt 332.71 346.40 -X THE LEGISLATION UNDER DISCUSSION, AND THUS MEASURING HOW MUCH DENOTATION INFORMATION THERE IS IN THE SENTENCE ENCODED BY THE DENOTATION SPACE. WHEREAS THE CONNOTATION PROBE TRIES TO CLASSIFY PARTISANSHIP, AND THUS MEASURING HOW MUCH CONNOTATION INFORMATION THERE IS
utt_0067 utt 346.40 351.30 -X IN THE ENCODED SENTENCE. NOW BECAUSE WE ARE ACTUALLY TRYING TO REMOVE CONNOTATION HERE,
utt_0068 utt 351.30 364.95 -X WE CONSTRUCT AN ADVERSARIAL LOSS, WHERE WHEN YOU COMBINE THESE TWO LOSSES TOGETHER AND BACKPROP THAT TO THE DENOTATION SPACE, WHAT YOU GET IS THAT THE DENOTATION SPACE WILL TRY TO ENCODE SENTENCES
utt_0070 utt 364.95 370.95 -X SUCH THAT THE CONNOTATION PREDICTION IS AT RANDOM, AND THAT IS WHY WE HAVE THE KL DIVERGENCE HERE.
utt_0071 utt 371.20 385.64 -X THIS EFFECTIVELY REMOVES ANY CONNOTATION INFORMATION FROM THE DENOTATION SPACE. AND ON THE OTHER HAND, BECAUSE THE DENOTATION PROBE LOSS IS STILL THE USUAL CLASSIFICATION LOSS, SO IT WILL ENCOURAGE THE DENOTATION SPACE TO PRESERVE AS MUCH DENOTATION INFORMATION AS POSSIBLE.
utt_0074 utt 386.40 390.95 -X AND NOTE THAT THE PROBES THEMSELVES ARE ALWAYS GRADIENT UPDATED BY THEIR OWN LOSSES,
utt_0075 utt 390.95 397.51 -X SO THEY WILL ALWAYS DILIGENTLY MEASURE INFORMATION REGARDLESS OF WHAT THE DENOTATION SPACE IS DOING.
utt_0076 utt 397.51 408.29 -X SO THAT'S ONE HALF THE MODEL, AND AGAIN THE OTHER HALF IS EXACTLY SYMMETRICAL, WHERE THE CONNOTATION SPACE WILL INSTEAD PRESERVE CONNOTATION AND USE AN ADVERSARIAL LOSS TO REMOVE DENOTATION.
utt_0078 utt 408.80 414.66 -X AND LASTLY WE HAVE A RECONSTRUCTION LOSS, WHICH ENSURES THAT WHEN YOU TAKE THESE TWO SPACES
utt_0079 utt 414.75 419.56 -X TOGETHER, THEY MUST PRESERVE ALL THE SEMANTIC MEANING OF THE ORIGINAL [PRETRAINED] SPACE,
utt_0080 utt 419.56 430.58 -X AS SUPPOSED TO DEGENERATIVELY REPLACE MEANING WITH PREDICTIVE FEATURES THAT ONLY MAXIMIZE PROBE ACCURACIES. AND FINALLY, WHEN YOU COMBINE ALL THESE LOSSES TOGETHER, THE WHOLE NETWORK IS
utt_0082 utt 430.58 437.22 -X TRAINED END-TO-END. SO, DOES THIS WORK? WELL LET'S START WITH A SMALL QUALITATIVE EXAMPLE
utt_0083 utt 438.05 443.78 -X HERE IS A PCA OF THE PRETRAINED SPACE VS. OUR DISENTANGLED DENOTATION SPACE. BECAUSE IT'S A PCA,
utt_0084 utt 443.97 455.18 -X DISTANCE ACTUALLY MATTERS HERE, AND YOU CAN SEE THAT ”UNDOCUMENTED WORKERS” AND “ILLEGAL ALIENS” ARE MUCH CLOSER TO EACH OTHER IN OUR DENOTATION SPACE, REFLECTING THE FACT THAT THESE TWO PHRASES
utt_0086 utt 455.18 460.87 -X DENOTE THE SAME REFERENCE IN THE EXTERNAL WORLD. AND HERE'S ANOTHER FAVORITE EXAMPLE OF MINE:
utt_0087 utt 460.87 470.70 -X “GOVERNMENT-RUN HEALTHCARE” VS. “PUBLIC OPTION”. AGAIN, DESPITE THEIR DIFFERENCES IN PARTISAN CONNOTATION, THEY MOSTLY REFER TO THE SAME THING. THEREFORE THEY'RE MUCH CLOSER IN OUR DENOTATION
utt_0089 utt 470.70 480.90 -X SPACE COMPARED TO THE PRETRAINED. AND THERE ARE MANY MORE EXAMPLES, PLEASE REFER TO OUR PAPER FOR THE FULL LIST. WHAT I DO WANT TO SHOW NOW IS THAT THESE AREN'T JUST SOME CHERRY-PICKED EXAMPLES,
utt_0091 utt 480.90 492.07 -X BUT IT ACTUALLY WORKS AT SCALE. SO HERE ARE SOME RANDOMLY SAMPLED PARTISAN WORDS, COLORED BY THEIR POLICY TOPIC. AND HERE YOU CAN SEE THAT THEY'RE SCATTERED AROUND WITH NO APPARENT STRUCTURE IN
utt_0093 utt 492.07 497.55 -X THE PRETRAINED SPACE. HOWEVER, WITH EXACTLY THE SAME T-SNE BUT FOR OUR DENOTATION SPACE,
utt_0094 utt 497.55 502.22 -X THEY'RE MUCH NICELY CLUSTERED ACCORDING TO THEIR POLICY DENOTATION. AND MEANWHILE,
utt_0095 utt 502.22 506.73 -X THE SAME IDEA WORKS ON THE CONNOTATION SPACE AS WELL. SO HERE ARE THE SAME WORDS AGAIN,
utt_0096 utt 506.73 511.59 -X JUST COLORED BY THEIR PARTISANSHIP. AND AGAIN NO APPARENT STRUCTURE IN THE PRETRAINED SPACE,
utt_0097 utt 511.59 518.07 -X BUT IN OUR CONNOTATION SPACE, THE CONNOTATION STRUCTURES ARE AS CLEAR AS LINEARLY SEPARABLE. SO
utt_0098 utt 518.12 522.67 -X THE PICTURES ARE LOOKING PRETTY GOOD, BUT HOW DO WE *QUANTITATIVELY* MEASURE SUCCESS?
utt_0099 utt 523.05 527.69 -X WELL, TO DO THAT, WE INTRODUCE A SIMPLE AND INTERPRETABLE HOMOGENEITY METRIC,
utt_0100 utt 527.69 537.04 -X BASED ON THE PERCENTAGE OF NEAREST NEIGHBORS THAT SHARES THE SAME COLOR AND HERE COLORS ARE POLICY DENOTATION, BUT IT COULD BE CONNOTATIONS AS WELL. SO FOR EXAMPLE, WE CAN MEASURE THE
utt_0102 utt 537.04 548.17 -X HOMOGENEITY OF THE NEIGHBORHOOD AROUND THE PHRASE “VIOLENCE AGAINST WOMEN ACT”. SUPPOSE WE TAKE ITS TOPminus four NEAREST NEIGHBORS, three OF WHICH ARE THE SAME PURPLE COLOR AS THE CENTER WORD. THAT IS, three OVER
utt_0104 utt 548.17 553.26 -X four NEIGHBOR WORDS HAVE THE SAME POLICY DENOTATION, SO ITS DENOTATION HOMOGENEITY IS three DIVIDED BY four OR
utt_0105 utt 554.25 559.85 -X seventy-five%. AND HERE'S ANOTHER EXAMPLE: LET'S MEASURE THE NEIGHBORHOOD OF “FAMILY FRIENDLY WORKPLACE ACT”,
utt_0106 utt 559.85 564.91 -X AND ALL four OF NEIGHBOR WORDS HAVE THE SAME GREEN COLOR AS THE CENTER WORD, SO ITS HOMOGENEITY
utt_0107 utt 564.94 579.66 -X IS one hundred. AND YOU CAN SEE HOW WE CAN COMPUTE THIS HOMOGENEITY OVER A LARGE SET OF WORDS TO APPROXIMATE HOW HOMOGENEOUS THE NEIGHBORHOOD STRUCTURES ARE WITH RESPECT TO SOME DESIRED ATTRIBUTE. AND THAT'S HOW WE QUANTITATIVELY EVALUATE OUR DISENTANGLE REPRESENTATIONS.
utt_0110 utt 580.49 583.82 -X ON THE LEFT CHART, WHEN WE ARE MEASURING DENOTATION HOMOGENEITY,
utt_0111 utt 583.85 589.33 -X UNSURPRISINGLY, OUR DENOTATION SPACE ENCODES MUCH MORE DENOTATION STRUCTURE THAN THE PRETRAINED ONE,
utt_0112 utt 589.33 593.97 -X WHEREAS OUR CONNOTATION SPACE REMOVES THAT DENOTATION STRUCTURE FROM THE PRETRAINED.
utt_0113 utt 593.97 598.03 -X CONVERSELY, FOR THE CHAR ON THE RIGHT, MEASURING CONNOTATION HOMOGENEITY,
utt_0114 utt 598.25 602.25 -X WE SEE THAT THE CONNOTATION SPACE SUCCESSFULLY ENCODES CONNOTATION STRUCTURE,
utt_0115 utt 602.25 606.32 -X WHEREAS THE DENOTATION SPACE REMOVES THAT CONNOTATION STRUCTURE FROM THE PRETRAINED.
utt_0116 utt 606.67 618.86 -X SO THAT CONCLUDES THE INTRINSIC EVALUATION OF OUR MODELS. LASTLY, SINCE WE AIM TO BE MORE THAN JUST A THEORETICAL EXERCISE, WE ALSO VERIFY THE USEFULNESS OF OUR DISENTANGLE REPRESENTATIONS
utt_0118 utt 618.86 630.00 -X IN DOWNSTREAM TASKS SUCH AS INFORMATION RETRIEVAL. SO HERE WE ARE USING A CORPUS OF POLITICAL NEWS ARTICLES COLLECTED FOR A SEMEVAL TASK WHERE EACH ARTICLE IS LABELED BY THE PARTISAN LEANING OF THE
utt_0120 utt 630.00 634.00 -X PUBLISHER. WE ALSO GATHER A COLLECTION OF SEARCH QUERIES FROM A RECENT GALLUP POLL.
utt_0121 utt 634.64 638.38 -X AND HERE WE SET UP A PRETTY STANDARD INFORMATION RETRIEVAL PIPELINE.
utt_0122 utt 638.73 651.83 -X WE START WITH A LARGE COLLECTION OF DOCUMENTS, AND THEN WE USE A FAST, TF-IDF BASED MODEL TO PRE-SELECT THE TOPminus five thousand MOST RELEVANT DOCUMENTS. THEN WE FEED THEM INTO A NEURAL RELEVANCE MATCHING
utt_0124 utt 651.83 662.77 -X MODEL TRAINED ON MS MARCO, AND THIS PROVIDES US AN END-TO-END RETRIEVAL OF THE TOPminus one hundred MOST RELEVANT DOCUMENTS. CRUCIALLY, DRMM USES AN EMBEDDING LAYER, WHERE WE CAN EASILY SWAP OUT THE
utt_0126 utt 662.77 672.59 -X PRETRAINED ONE WITH OUR DENOTATION OR CONNOTATION VECTORS, AND WE CAN COMPARE THE DIFFERENCES OF THE RETRIEVED DOCUMENTS USING DIVERSITY METRICS, SINCE EACH DOCUMENT IS LABELED WITH A PARTISANSHIP.
utt_0128 utt 673.23 686.29 -X SO HERE, HIGHER 𝛼-NDCG MEANS MORE DIVERSE, SO YOU CAN SEE THAT DENOTATION VECTORS RETRIEVED A MORE DIVERSE SET OF DOCUMENTS FOR BOTH THE TOPminus ten AND THE TOPminus ten0 RESULTS. AND TO VISUALIZE THIS,
utt_0130 utt 686.29 699.73 -X WE OBSERVE THAT THIS IS ESPECIALLY TRUE FOR RIGHT-LEANING QUERIES, WHERE IF YOU USE THE PRETRAINED SPACE, RIGHT-LEANING QUERIES RETURN OVERWHELMINGLY RIGHT-LEANING ARTICLES. BUT IF YOU USE OUR DENOTATION SPACE, THEN THE SAME QUERIES RETURN ARTICLES THAT ARE PARTISANLY BALANCED.
utt_0133 utt 700.59 705.01 -X SO JUST TO QUICKLY RECAP EVERYTHING. MOTIVATED BY TWO-FACTOR SEMANTICS,
utt_0134 utt 705.01 719.09 -X WE PROPOSE A MODEL WHICH REPRESENTS THE DENOTATION AND CONNOTATION OF LANGUAGE INDEPENDENTLY. WE CONFIRM THAT OUR DISENTANGLED REPRESENTATIONS ENCODE THE DESIRED STRUCTURE BY BOTH QUALITATIVELY EVALUATE THEIR CLUSTERS OF WELL-KNOWN POLITICAL EUPHEMISMS
utt_0137 utt 719.09 724.18 -X AS WELL AS QUANTITATIVELY MEASURE THEIR HOMOGENEITY. AND FOR EXTRINSIC APPLICATION,
utt_0138 utt 724.18 733.05 -X WE SHOWED THAT OUR DENOTATION SPACE IS CAPABLE OF IMPROVING THE VIEWPOINT DIVERSITY OF DOCUMENTS IN AN INFORMATION RETRIEVAL SETTING. AND THAT'S IT. THANK YOU SO MUCH FOR WATCHING.
