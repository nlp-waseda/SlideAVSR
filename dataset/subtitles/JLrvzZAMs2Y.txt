utt_0000 utt 0.02 4.94 -X HI EVERYONE, MY NAME IS ANUBHAV. IN THIS PRESENTATION, I AM GOING TO TALK ABOUT NER MQMRC,
utt_0001 utt 5.26 10.48 -X WHICH IS NAMED ENTITY RECOGNITION USING MULTI-QUESTION MACHINE READING COMPREHENSION.
utt_0002 utt 10.48 23.94 -X THE PRESENTATION IS OUTLINED AS FOLLOWS. WE ARE GOING TO TALK ABOUT WHAT IS NAMED ENTITY RECOGNITION, HOW IT WAS TRADITIONALLY SOLVED USING SEQUENCE LABELING TASK, HOW NER IS NOW BEING SOLVED USING MACHINE READING COMPREHENSION AS A SINGLE QUESTION ANSWERING TASK, AND THEN
utt_0005 utt 23.95 27.41 -X OUR PROPOSED SOLUTION OF MULTI-QUESTION MACHINE READING COMPREHENSION.
utt_0006 utt 27.41 37.75 -X COMING TO WHAT IS NAMED ENTITY RECOGNITION. NER IS BASICALLY A TASK TO CLASSIFY AND LOCATE NAMED ENTITIES IN AN UNSTRUCTURED TEXT. AS YOU CAN SEE IN THIS EXAMPLE, WE HAVE
utt_0008 utt 37.75 43.06 -X THIS UNSTRUCTURED TEXT WHERE WE ARE ABLE TO IDENTIFY MEN'S AS DEPARTMENT, RED AS COLOR,
utt_0009 utt 43.25 54.10 -X CANTABIL AS BRAND, AND SOLID AS THE PATTERN TYPE. NER IS USED IN MULTIPLE PLACES IN THE INDUSTRY SUCH AS IDENTIFYING PRODUCT ATTRIBUTES FROM PRODUCT TEXT, IDENTIFYING THE ATTRIBUTES FROM
utt_0011 utt 54.10 58.87 -X SEARCH QUERIES, AND ALSO IDENTIFYING SLOT VALUES FROM CUSTOMER CONVERSATIONS WITH ALEXA.
utt_0012 utt 58.87 69.17 -X TRADITIONALLY, NER HAS BEEN POSED AS A SEQUENCE LABELING TASK USING BI-LSTM ARCHITECTURE. THE UNSTRUCTURED TEXT GOES IN THE INPUT FOR THE MODEL AND OUT COMES THE BIO LABEL ENCODING FOR EACH OF
utt_0014 utt 69.17 73.91 -X THE RELEVANT TOKENS. IN THIS ARCHITECTURE, IF THERE ARE K ENTITIES, WE WOULD REQUIRE twoK PLUS
utt_0015 utt 73.97 88.50 -X one LABEL. THE PROS FOR THE SEQUENCE LABELING TASK ARE THAT IT CAN CLASSIFY MULTIPLE ENTITIES IN A SINGLE FORWARD PASS, WHICH IS WHY THERE IS FASTER TRAINING AND INFERENCE DUE TO A SINGLE FORWARD PASS FOR ALL ENTITIES. THE CONS INCLUDE THE LABEL SPACE INCREASES WITH THE NUMBER OF INCREASE IN
utt_0018 utt 88.50 98.84 -X THE ENTITIES, THE ENTITY INFORMATION IS NOT INCLUDED IN THE TEXT RATHER IT IS JUST PRESENT IN THE CLASSIFICATION LAYER, AND IF THERE ARE NEW LABELS TO BE ADDED IT NEEDS TO RETRAIN THE COMPLETE
utt_0020 utt 98.84 104.34 -X MODEL AGAIN. IN RECENT LITERATURE, NER HAS ALSO BEEN SOLVED USING MACHINE READING COMPREHENSION.
utt_0021 utt 104.34 119.27 -X MACHINE READING COMPREHENSION IS THE TASK TO READ AND COMPREHEND A GIVEN TEXT AND THEN ANSWER A QUESTION BASED ON IT. WHEN WE POSE NER AS A MACHINE READING COMPREHENSION, EACH ENTITY IS ASKED AS A QUESTION FOR THE CONTEXT AND THE ANSWER IS THE SPAN OF THE ENTITY IN THE UNSTRUCTURED TEXT. THIS
utt_0024 utt 119.27 129.66 -X APPROACH USES PRE-TRAINED WORD TRANSFORMER BASED ARCHITECTURE TO EXTRACT THE SPAN OF THE QUESTION FROM THE CONTEXT. ENTITY VALUE IS OBTAINED USING THE START AND THE END OR BIO LABEL. THE PROS OF
utt_0026 utt 130.20 135.00 -X NER SQMRC APPROACH IS THAT THE ENTITY INFORMATION IS PRESENT IN THE INPUT OF THE MODEL ITSELF.
utt_0027 utt 135.00 145.63 -X THE OUTPUT LABEL SPACE REMAINS CONSTANT AS THERE ARE ONLY TWO LABELS FOR START END OR THREE LABELS FOR BIOLABEL ENCODING FOR ANY KIND OF ENTITIES AND IT CAN ALSO GIVE ANSWERS FOR UNSEEN ENTITIES
utt_0029 utt 145.63 159.53 -X BECAUSE OF THE LARGE PRE-TRAINED WORD MODEL AND NO INCREASE IN THE LABEL SPACE. THE CON FOR THIS APPROACH IS THAT MULTIPLE ENTITIES CANNOT BE EXTRACTED IN A SINGLE FORWARD PASS BECAUSE EACH QUESTION IS JUST ONE SINGLE ENTITY. THIS LEADS TO SLOW TRAINING AND INFERENCE DUE TO MULTIPLE
utt_0032 utt 159.53 174.27 -X PROCESS FOR EXTRACTING ALL THE ENTITIES. WHICH BRINGS US TO OUR PROPOSED SOLUTION NER SQMRC WHICH IS BEST OF BOTH WORLDS. IT IS ABLE TO SCALE UP FOR ENTITIES VERY WELL. IT IS ALSO ABLE TO SCORE FOR MULTIPLE ENTITIES IN A SINGLE FORWARD PASS. THE TRAINING TIME AND THE INFERENCE
utt_0035 utt 174.27 185.01 -X TIME IMPROVES SUBSTANTIALLY BECAUSE OF THIS AND THE ENTITY INFORMATION IS PRESENT WITHIN THE MODEL INPUT AS WELL. OUR PROPOSED SOLUTION MQMRC ASKS QUESTIONS AROUND ALL THE ENTITIES IN A SINGLE
utt_0037 utt 185.01 196.58 -X FORWARD PASS. AS YOU CAN SEE THE DIFFERENCE FOR SQMRC EACH ENTITY IS ASKED A SEPARATE QUESTION WHEREAS FOR MQMRC ALL THE ENTITIES RELATED TO THE SAME CONTEXT ARE ASKED WITHIN A SINGLE FORWARD
utt_0039 utt 196.58 202.14 -X PASS. EACH ENTITY IS SEPARATED BY A SPECIAL TOKEN WHICH WE ADD TO THE WORD VOCABULARY CALLED ENT.
utt_0040 utt 202.30 213.09 -X EACH ENT TOKEN IS RESPONSIBLE FOR HOLDING THE INFORMATION FOR THE ADJACENT ENTITY. THE ENT EMBEDDING FOR EACH ATTRIBUTE IS THEN USED TO PRODUCE THE ENTITY SPECIFIC CONTEXT EMBEDDINGS
utt_0042 utt 213.09 217.86 -X BY PERFORMING THE ELEMENT WISE MULTIPLICATION OF ENT EMBEDDING AND THE CONTEXT EMBEDDINGS.
utt_0043 utt 217.86 232.61 -X THE ENTITY SPECIFIC CONTEXT EMBEDDING IS THEN PASSED ON TO A LINEAR CLASSIFIER LAYER TO PRODUCE THE SPAN ANSWER FOR EACH OF THE ENTITIES. TO CREATE THE INPUT FOR OUR ARCHITECTURE WE GROUP THE ENTITIES WHICH BELONG TO THE SAME PRODUCT. WE ALSO EXPLICITLY ADD ENTITIES WHICH
utt_0046 utt 232.61 242.24 -X ARE NOT PRESENT IN THE TEXT TO PROMOTE THE MODEL TO LEARN WHEN AN ENTITY IS NOT PRESENT. USING THIS DATA CREATION PROCESS WE ARE ABLE TO SUBSTANTIALLY REDUCE THE OVERALL VOLUME OF THE TRAINING DATA SET.
utt_0048 utt 242.24 251.04 -X THE PLOT ON THE RIGHT SHOWS THE NUMBER OF ENTITIES WHICH BELONG TO THE SAME PRODUCT IN THE RESPECTIVE DATA SETS. COMING TO THE RESULTS OF THE MODEL, FIRST TALKING ABOUT THE MODEL
utt_0050 utt 251.30 262.45 -X RUNTIME MQMRC ON AN AVERAGE HAS two point five TIMES FASTER TRAINING AND two point three TIMES FASTER INFERENCE AS COMPARED TO SQMRC APPROACH. TALKING ABOUT THE NER TASK PERFORMANCE OF MQMRC WITH RESPECT TO VARIOUS
utt_0052 utt 262.45 274.50 -X BASELINES FOR ALL THE BENCHMARK DATA SETS MQMRC IS ABLE TO OUTPERFORM SQMRC. IT IS ALSO ABLE TO REDUCE THE NUMBER OF MODELS FOR NER SL BY TRAINING A SINGLE MODEL INSTEAD OF FIVE DIFFERENT ENSEMBLE
utt_0054 utt 274.50 279.33 -X MODELS IN THE E-COMMERCE fivePT DATA SET. COMING TO THE FEW SHORT EXPERIMENTS WE OBSERVED THAT MQMRC
utt_0055 utt 279.39 289.70 -X IS ALSO ABLE TO SCALE WELL EVEN WHEN HAVING LESS NUMBER OF TRAINING SAMPLES. AS YOU CAN SEE FROM THE PLOT EVEN WHEN USING THE COMPLETE DATA SET THE NER SL APPROACH WAS ONLY ABLE TO REACH seventy-four point two nine%
utt_0057 utt 290.98 297.22 -X Fone SCORE WHEREAS MQMRC WAS ABLE TO PERFORM BETTER THAN THE BASELINE EVEN WITH AS LOW AS two point fiveK DURING
utt_0058 utt 297.32 302.66 -X TRAINING. WE ALSO EXPERIMENTED WITH DIFFERENT CONTEXT AND ENTITY EMBEDDING INTERACTION OPERATIONS.
utt_0059 utt 302.66 316.84 -X WE TRIED OPERATIONS SUCH AS DOING SOME OF THE ENTITY EMBEDDING WITH THE TOKEN EMBEDDINGS OR TAKING THE DIFFERENCE THE PRODUCT AND APPLYING ACTIVATION FUNCTIONS TO IT AS WELL AS TAKING THE MAX. THE RESULTS SUGGEST THAT HAVING ENTITY OPERATIONS WHICH CREATES A LARGE DISPARITY
utt_0062 utt 316.84 327.08 -X BETWEEN THE DIFFERENT ELEMENTS OF A TOKEN EMBEDDING HELPS US TO GET THE BETTER EMBEDDING SPACE. WE ALSO EXPERIMENTED WITH THE ENTITY ORDERING IN A QUESTION. KEEPING THE ENTITY ORDERING SAME IN THE
utt_0064 utt 327.08 341.51 -X QUESTION DURING TRAINING LEADS TO A DETERIORATION OF Fone BY one2.33% WHEN WE SHUFFLE THE ENTITY QUESTIONS DURING INFERENCE. THIS IS LIKELY DUE TO THE MODEL GIVING MORE WEIGHTAGE TO THE POSITION WHILE LEARNING THE ENTITY REPRESENTATIONS AND NOT FOCUSING ON THE ENTITY NAME. SHUFFLING THE ORDER
utt_0067 utt 341.51 351.76 -X OF ENTITIES DURING TRAINING ALLEVIATES THIS ISSUE. MQMRC HAS MULTIFOLD BENEFITS WHEN IMPLEMENTED IN PRODUCTION SYSTEMS. IT REDUCES THE MODEL TRAINING AND INFERENCE COST BY AN AVERAGE OF fifty-eight point eight two% IN OUR
utt_0069 utt 352.33 366.54 -X EXPERIMENTS. IT HAS FASTER MODEL RUN TIMES AS COMPARED TO THE NER-SQMRC APPROACH. IT REDUCES THE MODEL PROLIFERATION AND MAINTENANCE OF MULTIPLE MODELS BY TRAINING A SINGLE MODEL FOR A LARGE NUMBER OF ENTITIES AND IT IS ABLE TO PERFORM BETTER THAN BOTH NER-SQMRC AND NER-SL BASED
utt_0072 utt 367.21 377.95 -X APPROACHES. SO, TO CONCLUDE, WE PROPOSE POSING NER AS A MULTI-QUESTION MACHINE READING COMPREHENSION TASK BECAUSE IT IS MUCH MORE FASTER DURING TRAINING AND INFERENCE AND ALSO IS COST-EFFECTIVE
utt_0074 utt 377.95 385.29 -3.1309 IN PRODUCTION SYSTEMS AND SCALABLE FOR LARGE NUMBER OF ENTITIES WHILE MAINTAINING THE PERFORMANCE. THANK YOU.
