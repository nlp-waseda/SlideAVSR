utt_0000 utt 0.21 4.91 -X HI, MY NAME IS MINGCHAO JIANG. I'M A PH.D. STUDENT FROM RICE UNIVERSITY.
utt_0001 utt 5.07 17.30 -X TODAY, I'M REALLY GLAD TO SHARE A PROJECT ABOUT FEW-SHOT IMAGE CLASSIFICATION. IT IS CALLED JUST USE A LIBRARY OF PRE-TRAINED FEATURE EXTRACTORS AND A SIMPLE CLASSIFIER. THIS WORK IS
utt_0003 utt 17.30 23.28 -X COLLABORATED WITH ARKABANDHU CHOWDHURY, SWARAT CHAUDHURI, AND CHRIS JERMAINE.
utt_0004 utt 23.28 36.08 -X SO WHAT IS FEW-SHOT LEARNING? EVEN AT THE CURRENT AGE OF BIG DATA, THE ASSUMPTION OF HAVING A LARGE LABELED TRAINING SET IS NOT ALWAYS REALISTIC. IN COMPARISON TO THE TRADITIONAL LEARNING PROBLEM,
utt_0006 utt 36.11 46.48 -X WHERE WE HAVE CLASSES WITH MANY SAMPLES, THE FEW-SHOT LEARNING HAS CLASSES WITH FEW SAMPLES. THE FEW-SHOT SETTING REQUIRES OUR MODEL TO GENERALIZE WELL,
utt_0008 utt 46.48 59.29 -X BEYOND THE FEW AVAILABLE TRAINING INSTANCES. I'D LIKE TO SPECIFY THE PARTICULAR FUTURE PROBLEM WE'RE LOOKING HERE IS CALLED CROSS-DOMAIN FEW-SHOT LEARNING, WHICH IS THE MOST DIFFICULT AND GENERAL
utt_0010 utt 59.29 71.83 -X VARIANT OF FEW-SHOT LEARNING. WE WANT A FEW-SHOT LEARNER F, THAT CAN TAKE A VERY SMALL SAMPLE FROM CERTAIN DISTRIBUTION AND LEARN TO CLASSIFY THE FUTURE SAMPLES FROM THAT DISTRIBUTION.
utt_0012 utt 72.15 84.47 -X HOWEVER, THE FUTURE SAMPLES ARE VERY SMALL SO WE CANNOT AFFORD ANY DATA SPLIT. NEITHER THE SAMPLE OR ITS DISTRIBUTION IS AVAILABLE TO US WHEN DEVELOPING OR VALIDATING THE LEARNER F.
utt_0014 utt 84.47 91.45 -X THE GOAL IS TO BUILD A LEARNER F TRULY OFF THE SHELF AND WORKS ON VIRTUALLY ANY NEW DISTRIBUTION.
utt_0015 utt 91.77 104.22 -X RECENT WORK HAS SUGGESTED THAT THE TRANSFER LEARNING METHOD IS QUITE PROMISING IN THE FEW-SHOT SETTING. FOR EXAMPLE, THE BASELINE METHOD USED ONE LINEAR CLASSIFIER ON TOP OF ONE PRE-TRAINED CNN.
utt_0017 utt 104.60 118.17 -X SELECTING FROM UNIVERSAL REPRESENTATIONS APPROACH ARGUES THAT DIVERSITY IN FEATURES IS BEST OBTAINED THROUGH THE DIVERSITY IN DATA SETS. AND THE BIG TRANSFER MODEL TRAINED A VERY DEEP CNN ON
utt_0019 utt 118.17 125.24 -X A MASSIVE DATA SET. THE QUESTION IS: IS THERE AN EVEN SIMPLER METHOD? CONSIDER THIS IDEA:
utt_0020 utt 125.43 132.83 -X WE USE A DEEP CNN TRAINED ON A DIVERSE DATA SET -- SUCH AS RESNETeighteen TRAINED ON IMAGENET oneK.
utt_0021 utt 134.30 141.09 -X AFTER THAT, WE SIMPLY REMOVE THE CLASSIFIER ON TOP AND REPLACE THAT WITH ONE MLP. AND WE
utt_0022 utt 141.56 156.38 -X TRAIN THIS MLP FROM SCRATCH ON FEW-SHOT PROBLEMS. AND PERFORM A HYPER-PARAMETERS VALIDATION ON AN EXTERNAL DATA SET -- SUCH AS CUB BIRD. AND THAT'S IT! THAT'S A METHOD WE CALL LIBRARY-BASED LEARNER.
utt_0024 utt 156.48 170.19 -X BECAUSE WE ARE USING THE PUBLISHED LIBRARY-BASED CNN DIRECTLY. IN OUR CASE, THERE ARE nine DIFFERENT VARIANTS OF RESNET AND DENSENET. THE LIBRARY-BASED LEARNER HAS two COMPONENTS. THE FIRST
utt_0026 utt 170.19 182.21 -X ONE IS PRE-TRAINED FEATURE EXTRACTORS. AND THE SECOND ONE IS A SIMPLE CLASSIFIER. WE FOUND OUT A SINGLE HIGH-QUALITY DATA SET IS ACTUALLY ALL THAT IS NEEDED FOR FEW-SHOT LEARNING.
utt_0028 utt 182.24 195.30 -X AND ADDING MORE FEATURED EXTRACTORS TRAINED ON THAT DATA SET IS ACTUALLY THE BEST PATH TO ACHIEVE HIGHER PERFORMANCE. HERE WE COMPARE AGAINST nine STANDARD METHODS OVER eight DIFFERENT DATA SETS. AND
utt_0030 utt 195.30 201.48 -X WE FOUND OUT THE WORST LIBRARY-BASED LEARNER BEATS THE BEST STANDARD METHOD ON five OUT OF eight DATA SETS.
utt_0031 utt 201.48 213.27 -X TAKING THIS five WAY-five SHOT PROBLEM AS AN EXAMPLE. HERE WE SHOW THE PERFORMANCE ON THE AIRCRAFT DATA SET AND FUNGI DATA SET. THERE IS A MASSIVE GAP BETWEEN THE WORST LIBRARY-BASED LEARNER
utt_0033 utt 213.27 221.03 -X VERSUS THE BEST STANDARD METHOD. ON AIRCRAFT, IT'S sixty-one VERSUS forty-seven. AND ON FUNGI, IT'S seventy-seven VERSUS sixty-seven.
utt_0034 utt 221.89 226.70 -X WE ALSO NOTICED THERE IS A BIG GAP BETWEEN THE BEST AND WORST LIBRARY-BASED LEARNERS,
utt_0035 utt 226.70 234.02 -X WHICH BECAME ONE OF OUR CONCERNS. BECAUSE WE WANT THE BEST PERFORMANCE EVERY SINGLE TIME.
utt_0036 utt 234.02 245.99 -X HOWEVER, THERE IS NO CLEAR PATTERN ON WHICH FEATURE EXTRACTOR MIGHT WORK BETTER ON WHICH DATA SET. IT IS VERY NATURAL TO ASK: IS THAT POSSIBLE TO USE ALL THESE FEATURE EXTRACTORS
utt_0038 utt 245.99 259.59 -X TO DEVELOP AN EVEN BETTER FEW-SHOT LEARNER? WE FOUND A VERY SIMPLE APPROACH -- JUST ENSEMBLE ALL THOSE LIBRARY FEATURE EXTRACTORS. HERE WE CONCATENATE THE FEATURES EXTRACTED FROM ALL
utt_0040 utt 259.59 265.45 -X nine DIFFERENT LIBRARIES. AND WE ARE TALKING ABOUT thirteenK + FEATURES. WE JUST BUILT one MLP
utt_0041 utt 266.22 271.56 -X WITH Ltwo REGULARIZATION ON TOP OF THAT. AND WE CALL THIS METHOD FULL LIBRARY LEARNER.
utt_0042 utt 273.19 285.61 -X LET'S TAKE A LOOK AT ITS PERFORMANCE. HERE WE COMPARE THE FULL LIBRARY LEARNER VERSUS THE BEST SINGLE LEARNER. AS WE CAN TELL, THE FULL LIBRARY LEARNER ALWAYS BEATS THE BEST SINGLE LEARNER.
utt_0044 utt 285.67 297.42 -X ESPECIALLY WHEN WAYS INCREASE -- THAT MEANS THE FEW-SHOT PROBLEM IS GETTING MORE DIFFICULT. THE FULL LIBRARY LEARNER WINS EVEN MORE! WE WERE WONDERING WHY DOES LIBRARY-BASED LEARNER WORK
utt_0046 utt 297.42 303.18 -X SO WELL? WE FIND OUT THE FIRST REASON IS BECAUSE THE DIVERSITY OF HIGH-QUALITY FEATURE EXTRACTORS
utt_0047 utt 303.34 309.55 -X IS THE KEY FOR THE FEW-SHOT LEARNING. EVEN MANY TRAINED ON THE SAME DATA SET LIKE IMAGENET oneK
utt_0048 utt 309.58 315.86 -X IS ACTUALLY A GOOD OPTION. HERE WE HAVE DIFFERENT FEW-SHOT PROBLEMS AND THE TOP FEATURE THEY PICKED.
utt_0049 utt 316.01 320.82 -X AS WE CAN TELL, THE DIFFERENT FEW-SHOT PROBLEMS FAVOR DIFFERENT FEATURE EXTRACTORS.
utt_0050 utt 321.49 329.11 -X WE ALSO FIND OUT THE CNNS' DIVERSITY OUTWEIGHS THE DATA SIZE. WE COMPARE THE ENSEMBLE OF THE nine CNNS
utt_0051 utt 329.11 336.63 -X TRAINED ON THE IMAGENET oneK DATA SET VERSUS THE DEEPER CNN TRAINED ON THE FULL IMAGENET 2oneK.
utt_0052 utt 337.23 340.08 -X AND THE FIRST ONE OUT-PERFORMED THE LATTER ONE.
utt_0053 utt 340.50 345.14 -X ANOTHER REASON WE FIND OUT IS THE FEW-SHOT FINE-TUNING IS SURPRISINGLY EASY.
utt_0054 utt 345.52 351.86 -X EVEN WE'RE TALKING ABOUT thirteenK + FEATURES. THE WEIGHTS OF MLP TRAINED WITH ONE-SHOT
utt_0055 utt 351.92 363.45 -X ACTUALLY MATCHES THE WEIGHTS TRAINED WITH THE FULL DATA. THAT IS TELLING US one IMAGE OFFERS A LOT OF INFORMATION AND THE NUMBER OF TRAINING POINTS HAS SMALLER EFFECTS THAN WE EXPECTED.
utt_0059 utt 371.93 376.15 -6.2933 WITH MLP AND Ltwo REGULATION ON TOP. IT TURNED OUT IT'S VERY HARD TO BEAT! THE KEY NOVELTY HERE
