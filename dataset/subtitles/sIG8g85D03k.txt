utt_0000 utt 0.75 1.55 -X HELLO.
utt_0001 utt 1.55 5.20 -X I'M BRUNO TAILLÃ‰, PHD STUDENT AT SORBONNE UNIVERSITY AND BNP PARIBAS.
utt_0002 utt 5.45 10.80 -X TODAY I PRESENT MY WORK ON INCORRECT COMPARISONS IN END-TO-END RELATION EXTRACTION.
utt_0003 utt 10.80 14.19 -X JOINT WORK WITH VINCENT GUIGUE, GEOFFREY SCOUTHEETEN AND PATRICK GALLINARI.
utt_0004 utt 15.15 19.82 -X NAMED ENTITY RECOGNITION AND RELATION EXTRACTION ARE TWO KEY INFORMATION EXTRACTION TASKS,
utt_0005 utt 19.82 23.22 -X FOR EXAMPLE AT THE HEART OF KNOWLEDGE GRAPH CONSTRUCTION.
utt_0006 utt 23.22 23.95 -X GIVEN A SENTENCE,
utt_0007 utt 23.95 30.39 -X WE WOULD LIKE TO EXTRACT THE MENTIONS OF ENTITIES AS WELL AS THE RELATIONS EXPRESSES BETWEEN THEM.
utt_0008 utt 30.39 31.79 -X IN END-TO-END RELATIONS EXTRACTION,
utt_0009 utt 31.79 37.23 -X A SINGLE MODEL IS TRYING TO PERFORM BOTH TASKS AND TO MODEL THEIR INTERDEPENDENCY.
utt_0010 utt 37.23 39.54 -X NUMEROUS ARTICLES HAVE PROPOSED DIFFERENT MODELS,
utt_0011 utt 39.79 40.75 -X HOWEVER,
utt_0012 utt 40.81 43.28 -X BECAUSE SEVERAL EVALUATION SETUPS HAVE BEEN USED,
utt_0013 utt 43.28 46.90 -X SOME OF THEM PRESENT INCORRECT PERFORMANCE COMPARISON TO PREVIOUS WORK.
utt_0014 utt 47.57 51.38 -X OUR MAIN GOAL IN THIS ARTICLE IS TO RAISE AWARENESS ON THIS ISSUE.
utt_0015 utt 51.41 58.61 -X BUT IN ORDER TO BE USEFUL TO A LARGER AUDIENCE, I WILL FIRST PERFORM A QUICK LITERATURE REVIEW AND THEN COME TO ARE MORE SPECIFIC POINT.
utt_0017 utt 59.60 62.23 -X BEFORE TACKLING JOINT ENTITY AND RELATION EXTRACTION.
utt_0018 utt 62.23 65.33 -X LET'S SEE HOW WE WOULD PERFORM BOTH TASKS IN A PIPELINE APPROACH.
utt_0019 utt 66.25 71.22 -X WE WOULD FIRST PERFORM NAMED ENTITY RECOGNITION, OFTEN MODELED AS SEQUENCE LABELING.
utt_0020 utt 71.63 79.06 -X FOR EACH WORD, WE PREDICT A TAG THAT IS REFLECTIVE OF BOTH THE TYPE OF THE ENTITY AND THE POSITION OF THE WORD INSIDE THE MENTION.
utt_0021 utt 79.60 83.16 -X THE CLASSICAL NEURAL ARCHITECTURE TO DO SO IS THE BILSTM-CRF.
utt_0022 utt 84.08 89.27 -X TODAY WE WOULD MORE LIKELY FINETUNE A SIMPLE CLASSIFIER OVER A PRE-TRAINED LANGUAGE MODEL.
utt_0023 utt 90.80 94.23 -X NER CAN ALSO BE VIEWED AS A SPAN-LEVEL CLASSIFICATION TASK,
utt_0024 utt 94.58 98.29 -X WHICH IS USEFUL FOR NESTED OR OVERLAPPING MENTIONS.
utt_0025 utt 98.93 103.83 -X ALL POSSIBLE SUBSENTENCES ARE SEPARATELY CLASSIFIED AS ENTITY MENTIONS OR NOT.
utt_0026 utt 105.10 113.56 -X GIVEN THE OUTPUT OF NER, RELATION EXTRACTION IS THEN CAST AS CLASSIFICATION OF EVERY PAIR OF DETECTED ENTITIES.
utt_0028 utt 113.56 117.97 -X SUCH A CLASSIFIER TAKES AS INPUT A PAIR OF ENTITIES AND THE CONTEXT SENTENCE,
utt_0029 utt 117.97 121.40 -X AND IT MUST PREDICT WHICH RELATION IS EXPRESSED BETWEEN THE ENTITIES.
utt_0030 utt 122.48 127.83 -X ONE POPULAR ARCHITECTURE TO DO SO IS PIECEWISE POOLING, WHERE THE SENTENCE IS SPLIT INTO FIVE PARTS:
utt_0031 utt 127.83 133.85 -X THE TWO ENTITIES AND THREE SUBSENTENCES BEFORE, BETWEEN AND AFTER THE ENTITIES.
utt_0032 utt 135.06 140.63 -X EACH PART IS POOLED INTO A FIXED LENGTH REPRESENTATION, AND THEIR CONCATENATION IS FED TO A CLASSIFIER.
utt_0033 utt 141.07 147.13 -X TODAY WE CAN ALSO SIMPLY FEED BERT WITH THE CONCATENATION OF THE CANDIDATE ENTITIES AND THE SENTENCE,
utt_0034 utt 147.19 152.09 -X AND WE USE THE CLS TOKEN TO TRAIN A CLASSIFIER.
utt_0035 utt 152.09 158.07 -X SO THIS GIVES US THE PIPELINE APPROACH WHERE TWO SEPARATE MODELS ARE TRAINED SEPARATELY AND APPLIED SEQUENTIALLY.
utt_0036 utt 158.71 160.79 -X WHILE THIS ENABLES MODULARITY,
utt_0037 utt 160.85 167.77 -X IT ALSO PREVENTS FROM MODELING THE INTUITIVE INTERDEPENDENCY BETWEEN TASKS AND IT LEADS TO ERROR PROPAGATION.
utt_0039 utt 167.77 170.58 -X AND THIS IS WHY JOINT MODELS HAVE BEEN PROPOSED.
utt_0040 utt 171.47 175.54 -X SO THE FIRST APPROACH IS THE INCREMENTAL SETTING, PROPOSED BY LI AND JI.
utt_0041 utt 175.54 183.54 -X THE SENTENCE IS PARSED WORD BY WORD, AND FOR EACH WORD WE PERFORM AN ENTITY STEP AND RELATION STEP.
utt_0042 utt 183.60 188.66 -X WE BEGIN WITH ENTITY DETECTION AND AS SOON AS WE HAVE DETECTED SEVERAL ENTITIES,
utt_0043 utt 188.66 192.95 -X WE CAN EXAMINE POTENTIAL RELATIONS BETWEEN THEM.
utt_0044 utt 193.08 197.98 -X MIWA AND SASAKI PROPOSE TO CAST THIS JOINT TASK AS A TABLE FILLING PROBLEM.
utt_0045 utt 198.04 208.86 -X THEY NOTICED THAT THE ENTIRE INFORMATION WE WANT TO EXTRACT CAN BE REPRESENTED AS A TABLE WHERE THE DIAGONAL CONTAINS INFORMATION ON ENTITIES AND THE OTHER CELLS INFORMATION ON RELATIONS.
utt_0047 utt 208.86 212.41 -X THE TASK CAN THEN BE FRAMED AS SEQUENCE LABELING.
utt_0048 utt 212.41 215.51 -X THEY EXPLORE SEVERAL ORDERS FOR THIS SEQUENTIAL FILLING,
utt_0049 utt 216.08 218.58 -X AND WE CAN SEE THAT IF WE BEGIN WITH THE DIAGONAL,
utt_0050 utt 218.58 222.30 -X WE PERFORM ENTITY FIRST PREDICTION, SIMILARLY TO THE PIPELINE APPROACH.
utt_0051 utt 222.61 226.49 -X BUT WE CAN ALSO RECOVER THE INCREMENTAL SITTING BY CHANGING THIS ORDER.
utt_0052 utt 227.25 230.17 -X THIS SETTING HAS BEEN USED IN SEVERAL ADDITIONAL WORKS,
utt_0053 utt 230.17 239.29 -X INCLUDING A NEW WORK BY WANG AND LU PRESENTED AT THIS VERY CONFERENCE AND REPORTING NEW STATE OF THE ART RESULTS.
utt_0055 utt 239.29 241.72 -X IN BOTH INCREMENTAL AND TABLE FILLING SETTINGS,
utt_0056 utt 241.72 246.11 -X THE INTERDEPENDENCY IS MODELED IN THE SEQUENCE OF PREDICTIONS.
utt_0057 utt 246.58 253.18 -X ANOTHER APPROACH IS TO KEEP THE PIPELINE STRUCTURE BUT ENABLE INTERACTIONS IN A SHARED ENCODER.
utt_0058 utt 253.81 256.79 -X DEPENDING ON THE INFORMATION SHARED BY THE DECODERS,
utt_0059 utt 256.79 260.47 -X WE CAN DISTINGUISH TWO SUBCLASSES OF MODELS.
utt_0060 utt 260.47 263.61 -X THE ENTITY FILTERING MODELS KEEP THE PIPELINE APPROACH
utt_0061 utt 264.44 271.13 -X AND THE NER OUTPUTS ARE EXPLICITLY USED TO PERFORM RELATION CLASSIFICATION OF EVERY PAIR.
utt_0062 utt 271.13 272.63 -X IN MULTI-HEAD SELECTION MODELS,
utt_0063 utt 272.63 278.17 -X THE DECODERS DO NOT DIRECTLY SHARE INFORMATION OR THEY SHARE LESS EXPLICIT INFORMATION,
utt_0064 utt 278.17 280.31 -X SUCH AS NER LABEL EMBEDDINGS.
utt_0065 utt 280.57 287.26 -X IN THIS CASE, THE MODEL MUST MAKE A RELATION PREDICTION FOR EACH PAIR OF WORDS, SIMILARLY TO TABLE FILLING.
utt_0066 utt 287.86 288.76 -X HOWEVER,
utt_0067 utt 288.76 290.59 -X THE TABLE IS NOT FILLED SEQUENTIALLY,
utt_0068 utt 290.59 291.87 -X BUT IN TWO STEPS.
utt_0069 utt 291.96 295.20 -X ENTITIES THEN RELATIONS.
utt_0070 utt 295.48 299.39 -X SO WE CAN SEE THAT THERE ARE SEVERAL WORKS THAT USE ENTITY FILTERING
utt_0071 utt 299.58 301.95 -X FIRST WITH SEQUENCE TAGGING NER
utt_0072 utt 303.42 305.88 -X BUT ALSO SPAN-LEVEL NER.
utt_0073 utt 305.88 313.31 -X AND WE CAN NOTE THAT ENTITY FILTERING IS THE ONLY APPROACH THAT ENABLES TACKLING OVERLAPPING ENTITIES.
utt_0074 utt 314.90 321.92 -X WE CAN ALSO SEE THAT SEVERAL WORK USE MULTI-HEAD SELECTION.
utt_0075 utt 321.92 323.90 -X FINALLY, ON A MORE ORIGINAL NOTE,
utt_0076 utt 324.02 327.93 -X LI ET AL. FRAME END-TO-END RELATION EXTRACTION AS MULTI-TURN QUESTION ANSWERING.
utt_0077 utt 328.31 337.69 -X EACH RELATION CORRESPONDS TO A TEMPLATE OF SEVERAL QUESTIONS THAT ARE ANSWERED ONE AFTER THE OTHER WITH A BERT BASED QA MODEL.
utt_0078 utt 337.69 342.65 -X THIS GIVES US AN OVERVIEW OF PROPOSED MODELS WITH VERY DIVERSE APPROACHES,
utt_0079 utt 342.65 348.25 -X AND WE CAN NOTE AN EVOLUTION FROM INCREMENTAL SETTINGS TO SHARED ENCODER SETTINGS.
utt_0080 utt 348.25 361.18 -X SIMULTANEOUSLY, THE REPRESENTATIONS USED BY THESE MODELS HAVE FOLLOWED THE GENERAL NLP TREND AND WENT FROM HANDCRAFTED TO NON-CONTEXTUAL EMBEDDINGS AND FINALLY, LANGUAGE MODEL PRE-TRAINING.
utt_0082 utt 361.18 368.93 -X WE CAN WONDER WHICH ELEMENTS AMONG ALL THESE DIFFERENT PROPOSALS ARE THE MOST USEFUL FOR END-TO-END RELATION EXTRACTION
utt_0084 utt 369.63 376.32 -X AND ONE TOOL TO ANSWER THAT IS A COMMON BENCHMARK AND COMMON METRICS.
utt_0085 utt 377.95 385.15 -X HOWEVER, SEVERAL SETTINGS HAVE BEEN USED TO EVALUATE END-TO-END RELATION EXTRACTION.
utt_0086 utt 385.15 387.71 -X IN ALL SETTINGS WE GENERALLY REPORT TWO SCORES:
utt_0087 utt 387.99 393.44 -X - ONE FOR NER THAT TAKES INTO ACCOUNT THE DETECTION OF EVERY ENTITY, WHETHER INVOLVED IN A RELATION OR NOT.
utt_0088 utt 393.44 399.87 -X - AND ONE FOR RELATION EXTRACTION, ONLY FOCUSED ON THE RELATIONS.
utt_0089 utt 399.87 402.59 -X WE USE PRECISION, RECALL AND Fone SCORE FOR BOTH.
utt_0090 utt 402.59 408.54 -X BUT THIS INFORMATION IS NOT SUFFICIENT TO ENTIRELY DESCRIBE THE EVALUATION SETTING.
utt_0091 utt 408.54 414.75 -X INDEED, BEKOULIS ET AL. IDENTIFY THREE DIFFERENT SETTINGS: STRICT, BOUNDARIES AND RELAXED.
utt_0092 utt 415.80 422.18 -X THE STRICT SETTING IS THE MOST RESTRICTIVE, REQUIRING THAT BOTH THE ENTITY TYPE AND BOUNDARIES ARE CORRECTLY DETECTED,
utt_0093 utt 422.18 425.70 -X FOR BOTH NER AND RELATION EXTRACTION.
utt_0094 utt 425.70 429.09 -X IN THE BOUNDARIES SETTINGS, THE NER EVALUATION IS UNCHANGED
utt_0095 utt 429.37 434.02 -X BUT ONLY THE BOUNDARIES OF RELATION ARGUMENTS ARE TAKEN INTO ACCOUNT FOR RE.
utt_0096 utt 435.61 444.35 -X IN THE RELAXED SETTING, A MULTI-TOKEN ENTITY IS CORRECT IF AT LEAST ONE TOKEN IS ASSIGNED WITH THE CORRECT TYPE, WITH NO REQUIREMENTS ON THE BOUNDARIES.
utt_0097 utt 444.89 451.11 -X NER IS REDUCED TO TYPE CLASSIFICATION WITH THE ASSUMPTION THAT ENTITY BOUNDARIES ARE OBTAINED IN A SEPARATE PROCESS.
utt_0098 utt 452.06 456.10 -X WHILE HAVING SEVERAL EVALUATION SETTINGS IS NOT PROBLEMATIC IN ITSELF,
utt_0099 utt 456.32 464.13 -X THIS MULTIPLICATION BRINGS CONFUSION, AND IT HAS LED TO SEVERAL INCORRECT COMPARISONS.
utt_0100 utt 464.13 471.33 -X IN OUR PAPER, WE IDENTIFY SEVERAL INCORRECT COMPARISONS WITH RESPECT TO PREVIOUS WORKS.
utt_0101 utt 471.49 478.05 -X THE FIRST CAUSE IS CONFUSION BETWEEN BOUNDARIES AND STRICT SETTINGS, MAINLY ON ACE DATASETS.
utt_0102 utt 479.23 483.94 -X ANOTHER ISSUE IS DATASET ALTERATION DUE TO A DIFFERENT PREPROCESSING,
utt_0103 utt 484.38 490.82 -X AND THIS ISSUE IS PARTICULARLY CRITICAL FOR DATASETS WHICH ARE NOT PUBLICLY AVAILABLE,
utt_0104 utt 490.82 495.04 -X SUCH AS ACE DATASETS, FOR WHICH ONLY PREPROCESSING SCRIPTS ARE SHARED.
utt_0105 utt 496.00 500.74 -X FURTHERMORE, THERE CAN ALSO BE CONFUSION BETWEEN MICRO AND MICRO AVERAGE SCORES
utt_0106 utt 501.22 506.24 -X AS THE NORM DIFFERS BETWEEN EVALUATION SETUPS OR EVEN BETWEEN DATASETS.
utt_0107 utt 507.58 512.58 -X AND WE CAN ALSO MENTION MORE INSIDIOUS ISSUES THAT ARE LESS SPECIFIC TO END-TO-END RE.
utt_0108 utt 513.60 516.90 -X FOR EXAMPLE, CONFUSING DIFFERENT TRAIN AND TEST SPLITS,
utt_0109 utt 516.90 521.86 -X USING ONLY THE TRAIN SPLIT VS RETRAINING ON ADDITIONAL DEVELOPMENT DATA,
utt_0110 utt 522.50 527.97 -X AND FINALLY REPORTING THE RESULTS OF A SINGLE BEST RUN VS AVERAGING MULTIPLE ONES.
utt_0111 utt 528.54 536.20 -X WE INVITE YOU TO FIND MORE DETAILS IN THE PAPER WHERE WE REPORT A CURATED LIST OF PUBLISHED RESULTS ON FIVE DATASETS.
utt_0112 utt 537.50 545.70 -X SO NOW, IN THE FINAL PART OF THIS WORK, WE PROPOSE A SMALL EMPIRICAL STUDY TO QUANTIFY THE IMPACT OF SUCH DIFFERENCES IN EVALUATION.
utt_0113 utt 546.34 551.75 -X WE MAINLY FOCUS ON COMPARING BOUNDARIES AND STRICT RESULTS, WHICH IS THE MOST COMMON MISTAKE.
utt_0114 utt 551.87 559.62 -X WE CHOOSE A SIMPLE ENTITY FILTERING BASELINE INSPIRED BY THE RECENT WORK OF (EBERTS ET AL., two thousand and twenty).
utt_0115 utt 559.62 564.81 -X BUT TO OBTAIN RESULTS MORE BROADLY REPRESENTATIVE OF DIFFERENT TRENDS, WE VARY TWO ELEMENTS.
utt_0116 utt 565.06 571.17 -X FIRST, THE ENCODER IS EITHER BERT OR A MORE TRADITIONAL BILSTM WITH NON-CONTEXTUAL EMBEDDINGS.
utt_0117 utt 571.75 576.93 -X SECOND NER IS EITHER CAST AS SPAN-LEVEL CLASSIFICATION OR SEQUENCE TAGGING.
utt_0118 utt 578.37 584.45 -X WE PERFORM OUR EXPERIMENTS ON ACEfive AND CONLLfour, THE TWO MAIN BENCHMARKS FOR END-TO-END RE.
utt_0119 utt 586.59 597.64 -X FIRST, OUR SMALL STUDY SUGGESTS THAT SPAN-LEVEL NER IS NOT BENEFICIAL ON NON-OVERLAPPING ENTITIES, CONTRARY TO WHAT HAS BEEN CLAIMED.
utt_0120 utt 597.64 603.08 -X SECOND, WE CAN SEE THAT LANGUAGE MODEL PRE-TRAINING IS THE MAIN SOURCE FOR IMPROVED RESULTS.
utt_0121 utt 604.10 610.10 -X FURTHERMORE, WE EVALUATE THAT REPORTING BOUNDARIES SCORES INSTEAD OF STRICT SCORES
utt_0122 utt 610.18 614.73 -X RESULTS IN A PLUS *five%* RELATIVE (threePP) IMPROPER GAIN ON ACE0five.
utt_0123 utt 615.68 619.24 -X BUT SURPRISINGLY, IT HAS A VERY LIMITED IMPACT ON CONLLfour
utt_0124 utt 619.52 622.70 -X AND THIS REGARDLESS OFF THE DIFFERENT ARCHITECTURES.
utt_0125 utt 624.16 629.03 -X AND THIS CAN BE EXPLAINED BY ONE OVERLOOKED DIFFERENCE BETWEEN THESE TWO DATASETS.
utt_0126 utt 630.05 635.78 -X THAT IS THE FACT THAT THERE IS A BIJECTIVE MAPPING BETWEEN RELATION AND ARGUMENT TYPES IN CONLLfour.
utt_0127 utt 637.03 644.33 -X AND THIS EXPERIMENT SUGGESTS THAT MODELS HAVE LEARNED THIS MAPPING OR EVEN USE IT AS A SHALLOW HEURISTIC.
utt_0128 utt 645.00 648.78 -X AND WE CAN SEE THAT THIS MAPPING IS MUCH MORE COMPLEX IN ACEfive.
utt_0129 utt 650.18 655.43 -X TO CONCLUDE, WE CAN ONLY CALL FOR MORE COMPLETE REPORTS OF THE EVALUATION SETTINGS,
utt_0130 utt 655.43 661.48 -X INCLUDING EVERY METRIC SUBTLETY AS WELL AS THAT DATASET STATISTICS.
utt_0131 utt 661.48 667.53 -X IN THIS REGARD, WE ARE IN COMPLETE ADEQUATION WITH THE EMNLP two thousand and twenty REPRODUCIBILITY CHECKLIST.
utt_0132 utt 668.39 672.75 -X BUT WE ALSO THINK THAT IT WOULD BE BENEFICIAL TO UNIFY THE EVALUATION SETTINGS
utt_0133 utt 673.45 677.39 -X BY ALWAYS REPORTING BOTH STRICT AND BOUNDARIES SCORES,
utt_0134 utt 677.39 681.93 -X WHICH COULD ENABLE MORE MEANINGFUL CROSS-DATASET ANALYSES.
utt_0135 utt 682.57 683.91 -X ON A FINAL POINT,
utt_0136 utt 683.91 689.61 -X WE BELIEVE THAT A SINGLE Fone SCORE IS NOT ENOUGH TO ANALYZE AND UNDERSTAND THE PERFORMANCE OF A MODEL,
utt_0137 utt 690.60 693.83 -X AND WE CAN POINT TO SEVERAL WORKS THAT MEASURE, FOR EXAMPLE,
utt_0138 utt 693.83 697.10 -X THE IMPACT OF LEXICAL OVERLAP IN SPAN-BASED TASKS
utt_0139 utt 698.21 704.52 -X OR TWO WORKS PRESENTED AT THIS CONFERENCE ON SHALLOW HEURISTICS IN NEURAL RELATION EXTRACTION MODELS.
utt_0140 utt 704.52 707.56 -4.8244 THANK YOU FOR YOUR ATTENTION.
