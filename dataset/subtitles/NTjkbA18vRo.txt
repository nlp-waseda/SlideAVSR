utt_0000 utt 0.05 5.10 -X THIS IS PART OF OUR VIDEO SERIES ON HUMAN-CENTERED EVALUATIONS FOR NLP EXPLANATIONS.
utt_0001 utt 5.55 7.57 -X CHECK OUT THE LINK FOR MORE RESOURCES.
utt_0002 utt 8.08 10.03 -X YOU'RE CURRENTLY HERE ...
utt_0003 utt 10.09 15.36 -X I’LL SPEND THE REST OF MY SECTION OF THE TUTORIAL TALKING ABOUT THIS QUESTION: SHOULD
utt_0004 utt 15.47 18.99 -X MODEL EXPLANATIONS BE SIMILAR TO HUMAN EXPLANATIONS?
utt_0005 utt 19.44 26.07 -X OR IS THAT A FUNDAMENTALLY FLAWED OBJECTIVE, MAKING THE WHOLE IDEA OF PROXY EVALUATIONS KIND OF USELESS IN THIS CONTEXT?
utt_0007 utt 27.34 31.19 -X …AND AS YOU MIGHT EXPECT, THE ANSWER IS A LITTLE COMPLICATED.
utt_0008 utt 31.31 38.67 -X WE CAN THINK ABOUT WHAT HUMAN-ANNOTATED EXPLANATIONS DON’T DO, WHEN USED AS A GOLD-STANDARD FOR HUMAN EXPLANATIONS:
utt_0010 utt 39.47 46.48 -X THEY DON’T TELL US WHETHER THE EXPLANATION IS REALLY FAITHFULLY EXPLAINING THE MODEL BEHAVIOR IN AN ACTIONABLE WAY.
utt_0013 utt 52.05 58.74 -X THEY DON’T TELL US WHETHER THE EXPLANATION IS LIKELY TO BE USEFUL IN SOME DOWNSTREAM TASK INVOLVING A HUMAN AGENT.
utt_0015 utt 58.74 62.74 -X THIS IS WHAT APPLICATION-GROUNDED EVALUATIONS DO.
utt_0016 utt 63.06 68.95 -X WHAT THEY DO DO IS TELL US WHETHER THE MODEL EXPLANATION ADHERES TO OUR EXPECTATIONS FOR
utt_0017 utt 70.35 77.30 -X THE TASK: DID THE MODEL ATTEND TO CONSTRUCTIONS THAT WE RECOGNIZE AS PERSONAL ATTACKS?
utt_0019 utt 77.30 83.22 -X DID THE MODEL ATTEND TO THE INFORMATION WE THOUGHT WAS NECESSARY AND SUFFICIENT FOR ANSWERING THE QUERY?
utt_0021 utt 86.48 92.50 -X DID THE MODEL ATTEND TO THE CONTENT WE JUDGED AS DISCUSSING THE ONE PARTICULAR ASPECT OF A BEER?
utt_0023 utt 94.00 98.04 -X BUT OF COURSE, THIS BEGS THE QUESTION OF: HOW VALID ARE THESE EXPLANATIONS?
utt_0024 utt 99.79 106.97 -X ONE EXERCISE THAT HELPS ME THINK ABOUT THIS STUFF IS TO VISUALIZE THE PIECES OF AN EXPLAINED MODEL PREDICTION AND THE ELEMENTS BY WHICH WE CAN EVALUATE IT.
utt_0026 utt 107.47 112.95 -X WE HAVE THE MODEL PREDICTION, WHICH I GROUP WITH THE TRUE LABEL BECAUSE THEY ARE OFTEN CONSIDERED RELATIVE TO EACH OTHER.
utt_0028 utt 112.95 115.03 -X WE HAVE THE MODEL EXPLANATION.
utt_0029 utt 115.44 125.37 -X WE HAVE (OPTIONALLY) A HUMAN-ANNOTATED EXPLANATION, AND, AGAIN OPTIONALLY, A HUMAN AGENT TRYING TO ENGAGE WITH THE MODEL OUTPUT FOR SOME PURPOSE.
utt_0031 utt 125.37 131.38 -X CONVENTIONAL MACHINE LEARNING EVALUATION TENDS TO THINK ABOUT THE RELATIONSHIP BETWEEN TRUE AND PREDICTED LABEL.
utt_0033 utt 131.38 133.81 -X HOW ACCURATE WAS IT?
utt_0034 utt 133.81 141.24 -X FULLY AUTOMATED EVALUATION OF MODEL EXPLANATIONS TENDS TO LOOK AT THE RELATIONSHIP BETWEEN THE MODEL’S EXPLANATIONS AND ITS PREDICTION.
utt_0037 utt 144.73 151.90 -X ASKS WHETHER THE TOKENS IN A MODEL RATIONALE ARE SUFFICIENT TO LET THE MODEL MAKE THE PREDICTION IT WOULD HAVE MADE WITH THE FULL INPUT.
utt_0039 utt 153.56 161.53 -X APPLICATION-GROUNDED EVALUATIONS TEND TO LOOK AT HOW A HUMAN AGENT INTERACTS WITH A MODEL EXPLANATION IN TRYING TO MAKE SENSE OF THE MODEL’S PREDICTION.
utt_0041 utt 161.53 167.51 -X A VERY COMMON CONTROL CONDITION FOR THIS TYPE OF EVALUATION IS LOOKING AT HOW THE HUMAN AGENT INTERACTS WITH MODEL PREDICTION ALONE.
utt_0043 utt 169.17 179.54 -X AND THEN FINALLY, WE HAVE CONVENTIONAL PROXY EVALUATION WITH HUMAN EXPLANATIONS, WHERE WE ARE LOOKING AT THE RELATIONSHIP BETWEEN OUR HUMAN-ANNOTATED EXPLANATIONS AND OUR MODEL-GENERATED EXPLANATIONS.
utt_0046 utt 179.54 181.88 -X AND THE QUESTION REMAINS: WHAT CAN WE DO BEYOND THIS?
utt_0047 utt 181.88 189.59 -X ONE THING WE CAN DO IS TO LOOK AT THE RELATIONSHIP BETWEEN HUMAN-ANNOTATED EXPLANATIONS AND MODEL PREDICTIONS, AS SEVERAL RECENT PAPERS HAVE DONE.
utt_0049 utt 189.59 193.08 -X I AM GOING TO MENTION TWO IN PARTICULAR.
utt_0050 utt 193.17 205.96 -X IN “EVALUATING AND CHARACTERIZING HUMAN RATIONALES”, WE TOOK A NUMBER OF HUMAN RATIONALE DATASETS SUCH AS MULTIRC AND E-SNLI, AND LOOKED AT WHETHER THE TOKENS INCLUDED IN THE HUMAN-ANNOTATED
utt_0052 utt 205.96 213.69 -X RATIONALES WERE SUFFICIENT AND NECESSARY FOR TRAINED MODELS TO MAKE PREDICTIONS SIMILAR TO WHAT THEY WOULD MAKE WITH THE FULL DATA.
utt_0054 utt 214.01 220.31 -X BY DOING THIS, WE CAN GET A SENSE OF HOW GOOD A TARGET HUMAN-ANNOTATED RATIONALES ARE FOR MODEL RATIONALES.
utt_0056 utt 220.82 230.33 -X IF THE HUMAN RATIONALES IN A DATASET NEVER CONSIST OF ENOUGH INFORMATION FOR A MODEL TO MAKE A GOOD PREDICTION, THEN THEY PROBABLY SHOULDN’T BE USED AS A TARGET FOR PROXY EVALUATION.
utt_0059 utt 232.79 240.96 -X IN THEIR RECENT PAPER ABOUT “LEAKAGE-ADJUSTED SIMULATIBILITY”, PETER HASE AND HIS COAUTHORS DO SOMETHING SIMILAR FOR FREE-TEXT EXPLANATIONS.
utt_0061 utt 240.96 252.48 -X THEY LOOK AT HOW ACCURATE MODELS BECOME WHEN THE FREE-TEXT EXPLANATION IS APPENDED ONTO THE END OF THE INPUT, AND USE THESE VALUES TO MAKE INFERENCES ABOUT THE INFORMATIONAL CONTENT OF THESE EXPLANATIONS.
utt_0064 utt 252.48 254.49 -X THIS KIND OF WORK IS COOL
utt_0066 utt 258.43 261.28 -X WHAT RELATIONSHIP THEY HAVE WITH THE MODEL.
utt_0074 utt 297.50 301.92 -X LABEL THAT IS AT LEAST AS GOOD AS WHAT THE HUMAN EXPLANATION HAS TO THE TRUE LABEL?
utt_0076 utt 307.55 309.18 -X OF LOW-HANGING FRUIT.
utt_0079 utt 324.06 326.21 -X CONCRETE INFORMATIONAL CRITERION.
utt_0080 utt 326.21 331.58 -X THAT CONCLUDES MY DISCUSSION OF HUMAN EXPLANATIONS: HOW TO COLLECT THEM, HOW TO USE THEM AS A
utt_0081 utt 332.32 339.04 -X PROXY FOR EVALUATING MODEL EXPLANATIONS, AND THE VALIDITY OF SIMILARITY AS A METRIC OF
utt_0082 utt 340.06 341.83 -X SUCCESS IN THIS PROXY EVALUATION.
utt_0086 utt 374.01 375.49 -6.7810 THIS IS VIDEO ISN'T THE END.
