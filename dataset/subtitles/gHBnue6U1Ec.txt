utt_0001 utt 1.61 4.46 -X THIS IS TSAI-SHIEN CHEN FROM NATIONAL TAIWAN UNIVERSITY.
utt_0003 utt 4.46 6.51 -X I AM NOW INTRODUCING OUR WORK,
utt_0004 utt 6.51 10.16 -X NAMED ORIENTATION-AWARE VEHICLE RE-IDENTIFICATION
utt_0006 utt 10.22 12.78 -X WITH SEMANTICS-GUIDED PART ATTENTION NETWORK.
utt_0007 utt 12.97 18.99 -X THIS IS THE COLLABORATION WORK WITH CHIH-TING LIU, CHIH-WEI WU, AND PROF. CHIEN.
utt_0009 utt 20.37 25.68 -X VEHICLE RE-IDENTIFICATION FOCUSES ON MATCHING VEHICLE IMAGES WITH SAME IDENTITY.
utt_0012 utt 26.29 27.70 -X IN SOME HARD CASES,
utt_0013 utt 28.01 29.97 -X EVEN TWO DIFFERENT VEHICLES
utt_0014 utt 30.09 32.88 -X BUT WITH SIMILAR VIEWPOINTS AND VEHICLE TYPES
utt_0015 utt 33.10 35.92 -X WOULD HAVE SIMILAR APPEARANCES.
utt_0016 utt 36.17 37.43 -X IN CONTRAST,
utt_0017 utt 37.43 39.73 -X FOR TWO IMAGES FROM SAME VEHICLE,
utt_0018 utt 39.73 43.35 -X THEY STILL COULD HAVE DRAMATICALLY DIFFERENT APPEARANCES.
utt_0019 utt 43.95 45.04 -X IN SUCH CASE,
utt_0020 utt 45.04 48.08 -X IT WOULD BE HARD TO CORRECTLY CLASSIFY THEM
utt_0021 utt 48.17 50.48 -X BASED ON THE WHOLE IMAGE.
utt_0022 utt 50.64 51.63 -X BUT,
utt_0023 utt 51.69 56.66 -X IF WE ONLY FOCUS ON THE VEHICLE PARTS THAT BOTH APPEAR IN THE COMPARED IMAGES,
utt_0025 utt 56.66 59.00 -X WHICH IS SIDE-VIEW HERE,
utt_0026 utt 59.66 62.13 -X IT WOULD BE MUCH EASIER TO CLASSIFY THEM.
utt_0027 utt 62.93 67.00 -X SUCH IDEA MOTIVATED US TO DESIGN A FRAMEWORK WHICH CAN,
utt_0029 utt 67.00 69.97 -X FIRST, GENERATE THE PART ATTENTION MAPS
utt_0030 utt 70.06 75.70 -X TO MAKE OUR FRAMEWORK ABLE TO DISENTANGLE THE PART FEATURE FROM THE GLOBAL ONE
utt_0032 utt 76.46 83.06 -X AND EMPHASIZE ON THE CO-OCCURRENCE VEHICLE PARTS IN THE COMPARED IMAGES.
utt_0035 utt 83.34 85.56 -X TO GENERATE PART ATTENTION MAPS,
utt_0036 utt 85.56 89.59 -X IT IS INTUITIVE TO USE A REGULAR SEGMENTATION NETWORK
utt_0037 utt 89.65 93.30 -X WITH PIXEL-LEVEL PART LABEL FOR TRAINING.
utt_0038 utt 93.30 93.88 -X HOWEVER,
utt_0039 utt 94.07 95.80 -X SUCH LABEL IS EXPENSIVE
utt_0040 utt 95.99 99.48 -X AND HARD TO ACQUIRE IN THE REAL-WORLD SCENARIO.
utt_0042 utt 100.75 101.94 -X IN THIS PAPER,
utt_0043 utt 101.94 102.77 -X WE PROPOSED
utt_0044 utt 102.99 105.65 -X SEMANTICS-GUIDED PART ATTENTION NETWORK,
utt_0045 utt 105.71 107.16 -X ALSO CALLED SPAN,
utt_0046 utt 107.60 111.25 -X WHICH ONLY NEEDS CHEAPER IMAGE-LEVEL SEMANTIC LABEL
utt_0047 utt 111.28 114.61 -X AND CAN LEARN TO GENERATE THE PART ATTENTION MAPS.
utt_0048 utt 116.05 118.10 -X IN VEHICLE RE-ID PROBLEM,
utt_0049 utt 118.23 122.74 -X THE IMAGE-LEVEL SEMANTIC LABEL IS THE VIEWPOINT OF THE IMAGE.
utt_0051 utt 123.03 124.44 -X THAT IS,
utt_0052 utt 124.44 127.29 -X TAKE THIS VEHICLE IMAGE AS AN EXAMPLE,
utt_0053 utt 127.29 132.09 -X THE NETWORK ONLY NEEDS TO KNOW THAT THE FRONT AND LEFT FACES ARE VISIBLE
utt_0055 utt 132.44 135.67 -X AND CAN LEARN TO LOCALIZE WHERE THEY ARE.
utt_0057 utt 136.02 138.81 -X TO MAKE IT POSSIBLE,
utt_0058 utt 138.81 141.69 -X SPAN IS COMPOSED OF ONE SHARED FEATURE EXTRACTOR
utt_0059 utt 141.75 147.54 -X AND SEVERAL INDEPENDENT MASK GENERATORS TO PRODUCE THE MASK OF EACH PART.
utt_0061 utt 148.50 150.17 -X FOR EXAMPLE, IN OUR TASK,
utt_0062 utt 150.77 157.69 -X WE HAVE THREE GENERATORS TO RESPECTIVELY GENERATE THE FRONT, REAR AND SIDE VIEW MASKS.
utt_0064 utt 158.58 161.40 -X TO SUPERVISE THE LEARNING OF SPAN,
utt_0065 utt 161.46 163.61 -X WE DESIGNED A NOVEL LOSS FUNCTION,
utt_0066 utt 163.61 164.06 -X NAMED
utt_0067 utt 164.21 166.78 -X MASK RECONSTRUCTION LOSS.
utt_0068 utt 166.84 168.03 -X WITH THIS LOSS,
utt_0069 utt 168.03 171.16 -X THE ATTENTION MASKS OF THE VISIBLE PARTS
utt_0070 utt 171.19 174.43 -X WOULD JOINTLY RECONSTRUCT THE FOREGROUND MASK.
utt_0071 utt 175.61 177.59 -X FOR EXAMPLE, IN THIS IMAGE,
utt_0072 utt 177.59 181.02 -X THE VIEWPOINT OF THIS IMAGE IS REAR-SIDE
utt_0073 utt 181.37 186.04 -X WHICH MEANS THAT THE REAR AND SIDE VIEWS ARE VISIBLE IN THE IMAGE.
utt_0075 utt 186.04 186.97 -X HENCE,
utt_0076 utt 187.00 192.92 -X THE REAR AND SIDE ATTENTION MAPS SHOULD RECONSTRUCT THE WHOLE FOREGROUND MASK
utt_0078 utt 193.21 194.46 -X OF THE VEHICLE.
utt_0079 utt 196.18 198.14 -X TO SUPERVISE THIS LOSS,
utt_0080 utt 198.14 200.80 -X THE FOREGROUND MASK IS REQUIRED.
utt_0081 utt 200.89 201.92 -X THEREFORE,
utt_0082 utt 201.92 204.86 -X WE USED A TRADITIONAL SEGMENTATION METHOD,
utt_0083 utt 204.86 206.33 -X NAMED GRABCUT,
utt_0084 utt 206.68 215.04 -X TO AUTOMATICALLY GENERATE THE PRELIMINARY FOREGROUND MASK AND DID NOT USE ANY HUMAN-ANNOTATED PIXEL-LEVEL LABEL HERE.
utt_0087 utt 216.66 217.66 -X HOWEVER,
utt_0088 utt 217.69 220.06 -X AFTER APPLYING MASK RECONSTRUCTION LOSS,
utt_0089 utt 220.41 223.77 -X WE STILL FOUND THAT THE TRAINING RESULTS ARE UNSTABLE
utt_0090 utt 224.15 226.65 -X AND WOULD LEAD TO UNDESIRED ATTENTION MAPS
utt_0091 utt 226.78 230.27 -X FOR THE VEHICLE IMAGES WITH TWO VISIBLE PARTS.
utt_0092 utt 230.65 232.00 -X SOMETIMES,
utt_0093 utt 232.00 235.33 -X THE NETWORK WOULD ONLY USE SINGLE REPRESENTATIVE MASK
utt_0094 utt 235.45 237.79 -X TO PREDICT THE WHOLE VEHICLE MASK.
utt_0095 utt 238.87 239.68 -X OR,
utt_0096 utt 239.68 243.23 -X IT WOULD USE BOTH TWO MASKS WHICH HAVE THE WEIGHTS OF zero point five
utt_0098 utt 243.71 246.08 -X TO PREDICT THE WHOLE MASK.
utt_0099 utt 247.42 248.89 -X TO OVERCOME CASE I,
utt_0100 utt 249.37 251.77 -X WE DESIGNED THE AREA CONSTRAINT LOSS.
utt_0101 utt 251.87 252.80 -X THE LOSS
utt_0102 utt 252.83 255.81 -X WILL CONSTRAIN THE AREA OF EACH ATTENTION MAP,
utt_0103 utt 256.57 260.13 -X FORCING THE NETWORK TO GENERATE THE PART ATTENTION,
utt_0104 utt 260.13 261.28 -X NOT FOREGROUND ATTENTION.
utt_0105 utt 262.49 264.25 -X AS FOR TACKLING CASE II,
utt_0106 utt 264.35 267.14 -X WE INTRODUCED THE SPATIAL DIVERSITY LOSS.
utt_0107 utt 267.64 273.41 -X THE LOSS IS USED TO GUIDE EACH ATTENTION BRANCH TO GENERATE NON-OVERLAPPING PART MAPS.
utt_0109 utt 274.33 276.77 -X SUCH AS IN THIS CASE II EXAMPLE,
utt_0110 utt 277.02 280.99 -X THE NETWORK SHOULD LEARN TO GENERATE A SIDE AND A REAR MASK.
utt_0112 utt 282.33 285.86 -X AFTER SIMULTANEOUSLY TRAINED WITH THESE THREE LOSSES,
utt_0113 utt 286.43 290.21 -X OUR SPAN CAN FINALLY PRODUCE DESIRED RESULTS.
utt_0114 utt 291.55 296.90 -X HERE ARE SOME EXAMPLES OF THE PART ATTENTION MAPS GENERATED FROM OUR SPAN.
utt_0116 utt 296.90 298.21 -X FROM THESE RESULTS,
utt_0117 utt 298.21 301.89 -X WE CAN FIND THAT OUR SPAN IS ROBUST ON VARIOUS VEHICLES:
utt_0118 utt 302.40 304.51 -X FROM COMPACT CAR TO TRUCK,
utt_0119 utt 304.64 307.01 -X FROM BLACK ONES TO WHITE ONES,
utt_0120 utt 307.01 310.15 -X AND FROM THE FRONT VIEWPOINT TO THE REAR VIEWPOINT.
utt_0121 utt 311.81 317.63 -X IT IS ALSO WORTH MENTIONING THAT SPAN IS GENERAL-PURPOSED AND CAN BE USED TO OTHER DATASETS.
utt_0124 utt 318.21 319.27 -X HERE,
utt_0125 utt 319.27 322.47 -X WE SHOW THE EXAMPLE RESULT IN MULTI-DIGIT DATASET.
utt_0127 utt 322.97 324.13 -X IN THIS DATASET,
utt_0128 utt 324.29 326.24 -X THE SEMANTIC LABEL REPRESENTS
utt_0129 utt 326.27 328.71 -X WHICH DIGIT IS VISIBLE IN THE IMAGE.
utt_0130 utt 328.71 329.12 -X HENCE,
utt_0131 utt 330.17 334.59 -X THE NETWORK CAN LEARN TO GENERATE THE LOCALIZATION OF EACH DIGIT.
utt_0133 utt 335.36 336.07 -X ALSO,
utt_0134 utt 336.35 340.10 -X OUR SPAN CAN BE EXTENDED TO WEAKLY-SUPERVISED SEGMENTATION
utt_0136 utt 340.61 345.96 -X WHICH HAS MUCH WEAKER SUPERVISION SETTING THAN REGULAR SEGMENTATION
utt_0138 utt 346.14 350.69 -X BECAUSE IT ONLY REQUIRES IMAGE-LEVEL LABEL FOR TRAINING.
utt_0139 utt 351.07 353.86 -X WITH THE ATTENTION MAP OF EACH VEHICLE PART,
utt_0140 utt 353.98 362.15 -X OUR SECOND STEP IS TO MAKE OUR FRAMEWORK EMPHASIZE ON THE CO-OCCURRENCE PART IN THE COMPARED IMAGES HENCE,
utt_0144 utt 362.15 365.64 -X WE PROPOSED CO-OCCURRENCE PART-ATTENTIVE DISTANCE METRIC,
utt_0146 utt 365.82 368.16 -X WHICH IS ABBREVIATED AS CPDM.
utt_0147 utt 370.34 374.09 -X THE MECHANISM OF CPDM INCLUDES SEVERAL STEPS:
utt_0148 utt 374.37 375.08 -X FIRST,
utt_0149 utt 375.14 377.16 -X IT WOULD CALCULATE THE Lone NORM
utt_0150 utt 377.19 383.56 -X OF EACH PART ATTENTION MAP FROM SPAN TO EVALUATE THE AREA OF EACH VEHICLE PART.
utt_0152 utt 383.84 384.68 -X THEN,
utt_0153 utt 384.71 387.30 -X FOR GIVEN TWO COMPARED IMAGES,
utt_0154 utt 387.30 393.16 -X CO-OCCURRENCE ATTENTIVE MODULE WOULD EVALUATE THE ATTENTIVE WEIGHT FOR EACH VEHICLE PART.
utt_0157 utt 393.70 394.85 -X IN THIS CASE,
utt_0158 utt 394.85 400.74 -X THE FRONT AND REAR VIEWS WOULD BE TOTALLY IGNORED BECAUSE THEY ARE NOT CO-OCCURRENCE PART.
utt_0160 utt 400.74 402.44 -X IN CONTRAST,
utt_0161 utt 402.63 405.83 -X THE MODULE WOULD GIVE LARGER WEIGHT ON THE SIDE VIEW.
utt_0162 utt 407.46 411.56 -X HERE IS THE ARCHITECTURE OF OUR PROPOSED VEHICLE RE-ID FRAMEWORK.
utt_0164 utt 411.56 414.22 -X GIVEN AN INPUT VEHICLE IMAGE,
utt_0165 utt 414.40 419.05 -X SPAN WOULD FIRST GENERATE THE ATTENTION MAP FOR EACH VIEW OF THE IMAGE.
utt_0167 utt 420.07 420.81 -X THEN,
utt_0168 utt 420.81 422.70 -X THE FIRST STAGE CNN MODEL
utt_0169 utt 422.76 425.67 -X EXTRACT FIRST STAGE FEATURE MAP OF THE IMAGE.
utt_0170 utt 427.43 428.42 -X AFTERWARDS,
utt_0171 utt 428.42 432.04 -X WE APPLIED FOUR INDEPENDENT SECOND STAGE CNN MODELS
utt_0172 utt 432.10 436.97 -X TO RESPECTIVELY EXTRACT ONE GLOBAL AND THREE PART FEATURES.
utt_0174 utt 437.80 439.18 -X ESPECIALLY,
utt_0175 utt 439.18 440.87 -X TO EXTRACT THE PART FEATURES,
utt_0176 utt 440.93 445.58 -X THE MASKS GENERATED FROM SPAN ARE USED AS SPATIAL ATTENTION MAPS
utt_0178 utt 445.64 449.83 -X TO MAKE OUR FRAMEWORK DISENTANGLE THE PART FEATURE.
utt_0179 utt 451.30 452.14 -X FINALLY,
utt_0180 utt 452.33 459.11 -X ALL GLOBAL AND PART FEATURES ARE CONCATENATED AS ONE REPRESENTATIVE EMBEDDING OF THE IMAGE
utt_0182 utt 459.88 463.66 -X WHICH IS SUPERVISED BY THE IDENTITY CLASSIFICATION LOSS.
utt_0184 utt 464.39 465.45 -X IN THIS WORK,
utt_0185 utt 465.51 470.25 -X WE ALSO APPLIED TRIPLET LOSS TO SUPERVISE THE LEARNING OF OUR FRAMEWORK.
utt_0187 utt 471.11 471.82 -X HENCE,
utt_0188 utt 471.91 473.71 -X GIVEN THE COMPARED IMAGES,
utt_0189 utt 473.96 476.11 -X BASED ON THEIR ATTENTION MAPS,
utt_0190 utt 476.42 482.00 -X THE CO-OCCURRENCE ATTENTIVE MODULE WOULD CALCULATE THE ATTENTIVE WEIGHT OF EACH PART
utt_0192 utt 482.57 484.33 -X AND CAN MAKE THE FRAMEWORK
utt_0193 utt 484.49 487.08 -X EMPHASIZE ON THE CO-OCCURRENCE PARTS.
utt_0194 utt 488.36 489.04 -X LAST,
utt_0195 utt 489.32 490.99 -X WITH ATTENTIVE WEIGHTS,
utt_0196 utt 491.72 494.73 -X CPDM CALCULATES A WEIGHTED Ltwo FEATURE DISTANCE
utt_0197 utt 495.08 497.87 -X WHICH IS SUPERVISED BY THE TRIPLET LOSS.
utt_0198 utt 498.92 500.04 -X IN THIS PAGE,
utt_0199 utt 500.04 505.07 -X WE SHOW OUR SUPERIOR PERFORMANCE IN THE TWO LARGE-SCALE VEHICLE RE-ID DATASETS,
utt_0201 utt 505.10 507.69 -X INCLUDING VERI AND CITYFLOW DATASETS,
utt_0202 utt 507.75 509.87 -X COMPARED TO THE STATE-OF-THE-ARTS,
utt_0203 utt 510.02 514.38 -X WHICH INDICATES THE EFFICIENCY OF OUR PROPOSED SPAN AND CPDM.
utt_0205 utt 515.88 520.14 -X WE ALSO SHOW THE COMPARISON OF GENERATED PART ATTENTION MAPS
utt_0207 utt 520.17 522.00 -X WITH TWO PREVIOUS WORKS.
utt_0208 utt 522.28 526.29 -X WE CAN FIND THAT THE ATTENTION MAPS GENERATED FROM PREVIOUS WORKS
utt_0210 utt 526.41 527.95 -X ARE USUALLY NOISY
utt_0211 utt 528.11 530.13 -X OR UNSTABLE.
utt_0212 utt 530.13 531.15 -X BY COMPARISON,
utt_0213 utt 531.15 534.57 -X OUR SPAN CAN PRODUCE MORE ROBUST ATTENTION MAPS,
utt_0214 utt 534.70 538.64 -X EVEN WITH ONLY THE IMAGE-LEVEL LABEL FOR TRAINING.
utt_0215 utt 539.31 541.87 -X THAT’S THE BRIEF INTRODUCTION OF OUR PAPER.
utt_0216 utt 541.90 545.42 -X MORE DETAILS CAN BE FOUND IN OUR PAPER WEBSITE.
utt_0217 utt 545.42 546.77 -2.4175 THANKS FOR WATCHING.
