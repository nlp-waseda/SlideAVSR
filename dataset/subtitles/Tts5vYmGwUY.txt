utt_0000 utt 1.36 6.67 -X HI THANKS FOR TUNING IN I'M EDUARDO FONSECA FROM THE MUSIC TECHNOLOGY GROUP IN BARCELONA
utt_0001 utt 7.15 13.49 -X IN THIS VIDEO I'M PRESENTING OUR WASPAA PAPER ON SELF-SUPERVISED LEARNING OF AUDIO REPRESENTATIONS
utt_0002 utt 17.00 22.29 -X OUR PAPER IS TITLED SELF-SUPERVISED LEARNING FROM AUTOMATICALLY SEPARATED SOUND SCENES
utt_0003 utt 23.05 26.26 -X THIS WORK WAS DONE DURING AN INTERNSHIP AT GOOGLE RESEARCH
utt_0004 utt 26.38 30.54 -X AND IN COLLABORATION WITH MULTIPLE RESEARCHERS AT GOOGLE AS YOU CAN SEE IN THE SLIDE
utt_0005 utt 33.13 36.67 -X OUR TASK IS TO LEARN AUDIO REPRESENTATIONS FROM UNLABELED DATA
utt_0006 utt 36.91 47.70 -X TO DO THIS WE TURN TO SELF-SUPERVISED LEARNING WHERE WE LEARN REPRESENTATIONS WITHOUT EXPLICIT HUMAN SUPERVISION THE SUCCESS OF THESE METHODS RELIES ON THE DESIGN OF PROXY TASKS
utt_0008 utt 47.88 61.71 -X BY SOLVING THESE TASKS WE LEARN MAPPINGS FROM INPUTS TO LOW DIMENSIONAL REPRESENTATIONS WHICH CAN THEN BE USED FOR DOWNSTREAM TASKS SPECIFICALLY OUR WORK FOCUSES ON CONTRASTIVE LEARNING WHICH
utt_0010 utt 61.71 67.95 -X IS BASICALLY LEARNING BY COMPARING SO WE COMPARE PAIRS OF POSITIVES AND PAIRS OF NEGATIVE EXAMPLES
utt_0011 utt 68.46 79.92 -X THE GOAL IS TO PRODUCE A SEMANTICALLY STRUCTURED EMBEDDING SPACE WHERE REPRESENTATIONS FOR SIMILAR EXAMPLES ARE PULLED TOGETHER AND REPRESENTATIONS FOR DISSIMILAR EXAMPLES ARE FURTHER AWAY
utt_0013 utt 82.32 90.93 -X SO THE KEY FACTOR HERE IS HOW WE DESIGN THESE PROXY TASKS AND THERE ARE TWO MAIN QUESTIONS HOW TO GENERATE THE PAIRS OF POSITIVE EXAMPLES
utt_0015 utt 91.28 96.72 -X AND ONCE WE HAVE THEM HOW TO COMPARE THEM IN ORDER TO LEARN USEFUL REPRESENTATIONS
utt_0016 utt 99.25 103.39 -X ONE COMMON WAY TO GENERATE PAIRS OF POSITIVES IS THROUGH DATA AUGMENTATION
utt_0017 utt 103.47 107.86 -X IN ORDER TO HAVE DIFFERENTLY AUGMENTED VIEWS OF THE SAME INPUT EXAMPLE
utt_0018 utt 109.01 116.98 -X FOR CONTRASTIVE LEARNING OF AUDIO REPRESENTATIONS IT IS POPULAR TO EMPLOY COMPOSITIONS OF AUGMENTATIONS USING METHODS LIKE THOSE IN THE SLIDE
utt_0020 utt 120.49 131.67 -X IN MOST CASES THESE AUGMENTATIONS INTRODUCE ARTIFICIAL HANDCRAFTED TRANSFORMATIONS EACH WITH ONE OR MORE PARAMETERS THAT MUST BE TUNED SO WHEN WE COMBINE
utt_0022 utt 131.67 137.04 -X MANY OF THESE THINGS TOGETHER WE RUN THE RISK OF INTRODUCING A SOMEWHAT UNREALISTIC DOMAIN SHIFT
utt_0023 utt 139.09 144.23 -X ON THE OTHER HAND SOUND SCENES CONSIST OF TIME VARYING COLLECTIONS OF SOUND SOURCES
utt_0024 utt 144.30 146.90 -X EACH GENERATING CHARACTERISTIC SOUND EVENTS
utt_0025 utt 147.76 157.97 -X THE ASSOCIATION OF THESE CONSTITUENT SOUND EVENTS WITH A MIXTURE AND EACH OTHER IS NOT RANDOM BUT SEMANTICALLY CONSTRAINED IN THE SENSE THAT NOT ALL THE CLASSES COEXIST NATURALLY
utt_0027 utt 160.59 171.64 -X WITH THIS MOTIVATION WE EXPLORE THE USE OF AUTOMATIC SOUND SEPARATION TO GENERATE VIEWS FOR CONTRASTIVE LEARNING IN PARTICULAR WE PROPOSE TO DECOMPOSE A SOUND SCENE
utt_0029 utt 171.66 183.00 -X WHICH IN GENERAL IS A MIXTURE OF MULTIPLE EVENTS INTO MULTIPLE SEPARATED CHANNELS THAT SHARE SEMANTICS WITH A MIXTURE AND WITH EACH OTHER COMPARED TO PREVIOUS APPROACHES
utt_0031 utt 183.00 193.78 -X SOUND SEPARATION IS AN INPUT DEPENDENT PROCESS THAT PRODUCES ECOLOGICALLY VALID VIEWS AND IT ALSO REDUCES THE NEED FOR PARAMETER TUNING ONCE YOU HAVE A SEPARATION MODEL
utt_0033 utt 196.08 207.80 -X SPECIFICALLY THE COMPARISON OF THE INPUT MIXTURE AND ONE OF THE SEPARATED CHANNELS SHOULD BE SUITABLE FOR CONTRASTIVE LEARNING ON THE ONE HAND THE MUTUAL INFORMATION BETWEEN VIEWS IS REDUCED
utt_0035 utt 208.08 212.47 -X AS SOME SOUND COMPONENTS FROM THE MIXTURE ARE NO LONGER IN THE SEPARATED CHANNEL
utt_0036 utt 213.58 221.94 -X ALSO SOME RELEVANT SEMANTIC INFORMATION IS PRESERVED AS THE SOUNDS PRESENT IN THE SEPARATED CHANNEL ARE ALSO PRESENT IN THE INPUT MIXTURE
utt_0038 utt 221.94 231.76 -X THE SECOND QUESTION IS HOW TO COMPARE PAIRS OF EXAMPLES THERE ARE TWO POPULAR PROXY TASKS
utt_0039 utt 231.76 238.10 -X FOR CONTRASTIVE AUDIO REPRESENTATION LEARNING SIMILARITY MAXIMIZATION ALSO KNOWN AS SIMCLR
utt_0040 utt 238.64 244.28 -X CONSISTS OF MAXIMIZING THE SIMILARITY BETWEEN DIFFERENTLY AUGMENTED VIEWS OF THE SAME EXAMPLE
utt_0041 utt 246.03 255.77 -X COINCIDENCE PREDICTION CONSISTS OF PREDICTING WHETHER A PAIR OF EXAMPLES OCCURS WITHIN A TEMPORAL PROXIMITY TYPICALLY A FEW SECONDS OF AN AUDIO CLIP
utt_0043 utt 257.78 270.85 -X HERE WE PROPOSE TO OPTIMIZE BOTH TASKS JOINTLY AS A MULTI-TASK OBJECTIVE THIS CAN FAVOR LEARNING COMPLEMENTARY INFORMATION BECAUSE BOTH TASKS SHARE THE SAME GOAL OF A SEMANTICALLY
utt_0045 utt 270.85 276.89 -X STRUCTURED EMBEDDING SPACE BUT EACH TASK PURSUES THIS GOAL IN A SLIGHTLY DIFFERENT WAY
utt_0046 utt 278.42 291.77 -X SIMILARITY MAXIMIZATION AIMS TO COLLOCATE THE REPRESENTATIONS OF POSITIVES AT THE SAME SPOT IN THE EMBEDDING SPACE WHEREAS COINCIDENCE PREDICTION IS BASED ON A WEAKER CONDITION ESTABLISHING A
utt_0048 utt 291.77 297.00 -X RELATIONSHIP BETWEEN THE REPRESENTATIONS BUT NOT NECESSARILY REQUIRING THEIR COLLOCATION
utt_0049 utt 300.47 308.95 -X AND THIS IS OUR PROPOSED APPROACH IT IS COMPOSED OF A FRONT-END THAT INCLUDES SOUND SEPARATION AND AUGMENTATION TO GENERATE PAIRS OF EXAMPLES
utt_0051 utt 309.27 322.89 -X AND THE BACK-END WITH TWO PROXY TASKS SIMILARITY MAXIMIZATION AND COINCIDENCE PREDICTION EACH WITH A CORRESPONDING LOSS FUNCTION HERE THE INPUT IS AUDIO WAVEFORMS WHICH ARE CONVERTED
utt_0053 utt 322.89 328.74 -X TO MEL SPECTROGRAMS AND THE OUTPUT IS A LOW DIMENSIONAL REPRESENTATION H AFTER THE ENCODER
utt_0054 utt 332.79 336.17 -X IN THE AUGMENTATION FRONT END EVERY INCOMING MIXTURE
utt_0055 utt 336.31 346.42 -X IS SEPARATED INTO TWO OUTPUT CHANNELS FROM WHICH ONE IS RANDOMLY SELECTED FOR EACH TASK AND THEN THE OUTCOME GOES THROUGH A DATA AUGMENTATION BLOCK
utt_0057 utt 349.65 360.65 -X FOR SOUND SEPARATION WE USE A MODEL TRAINED WITH MIXTURE INVARIANT TRAINING MIXIT IS A METHOD IN WHICH TRAINING EXAMPLES ARE CONSTRUCTED BY MIXING AUDIO CLIPS
utt_0059 utt 360.92 366.52 -X AND THE MODEL IS TASKED TO SEPARATE THE RESULTING MIXTURES INTO A NUMBER OF LATENT SOURCES
utt_0060 utt 368.02 380.60 -X THIS METHOD IS FULLY UNSUPERVISED SO IT ALLOWS LEVERAGING LARGE AMOUNTS OF UNLABELED DATA IN OUR CASE AUDIOSET ALSO IT HAS SHOWN PROMISING RESULTS IN UNIVERSAL SOUND SEPARATION
utt_0062 utt 382.42 389.14 -X OUR SEPARATION MODEL IS BASED ON AN IMPROVED TIME DOMAIN CONVOLUTIONAL NETWORK WHICH IS SIMILAR TO A CONV-TASNET
utt_0064 utt 391.64 396.60 -X NOW IT IS TYPICALLY BENEFICIAL TO USE A COMPOSITION OF SEVERAL AUGMENTATIONS
utt_0065 utt 396.95 402.95 -X IN THIS WAY WE DEFINE A MORE CHALLENGING LEARNING TASK WHICH LEADS TO MORE EFFECTIVE REPRESENTATIONS
utt_0066 utt 403.54 415.53 -X IN OUR CASE WE COMBINE SOUND SEPARATION WITH TEMPORAL PROXIMITY SAMPLING AND SPECAUGMENT TEMPORAL PROXIMITY SAMPLING CONSISTS OF RANDOMLY SELECTING TWO AUDIO SNIPPETS
utt_0068 utt 415.53 427.77 -X TO CONSTRUCT PAIRS OF EXAMPLES INSTEAD OF USING THE ENTIRE AUDIOSET CLIPS WE USE SNIPPETS OF ONE SECOND THEN SPECAUGMENT INCLUDES TIME WARPING AND TIME AND FREQUENCY MASKING
utt_0070 utt 430.29 436.98 -X SO IN THIS WAY WE CREATE OUR PAIRS OF EXAMPLES NOTE THAT EACH DATA AUGMENTATION BLOCK
utt_0071 utt 437.24 441.72 -X IS ACTUALLY A DIFFERENT INSTANCE OF THE SAME AUGMENTATION POLICY
utt_0072 utt 442.58 454.31 -X ALSO THE SLIDE ILLUSTRATES THE CREATION OF POSITIVE EXAMPLES WHERE WE COMPARE A MIXTURE WITH ONE OF THE SEPARATED CHANNELS TO CREATE A PAIR OF NEGATIVE EXAMPLES WE WOULD COMPARE DIFFERENT CLIPS
utt_0074 utt 457.11 471.87 -X AND FINALLY OUR PAIRS OF EXAMPLES FEED THE TWO PROXY TASKS FOR THE SIMILARITY MAXIMIZATION TASK WE USED AN ENCODER TO EXTRACT LOW DIMENSIONAL EMBEDDINGS H AND THEN A SIMILARITY HEAD TO MAP
utt_0076 utt 471.87 483.67 -X TO THE EMBEDDINGS THAT FEED THE CONTRASTIVE LOSS THIS LOSS MAXIMIZES THE AGREEMENT BETWEEN EMBEDDINGS THAT CORRESPOND TO DIFFERENTLY AUGMENTED VIEWS OF THE INITIAL AUDIO EXAMPLE
utt_0078 utt 485.53 489.37 -X FOR THE COINCIDENCE PREDICTION TASK WE USE THE SAME ENCODER
utt_0079 utt 489.75 503.58 -X TO EXTRACT EMBEDDINGS H THEN WE CONCATENATE THE PAIR OF EMBEDDINGS AND THEN A COINCIDENCE HEAD IS TASKED TO PREDICT THE PROBABILITY THAT THESE TWO GUYS COINCIDE WITHIN THE SAME AUDIO CLIP
utt_0081 utt 504.82 509.53 -X THIS IS A BINARY CLASSIFICATION TASK AND SO WE USE A BINARY CROSS ENTROPY LOSS
utt_0082 utt 511.48 519.58 -X SO WE TRAIN ALL THIS UNTIL CONVERGENCE AND ONCE THE TRAINING IS OVER THE REPRESENTATION H IS USED FOR DOWNSTREAM TASKS
utt_0084 utt 522.42 535.35 -X AS FOR IMPLEMENTATION DETAILS WE SEPARATE INPUT MIXTURES INTO TWO CHANNELS AS IT WAS PROVEN MORE PRACTICAL THAN USING A LARGER NUMBER OUR ENCODER IS A CNNfourteen FROM THE PANN'S PAPER
utt_0086 utt 535.89 542.55 -X WHICH HAS A VGG STYLE ARCHITECTURE AND WE OBTAIN OUR REPRESENTATION H AFTER A BOTTLENECK TO one hundred and twenty-eight
utt_0087 utt 542.65 550.71 -X DIMENSIONS ALSO OUR TWO HEADS CONSIST OF A SIMPLE MULTI-LAYER PERCEPTRON WITH A RELU NON-LINEARITY
utt_0088 utt 550.71 559.42 -X FOR EVALUATION WE EMPLOYED TWO METHODS THE FIRST ONE CONSISTS OF A DOWNSTREAM CLASSIFICATION TASK
utt_0089 utt 559.42 572.80 -X WITH A SHALLOW MODEL SPECIFICALLY WE USE THE TRAINED ENCODER AS A FEATURE EXTRACTOR AND WE COMPUTE FEATURES ON AUDIOSET THEN WE TRAIN AND EVALUATE A SHALLOW NETWORK ON TOP
utt_0091 utt 572.80 586.49 -X OF THESE FEATURES IN ADDITION WE ALSO EVALUATE THE EMBEDDINGS FOLLOWING A QUERY BY EXAMPLE RETRIEVAL TASK BUT IN THIS VIDEO WE FOCUS ONLY ON THE RESULTS FOR THE SHALLOW MODEL CLASSIFICATION
utt_0093 utt 590.20 602.19 -X AND HERE'S THE SUMMARY OF THE MAIN RESULTS FIRST WE SHOW THE UTILITY OF SOUND SEPARATION FOR CONTRASTIVE LEARNING OF AUDIO REPRESENTATIONS THE TABLE SHOWS THE PERFORMANCE OBTAINED BY CREATING
utt_0095 utt 602.19 606.62 -X PAIRS OF EXAMPLES IN DIFFERENT WAYS COUPLED WITH THE SIMILARITY MAXIMIZATION BACKEND
utt_0096 utt 607.19 614.33 -X IN PARTICULAR TEMPORAL PROXIMITY SAMPLING IS ALWAYS APPLIED AND SOUND SEPARATION AND
utt_0097 utt 614.33 622.30 -X SPECAUGMENT ARE APPLIED AS INDICATED IN THE COLUMN SO THE FIRST ROW CORRESPONDS TO ONLY TEMPORARY
utt_0098 utt 622.30 636.80 -X PROXIMITY SAMPLING AND THEN IN THE SECOND ROW WE ADD ONLY SPECAUGMENT OR ONLY SOUND SEPARATION IN THE THIRD ROW AND FINALLY WE COMBINE EVERYTHING SO WE SEE THAT SOUND SEPARATION PRE-PROCESSING
utt_0100 utt 636.92 643.12 -X OUTPERFORMS SPECAUGMENT YET THE BEST PERFORMANCE IS OBTAINED FROM THEIR COMPOSITION
utt_0101 utt 644.47 656.70 -X THIS INDICATES THAT COMPARING THE INPUT MIXTURE WITH THE SEPARATED CHANNELS PROVIDES BETTER REPRESENTATIONS THAN THE CONVENTIONAL SETTING OF USING ONLY THE INPUT CLIP ALSO RESULTS
utt_0103 utt 656.70 670.97 -X SHOW THAT SOUND SEPARATION CAN BE SUCCESSFULLY COMBINED WITH OTHER COMMONLY USED AUGMENTATIONS DURING OUR EXPERIMENTS WE SAW THAT SOUND SEPARATION IS BENEFICIAL EVEN WHEN THE
utt_0105 utt 670.97 676.19 -X SEPARATION IS LESS THAN PERFECT THIS CAN HAPPEN WHEN THE INPUT MIXTURES ARE DIFFICULT TO SEPARATE
utt_0106 utt 677.11 682.88 -X SO WE WANTED TO SEE WHETHER THE PROCESSING PROVIDED BY A SEPARATION MODEL BEFORE CONVERGENCE
utt_0107 utt 683.00 696.54 -X CAN ALSO BE A VALID FORM OF AUGMENTATION TO THIS END WE EXPERIMENT WITH SEPARATION EXAMPLES GENERATED BY MULTIPLE TRAINING CHECKPOINTS OF A SEPARATION NETWORK WE VIEW THESE SEPARATION
utt_0109 utt 696.54 710.43 -X CHECKPOINTS AS AUDIO PROCESSORS AND WE IDENTIFY FOUR TYPES THE SLIDE SHOWS THE SPECTROGRAMS OF THE TWO SEPARATED CHANNELS OBTAINED WITH FOUR CHECKPOINTS OF THE SAME SEPARATION MODEL
utt_0111 utt 711.51 719.10 -X AND THIS IS FOR AN INPUT MIXTURE THAT CONTAINS A GUITAR MELODY FOLLOWED BY THE SOUND OF APPLAUSE
utt_0112 utt 719.74 723.68 -X YOU CAN SEE THE PROCESSOR'S NAME ALONG WITH THE NUMBER OF TRAINING STEPS
utt_0113 utt 723.71 729.34 -X HERE AT THE TOP SO THE PROCESSORS INCLUDE SEPARATION AFTER CONVERGENCE Stwo
utt_0114 utt 730.46 736.96 -X WHICH IS WHAT WE USED FOR THE RESULTS IN THE PREVIOUS SLIDE SEPARATION BEFORE CONVERGENCE Sone
utt_0115 utt 738.20 741.71 -X WHERE THE SEPARATION PERFORMANCE IS MORE LIMITED AS YOU CAN SEE IN THE SLIDE
utt_0116 utt 742.49 755.97 -X AND THEN WE ALSO HAVE A FILTERING EFFECT WHERE SOURCES ARE NOT REALLY SEPARATED AND THE OUTPUT CHANNELS ARE DIFFERENTLY FILTERED VERSIONS OF THE INPUT AND FINALLY A NOISE PROCESS WHERE OUTPUTS
utt_0118 utt 755.97 762.78 -X ARE PRODUCED BY THE SEPARATION MODEL UNTRAINED FEATURING CLEARLY AUDIBLE AND WIDEBAND NOISE
utt_0119 utt 766.27 774.43 -X FROM THE RESULTS WE CAN MAKE TWO OBSERVATIONS BY LOOKING AT THE TOP SECTION OF THE TABLE
utt_0120 utt 774.43 786.59 -X WE SEE THAT ALL PROCESSORS PROVIDE VALID FORMS OF AUGMENTATION FOR GENERATING POSITIVES AND BY LOOKING AT THE BOTTOM SECTION WE SEE THAT COMBINING SOME OF THEM USING AN OR
utt_0122 utt 786.59 793.25 -X RULE CAN BE HELPFUL THIS MEANS APPLYING ONLY ONE OF THE PROCESSORS SELECTED RANDOMLY AT A TIME
utt_0123 utt 796.00 809.30 -X FINALLY WE SHOW THE RESULTS WHEN TRAINING THE ENTIRE FRAMEWORK MEANING JOINTLY OPTIMIZING BOTH PROXY TASKS HERE WE OBSERVE SMALL BOOSTS ACROSS THE BOARD SO THESE RESULTS SUGGEST THAT
utt_0125 utt 809.30 819.87 -X THE KEY INGREDIENT HERE IS NOT THE QUALITY OF THE SOUND SEPARATION PROCESS BUT RATHER THE COMBINATION OF DIVERSE PROCESSING PROVIDED BY THE SEPARATION MODEL AS LEARNING PROGRESSES
utt_0127 utt 823.32 827.07 -X TO PUT OUR RESULTS INTO CONTEXT WE COMPARE THEM WITH PREVIOUS WORK
utt_0128 utt 827.23 831.84 -X AND WE SEE THAT THE PROPOSED FRAMEWORK OUTPERFORMS SOME PAST APPROACHES
utt_0129 utt 831.93 838.53 -X INCLUDING SOME MULTIMODAL ONES THAT USE AUDIO AND VIDEO OR AUDIO VIDEO AND TEXT
utt_0130 utt 839.32 844.67 -X ALSO OUR SYSTEM IS COMPETITIVE WITH THE STATE OF THE ART UNDER SIMILAR EVALUATION SETTINGS
utt_0131 utt 847.55 858.59 -X WE CONCLUDE WITH THE MAIN TAKEAWAYS WE'VE SHOWN THAT SOUND SEPARATION CAN BE SEEN AS A VALID AUGMENTATION TO GENERATE VIEWS FOR CONTRASTIVE LEARNING AND THAT LEARNING TO ASSOCIATE
utt_0133 utt 858.59 869.86 -X SOUND MIXTURES WITH THEIR CONSTITUENT SEPARATED CHANNELS ELICITS SEMANTIC STRUCTURE IN THE LEARNED REPRESENTATION WE'VE ALSO DEMONSTRATED THAT SOUND SEPARATION CAN BE SUCCESSFULLY COMBINED
utt_0135 utt 869.86 880.51 -X WITH OTHER COMMONLY USED AUGMENTATIONS WE DISCOVERED THAT THE TRANSFORMATIONS PROVIDED BY DIFFERENT CHECKPOINTS OF THE SAME SEPARATION MODEL AS LEARNING PROGRESSES ARE VALID FORMS OF
utt_0137 utt 880.51 885.81 -X AUGMENTATION FOR GENERATING POSITIVES AND THAT SOMETIMES CAN BE COMPLEMENTARY IN ADDITION
utt_0138 utt 885.95 897.57 -X WE SHOW THE BENEFIT OF JOINTLY TRAINING THE PROXY TASKS OF SIMILARITY MAXIMIZATION AND COINCIDENCE PREDICTION AND THAT'S ALL THANKS FOR LISTENING AND PLEASE CHECK OUR PAPER FOR MORE DETAILS
