utt_0000 utt 0.06 5.17 -X THIS IS PART OF OUR VIDEO SERIES ON HUMAN-CENTERED EVALUATIONS FOR NLP EXPLANATIONS.
utt_0001 utt 5.58 9.49 -X CHECK OUT THE LINK FOR MORE RESOURCES. YOU'RE CURRENTLY HERE ...
utt_0002 utt 9.49 15.92 -X A MIDDLE GROUND BETWEEN TOTALLY AUTOMATED EVALUATIONS OF EXPLANATIONS AND REAL HUMAN SUBJECT EXPERIMENTATION IS TO COLLECT
utt_0004 utt 16.56 21.84 -X HUMAN-ANNOTATED EXPLANATIONS, AND USE THEM AS A GOLD-STANDARD FOR MODEL-GENERATED EXPLANATIONS.
utt_0005 utt 21.84 26.71 -X THE IDEA HERE IS THAT, BECAUSE MANY LABELS IN NLP ARE DEFINED BY HUMAN ANNOTATORS,
utt_0006 utt 26.71 33.39 -X THEN HUMAN-PROVIDED EXPLANATIONS FOR THOSE SHOULD HAVE A SIMILAR CLAIM ON LEGITIMACY AS A GOLD-STANDARD FOR MODEL OUTPUT.
utt_0008 utt 43.76 55.38 -X HOWEVER, BEFORE WE TRY TO USE THIS EXTRA ANNOTATION TO EVALUATE OUR MODELS, WE FIRST HAVE TO COLLECT IT. THIS IS A NUANCED PROCESS WITH A LOT OF DESIGN CONSIDERATIONS, INCLUDING WHAT KIND
utt_0010 utt 61.58 67.51 -X OF EXPLANATIONS WE ARE COLLECTING, THE INTERFACE AND INSTRUCTIONS USED BY HUMAN ANNOTATORS,
utt_0012 utt 71.31 79.60 -X SARAH WIEGREFFE AND ANA MARASOVIC HAVE A NICE RECENT SURVEY OF EXTANT HUMAN EXPLANATION DATASETS, IN WHICH THEY DISCUSS SOME OF THESE ISSUES.
utt_0014 utt 79.60 89.27 -X BEFORE I DISCUSS THESE DESIGN ISSUES IN THE ABSTRACT, I AM GOING TO GO THROUGH A FEW PROMINENT EXAMPLES OF EXPLANATION DATASETS, IN ORDER TO GIVE A CONCRETE IDEA OF WHAT THESE THINGS LOOK LIKE,
utt_0016 utt 89.42 93.94 -X AS WELL AS THE INTERFACES USED TO COLLECT THEM (WHERE THAT INFORMATION IS AVAILABLE).
utt_0017 utt 93.94 98.48 -X THE FIRST ONE I’LL MENTION IS THE BEERADVOCATE DATASET, RELEASED BY JULIAN MCCAULEY IN two thousand and twelve.
utt_0018 utt 99.06 104.45 -X THIS DATASET TOOK A SMALL SAMPLE OF AN EXISTING DATASET OF MULTI-ASPECT BEER REVIEWS
utt_0019 utt 105.36 111.03 -X AND HAD ANNOTATORS IDENTIFY THE SENTENCES IN EACH REVIEW THAT PERTAINED TO EACH ASPECT
utt_0020 utt 112.30 114.26 -X (TASTE, SMELL AND APPEARANCE).
utt_0021 utt 114.90 125.37 -X …AND HERE’S AN EXAMPLE OF THE RESULT, TAKEN FROM A LATER PAPER. YOU CAN SEE HERE THAT THE REVIEW COVERS MULTIPLE ASPECTS OF THE BEER, ONLY SOME OF IT PERTAINING TO ITS APPEARANCE SPECIFICALLY.
utt_0023 utt 125.52 136.63 -X THE NEXT EXAMPLE IS THE WIKIATTACK DATASET, WHICH WAS SIMILARLY SAMPLED FROM A LARGER EXISTING DATASET, SPECIFICALLY THE WIKITOXIC DATASET PUBLISHED BY JIGSAW IN two thousand and seventeen.
utt_0026 utt 145.81 149.94 -X BY HIGHLIGHTING ALL PERSONAL ATTACK SPANS WITHIN EACH COMMENT.
utt_0027 utt 150.16 152.09 -X THE RESULT LOOKS LIKE THE FOLLOWING,
utt_0028 utt 152.09 155.22 -X WITH EACH INDIVIDUAL PERSONAL ATTACK IDENTIFIED WITHIN THE TEXT.
utt_0029 utt 155.76 159.70 -X THIS DATASET WAS COLLECTED USING THE BRAT NLP ANNOTATION TOOL,
utt_0030 utt 160.21 163.54 -X WHICH HAS BEEN HISTORICALLY POPULAR BUT IS A LITTLE DATED NOW.
utt_0031 utt 163.79 173.50 -X MULTIRC, BY CONTRAST, IS A READING COMPREHENSION DATASET WHEREIN THE EXPLANATIONS WERE COLLECTED ALONGSIDE THE DOCUMENT-LEVEL LABELS.
utt_0033 utt 173.50 183.90 -X EACH MULTIRC EXAMPLE LOOKS LIKE THE FOLLOWING: A MULTI-SENTENCE DOCUMENT, AND A QUESTION ABOUT THE DOCUMENT, SPECIFICALLY ONE THAT REQUIRES INFORMATION FROM MULTIPLE SENTENCES TO ANSWER.
utt_0035 utt 184.44 188.95 -X AND THEN THE EXPLANATION HERE IS THE SENTENCES NEEDED TO ANSWER THE QUESTION.
utt_0036 utt 189.17 192.38 -X THIS DATASET WAS COLLECTED IN A TWO-STEP PROCESS.
utt_0037 utt 192.56 197.24 -X FIRST, ANNOTATORS WERE ASKED TO GENERATE CLAIMS GROUNDED IN MULTIPLE SENTENCES.
utt_0038 utt 197.75 206.62 -X THEN, THE CLAIMS WERE VALIDATED BY HAVING A SECOND ROUND OF ANNOTATORS TRY TO ANSWER THEM WITH ONLY THE DESIGNATED SENTENCES. ANOTHER RECENT READING COMPREHENSION EXAMPLE
utt_0040 utt 207.12 212.50 -X IS FEVER, WHICH INVOLVES VERIFYING OR REFUTING CLAIMS EXTRACTED FROM WIKIPEDIA ARTICLES.
utt_0041 utt 212.50 216.22 -X MANY OF THESE CLAIMS REQUIRE REASONING OVER MULTIPLE DOCUMENTS,
utt_0042 utt 216.22 218.10 -X AS IN THIS EXAMPLE, ALTHOUGH SOME DO NOT.
utt_0043 utt 218.74 224.03 -X BY CONTRAST WITH MULTIRC, FEVER PROVIDED SENTENCES TO ANNOTATORS,
utt_0044 utt 224.03 225.98 -X ASKING THEM TO GENERATE CLAIMS FROM THEM.
utt_0045 utt 225.98 229.37 -X BUT LIKE MULTIRC, IT INVOLVED A SECOND VALIDATION STEP,
utt_0046 utt 229.37 233.02 -X ASKING OTHER ANNOTATORS TO REVIEW THE CLAIMS AND SUPPORTING SENTENCES.
utt_0047 utt 233.02 237.47 -X ALL OF THE PREVIOUS EXAMPLES HAVE BEEN “RATIONALE” OR “FEATURE-ATTRIBUTION” STYLE
utt_0048 utt 237.65 252.35 -X EXAMPLES, WHICH IDENTIFY TOKENS OR SENTENCES FROM THE INPUT AS EXPLANATORY. THE NEXT FEW EXAMPLES I WILL SHOW INCLUDE NATURAL LANGUAGE, OR FREE-TEXT EXPLANATIONS ALONGSIDE, OR INSTEAD OF, RATIONALES.
utt_0050 utt 254.49 264.99 -X THE FIRST SUCH EXAMPLE IS COS-E, WHICH ASKS ANNOTATORS TO PROVIDE BOTH RATIONALES AND FREE-TEXT EXPLANATIONS JUSTIFYING MULTIPLE CHOICE ANSWERS TO COMMON SENSE QUESTIONS.
utt_0052 utt 264.99 272.38 -X THESE THREE INSTANCES SHOW THE FORMAT. IN ADDITION TO REQUIRING DIFFERENT EXPLANATIONS, COS-E DIFFERS
utt_0054 utt 277.69 284.06 -X E-SNLI IS A TEXTUAL ENTAILMENT DATASET, ASKING ANNOTATORS WHETHER A PREMISE SENTENCE
utt_0055 utt 284.63 290.56 -X ENTAILS, CONTRADICTS, OR IS NEUTRAL TOWARD A HYPOTHESIS SENTENCE. LIKE COS-E,
utt_0056 utt 290.68 296.60 -X ANNOTATORS WERE ASKED TO PROVIDE BOTH RATIONALES AND FREE-TEXT EXPLANATIONS FOR THEIR LABELS.
utt_0057 utt 297.53 301.80 -X HOWEVER, THE FREE-TEXT EXPLANATIONS WERE REQUIRED TO CONTAIN THE TOKENS
utt_0058 utt 301.91 306.04 -X HIGHLIGHTED IN THE RATIONALES, WHICH IS AN INTERESTING QUALITY ASSURANCE MECHANISM.
utt_0059 utt 306.04 314.46 -X HERE ARE SEVERAL EXAMPLES. AS YOU CAN SEE, EACH FREE-TEXT EXPLANATION INCLUDES THE TOKENS HIGHLIGHTED IN THE PREMISE AND HYPOTHESIS SENTENCES.
utt_0061 utt 325.50 335.58 -X FINALLY, THE CONTRASTIVE EDITS DATASET COLLECTS COUNTERFACTUAL EXPLANATIONS IN THE FORM OF FREE-TEXT MODIFICATIONS TO THE INPUT TEXT, DESIGNED TO FLIP THE LABEL OF EACH TEXT.
utt_0063 utt 339.74 341.15 -X AS YOU CAN SEE, THERE
utt_0065 utt 352.54 356.45 -X THIS DATASET USED A SIMPLE INTERFACE FOR COLLECTING THESE CONTRASTIVE EDITS.
utt_0066 utt 358.91 363.23 -X SO, I’VE SHOWN YOU six EXAMPLES OF EXPLANATORY DATASETS,
utt_0067 utt 363.23 367.62 -X EXEMPLIFYING DIFFERENT APPROACHES TO THESE DESIGN ISSUES IN COLLECTING SUCH DATASETS.
utt_0068 utt 367.62 370.24 -X NOW I WILL BRIEFLY TALK ABOUT EACH ISSUE ONE BY ONE.
utt_0069 utt 370.81 375.65 -X AS WE’VE SEEN, MOST EXPLANATIONS COLLECTED ARE OF THE RATIONALE STYLE,
utt_0070 utt 376.00 380.83 -X IDENTIFYING KEY TOKENS AND SENTENCES IN THE INPUT THAT ARE RELEVANT TO THE DOCUMENT-LEVEL LABEL,
utt_0071 utt 381.40 384.74 -X BUT FREE-TEXT EXPLANATIONS ARE INCREASINGLY POPULAR.
utt_0072 utt 385.24 389.38 -X UNMENTIONED IN THIS TUTORIAL, BUT DISCUSSED IN SARAH WIEGREFFE’S REVIEW
utt_0073 utt 389.47 399.62 -X ARE SO-CALLED “STRUCTURED” EXPLANATIONS, CONSISTING OF SETS OF RATIONALES OR FREE-TEXT LOCKED INTO A PARTICULAR FORMAT APPROPRIATE TO THE MEDIUM.
utt_0075 utt 399.62 405.28 -X FINALLY, EXAMPLE- OR CASE-BASED EXPLANATIONS DON’T REALLY HAVE A PRESENCE IN THIS LITERATURE YET,
utt_0076 utt 405.28 416.58 -X AT LEAST NOT UNDER THE NAME. GOLD-STANDARD EXPLANATORY EXAMPLES FOR CLASSIFICATION DECISIONS DO EXIST FOR PARTICULAR TASKS, LIKE IDENTIFYING PRECEDENT FOR LEGAL DECISIONS,
utt_0078 utt 416.86 421.63 -X OR IDENTIFYING RELEVANT DOCUMENTS IN MULTI-DOCUMENT QUESTION-ANSWERING,
utt_0079 utt 421.63 426.31 -X BUT THESE TASKS HAVEN’T BEEN COLLECTED TOGETHER UNDER THIS PARTICULAR CONCEPTUAL UMBRELLA (YET).
utt_0080 utt 426.31 433.99 -X AND OF COURSE, THERE IS A LOT OF CONCEPTUAL OVERLAP BETWEEN THE IDEA OF GOLD-STANDARD EXPLANATORY EXAMPLES AND OTHER FAMILIES OF METHODS BASED ON RETRIEVAL,
utt_0082 utt 433.99 438.95 -X SUCH AS NEAREST-NEIGHBOR BASED CLASSIFICATION, INFORMATION RETRIEVAL, AND RECOMMENDATION.
utt_0083 utt 438.95 442.59 -X ANNOTATOR INSTRUCTIONS ARE ANOTHER BIG DESIGN CONSIDERATION.
utt_0084 utt 444.29 447.99 -X IN SOME TASKS WE DIRECTLY ASK ANNOTATORS TO JUSTIFY THE LABEL
utt_0085 utt 448.06 459.46 -X USING EXPLICITLY CAUSAL LANGUAGE. AN EXAMPLE IS E-SNLI, WHERE ANNOTATORS WERE DIRECTED TO IDENTIFY THE TOKENS THAT JUSTIFIED THE ENTAILMENT OR CONTRADICTION OF THE TWO SENTENCES.
utt_0087 utt 459.46 465.32 -X IN OTHERS, WE CAN USE TASK-SPECIFIC LANGUAGE TO SOLICIT EXPLANATIONS WHICH ARE
utt_0089 utt 471.84 477.19 -X IS THE PERSONAL ATTACK DETECTION TASK, WHERE WE ASK ANNOTATORS TO IDENTIFY ALL PERSONAL ATTACKS
utt_0090 utt 477.31 481.16 -X AS A WAY TO JUSTIFY THE LABEL OF “CONTAINS PERSONAL ATTACKS” OR NOT.
utt_0091 utt 481.73 492.48 -X IN BOTH CASES, THERE IS ALSO A DISTINCTION BETWEEN ASKING FOR A “COMPREHENSIVE” EXPLANATION THAT INCLUDES ALL EVIDENCE, VERSUS A “SUFFICIENT” EXPLANATION
utt_0093 utt 492.64 499.46 -X THAT CONSISTS OF JUST ENOUGH TO JUSTIFY THE LABEL. IN THE PERSONAL ATTACK DETECTION TASK,
utt_0094 utt 499.46 504.45 -X WE COULD HAVE ASKED ANNOTATORS TO IDENTIFY JUST ENOUGH PERSONAL ATTACKS TO JUSTIFY THE LABEL.
utt_0095 utt 504.45 508.96 -X BUT FOR OUR DOWNSTREAM PURPOSE WE DECIDED WE WANTED TO TARGET MORE COMPREHENSIVE EXPLANATIONS.
utt_0096 utt 508.96 513.06 -X FINALLY, A SOMEWHAT DIFFERENT APPROACH IS THE IDEA OF “CONTRASTIVE” EXPLANATIONS,
utt_0097 utt 513.06 520.36 -X WHERE WE ASK ANNOTATORS TO CHANGE THE INPUT IN WAYS THEY THINK WOULD CHANGE THE LABEL. THE CONTRASTIVE EDITS DATASET IS AN EXAMPLE OF THIS.
utt_0099 utt 537.31 547.78 -X CREATING AND DEPLOYING AN INTERFACE FOR COLLECTING EXPLANATIONS IS OFTEN A HUGELY LABOR-INTENSIVE PART OF THE PROCESS, ONE WHICH TENDS TO GET GLOSSED OVER IN THE LITERATURE.
utt_0101 utt 549.09 553.06 -X WHILE THERE ARE NLP ANNOTATION TOOLS OUT THERE,
utt_0102 utt 553.06 557.45 -X THE MOST COMMON WAY RESEARCHERS DO THIS IS TO CREATE THEIR OWN AD-HOC SYSTEMS.
utt_0103 utt 579.43 582.31 -X CONTROLLING QUALITY IS ANOTHER HUGE ISSUE. CROWD
utt_0104 utt 585.25 588.68 -X WORKERS HAVE LOTS OF ISSUES THAT CAN LEAD TO LOW-QUALITY RESULTS.
utt_0105 utt 589.22 595.53 -X LIKE NORMAL NLP LABELING, ONE SOLUTION IS SIMPLY TO COLLECT MULTIPLE ANNOTATIONS PER ITEM.
utt_0106 utt 595.53 598.25 -X BUT BECAUSE EXPLANATIONS ARE A VERY PARTICULAR TYPE OF PRODUCT,
utt_0107 utt 598.25 602.54 -X THERE ARE PARTICULAR WAYS WE CAN VALIDATE THEM, DEPENDING ON THE PROPERTIES WE WANT THEM TO HOLD.
utt_0108 utt 603.24 614.70 -X IF WE WANT EXPLANATIONS TO CONTAIN THE SUFFICIENT INFORMATION NEEDED TO PREDICT THE TRUE LABEL, FOR EXAMPLE, WE CAN DO AS MULTIRC DID, AND EVALUATE THIS DIRECTLY USING A SECOND GROUP OF ANNOTATORS.
utt_0110 utt 616.77 626.73 -X WE CAN ALSO APPLY COMMON-SENSE HEURISTICS, LIKE THAT A FREE-TEXT EXPLANATION SHOULD CONTAIN THE RATIONALE TOKENS WITHOUT OTHERWISE BEING TOO SIMILAR TO THE INPUT, AS IN E-SNLI.
utt_0112 utt 629.25 632.23 -X THIS IS VIDEO ISN'T THE END. YOU'RE CURRENTLY HERE.
utt_0113 utt 632.90 639.53 -3.8298 IF YOU'RE LOST, CLICK ON THE LINK BELOW FOR SOURCE CODE, SLIDES, AND OTHER RESOURCES.
