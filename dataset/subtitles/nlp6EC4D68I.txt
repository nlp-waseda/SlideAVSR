utt_0000 utt 0.32 5.58 -X HI, I'M JIACHENG. THIS VIDEO INTRODUCES OUR WORK: LEARNING THE BEST POOLING STRATEGY FOR VISUAL SEMANTIC EMBEDDING.
utt_0002 utt 10.87 17.52 -X IT HAS BEEN WIDELY USED FOR CROSS-MODAL RETRIEVAL AND RECENTLY ALSO BEEN USED FOR LANGUAGE-BASED VISUAL PRE-TRAINING.
utt_0004 utt 18.19 22.55 -X THE MODEL USUALLY CONTAINS TWO OR MULTIPLE BRANCHES OF ENCODERS FOR VISION OR LANGUAGE DATA.
utt_0005 utt 22.55 28.24 -X THE ENCODERS MAP INPUT DATA TO A SHARED EMBEDDING SPACE, WHERE THE SIMILARITY CAN BE MEASURED.
utt_0006 utt 28.24 30.45 -X AND THE ENTIRE MODEL IS TRAINED BY CONTRASTIVE LEARNING.
utt_0007 utt 32.53 37.75 -X A SERIES OF RECENT WORKS IMPROVED VISUAL SEMANTIC EMBEDDING WITH BETTER FEATURE AGGREGATORS,
utt_0008 utt 37.75 40.15 -X ESPECIALLY FOCUSING ON VISUAL FEATURE AGGREGATION.
utt_0009 utt 40.15 44.85 -X THEY USE SEQUENCE MODELS, GRAPH NETS, OR ATTENTION-BASED MODELS TO BETTER CONTEXTUALIZE THE INPUT FEATURES.
utt_0010 utt 46.48 53.05 -X HOWEVER, WE HAD AN EMPIRICAL FINDING SHOWING THAT SIMPLE POOLING FUNCTIONS CAN BE SURPRISINGLY EFFECTIVE AS THE FEATURE AGGREGATOR.
utt_0011 utt 53.05 54.90 -X THE EMPIRICAL STUDY IS AS FOLLOWING.
utt_0012 utt 55.19 59.86 -X FIRST, WE DEFINE A SET OF POSSIBLE POOLING FUNCTIONS INCLUDING AVERAGE POOLING, MAX POOLING, ETC.
utt_0014 utt 60.79 66.94 -X THEN WE PICK ONE POOLING FUNCTION FROM THE FUNCTION SET FOR EACH MODEL BRANCH AND WE TRAIN THE VSE MODEL AND DO THE VALIDATION.
utt_0016 utt 67.96 72.19 -X REPEAT STEP ONE AND STEP TWO TO FIND THE BEST POOLING FUNCTION FOR EACH MODEL BRANCH.
utt_0017 utt 72.69 80.25 -X THE SEARCH COMPLEXITY DEPENDS ON BOTH THE NUMBER OF POOLING FUNCTIONS AND THE NUMBER OF MODEL BRANCHES.
utt_0019 utt 80.25 84.41 -X FOR SIMPLICITY, WE ONLY SEARCH FOR THE BEST VISUAL POOLING FUNCTION HERE AND FIX THE TEXT BRANCH.
utt_0020 utt 84.41 86.27 -X AND THE RESULTS ARE SHOWN IN THE TABLE.
utt_0021 utt 86.90 90.40 -X THIS EXPERIMENT IS RUN FOR TWO DIFFERENT IMAGE FEATURE EXTRACTORS.
utt_0022 utt 90.40 96.09 -X THE REGION FEATURE IS THE ROI FEATURE FROM A PRE-TRAINED OBJECT DETECTOR, AND THE BEST POOLING FUNCTION IS THE MAX POOL.
utt_0023 utt 96.86 100.64 -X THE GRID FEATURE IS THE FLATTENED FEATURE MAPS FROM A STANDARD CONVNET,
utt_0024 utt 100.64 103.49 -X AND THE BEST POOLING FUNCTION IS KMAX POOL WITH K BEING twenty.
utt_0025 utt 103.87 111.74 -X FOR BOTH FEATURE EXTRACTORS THE MANUALLY-SELECTED POOLING FUNCTION OUTPERFORMS THE COMPLEX AGGREGATION MODELS FROM PREVIOUS WORKS.
utt_0027 utt 111.74 117.25 -X NOW WE HAVE THE FOLLOWING OBSERVATIONS. FIRST, THE BEST-SELECTED POOLING FUNCTION CAN BE BOTH SIMPLE AND EFFECTIVE.
utt_0028 utt 117.37 121.67 -X HOWEVER, THE BEST POOLING FUNCTION VARIES WHEN DATA MODALITY OR FEATURE EXTRACTOR CHANGES,
utt_0029 utt 121.67 124.26 -X THUS MANUAL SEARCH IS NECESSARY HERE.
utt_0030 utt 124.26 129.95 -X THE SEARCH REQUIRES REPETITIVE EXPERIMENTS, WHICH CAN BE COSTLY WHEN THERE ARE MULTIPLE MODALITIES AND FEATURE EXTRACTORS.
utt_0031 utt 130.11 133.51 -X ALSO, THE SEARCH COULD BE HARDER WHEN FEATURES HAVE VARIABLE LENGTHS.
utt_0032 utt 134.30 138.85 -X THEREFORE, WE WANT A GENERAL POOLING OPERATOR THAT CAN GENERALIZE OVER VARIOUS POOLING FUNCTIONS,
utt_0033 utt 138.85 143.43 -X AND BE TRAINED TO APPROXIMATE THE PROPER POOLING STRATEGY BASED ON THE DATA MODALITY AND FEATURE EXTRACTOR.
utt_0034 utt 143.43 145.70 -X AND CAN NATURALLY HANDLE FEATURES WITH VARIABLE LENGTH.
utt_0035 utt 146.85 150.60 -X ACCORDING TO THESE MOTIVATIONS, WE PROPOSE THE GENERALIZED POOLING OPERATOR.
utt_0036 utt 150.60 154.98 -X FIRST, WE DEFINE THE GLOBAL POOLING FUNCTION AS A WEIGHTED SUM OVER SORTED INPUT FEATURES.
utt_0037 utt 154.98 158.85 -X WE ILLUSTRATE THE MODEL BY ASSUMING THE FEATURE DIMENSIONS TO BE THREE.
utt_0038 utt 158.85 161.09 -X EACH FEATURE VECTOR HERE IS A COLUMN.
utt_0039 utt 162.02 165.32 -X THE SORTING OPERATION HERE SORTS EACH FEATURE DIMENSION SEPARATELY,
utt_0040 utt 165.51 168.20 -X MAKING EACH HORIZONTAL ROW MONOTONICALLY DECREASING.
utt_0041 utt 168.71 174.22 -X WE THEN PERFORM A WEIGHTED SUM FOR EACH FEATURE DIMENSION OF THE FEATURE VALUES, BASED ON THE POOLING COEFFICIENTS THETA.
utt_0042 utt 175.40 179.18 -X THE RIGHT FIGURE SHOWS THE POOLING COEFFICIENTS FOR SEVERAL COMMON POOLING FUNCTIONS.
utt_0043 utt 179.78 182.51 -X THE NEXT STEP IS TO GENERATE THESE POOLING COEFFICIENTS.
utt_0044 utt 182.60 185.10 -X TO MAKE THE POOLING OPERATOR HANDLE VARIABLE LENGTH,
utt_0045 utt 185.10 190.38 -X INSTEAD OF MAKING THETA A TRAINABLE VECTOR, WE NEED A COEFFICIENT GENERATOR HERE CONDITIONED ON THE LENGTH.
utt_0046 utt 190.95 193.90 -X IT'S NATURAL TO PARAMETERIZE THE GENERATOR WITH A SEQUENCE MODEL.
utt_0047 utt 196.58 198.96 -X THE FINAL IMPLEMENTATION IS ILLUSTRATED HERE.
utt_0048 utt 200.30 206.67 -X WE CHOOSE BIDIRECTIONAL GRU AS THE SEQUENCE MODEL BECAUSE IT'S SIMPLE AND PARAMETER-EFFICIENT, AND CAN MINIMIZE THE COMPUTATIONAL OVERHEAD.
utt_0050 utt 206.86 212.40 -X WE EMBED THE POSITION INDEX WITH TRIGONOMETRIC POSITIONAL ENCODING, AS IT CAN PRESERVE USEFUL PRIOR INFORMATION.
utt_0051 utt 213.74 220.81 -X FOR BETTER GENERALIZATION, WE RANDOMLY DROP twenty% OF THE INPUT VECTORS DURING THE TRAINING TO PERTURB THE SIZE OF THE FEATURE.
utt_0052 utt 220.81 224.08 -X FINALLY, WE BUILD UP OUR VSE FRAMEWORK BY FOLLOWING THE STANDARD VSE++
utt_0053 utt 224.75 228.46 -X AND USE THE PROPOSED GPO AS THE FEATURE AGGREGATOR FOR ALL MODEL BRANCHES.
utt_0054 utt 228.72 231.12 -X AND WE CALL THE FINAL FRAMEWORK VSE INFINITY.
utt_0055 utt 232.11 238.03 -X ON IMAGE-TEXT MATCHING, WE SHOW THAT VSE INFINITY IS BETTER THAN THE BEST PREVIOUS VSE-BASED METHOD, VSRN.
utt_0056 utt 238.45 242.29 -X AND WE FURTHER COMPARE VSE INFINITY WITH METHODS BASED ON VISION-LANGUAGE BERT.
utt_0057 utt 243.31 247.63 -X BY USING A STRONGER IMAGE BACKBONE TO COMPENSATE FOR THE LACK OF LARGE-SCALE PRE-TRAINING,
utt_0058 utt 247.63 251.70 -X VSE INFINITY'S PERFORMANCE CAN BE VERY CLOSE TO THE STATE-OF-THE-ART VISION-LANGUAGE BERT.
utt_0059 utt 252.59 263.57 -X IT'S WORTH NOTING THAT VISION-LANGUAGE BERT APPLIES DEEP CROSS-MODAL ATTENTION OVER EVERY PAIR OF IMAGE AND TEXT DATA WHICH IS ESSENTIALLY DIFFERENT FROM VSE-BASED METHODS, WHERE THE ONLY CROSS-MODAL INTERACTION IS THE DOT-PRODUCT BETWEEN EMBEDDING VECTORS.
utt_0061 utt 263.76 269.43 -X WE USE A SIMULATION HERE TO SHOW THAT VSE-BASED METHODS ARE MUCH FASTER FOR LARGE-SCALE RETRIEVAL.
utt_0062 utt 269.43 276.92 -X FOR VIDEO-TEXT MATCHING, WE APPLY THE GPO TO EXISTING METHODS FOR FRAME-LEVEL FEATURE AGGREGATION AND TEXT FEATURE AGGREGATION, AND FIND CONSISTENT IMPROVEMENTS.
utt_0064 utt 276.92 285.72 -X WE COMPARE GPO WITH OTHER LEARNABLE POOLING OPERATORS FROM THE LITERATURE AND FIND THAT GPO IS BETTER THAN THESE POOLING OPERATORS FOR VSE OVER DIFFERENT COMBINATIONS OF FEATURES.
utt_0066 utt 287.51 295.58 -X WE ALSO VISUALIZE THE POOLING COEFFICIENTS LEARNED BY GPO FOR DIFFERENT COMBINATIONS OF IMAGE AND TEXT FEATURES.
utt_0067 utt 295.58 299.83 -3.7870 THANK YOU FOR WATCHING. PLEASE FIND MORE DETAILS IN OUR PAPER AND CHECK OUR CODE ON OUR PROJECT PAGE.
