utt_0000 utt 6.80 19.60 -X IN THIS WORK WE FOCUS ON TRAINING VIDEO LANGUAGE MODELS FROM VIDEO-TEXT PAIRS WE PROPOSE LAVILA A NEW APPROACH TO LEARN VIDEO LANGUAGE REPRESENTATIONS BY LEVERAGING LARGE LANGUAGE
utt_0002 utt 19.60 31.39 -X MODELS WE ACHIEVED THIS BY REPURPOSING PRE-TRAINED LARGE LANGUAGE MODELS TO BE CONDITIONED ON VISUAL INPUT AND THEN FINE TUNE THEM WITH THE HUMAN NARRATIONS TO BECOME AUTOMATIC VIDEO NARRATORS
utt_0004 utt 32.27 38.56 -X THE AUTO-GENERATED NARRATIONS OFFER SEVERAL ADVANTAGES INCLUDING DENSE COVERAGE OF LONG VIDEOS
utt_0005 utt 38.96 50.68 -X MUCH HIGHER DIVERSITY OF TEXT AND COMPARED TO HUMAN ANNOTATIONS GENERATING NARRATIONS FROM LARGE LANGUAGE MODELS IS FREE THE VIDEO LANGUAGE EMBEDDING LEARNED CONTRASTIVELY
utt_0007 utt 50.68 59.52 -X WITH THESE NARRATIONS OUTPERFORMS THE PREVIOUS STATE-OF-THE-ART ON MULTIPLE FIRST PERSON AND THIRD-PERSON VIDEO TASKS BOTH IN ZERO SHOT AND FINE-TUNED SETUPS
utt_0009 utt 62.00 72.48 -X WE DIVE DEEPER INTO THE DETAILS OF THE APPROACH WE USE A DUAL ENCODER MODEL TO LEARN VIDEO LANGUAGE REPRESENTATIONS IT CONTAINS A VIDEO ENCODER AND A TEXT ENCODER
utt_0011 utt 73.04 83.05 -X FOR VIDEO ENCODER WE USE TIMESFORMER WHERE WE INITIALIZE THE SPATIAL ATTENTIONS USING CRYPTO TRAIN MODEL WEIGHTS AND THEN THE TEMPORAL ATTENTION WITH ZEROS
utt_0013 utt 84.24 97.91 -X FOR TEXT ENCODER WE USE A GPTtwo BASED TRANSFORMER WHICH IS ALSO INITIALIZED FROM CLIP'S MODEL WEIGHTS ON TOP OF BOTH VIDEO AND TEXT ENCODER WE USE A PROJECTION HEAD TO OBTAIN THE GLOBAL VISUAL
utt_0015 utt 97.91 111.16 -X AND TEXTUAL EMBEDDINGS RESPECTIVELY CONTRASTIVE LAWS SUCH AS INFONCE LEARNS GLOBAL EMBEDDINGS THAT ASSOCIATE CORRESPONDING VIDEO AND TEXT EMBEDDINGS WITHIN A BATCH OF SAMPLES THE PRO
utt_0017 utt 111.16 116.90 -X TRAINING DATA INCLUDING BOTH HUMAN ANNOTATED VIDEO TEXT PAIRS AND THEN PSEUDO-CAPTION VIDEO CLIPS
utt_0018 utt 118.84 124.78 -X THE PSEUDO-CAPTION NARRATIONS TAKE ADVANTAGE OF LARGE LANGUAGE MODELS SUPERVISION FROM TWO ASPECTS
utt_0019 utt 125.08 138.94 -X NAMELY REPHRASER AND NARRATOR. REPHRASER IS A STANDARD LARGE LANGUAGE MODEL THAT PARAPHRASES NARRATIONS IN EXISTING CLIPS IT SERVES TO AUGMENT THE TEXT INPUT NARRATOR IS A VISUALLY CONDITIONED
utt_0021 utt 138.94 151.50 -X LARGE LANGUAGE MODEL THAT PSEUDO-LABELS EXISTING AND NEW CLIPS WITH NARRATIONS IT GENERATES NEW DESCRIPTIONS OF THE ACTION TAKING PLACE AND CAN POTENTIALLY FOCUSES ON OTHER OBJECTS WHICH HUMANS
utt_0023 utt 151.50 157.83 -X ARE INTERACTING WITH WE PROVIDE TWO EXAMPLES OF THE OUTPUT BY REFRESHER AND NARRATORS ON THE RIGHT
utt_0024 utt 160.98 167.55 -X REPHRASER IS A TEXT TO TEXT MODEL IMPLEMENTED BY AN ENCODER-DECODER ARCHITECTURE SUCH AS Tfive
utt_0025 utt 168.63 173.95 -X WE OBSERVE THAT REPHRASER CAN DO BASIC MANIPULATIONS SUCH AS REPLACING SYNONYMS
utt_0026 utt 174.26 179.84 -X OR CHANGING WORD ORDER WHICH SERVES AS AN EFFICIENT WAY OF AUTOMATIC DATA AUGMENTATION
utt_0027 utt 181.78 194.91 -X NARRATOR CONTAINS A VISUAL ENCODER AND A TEXT DECODER WITH ATTENTION POOLING IN BETWEEN TO ALIGN THE DIMENSION BETWEEN VISION AND TEXT THE TEXT DECODER FOLLOWS STANDARD LARGE LANGUAGE MODELS
utt_0029 utt 194.91 201.12 -X SUCH AS A GPTtwo-LARGE MODEL THIS ENABLES NARRATOR TO BE INITIALIZED FROM PRE-TRAINED WEIGHTS
utt_0030 utt 201.46 213.55 -X TAKING FULL ADVANTAGE OF LARGE LANGUAGE MODELS CAPACITY WE ADD A FEW ADDITIONAL CROSS-ATTENTION MODULES BEFORE EACH OF THE GPTtwo BLOCK TO PROVIDE VISUAL CONDITIONING WE
utt_0032 utt 213.55 219.23 -X KEEP THE GPTtwo TRANSFORMER BLOCKS FROZEN AND ONLY FINE-TUNE THE CROSS-ATTENTION MODULES IN BETWEEN
utt_0033 utt 220.28 232.93 -X THE VISUAL ENCODER TO GENERATE VIDEO EMBEDDINGS FOR CONDITIONING ARE OBTAINED FROM THE DUAL-ENCODER MODEL'S VISUAL PART WE TRAIN THE NARRATOR ON ALL OR SUBSET OF
utt_0035 utt 232.93 245.30 -X HUMAN ANNOTATED VIDEO NARRATION PAIRS USING CAPTIONING LOSS DURING INFERENCE WE QUERY THE NARRATOR BY FEEDING THE VISUAL EMBEDDING AND THE SPECIAL START-OF-SENTENCE TOKEN AT EACH
utt_0037 utt 245.30 251.14 -X TIME STEP THE TEXT DECODER PRODUCES A LIKELIHOOD OVER ALL POSSIBLE TOKENS IN THE VOCABULARY SET
utt_0038 utt 251.90 257.97 -X WE USE NUCLEAR SAMPLING IN AN AUTO REGRESSIVE WAY UNTIL A SPECIAL END-OF-SENTENCE TOKEN IS REACHED
utt_0039 utt 258.36 262.98 -X THIS PROCESS CAN REPEAT FOR MULTIPLE TIMES TO GENERATE DIVERSE NARRATIONS
utt_0040 utt 264.57 268.99 -X HERE WE DESCRIBE HOW THE NARRATOR DENSELY NARRATES THE ENTIRE LONG VIDEO
utt_0041 utt 269.95 275.62 -X FOR EXISTING VIDEO CLIPS THAT HAVE BEEN ANNOTATED BY HUMAN WE DIRECTLY RECAPTION THEM
utt_0042 utt 276.29 281.71 -X FOR THE REMAINING PART WE FIRST UNIFORMLY SAMPLE VIDEO CLIPS FROM UNLABELED INTERVALS
utt_0043 utt 281.76 293.70 -X AND THEN DENSELY CAPTIONED ON THESE CLIPS NEXT WE FILTER OUT CLIPS WITH LOW SIMILARITY SCORE BETWEEN THE VIDEO CLIPS AND THE GENERATING NARRATIONS USING A PRE-TRAINED DUAL ENCODER MODEL
utt_0045 utt 296.00 306.58 -X WE TRAIN BOTH THE NARRATOR AND THE DUAL-ENCODER MODEL ON THE VIDEO NARRATION PAIRS FROM EGOminus fourD VIDEOS THAT ARE NOT IN THE VALIDATION OR TESTING SPLIT OF THE EGOminus fourD BENCHMARKS
utt_0047 utt 307.26 319.06 -X THE PRE-TRAINING DATA SET INCLUDES AROUND four MILLIONS OF HUMAN ANNOTATED CLIPS WITH AN AVERAGE LENGTH OF ABOUT A SECOND WITH THE HELP OF NARRATOR AND REPHRASER WE EXPAND
utt_0049 utt 319.06 331.64 -X THE NARRATION CORPUS BY ten TIMES AND THREE TIMES RESPECTIVELY WE CONDUCT A ZERO SHOT OR FINE-TUNING EXPERIMENTS ON A WIDE RANGE OF DOWNSTREAM BENCHMARKS INCLUDING EPIC-KITCHEN
utt_0051 utt 331.71 340.28 -X one hundred EGOfourD BENCHMARKS EGTEA AND CHARADES EGO IN THE ZERO SHOT SETUP WE APPLY THE PRE-TRAINED
utt_0052 utt 340.28 352.92 -X DUAL ENCODER DIRECTLY ON THE DOWNSTREAM VALIDATION SPLIT WITHOUT ANY TUNING IN THE FINE-TUNE SETUP WE TAKE THE PRE-TRAINED DUAL ENCODER AND PERFORM END-TO-END FINE TUNING ON THE TRAINING SPLIT
utt_0054 utt 356.26 368.58 -X THE RESULTING MODEL LEARNED ON THESE EXPANDING NARRATIONS SET A NEW STATE-OF-THE-ART ON A WIDE RANGE OF DOWNSTREAM TASKS ACROSS CHALLENGING DATA SETS FOR MULTIPLE INSTANCE VIDEO RETRIEVAL ON EPIC
utt_0056 utt 370.40 375.18 -X KITCHEN one hundred WE ACHIEVED A seven point one% ABSOLUTE GAME ON AVERAGE NDCG AFTER FINE TUNING
utt_0057 utt 375.75 383.82 -X FOR MULTIPLE CHOICE QUESTION ANSWERING ON EGOfourD WE ACHIEVE five point nine% ABSOLUTE GAIN ON INTRA-VIDEO ACCURACY
utt_0058 utt 384.90 392.92 -X FOR VIDEO RECOGNITION ON EGTEA WE ACHIEVED A ten point one% ABSOLUTE GAIN ON TOP ONE ACCURACY AFTER
utt_0059 utt 392.92 400.49 -X FINE TUNING WE OBSERVE POSITIVE SCALING BEHAVIOR ON INCREASING PRE-TRAINING DATA AND MODEL SIZE
utt_0060 utt 401.13 406.57 -X FOR DATA SCALING WE CAN SEE A CONSISTENT PERFORMANCE GAIN WITH INCREASING PRE-TRAINING DATA
utt_0061 utt 407.11 415.75 -X NOTABLY LAVILA TRAINED WITH ONLY HALF OF THE NARRATIONS FROM THE EGOfourD DATA SET OUTPERFORMS BASELINE MODELS TRAINED ON A FULL DATA SET
utt_0063 utt 417.51 429.10 -X FOR MODEL SCALING WE COMPARE THE ZERO SHORT RETRIEVAL RESULT BY PROGRESSIVELY INCREASING THE SIZE OF NARRATOR'S VIDEO ENCODER SIZE FROM BASE TO TIMES FORMER LARGE WHILE FIXING THE DUAL
utt_0065 utt 429.10 435.29 -X ENCODER ARCHITECTURE THE RETRIEVAL PERFORMANCE STEADILY INCREASES WHILE NARRATOR BECOMES STRONGER
utt_0066 utt 435.85 440.04 -X WE ALSO VARY THE DUAL ENCODER ARCHITECTURE AND SHOW SIMILAR TRENDS
utt_0067 utt 440.90 454.09 -X BOTH PHENOMENA SUGGEST THAT LAVILA CAN SCALE TO LARGER MODELS FOR MORE DETAILS PLEASE CHECK OUT OUR GITHUB REPO AND PLAY WITH THE DEMOS THAT ARE AVAILABLE ONLINE THANKS FOR WATCHING
