utt_0000 utt 0.59 4.62 -X HI! THIS IS ZHAO CHEN (赵琛) FROM KAUST PRESENTING OUR WORK REtwoTAL.
utt_0001 utt 7.63 21.04 -X LONG-FORM VIDEO UNDERSTANDING TASKS SUCH AS TEMPORAL ACTION LOCALIZATION IS EXTREMELY MEMORY INTENSIVE TO TRAIN END TO END. TO FACILITATE MEMORY EFFICIENT TRAINING, WE PROPOSE REtwoTAL.
utt_0003 utt 21.97 35.32 -X GIVEN ANY RESIDUAL MODULES, SUCH AS RESNET BLOCKS AND SELF-ATTENTION BLOCKS, WE REPLICATE THE INPUT AND REWIRE THE SKIP CONNECTIONS TO CONVERT THEM INTO NEW MODULES WITH THE SAME PARAMETERS.
utt_0005 utt 35.66 49.81 -X THE NETWORK WITH THE NEW MODULES CAN REUSE THE PARAMETERS IN THE PREVIOUS MODULES WITHOUT TRAINING FROM SCRATCH. THE OBTAINED MODULE IS REVERSIBLE, MEANING THAT WE CAN RECONSTRUCT THE INPUT FROM THE OUTPUT.
utt_0007 utt 49.94 62.36 -X THEREFORE THE INTERMEDIATE FEATURES DON'T NEED TO STORE IN MEMORY FOR BACK PROPAGATION AND THE GPU MEMORY USAGE IS DRAMATICALLY REDUCED. LET ME DESCRIBE THIS WORK IN MORE DETAILS.
utt_0009 utt 63.83 76.76 -X TEMPORAL ELECTION LOCALIZATION, TAL FOR SHORT AIMS TO RECOGNIZE ACTIONS IN A LONG VIDEO AND LOCALIZE THEIR START AND END TIME STAMPS. IT IS A TYPICAL LONG-FORM VIDEO UNDERSTANDING TASK,
utt_0011 utt 76.92 90.74 -X WHICH NEEDS TO REASON AMONG A LARGE NUMBER OF VIDEO FRAMES. THIS IS A WIDELY ADOPTED TAL ARCHITECTURE, WHICH COMPRISES A BACKBONE AND A LOCALIZER. THE BACKBONE SUCH AS SLOWFAST AND
utt_0013 utt 90.74 97.15 -X VIDEO SWIN TRANSFORMER EXTRACTS SHORT-TERM VIDEO FEATURES. THE LOCALIZER SUCH AS VSGN
utt_0014 utt 97.33 103.48 -X AND ACTIONFORMER PERFORMS A LONG-TERM TEMPERAL AGGREGATION AND OUTPUTS THE PREDICTION RESULTS.
utt_0015 utt 104.76 112.41 -X THE IDEAL OF TRAINING TAL IS END TO END, MEANING TRAINING THE BACKBONE AND THE LOCALIZER AT THE SAME TIME
utt_0016 utt 112.82 126.65 -X BUT IT IS EXTREMELY MEMORY EXPENSIVE IN THE BACKBONE. THIS IS BECAUSE DURING TRAINING ALL THE INTERMEDIATE ACTIVATIONS FROM ALL FRAMES NEED TO BE STORED IN MEMORY FOR BACK PROPAGATION.
utt_0018 utt 126.65 131.93 -X THEREFORE, MOST WORKS ADOPT TWO-STEP OR FEATURE-BASED TRAINING AS A COMPROMISE,
utt_0019 utt 132.22 146.49 -X WHICH FIRST EXTRACT SHORT-TERM FEATURES USING A PRE-TRAINED BACKBONE AND THEN TRAIN THE LOCALIZER ONLY WITH THE PRE-EXTRACT FEATURES. THIS INEVITABLY SACRIFICES THE ACCURACY, SINCE THE BACKBONE
utt_0021 utt 146.49 149.50 -X CANNOT ADAPT TO THE TAL TASK AND THE DATASET.
utt_0022 utt 150.97 162.08 -X IF END-TO-END TRAINING IS THE WAY TO GO, CAN WE FIND SUCH TYPE OF A NETWORK AS THE BACKBONE THAT USES LESS MEMORY BUT ACHIEVES COMPARABLE PERFORMANCE ?
utt_0023 utt 162.14 176.42 -X THE ANSWER IS YES. REVERSIBLE NETWORKS, WHICH CAN RECONSTRUCT THE INPUT FROM THE OUTPUT, DON'T NEED TO STORE INTERMEDIATE ACTIVATIONS IN GPU MEMORY, THUS CAN DRAMATICALLY REDUCE GPU MEMORY USAGE.
utt_0025 utt 177.41 190.42 -X MULTIPLE REVERSIBLE NETWORKS HAVE BEEN PROPOSED IN THE LITERATURE TO REDUCE MEMORY WHILE PRESERVING ACCURACY, SUCH AS REV-VIT AND REVNET. HOWEVER, THESE WORKS
utt_0027 utt 190.42 196.16 -X ONLY FOCUS ON SPECIFIC ARCHITECTURES AND THEIR MODELS NEED TO TRAIN FROM SCRATCH.
utt_0028 utt 196.16 208.39 -X THEREFORE, IT IS NOT A GOOD IDEA TO DIRECTLY APPLY THESE REVERSIBLE NETWORKS TO TAL, WHICH NEEDS MULTIPLE ROUNDS OF BACKBONE PRE-TRAINING ON DIFFERENT DATASETS TO REACH A GOOD PERFORMANCE.
utt_0030 utt 210.59 215.85 -X TO FACILITATE END-TO-END TRAINING WITH MINIMAL PRE-TRAINING EFFORT, IN THIS WORK,
utt_0031 utt 215.85 229.03 -X WE PROPOSE REtwoTAL. LET'S CONSIDER ONE COMMONLY USED STRUCTURE IN MODERN NEURAL NETWORKS, THE RESIDUAL MODULE, WHICH HAS A SKIP CONNECTION AROUND A STACK OF OPERATIONS.
utt_0033 utt 229.63 234.47 -X FOR EXAMPLE, THE RESIDENT BLOCK OR THE SELF-ATTENTION BLOCKS IN TRANSFORMERS.
utt_0034 utt 235.30 242.25 -X GIVEN ANY RESIDUAL MODULE, WE CAN CONVERT IT INTO A REVERSE BLOCK BY REWIRING, SUCH THAT WE
utt_0035 utt 242.25 251.08 -X CAN AUTOMATICALLY OBTAIN REVERSIBLE NETWORKS FROM THE LARGE COLLECTION OF VARIOUS EXISTING OR EVEN FUTURE NETWORK ARCHITECTURES.
utt_0036 utt 256.33 263.47 -X HERE IS A NETWORK OF THREE RESIDUAL MODULES. WE MAKE A TWO PATHWAY NETWORK BY REWIRING.
utt_0037 utt 263.47 275.02 -X WE REPLICATE THE ORIGINAL INPUT AS THE INPUT TO THE SECOND PATHWAY, AND REWIRE ALL SKIP CONNECTIONS TO MAKE THEM SKIP TWO BLOCKS INSTEAD OF ONE BLOCK TO CONVERT THE NETWORK INTO
utt_0039 utt 276.07 283.05 -X ONE NETWORK WITH THREE NEW MODULES. THE NEW NETWORK HAS THE SAME PARAMETERS IN Fone, Ftwo AND
utt_0040 utt 283.50 289.77 -X Fthree AS THE ORIGINAL NETWORK, THUS IT CAN REUSE THE PARAMETERS WITHOUT TRAINING FROM SCRATCH.
utt_0041 utt 290.31 299.21 -X THE REWIRED NETWORK IS REVERSIBLE OR INVERTIBLE, WHICH CAN BE EASILY PROVED WITH THE MATHEMATICAL FORMULATIONS.
utt_0042 utt 299.63 308.05 -X THIS IS THE OBTAINED FORWARD PROCESS. AND THIS IS THE REVERSE PROCESS TO RECONSTRUCT THE INPUT FROM THE OUTPUT.
utt_0043 utt 308.78 319.28 -X DUE TO THE REVERSIBLE PROPERTY OF THE NETWORK, WE DON'T NEED TO STORE INTERMEDIATE ACTIVATIONS IN MEMORY AND CAN RECONSTRUCT THOSE DURING BACK PROPAGATION.
utt_0044 utt 319.28 322.80 -X THEREFORE,GPU MEMORY CONSUMPTION IS DRAMATICALLY REDUCED.
utt_0045 utt 325.36 330.38 -X THE STEPS OF USING THE REWIRING STRATEGY FOR TAL END-TO-END TRAINING IS THE FOLLOWING.
utt_0046 utt 331.56 339.57 -X PREPARE A VIDEO MODEL WITH PRE-TRAINED PARAMETERS. REWIRE THE RESIDUAL CONNECTION IN THE MODEL AS PROPOSED.
utt_0047 utt 340.40 352.34 -X LOAD THE PRE-TAINED PARAMETERS INTO THE REWIRED MODEL. FINETUNE FOR SEVERAL EPOCHS ON THE PRE-TRAINING TASK, AND IT IS READY FOR USE FOR TAL AND OTHER TASKS.
utt_0049 utt 354.03 355.44 -X EXPERIMENTAL RESULTS.
utt_0050 utt 357.14 371.36 -X WE COMPARE ORIGINAL NON-REVERSIBLE NETWORKS TO OUR REWIRED REVERSIBLE COUNTERPARTS IN TERMS OF MEMORY CONSUMPTION AND ACCURACY. WE EXPERIMENTED ON DIFFERENT VARIANTS OF VIDEO SWIN TRANSFORMERS
utt_0052 utt 371.36 379.99 -X AND SLOWFAST NETWORKS FOR ALL THE VARIANTS. OUR REVERSIBLE NETWORKS USE SIGNIFICANTLY LESS GPU
utt_0053 utt 379.99 387.86 -X MEMORY AND THEIR USE OF MEMORY KEEPS CONSTANT AS THE NETWORKS BECOME DEEPER. AT THE SAME TIME,
utt_0054 utt 387.86 395.41 -X THE ACCURACY OF OUR REWIRED REVERSIBLE NETWORKS IS COMPARABLE TO THE ORIGINAL NON-REVERSIBLE NETWORKS.
utt_0055 utt 397.33 411.19 -X THIS TABLE SHOWS THE MEAN AP COMPARISON WITH THE PREVIOUS REPRESENTATIVE TAL METHODS ON THE TWO DATASETS ACTIVITYNET AND THUMOS. OUR REtwoTAL SETS A NEW STATE-OF-THE-ART
utt_0057 utt 411.19 417.98 -X PERFORMANCE ON ACTIVITYNET AND REACHES THE HIGHEST AMONG ALL RGB-ONLY METHODS ON THUMOS.
utt_0058 utt 418.65 429.11 -X IF WE COMPARE THE RESULTS OF REtwoTAL END-TO-END TRAINING TO THEIR COUNTERPARTS OF FEATURE-BASED TRAINING, REtwoTAL SIGNIFICANTLY IMPROVES THE PERFORMANCE.
utt_0060 utt 430.74 431.83 -X CONCLUSIONS.
utt_0061 utt 433.01 443.20 -X WE PROPOSED A REWIRING STRATEGY TO CONVERT OFF-THE-SHELF MODELS INTO REVERSIBLE MODELS WITHOUT THE NEED TO DESIGN OR TRAIN FROM SCRATCH.
utt_0063 utt 443.20 448.99 -X USING OUR REtwo MODELS, WE CAN DO END-TO-END TRAINING WITH DRAMATICALLY REDUCED MEMORY COST,
utt_0064 utt 449.14 455.32 -X AND ACHIEVE SIGNIFICANTLY HIGHER ACCURACY THAN THE TWO-STEP TRAINING.
utt_0065 utt 455.32 465.95 -X WE DEMONSTRATED THE EFFECTIVENESS OF OUR REtwo STRATEGY WITH THE MEMORY INTENSIVE TASK TEMPORAL ACTION LOCALIZATION, TAL. THE STRATEGY AND THE MODELS OF REtwo
utt_0067 utt 466.36 472.61 -X CAN ALSO BE APPLIED TO OTHER TASKS. YOU'RE WELCOME TO TRY IT.
utt_0068 utt 472.61 474.37 -2.0902 THANKS FOR THE ATTENTION!
