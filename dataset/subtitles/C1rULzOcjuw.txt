utt_0000 utt 4.88 11.60 -X WE PRESENT MOBILENERF, A NERF THAT CAN RUN ON A VARIETY OF COMMON DEVICES IN REAL TIME.
utt_0002 utt 11.60 15.73 -X IN OUR APPROACH, WE USE TRIANGLE MESHES AS THE NERF REPRESENTATION,
utt_0003 utt 15.73 19.47 -X AS THEY NOT ONLY PROVIDE SUPERB COMPATIBILITY ACROSS DIFFERENT DEVICES,
utt_0004 utt 19.79 22.64 -X BUT ALSO MAKE OUR METHOD VERY FAST TO RUN.
utt_0005 utt 23.69 27.99 -X WE COUPLE THE TRIANGLE MESH WITH A TEXTURE MAP TO ADD DETAILS.
utt_0006 utt 27.99 32.72 -X THE TEXTURES CONTAIN FEATURES INSTEAD OF RGB COLORS TO MODEL VIEW DEPENDENT COLORS.
utt_0008 utt 33.29 37.17 -X DURING RENDERING, WE RASTERIZE THE MESH TO OBTAIN A FEATURE IMAGE.
utt_0009 utt 37.36 44.21 -X THEN WE USE AN MLP IMPLEMENTED IN A FRAGMENT SHADER TO CONVERT THE FEATURES AND VIEWING DIRECTIONS INTO PIXEL COLORS.
utt_0011 utt 45.29 49.91 -X THE MESH-BASE REPRESENTATION OF OUR METHOD ENABLES A VARIETY OF APPLICATIONS,
utt_0012 utt 50.29 60.37 -X INCLUDING REAL-TIME RENDERING, SCENE COMPOSITION, AND INTERACTIVE MANIPULATION.
utt_0013 utt 60.37 64.24 -X OUR WORK IS MOTIVATED BY SEVERAL ISSUES OF CLASSIC NERF METHODS.
utt_0014 utt 64.75 72.24 -X FIRST, CLASSIC NERF METHODS RELY ON A VOLUMETRIC RENDERING ALGORITHM THAT SAMPLES POINTS ALONG THE RAY FOR EACH PIXEL,
utt_0016 utt 72.24 80.31 -X EVALUATES A LARGE MLP FOR EACH SAMPLED POINT, AND INTEGRATES DENSITY AND RADIANCE OF THOSE SAMPLED POINTS TO OBTAIN THE FINAL PIXEL COLOR.
utt_0018 utt 80.82 86.68 -X THIS RENDERING PROCESS IS FAR TOO SLOW FOR INTERACTIVE VISUALIZATION.
utt_0019 utt 86.68 92.21 -X RECENT METHODS SPEED UP INFERENCE BY BAKING THE MLP EVALUATION RESULTS INTO SPARSE threeD VOXEL GRIDS.
utt_0021 utt 92.37 95.03 -X FOR EXAMPLE, SNERG AND PLENOCTREES.
utt_0022 utt 95.63 100.28 -X HOWEVER, THEY REQUIRE A SIGNIFICANT AMOUNT OF GPU MEMORY TO STORE THE threeD DATA,
utt_0023 utt 100.40 105.46 -X WHICH PREVENTS THEM FROM RUNNING EFFECTIVELY ON COMMON MOBILE DEVICES.
utt_0024 utt 105.46 109.46 -X IN ADDITION, MOST METHODS NEED CUDA AND HIGH-END ACCELERATORS.
utt_0025 utt 109.46 116.56 -X NEITHER OF WHICH IS AVAILABLE ON MOBILE DEVICES, AND THEY HAVE COMPATIBILITY ISSUES WITH MANY HARDWARE PLATFORMS.
utt_0027 utt 117.65 122.77 -X IN OUR APPROACH, WE PROPOSE TO USE TEXTURED TRIANGLE MESH AS THE NERF REPRESENTATION.
utt_0029 utt 123.12 126.90 -X THIS GIVES US SEVERAL UNIQUE ADVANTAGES.
utt_0030 utt 126.90 133.24 -X FIRST, IT MAKES OUR METHOD HIGHLY COMPATIBLE, BECAUSE GPUS ARE NATIVELY DESIGNED TO RENDER TRIANGLE MESHES.
utt_0032 utt 133.71 141.24 -X SECOND, OUR METHOD CAN RUN VERY FAST, BECAUSE GPU HARDWARE IS SPECIALIZED FOR EFFICIENT MESH RENDERING.
utt_0034 utt 141.24 144.63 -X THIRD, OUR METHOD STORES twoD TEXTURES FOR THE TRIANGLE MESHES,
utt_0035 utt 144.75 151.09 -X THEREFORE IT CONSUMES MUCH LESS MEMORY THAN PREVIOUS METHODS THAT STORE threeD DATA.
utt_0037 utt 151.09 154.07 -X HERE IS AN EXAMPLE TRIANGLE MESH EXTRACTED BY OUR METHOD.
utt_0038 utt 154.45 159.22 -X NOTICE THAT THE TRIANGLE FACES DO NOT NEED TO ALIGN PERFECTLY WITH THE OBJECT'S SURFACE.
utt_0040 utt 160.11 167.99 -X NONETHELESS, AFTER WE PUT THE TEXTURES WITH TRANSPARENCY ONTO THE TRIANGLES AND RENDER THEM, WE OBTAIN HIGH QUALITY RENDERING RESULTS.
utt_0042 utt 168.63 173.43 -X WE COUPLE THE TRIANGLE MESH WITH A TEXTURE MAP TO ADD FINE-GRAINED DETAILS AND COLORS.
utt_0044 utt 174.00 180.25 -X TO MODEL VIEW DEPENDENT COLORS, WE STORE eight-D FEATURES INSTEAD OF three-D RGB COLORS IN THE TEXTURES.
utt_0046 utt 181.14 184.25 -X AFTER MESH RASTERIZATION, WE OBTAIN A FEATURE IMAGE,
utt_0047 utt 184.25 188.60 -X WHERE EACH PIXEL STORES A FEATURE VECTOR RETRIEVED FROM THE TEXTURE IMAGE,
utt_0048 utt 188.60 190.68 -X AND A VIEWING DIRECTION.
utt_0049 utt 190.68 197.98 -X NEXT, WE USE A TINY MLP IMPLEMENTED IN A FRAGMENT SHADER TO CONVERT THE PER-PIXEL FEATURES AND VIEWING DIRECTIONS INTO PIXEL COLORS.
utt_0051 utt 199.32 207.51 -X TO TRAIN OUR METHOD, WE USE A MESH LIKE THE ONE SHOWN HERE WITH OPTIMIZABLE VERTEX POSITIONS TO REPRESENT THE GEOMETRY OF THE SCENE.
utt_0053 utt 207.51 211.13 -X THE MESH IS CONSTRUCTED BY STORING A GRID OF OPTIMIZABLE VERTICES,
utt_0054 utt 211.13 214.20 -X AND THEN CONNECTING ADJACENT VERTICES TO FORM FACES.
utt_0055 utt 215.48 219.06 -X WE USE MLPS TO STORE THE PROPERTIES OF POINTS ON THE MESH SURFACE.
utt_0056 utt 219.22 224.03 -X THE FEATURE FIELD MLP PREDICTS THE FEATURE VECTOR OF ANY threeD POINT ON THE MESH.
utt_0057 utt 224.03 227.29 -X THE OPACITY FIELD MLP PREDICTS THE POINTâ€™S OPACITY.
utt_0058 utt 227.61 231.67 -X THE SMALL MLP, ACTING AS A NEURAL DEFERRED RENDERER,
utt_0059 utt 231.67 236.03 -X CONVERTS THE PER-POINT FEATURE VECTOR AND THE VIEWING DIRECTION INTO A VIEW DEPENDENT COLOR.
utt_0061 utt 236.57 241.72 -X DURING TRAINING, FOR EACH PIXEL, WE SHOOT A RAY AND COMPUTE INTERSECTIONS WITH THE MESH.
utt_0063 utt 242.04 247.23 -X THEN WE USE THE MLPS TO COMPUTE THE OPACITY, FEATURE, AND COLOR OF EACH INTERSECTION.
utt_0065 utt 247.67 258.07 -X FINALLY, WE ALPHA-COMPOSITE THE COLORS OF THE INTERSECTION POINTS TO OBTAIN THE PIXEL COLOR, WE SUPERVISE THE ENTIRE MODEL WITH AN Ltwo LOSS BETWEEN THE OUTPUT PIXEL COLOR AND THE GROUND TRUTH.
utt_0068 utt 259.16 263.26 -X THE PREVIOUS TRAINING STAGE USES CONTINUOUS OPACITY, SIMILAR TO NERF.
utt_0069 utt 263.96 270.52 -X HOWEVER, RENDERING PIPELINES IMPLEMENTED IN TYPICAL HARDWARE DO NOT NATIVELY SUPPORT RENDERING OF SEMI-TRANSPARENT MESHES.
utt_0071 utt 270.71 275.20 -X THEREFORE, WE BINARIZE THE PER-POINT OPACITY VIA SPECIALIZED LOSS FUNCTIONS.
utt_0072 utt 275.64 279.52 -X DISCRETIZING OPACITY CAN INTRODUCE ALIASING, AS SHOWN ON THE RIGHT.
utt_0073 utt 279.90 283.74 -X WE OVERCOME THIS ISSUE BY EMPLOYING ANTI-ALIASING BY SUPER-SAMPLING.
utt_0074 utt 283.90 289.88 -X THAT IS, WE DIVIDE EACH PIXEL INTO FOUR SUB-PIXELS, PERFORM THE COMPUTATION FOR EACH SUB-PIXEL,
utt_0075 utt 290.04 294.62 -X AND AVERAGE THE COLORS OF THE SUB-PIXELS TO OBTAIN THE COLOR OF THE ORIGINAL PIXEL.
utt_0076 utt 294.90 298.94 -X THIS SUPER-SAMPLING STEP CAN GIVE US SMOOTH BOUNDARIES SIMILAR TO THE GROUND TRUTH.
utt_0078 utt 300.12 303.96 -X AFTER THE SECOND TRAINING STAGE, WE CAN NOW EXPORT THE NERF SCENE.
utt_0079 utt 304.09 307.74 -X WE EXTRACT THE MESH BY STORING VISIBLE TRIANGLES IN OBJ FILES,
utt_0080 utt 307.99 312.38 -X BAKE TEXTURES BY STORING THE FEATURES AND ALPHA INTO PNG TEXTURE IMAGES,
utt_0081 utt 312.73 317.85 -X AND CACHE THE NEURAL RENDERER BY STORING THE WEIGHTS OF THE VIEW-DEPENDENT MLP IN A JSON FILE.
utt_0083 utt 318.26 322.78 -X THESE ARE ALL WE NEED TO VIEW OUR NERF SCENES IN REAL TIME.
utt_0084 utt 322.78 325.63 -X OUR NERF VIEWER IS A SIMPLE WEBGL APPLICATION.
utt_0085 utt 325.63 328.51 -X AND A LIVE DEMO IS AVAILABLE ON OUR PROJECT PAGE.
utt_0086 utt 330.94 333.79 -X WE HAVE TESTED OUR METHOD ON A VARIETY OF DEVICES,
utt_0087 utt 333.88 338.97 -X INCLUDING TWO PHONES, A TABLET, A CHROMEBOOK, A GAMING LAPTOP, AND A DESKTOP.
utt_0088 utt 339.51 343.39 -X THE MOBILE DEVICES ONLY HAVE LOW-POWER INTEGRATED GPUS.
utt_0089 utt 344.25 348.96 -X WE COMPARE WITH SNERG, AS IT IS THE ONLY NERF MODEL THAT CAN RUN ON COMMON DEVICES.
utt_0091 utt 349.34 355.07 -X FROM THIS TABLE, WE CAN FIND THAT OUR METHOD CONSUMES MUCH LESS GPU MEMORY COMPARED TO SNERG.
utt_0093 utt 355.77 361.41 -X THIS TABLE SHOWS THE RENDERING SPEED OF OUR METHOD AND SNERG IN FRAMES PER SECOND.
utt_0095 utt 361.41 365.57 -X NOTE THAT SNERG IS UNABLE TO RUN ON PHONES DUE TO MEMORY ISSUES,
utt_0096 utt 365.57 368.99 -X AND IT CANNOT RUN ON THE TABLET DEVICE DUE TO COMPATIBILITY ISSUES.
utt_0097 utt 369.34 374.11 -X IN CONTRAST, OUR METHOD CAN SUCCESSFULLY RUN ON THOSE DEVICES AT HIGH FRAME RATES.
utt_0099 utt 374.81 380.29 -X ON THE OTHER SETTINGS WHERE SNERG IS RUNNABLE, OUR METHOD IS ON AVERAGE ten TIMES FASTER.
utt_0100 utt 380.83 384.74 -X THIS TABLE SHOWS THE RENDERING QUALITY IN COMMON EVALUATION METRICS.
utt_0101 utt 385.05 389.44 -X OUR METHOD HAS ROUGHLY THE SAME IMAGE QUALITY AS SNERG.
utt_0102 utt 389.44 391.90 -X HERE WE SHOW SOME VISUAL COMPARISONS.
utt_0103 utt 391.90 397.09 -X WHEN THE CAMERA IS AT AN APPROPRIATE DISTANCE, OUR METHOD AND SNERG ARE BOTH CLOSE TO THE
utt_0105 utt 398.30 402.62 -X HOWEVER, WHEN WE MOVE THE CAMERA TO ZOOM-IN ONTO THE OBJECTS,
utt_0106 utt 402.62 404.77 -X SNERG TENDS TO RENDER OVER-SMOOTHED IMAGES.
utt_0107 utt 405.69 409.38 -X MORE RECENTLY, WITH THE OPTIMIZATIONS SUGGESTED BY NOERI,
utt_0108 utt 409.40 413.79 -X WE HAVE GREATLY IMPROVED THE RENDERING SPEED OF THE MLP FRAGMENT SHADER.
utt_0109 utt 414.14 418.85 -X IN OUR DEFERRED RENDERING SETTING, THE SPEED HAS BEEN IMPROVED BY MORE THAN thirty PERCENT.
utt_0111 utt 419.39 429.83 -X MORE IMPRESSIVELY, AT THIS POINT, THE MLP SHADER IS SO FAST THAT FORWARD RENDERING WITHOUT THE INTERMEDIATE FEATURE IMAGE IS EVEN FASTER THAN DEFERRED RENDERING ON SOME MOBILE DEVICES.
utt_0114 utt 430.33 434.63 -X IT TRIPLES THE RENDERING SPEED COMPARED TO OUR ORIGINAL IMPLEMENTATION.
utt_0115 utt 434.88 441.03 -X THE EXPLICIT MESH REPRESENTATION PROVIDED BY OUR METHOD GIVES US DIRECT EDITING CONTROL OVER THE NERF OBJECTS.
utt_0117 utt 441.21 447.07 -X FOR EXAMPLE, WE CAN BEND THE BRANCH OF THIS FICUS SCENE BY EDITING ITS TRIANGLE MESH.
utt_0119 utt 447.07 450.59 -X SIMILARLY, WE CAN REMOVE THE HORNS, THE LEAVES, AND THE T-REX.
utt_0120 utt 452.03 455.01 -X WE CAN ALSO PUT DIFFERENT NERF OBJECTS TOGETHER.
utt_0123 utt 462.88 465.73 -X WE CAN MANIPULATE THE NERF OBJECTS IN REAL TIME.
utt_0126 utt 482.82 485.28 -X THERE ARE SOME LIMITATIONS OF OUR METHOD.
utt_0127 utt 485.82 491.62 -X FIRST, SIMILAR TO OTHER NERF, OUR METHOD MAY NOT PRODUCE THE CORRECT GEOMETRY.
utt_0128 utt 491.62 496.93 -X SECOND, IT USES BINARY OPACITIES, SO IT CANNOT HANDLE SCENES WITH SEMI-TRANSPARENT PARTS.
utt_0130 utt 497.70 503.52 -X THIRD, IT USES FIXED MESH AND TEXTURE RESOLUTIONS, WHICH MAY BE TOO COARSE FOR CLOSE-UP VIEWS.
utt_0131 utt 504.26 505.41 -X THAT IS ALL.
utt_0132 utt 505.41 506.47 -3.6675 THANK YOU FOR WATCHING!
