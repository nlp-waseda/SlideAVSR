utt_0000 utt 0.13 2.93 -X HELLO EVERYONE, I’M SRIRAM FROM NEC LABS AMERICA.
utt_0001 utt 2.93 7.44 -X THIS VIDEO IS ABOUT OUR PAPER DIVIDE-AND-CONQUER FOR LANE AWARE DIVERSE TRAJECTORY PREDICTION.
utt_0002 utt 7.72 12.02 -X CONSIDER THIS SCENARIO WHERE CARS ARE MOVING IN THE OPPOSITE DIRECTIONS.
utt_0003 utt 12.02 17.81 -X IMPLAUSIBLE PREDICTIONS OF AGENTS THAT INTERACT WITH SDV’S ROUTE CAN MAKE SDV TAKE CONSERVATIVE ACTIONS.
utt_0005 utt 17.81 23.83 -X MOREOVER, PREDICTIONS THAT DO NOT FOLLOW ROAD SEMANTICS CAN ALSO LEAD TO SDVS EXECUTING DANGEROUS MANEUVERS.
utt_0008 utt 29.72 31.09 -X YIELD OR ACCELERATE.
utt_0009 utt 31.09 36.44 -X THIS REQUIRES SDV TO HAVE MULTIMODAL PREDICTIONS THAT COVER DIFFERENT MODES OF DISTRIBUTION.
utt_0010 utt 36.91 43.16 -X OVERALL, HAVING MULTIMODAL PREDICTIONS WITH STRONG SEMANTIC COUPLING IS CRUCIAL TO HAVE A RELIABLE PREDICTION SYSTEM.
utt_0012 utt 44.11 46.45 -X OUR WORK IS DIVIDED IN TWO PARTS.
utt_0013 utt 46.45 56.44 -X FIRST WE FOCUS ON PREDICTING MULTIMODAL OUTPUTS USING MULTI-CHOICE OBJECTIVES AND THEN PROPOSE ALAN TO OBTAIN STRONG SEMANTIC COUPLING BY IMPOSING CONSTRAINTS ON THE DRIVING BEHAVIOR.
utt_0015 utt 56.98 63.13 -X SPECIFICALLY, WE FOCUS ON USING MULTI-CHOICE LEARNING AND PROPOSE A BETTER INITIALIZATION TO WINNER TAKES ALL LOSS.
utt_0017 utt 64.34 65.45 -X PREVIOUS WORKS
utt_0018 utt 65.68 72.44 -X VANILLA OBJECTIVE DEPENDS ON INITIALIZATION FOR OPTIMAL CONVERGENCE AND HENCE, THIS MAKES THE TRAINING PROCESS VERY BRITTLE.
utt_0020 utt 72.76 79.26 -X FOLLOW UP WORKS SUCH AS RWTA SOLVE THE CONVERGENCE PROBLEM BY ADDING RESIDUAL WEIGHT TO THE NON-WINNERS.
utt_0021 utt 80.44 88.86 -X AND FINALLY, EWTA WAS PROPOSED TO MAKE THE TRAINING MORE STABLE BUT ALL THESE WORKS STILL SUFFER FROM THE PROBLEM OF OUTPUTS OCCUPYING SPURIOUS MODES.
utt_0023 utt 90.68 95.87 -X WE PROPOSE DIVIDE AND CONQUER OBJECTIVE THAT ACTS A BETTER INITIALIZATION TO WINNER TAKES ALL LOSS.
utt_0025 utt 95.99 105.87 -X CONSIDER THIS TOY EXAMPLE SHOWN ABOVE, IN THE FIRST STAGE, DAC PENALIZES ALL HYPOTHESES WITH THE ENTIRE DATA CAUSING ALL PREDICTIONS TO GO TOWARDS THE CENTERS OF THE DISTRIBUTION
utt_0027 utt 106.42 114.91 -X THEN WE DIVIDE THESE HYPOTHESES INTO TWO SETS WHERE FIRST SET CONSISTS OF INITIAL HALF OF THE OUTPUTS AND REST OF THE HYPOTHESES GO IN THE SECOND SET.
utt_0029 utt 114.91 120.86 -X WE THEN BACKPROPAGATE THROUGH THE SET OF OUTPUTS WHICH CONTAIN THE PREDICTION CLOSEST WITH THAT OF THE GROUND TRUTH.
utt_0031 utt 121.56 126.01 -X THIS LEADS TO SETS FORMING CENTROIDAL VORONOI TESSELLATION AT EVERY STAGE.
utt_0032 utt 126.43 134.50 -X WE PERFORM THIS ITERATIVELY, WHERE EVERY SET AFTER FEW THOUSAND ITERATIONS IS BROKEN DOWN INTO HALVES LEADING TO WINNER TAKES OBJECTIVE IN THE END.
utt_0034 utt 135.84 141.79 -X WE CONDUCT EXPERIMENTS OF DAC ON A CAR PEDESTRIAN DATASET THAT CONTAINS MULTIMODAL GROUND TRUTHS.
utt_0035 utt 141.98 147.68 -X WE EVALUATE HOW WELL THE MULTI-HYPOTHESIS PREDICTIONS CAPTURE THE TRUE DISTRIBUTION OF SAMPLES IN THE TEST SET.
utt_0037 utt 148.25 155.94 -X WE USE EVALUATION METRICS SUCH AS ORACLE ERROR AND EMD DISTANCE AND SHOW THAT PROPOSED DAC OUTPERFORMS OTHER VARIANTS OF WTA.
utt_0039 utt 157.89 162.85 -X WE NOW DESCRIBE ALAN NETWORK THAT USES EXISTING LANE CENTERLINES AS ANCHORS.
utt_0040 utt 163.68 168.84 -X WE UTILIZE BOTH RASTERIZED TOP VIEW INFORMATION AND LANE CENTERLINES AS INPUT TO OUR NETWORK.
utt_0041 utt 169.34 176.84 -X FIRST WE FEED THE INPUT THROUGH A SET OF PAST TRAJECTORY AND CENTERLINE ENCODER TO OBTAIN A SET OF FEATURES FOR EVERY LANE AND EVERY AGENT.
utt_0043 utt 177.79 183.49 -X WE CONCATENATE THESE FEATURE INPUTS AND PLACE THEM IN THE CHANNEL DIMENSION AT THEIR CORRESPONDING LOCATION ON THE MAP.
utt_0045 utt 184.39 188.90 -X ALONG WITH TOP VIEW RASTERIZED BEV WE PASS IT THROUGH A MULTI-AGENT CONVOLUTIONAL ENCODER.
utt_0046 utt 188.90 193.41 -X WE THEN EXTRACT HYPERCOLUMN DESCRIPTORS OF EVERY AGENT.
utt_0047 utt 193.89 205.54 -X THE INTUITION IS TO CAPTURE INTERACTIONS AT DIFFERENT SCALES WHERE HIGHER CONVOLUTIONAL LAYERS CAPTURE THE GLOBAL CONTEXT WHILE LOW-LEVEL FEATURES RETAIN NEARBY INTERACTIONS.
utt_0049 utt 205.54 209.93 -X THEN THE DESCRIPTORS ARE FED THROUGH A SET OF MLPS IN THE HYPERCOLUMN TRAJECTORY DECODER
utt_0050 utt 211.17 217.03 -X AS OUR PRIMARY OUTPUTS WE PREDICT TRAJECTORIES IN NORMAL TANGENTIAL COORDINATES WITH RESPECT TO EVERY INPUT LANE.
utt_0052 utt 217.96 230.28 -X FURTHER, WE MAKE AUXILIARY PREDICTIONS IN GLOBAL CARTESIAN COORDINATES OF THE MAP TO REGULARIZE OUR ANCHOR OUTPUTS BECAUSE TWO TRAJECTORIES HAVING SAME NT VALUES CAN HAVE COMPLETELY DIFFERENT DYNAMICS BASED ON THE INPUT ANCHOR.
utt_0055 utt 231.62 239.15 -X OUR INVERSE OPTIMAL CONTROL MODULE THEN OUTPUTS A SCORE FOR THESE PREDICTIONS BY MAXIMIZING THE REWARD BASED ON THEIR DISTANCE WITH THE GROUND TRUTH.
utt_0057 utt 240.30 242.99 -X WE SUPERVISE OUR NETWORK BASED ON three OBJECTIVES.
utt_0058 utt 243.40 249.39 -X FIRST, IS A WINNER-TAKES-ALL BASED RECONSTRUCTION LOSS WHICH IS INITIALIZED USING OUR PROPOSED DAC APPROACH.
utt_0060 utt 250.02 253.84 -X FURTHER, WE REGULARIZE OUR PRIMARY AND AUXILIARY PREDICTIONS WITH EACH OTHER.
utt_0061 utt 254.57 258.09 -X AND FINALLY, WE HAVE AN IOC BASED CROSS-ENTROPY LOSS.
utt_0062 utt 259.98 262.25 -X HERE ARE THE VIDEO RESULTS FROM OUR WORK.
utt_0063 utt 262.41 266.93 -X AS SEEN ALAN SHOWS MULTIMODAL PREDICTIONS WITH TIGHT SEMANTIC COUPLING.
utt_0064 utt 268.97 274.64 -X ALAN PROVIDES ON PAR OR BETTER PERFORMANCE WITH OTHER STATE OF THE ART METHODS FROM NUSCENES BENCHMARK.
utt_0066 utt 274.80 281.33 -X MOREOVER, IT ACHIEVES SIGNIFICANTLY LOW OFFROADRATE INDICATING STRONG ALIGNMENT WITH ROAD SEMANTICS.
utt_0067 utt 281.39 289.55 -X IN CONCLUSION, WE PROPOSED DAC TO LEARN BETTER MULTIMODAL PREDICTIONS AND ALAN TO IMPOSE CONSTRAINTS IN DRIVING BEHAVIOR FOR TRAJECTORY PREDICTIONS.
utt_0069 utt 289.90 296.82 -X WE LEARN BETTER MULTIMODALITY IN ALAN USING DAC, WHILE WE OBTAIN STRONG SEMANTIC ALIGNMENT WITH THE HELP OF ANCHOR LANES.
utt_0071 utt 296.85 299.35 -X KINDLY REFER TO OUR PAPER FOR MORE DETAILS.
utt_0072 utt 299.35 300.05 -2.4892 THANK YOU!
