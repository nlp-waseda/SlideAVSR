utt_0000 utt 0.59 6.10 -X WHAT THINGS DO YOU SEE IN THIS IMAGE? WELL, THERE'S A CIRCLE, A HEXAGON, AND A STAR.
utt_0001 utt 6.64 12.21 -X WHAT YOU'VE JUST DONE IS SOMETHING CALLED SET PREDICTION. THE ANSWER IS A COLLECTION OF OBJECTS,
utt_0002 utt 12.21 16.14 -X BUT THE ORDER THAT THESE OBJECTS ARE IN DOESN'T MATTER, THEY CAN BE IN ANY ORDER.
utt_0003 utt 16.97 22.48 -X THIS MAKES THE COLLECTION OF OBJECTS A SET. MANY THINGS CAN BE THOUGHT OF AS A SET,
utt_0004 utt 22.48 27.34 -X LIKE AGENTS IN AN ENVIRONMENT, THE INSTRUMENTS IN A PIECE OF MUSIC, OR ATOMS IN A MOLECULE.
utt_0005 utt 28.01 36.95 -X IN GENERAL, THINKING ABOUT PROBLEMS IN TERMS OF SETS IS REALLY USEFUL WHENEVER WE HAVE COLLECTIONS OF INDIVIDUAL THINGS, BUT WE DON'T CARE ABOUT THE ORDER OF THE THINGS.
utt_0007 utt 37.68 47.63 -X THE PROBLEM OF SET PREDICTION IS ABOUT THIS ABSTRACTION OF, SAY, AN IMAGE WITH LOTS AND LOTS OF PIXELS TO A SMALL SET OF DISCRETE OBJECTS, WHICH IS MUCH EASIER TO REASON ABOUT.
utt_0009 utt 48.43 59.28 -X HUMANS DO THIS FROM A YOUNG AGE WITHOUT EVEN THINKING ABOUT IT, AND WE WOULD LOVE OUR MACHINE LEARNING MODELS TO BE ABLE TO DO THE SAME. SO, HOW DO WE GET THEM TO DO SET PREDICTION?
utt_0011 utt 59.57 70.19 -X IT TURNS OUT THAT WHILE SETS ARE REALLY USEFUL, THE FACT THAT THEY ARE ORDERLESS ALSO MAKES THEM QUITE TRICKY TO WORK WITH. IN PARTICULAR, IN THIS VIDEO I WILL TELL YOU ABOUT A NEW KIND
utt_0013 utt 70.19 75.09 -X OF TRICKINESS, WHICH WILL LEAD US TO THE FRESH-NEW CONCEPT OF SOMETHING CALLED MULTISET-EQUIVARIANCE.
utt_0014 utt 75.82 80.02 -X THIS LETS US UNDERSTAND WHY SELF ATTENTION (LIKE USED IN A TRANSFORMER)
utt_0015 utt 80.05 84.37 -X IS NOT ABLE TO REPRESENT SOME FUNCTIONS THAT WOULD BE USEFUL FOR SET PREDICTION,
utt_0016 utt 84.46 87.22 -X AND WE'LL GET TO A CONCRETE EXAMPLE OF THIS LATER.
utt_0017 utt 87.92 96.47 -X AFTER THIS, I'LL SHOW YOU THE ONLY SET PREDICTOR I KNOW OF THAT HAS THIS NEW PROPERTY THAT WE WANT, AND HOW TO IMPROVE IT WITH IMPLICIT DIFFERENTIATION.
utt_0019 utt 97.14 101.27 -X BUT FIRST THINGS FIRST, LET'S START WITH WHAT EXACTLY I MEAN BY A SET.
utt_0020 utt 103.50 107.57 -X WHEN I SAY SET, WHAT I MEAN IS AN UNORDERED COLLECTION OF VECTORS.
utt_0021 utt 107.89 111.60 -X EACH VECTOR DESCRIBES SOME INFORMATION ABOUT AN ELEMENT IN THE SET.
utt_0022 utt 112.02 116.76 -X FOR EXAMPLE, IF WE HAVE A SET OF OBJECTS, THE VECTORS COULD ENCODE THE POSITION,
utt_0023 utt 116.76 119.60 -X COLOUR, AND OTHER PROPERTIES FOR EACH OBJECT.
utt_0024 utt 119.82 122.87 -X WE WILL ALWAYS HAVE A FINITE NUMBER OF VECTORS IN OUR SET,
utt_0025 utt 122.96 128.15 -X AND SET PREDICTION IS SIMPLY THE PROBLEM WHERE THE OUTPUT THAT WE WANT IS SUCH A SET OF VECTORS.
utt_0026 utt 129.62 133.40 -X SO FAR, I'VE BEEN TALKING ABOUT SETS AS YOU WOULD EXPECT THEM FROM MATHS.
utt_0027 utt 134.00 145.91 -X HOWEVER, FOR COMPUTATION WE HAVE TO STORE THESE ORDERLESS THINGS IN COMPUTER MEMORY, AND MEMORY IS VERY MUCH ORDERED. THE TYPICAL SOLUTION IS TO JUST STORE THEM AS A LIST OF VECTORS IN SOME ORDER:
utt_0029 utt 146.26 151.32 -X THE ORDER OF THE ELEMENTS IS IRRELEVANT, SO WE CAN JUST PICK ANY ARBITRARY ORDER THAT WE WANT.
utt_0030 utt 151.92 156.47 -X THIS ALLOWS US TO USE LISTS, WHICH ARE MUCH EASIER TO WORK WITH, TO REPRESENT SETS.
utt_0031 utt 158.13 163.09 -X IT'S ACTUALLY BETTER TO THINK OF THESE LISTS AS REPRESENTING MULTISETS RATHER THAN SETS,
utt_0032 utt 163.09 167.96 -X SO DUPLICATES ARE ALLOWED. WHY IS THIS THE CASE? FOR EXAMPLE,
utt_0033 utt 168.08 174.84 -X [A, A, B] AND [A, A+EPSILON, B] ARE ALMOST THE SAME IN TERMS OF THE LIST REPRESENTATION.
utt_0034 utt 174.84 184.89 -X BUT IN SETS, DUPLICATE ELEMENTS ARE MEANINGLESS, SO THE SETS THEY WOULD REPRESENT ARE WILDLY DIFFERENT, THEY'RE NOT EVEN THE SAME SIZE. THIS MEANS THAT MAKING A TINY CHANGE TO THE LIST
utt_0036 utt 184.89 196.53 -X REPRESENTATION BY EPSILON COULD LEAD TO A BIG, DISCONTINUOUS CHANGE IN THE SET, AND WE'D LIKE TO AVOID SUCH DISCONTINUITIES IF POSSIBLE. IF WE INSTEAD THINK OF THESE LISTS AS REPRESENTING
utt_0038 utt 196.53 201.17 -X MULTISETS, WE JUST KEEP ANY DUPLICATES, SO THERE IS NO PROBLEM WITH DISCONTINUITIES.
utt_0039 utt 202.32 205.65 -X THIS IS ACTUALLY QUITE IMPORTANT, SO KEEP THIS IN MIND FOR LATER:
utt_0040 utt 205.65 210.55 -X YOU SHOULD THINK OF EXISTING SET PREDICTION MODELS AS OPERATING ON MULTISETS, NOT SETS.
utt_0041 utt 211.31 216.34 -X THIS MAY SEEM A BIT PEDANTIC FOR NOW, BUT WE'LL SEE LATER WHY THIS DISTINCTION IS SO IMPORTANT
utt_0042 utt 216.53 220.06 -X EVEN IF WE NEVER ACTUALLY ENCOUNTER ANY EXACT DUPLICATES.
utt_0043 utt 222.61 225.75 -X NOW THAT WE HAVE A LIST REPRESENTATION OF OUR MULTISET,
utt_0044 utt 225.78 230.23 -X WE HAVE TO BE CAREFUL TO STILL TREAT THESE LISTS AS IF THEY WERE ORDERLESS.
utt_0045 utt 230.55 235.96 -X WE PICKED AN ORDER COMPLETELY ARBITRARILY, SO WE WOULD LIKE OUR MODEL TO NOT RELY ON THIS ORDER.
utt_0046 utt 236.76 240.02 -X THERE ARE TWO PROPERTIES IN THE LITERATURE THAT ARE HELPFUL FOR THIS.
utt_0047 utt 241.24 252.73 -X PERMUTATION-INVARIANCE MEANS THAT WHEN CHANGING THE ORDER OF THE INPUT, THE OUTPUT VECTOR MUST STAY THE SAME. FOR EXAMPLE, IF YOU SUM THESE ELEMENTS TOGETHER, YOU CAN SUM THEM IN ANY
utt_0049 utt 252.73 259.29 -X ORDER YOU WANT WITHOUT CHANGING THE RESULT. PERMUTATION-EQUIVARIANCE OR AS I PREFER,
utt_0050 utt 259.29 263.58 -X SET-EQUIVARIANCE, MEANS THAT WHEN CHANGING THE ORDER OF THE INPUT,
utt_0051 utt 263.58 273.40 -X THE OUTPUT LIST SHOULD CHANGE ORDER IN THE SAME WAY. YOU CAN THINK OF EACH INPUT ELEMENT AS BEING LINKED TO AN OUTPUT ELEMENT, AND THEY ALWAYS HAVE TO CHANGE ORDER TOGETHER.
utt_0053 utt 274.29 284.44 -X THIS PROPERTY OF SET-EQUIVARIANCE IS REALLY THE KEY BEHIND HOW MANY OF THE RECENT SET PREDICTION MODELS WORK, LIKE SLOT ATTENTION AND DETR, THE TRANSFORMERS FOR OBJECT DETECTION: YOU START WITH
utt_0055 utt 284.44 294.01 -X SOME INITIALIZATION FOR A SET ON THE LEFT, AND IT GETS UPDATED WITH A BUNCH OF SET-EQUIVARIANT STEPS -- CONDITIONED ON AN INPUT -- UNTIL YOU GET THE SET THAT YOU WANT TO PREDICT ON THE RIGHT.
utt_0057 utt 294.39 309.18 -X FOR EXAMPLE, IN DETR EACH OF THESE SET-EQUIVARIANT STEPS IS SELF-ATTENTION OR AN MLP THAT IS SHARED ACROSS ELEMENTS. AND IN THE PAST, IT WAS THOUGHT THAT SET-EQUIVARIANCE IS EXACTLY WHAT WE WANT FOR THESE SET PREDICTORS, BUT I'M HERE TODAY TO CHANGE THAT.
utt_0060 utt 310.74 322.20 -X ONE PROPERTY OF SET-EQUIVARIANCE IS THAT IF YOU HAVE AN INPUT WITH DUPLICATES IN IT, THEN THEY MUST STAY DUPLICATES IN THE OUTPUT. LET'S SAY THIS SECOND OUTPUT IS NOT C, BUT SOMETHING DIFFERENT,
utt_0062 utt 322.20 326.65 -X AN E, AND I'LL JUST KEEP A COPY OF THE OUTPUT ON THE SIDE FOR US TO KEEP IN MIND.
utt_0063 utt 328.05 332.31 -X SET-EQUIVARIANCE MEANS THAT WHEN SWAPPING THE FIRST TWO ELEMENTS IN THE INPUT,
utt_0064 utt 332.37 337.53 -X WE MUST ALSO SWAP THE FIRST TWO ELEMENTS IN THE OUTPUT, THAT'S JUST WHAT SET-EQUIVARIANCE IS.
utt_0065 utt 338.04 342.68 -X NOW, THE INPUT REPRESENTATION IS STILL EXACTLY THE SAME AS BEFORE,
utt_0066 utt 342.68 347.52 -X WE JUST SWAPPED TWO A'S, THEREFORE THE OUTPUT MUST ALSO STILL BE THE SAME.
utt_0067 utt 347.77 355.96 -X SO, WE HAVE THAT [E, C, D] IS EQUAL TO [C, E, D], SO C AND E MUST BE THE SAME.
utt_0068 utt 356.28 365.56 -X IN OTHER WORDS, A SET-EQUIVARIANT FUNCTION MUST MAP TWO EQUAL ELEMENTS IN THE INPUT TO TWO EQUAL ELEMENTS IN THE OUTPUT, THERE'S NO WAY AROUND THAT.
utt_0070 utt 366.58 376.67 -X SELF ATTENTION AND SLOT ATTENTION ARE SET-EQUIVARIANT, AND THEY HAVE THIS EXACT LIMITATION. FOR EXAMPLE IN SELF-ATTENTION, IF YOU HAVE TWO EQUAL ELEMENTS SOMEWHERE IN THE NETWORK,
utt_0072 utt 376.67 385.66 -X YOU WILL GET TWO EQUAL QUERIES, WHICH WILL GIVE YOU TWO EQUAL ELEMENTS, WHICH WILL GIVE YOU TWO EQUAL QUERIES, AND SO ON. IS THIS ACTUALLY A PROBLEM THOUGH?
utt_0074 utt 390.52 394.94 -X HERE'S A FUNCTION THAT COULD BE USEFUL WHEN SOLVING A MORE COMPLEX SET PREDICTION TASK,
utt_0075 utt 394.94 398.49 -X BUT IT'S IMPOSSIBLE TO MODEL WITH SET-EQUIVARIANT FUNCTIONS.
utt_0076 utt 399.80 405.41 -X IT TAKES A LIST OF TWO SCALARS (REPRESENTING A MULTISET) AS INPUT AND PUSHES THEM APART.
utt_0077 utt 405.69 411.36 -X WHICHEVER OF THE TWO INPUTS IS THE SMALLER VALUE, WE SUBTRACT one FROM IT, AND WHICHEVER IS BIGGER,
utt_0078 utt 411.36 416.48 -X WE ADD ONE TO IT. FOR EXAMPLE, AN INPUT [one, two] GETS PUSHED APART TO BE [zero, three].
utt_0079 utt 417.34 422.08 -X THE SMALLER one GETS TURNED INTO THE zero, THE BIGGER two GETS TURNED INTO THE three.
utt_0080 utt 422.08 427.20 -X IF YOU TRY A FEW VALUES, YOU'LL SEE THAT IT BEHAVES JUST LIKE A SET-EQUIVARIANT FUNCTION:
utt_0081 utt 427.20 429.85 -X CHANGING THE INPUT ORDER MEANS CHANGING THE OUTPUT ORDER.
utt_0082 utt 430.49 441.18 -X HOWEVER, WHAT HAPPENS WHEN WE PUT IN TWO EQUAL VALUES? WELL, WE CAN BREAK THE TIE IN THE OUTPUT ORDER ARBITRARILY, SO LET'S SAY [zero, zero] GETS PUSHED APART TO BE [minus one, one]. NOW WE'RE MAPPING
utt_0084 utt 442.68 452.86 -X TWO EQUAL ELEMENTS TO TWO DIFFERENT ELEMENTS, SO IT CAN'T BE SET-EQUIVARIANT. AND INDEED, CHANGING THE INPUT ORDER DOES NOT CHANGE THE OUTPUT ORDER, WHICH IT WOULD HAVE TO DO FOR SET-EQUIVARIANCE.
utt_0086 utt 454.23 462.46 -X THIS IS A BIT STRANGE: MOST OF THE TIME THIS FUNCTION IS SET-EQUIVARIANT, AND WHEN IT'S NOT EQUIVARIANT WHEN THE INPUTS ARE EQUAL,
utt_0088 utt 462.78 467.78 -X WE DON'T REALLY CARE ABOUT THE OUTPUT ORDER ANYWAY. THE OUTPUT REPRESENTS A MULTISET,
utt_0089 utt 467.78 472.67 -X SO THE ORDER DOESN'T REALLY MATTER, EITHER WAY TO RESOLVE THE TIE WOULD BE FINE.
utt_0090 utt 472.92 482.53 -X SO, IT SEEMS THAT SET-EQUIVARIANCE IS TOO RESTRICTIVE, THERE ARE THESE PERFECTLY REASONABLE FUNCTIONS FOR MULTISETS THAT ARE IMPOSSIBLE TO MODEL WITH SET-EQUIVARIANCE.
utt_0092 utt 487.10 490.05 -X HOW DO WE SOLVE THIS? AS A REMINDER,
utt_0093 utt 490.05 495.04 -X THE KEY DIFFERENCE BETWEEN MULTISETS AND SETS IS WHETHER DUPLICATE ELEMENTS ARE ALLOWED OR NOT.
utt_0094 utt 495.90 500.96 -X SO, WE WANT A NEW PROPERTY THAT IS STILL RELATED TO THE TRADITIONAL SET-EQUIVARIANCE,
utt_0095 utt 501.31 504.93 -X BUT IT SHOULD DO SOMETHING DIFFERENT ABOUT THESE MULTIPLE EQUAL ELEMENTS.
utt_0096 utt 505.31 516.56 -X AND THAT'S EXACTLY WHAT THIS VIDEO IS ABOUT. THE NEW PROPERTY MULTISET-EQUIVARIANCE STILL MEANS THAT CHANGING THE ORDER OF THE INPUT SHOULD CHANGE ONLY THE ORDER OF THE OUTPUT, BUT
utt_0098 utt 516.56 527.01 -X SPECIFICALLY FOR EQUAL ELEMENTS IN THE INPUT, WE NO LONGER CARE ABOUT THEIR OUTPUT ORDER AT ALL. WE CAN PUT OUTPUTS FOR EQUAL ELEMENTS IN ANY ORDER, THEY DON'T NEED TO BE LINKED TOGETHER LIKE BEFORE.
utt_0100 utt 527.68 531.87 -X THIS MAKES MULTISET-EQUIVARIANCE A RELAXATION OF SET-EQUIVARIANCE.
utt_0101 utt 532.77 536.55 -X THIS IS REALLY WHAT ALLOWS EQUAL ELEMENTS TO BE MAPPED TO DIFFERENT ELEMENTS,
utt_0102 utt 536.55 540.77 -X SO WE CAN TURN AN INPUT WITH DUPLICATES INTO AN OUTPUT WITHOUT DUPLICATES.
utt_0103 utt 541.44 546.11 -X THE PUSH APART FUNCTION IS NOT SET-EQUIVARIANT, BUT IT IS MULTISET-EQUIVARIANT.
utt_0104 utt 550.75 561.41 -X AS AN ASIDE HERE, YOU MIGHT HAVE NOTICED THAT THIS PUSH_APART FUNCTION SEEMS TO BE DISCONTINUOUS BECAUSE THERE IS THIS 'IF' HERE. USUALLY, WE DON'T LIKE DISCONTINUOUS THINGS, SINCE IT OFTEN MAKES A
utt_0106 utt 561.41 566.91 -X MODEL DIFFICULT OR IMPOSSIBLE TO TRAIN AND WE LOSE A LOT OF NICE GUARANTEES ABOUT ITS PERFORMANCE.
utt_0107 utt 567.52 577.92 -X BUT IN THIS CASE, IT'S ACTUALLY NOT A PROBLEM, BECAUSE IT'S A VERY SPECIFIC KIND OF DISCONTINUITY. THE KEY HERE IS THAT EVEN THOUGH THE LIST REPRESENTATION IS INDEED DISCONTINUOUS,
utt_0109 utt 577.98 590.31 -X THE MULTISET THAT IT REPRESENTS ISN'T. BASICALLY, IT ONLY SEEMS DISCONTINUOUS BECAUSE THE ELEMENTS ARE BEING SWAPPED HERE. IF WE CONSIDER WHAT THESE LISTS ARE REPRESENTING, MULTISETS,
utt_0111 utt 590.31 593.38 -X THE ORDER DOESN'T MATTER, SO THIS DISCONTINUITY DISAPPEARS.
utt_0112 utt 594.27 599.04 -X FUNCTIONS THAT ARE NOT SET-EQUIVARIANT BUT MULTISET-EQUIVARIANT WILL TEND TO BE LIKE THIS,
utt_0113 utt 599.04 602.60 -X WHERE EVEN THOUGH THE LIST REPRESENTATION LOOKS DISCONTINUOUS AND TROUBLESOME,
utt_0114 utt 602.72 607.75 -X THE UNDERLYING MULTISET IS STILL NICE AND CONTINUOUS, SO LEARNING ISN'T A PROBLEM.
utt_0115 utt 612.67 621.51 -X IF THERE IS ONE THING I WANT YOU TO TAKE AWAY FROM THIS VIDEO, IT'S THIS SUMMARY OF ALL THE PROPERTIES THAT I'VE BEEN TALKING ABOUT. WHEN A FUNCTION IS SET-EQUIVARIANT,
utt_0117 utt 621.51 624.96 -X IT MEANS THAT THE INPUTS ARE EXCHANGEABLE WITHOUT AFFECTING THE RESULTS,
utt_0118 utt 625.15 629.03 -X BUT WE HAVE THIS LIMITATION OF NOT BEING ABLE TO SEPARATE EQUAL ELEMENTS.
utt_0119 utt 629.57 642.66 -X WHEN A FUNCTION IS NOT EQUIVARIANT AT ALL, IT MEANS THAT WE CAN SEPARATE EQUAL ELEMENTS IN THE INPUT, BUT NOW WE DON'T HAVE THE NICE PROPERTY THAT THE ELEMENTS ARE EXCHANGEABLE WITHOUT AFFECTING THE RESULTS, SO WE LOSE A LOT OF SAMPLE EFFICIENCY.
utt_0122 utt 644.06 654.92 -X THERE ARE N! NUMBER OF LIST REPRESENTATIONS FOR THE SAME MULTISET, SO IT'S DIFFICULT FOR A MODEL TO LEARN TO NEVER RELY ON THE ORDER. LAST BUT NOT LEAST, WHEN A FUNCTION IS NOT
utt_0124 utt 654.92 660.29 -X SET-EQUIVARIANT BUT IT IS MULTISET-EQUIVARIANT, THEN WE REALLY GET THE BEST OF BOTH WORLDS:
utt_0125 utt 660.58 669.61 -X THE ELEMENTS ARE EXCHANGABLE WHICH GIVES US GOOD SAMPLE COMPLEXITY, AND WE ARE ALSO ABLE TO SEPARATE EQUAL ELEMENTS, SO WE ARE LESS RESTRICTED THAN WITH SET-EQUIVARIANCE.
utt_0127 utt 669.86 677.48 -X I WON'T GO INTO IT HERE, BUT IN THE PAPER THERE IS AN EXPERIMENT THAT CONFIRMS EXACTLY THESE PROPERTIES ON SAMPLE EFFICIENCY AND SEPARATING EQUALS.
utt_0129 utt 678.69 688.49 -X AND IT'S IMPORTANT THAT THIS INABILITY TO SEPARATE EQUAL ELEMENTS IS A PROBLEM EVEN WHEN THERE AREN'T EVER ANY EXACT DUPLICATES. BECAUSE WE'RE WORKING WITH CONTINUOUS MODELS,
utt_0131 utt 688.49 694.12 -X THE BEHAVIOUR FOR EXACTLY EQUAL ELEMENTS WILL BE SIMILAR TO THE BEHAVIOUR FOR SIMILAR ELEMENTS.
utt_0132 utt 694.50 703.11 -X IMPOSSIBILITY TO SEPARATE EQUAL ELEMENTS MEANS DIFFICULTY WHEN TRYING TO SEPARATE SIMILAR ELEMENTS, AND THERE ARE LOTS OF REASONS WHY ELEMENTS MIGHT BE SIMILAR.
utt_0134 utt 703.55 707.46 -X - WE OFTEN USE SMALL RANDOM INITIALIZATIONS FOR THE MULTISET,
utt_0135 utt 707.46 711.85 -X - OUR MODEL CAN HAVE OVERSMOOTHING ISSUES LIKE WE OFTEN SEE IN GRAPH NEURAL NETWORKS,
utt_0136 utt 711.85 716.39 -X - RELUS COULD BE ZEROING OUT ELEMENTS OR SIGMOIDS AND TANH'S COULD BE SATURATED,
utt_0137 utt 716.39 721.80 -X - OR SIMILAR ELEMENTS COULD OCCUR SIMPLY BECAUSE THEY ARE PART OF THE DATASET THAT IS BEING USED.
utt_0138 utt 722.44 727.50 -X SO, WHILE MULTISET-EQUIVARIANCE SPECIFIES WHAT HAPPENS FOR EXACTLY EQUAL ELEMENTS,
utt_0139 utt 727.68 732.42 -X IT'S REALLY MORE ABOUT HOW WELL WE CAN DO FOR SIMILAR ELEMENTS, WHICH ARE MUCH MORE COMMON.
utt_0140 utt 737.44 747.85 -X IF WE TAKE A STEP BACK AND LOOK AT THE LANDSCAPE OF SET PREDICTORS, WE HAVE QUITE A FEW THAT ARE SET-EQUIVARIANT, WE HAVE PLENTY THAT ARE NOT EQUIVARIANT, AND JUST ONE THAT HAS THIS PROPERTY
utt_0142 utt 747.85 754.15 -X THAT WE LIKE, MULTISET-EQUIVARIANCE WITHOUT SET-EQUIVARIANCE. SO WHAT IS THIS DSPN THING?
utt_0143 utt 754.47 758.28 -X IT STANDS FOR DEEP SET PREDICTION NETWORKS, WHICH IS A PAPER FROM NEURIPS two thousand and nineteen.
utt_0144 utt 759.14 762.92 -X THE DETAILS OF WHY THIS WORKS AREN'T SO IMPORTANT, BUT JUST VERY QUICKLY:
utt_0145 utt 763.04 773.26 -X THE BASIC IDEA IS THAT WE FIRST RELATE A MULTISET Y TO AN INPUT VECTOR Z THROUGH A LOSS IN LATENT SPACE, LIKE THIS ONE HERE. THEN, WE TRY TO FIND A MULTISET THAT
utt_0147 utt 773.26 778.28 -X MINIMIZES THIS LOSS BY GRADIENT DESCENT, AND THE BEST MULTISET WE FIND IS OUR PREDICTION.
utt_0148 utt 778.57 783.59 -X SO, IN THE FORWARDS PASS, WE DO THIS OPTIMIZATION OF THE MULTISET CONDITIONED ON THE INPUT.
utt_0149 utt 783.94 787.88 -X AND IN THE BACKWARDS PASS, WE DIFFERENTIATE THROUGH EACH STEP OF THE OPTIMIZATION.
utt_0150 utt 788.68 798.89 -X IT'S OKAY IF THAT DIDN'T MAKE MUCH SENSE. THE IMPORTANT PART IS THAT THERE IS SOMETHING PARTICULAR ABOUT THESE GRADIENT DESCENT STEPS. IN THIS SET ENCODER THAT GETS DIFFERENTIATED,
utt_0152 utt 798.98 810.67 -X NUMERICAL SORTING CAN BE USED. ITS GRADIENT IS USED TO UPDATE THE MULTISET, AND YOU CAN PROVE THAT THIS GRADIENT OF THE SORTING IS NOT SET-EQUIVARIANT BUT IT IS MULTISET-EQUIVARIANT,
utt_0154 utt 810.67 821.45 -X EXACTLY THE PROPERTY THAT WE WANTED. AND TO ME, THIS IS REALLY COOL, BECAUSE IN THE DSPN PAPER, THEY FOUND THAT HAVING A SORTING IN THERE WAS MUCH BETTER THAN ALTERNATIVES LIKE
utt_0156 utt 821.45 825.74 -X SUM OR MEAN POOLING, BUT THEY DIDN'T HAVE A GREAT EXPLANATION AS FOR WHY THIS IS THE CASE.
utt_0157 utt 826.15 831.02 -X NOW WE KNOW, WITH THE SUM AND MEAN POOLING, YOU GET SET-EQUIVARIANT GRADIENTS,
utt_0158 utt 831.02 836.84 -X BUT WITH SORTING, THESE GRADIENTS ARE MULTISET-EQUIVARIANT AND NOT SET-EQUIVARIANT. SO,
utt_0159 utt 836.84 841.45 -X BY USING SORTING IN THE ENCODER, DSPN IS ABLE TO SEPARATE EQUAL ELEMENTS.
utt_0160 utt 845.10 852.49 -X DESPITE THIS, IT TURNS OUT THAT ONE OF THE STATE-OF-THE-ART MODELS SLOT ATTENTION WORKS REMARKABLY WELL AND IS OFTEN ACTUALLY BETTER THAN DSPN,
utt_0162 utt 852.62 859.15 -X EVEN THOUGH SLOT ATTENTION IS SET-EQUIVARIANT. SO, LET'S SEE WHAT ELSE ABOUT DSPN WE CAN IMPROVE.
utt_0163 utt 860.01 864.49 -X ONE OF THE MAIN MECHANISMS IN DSPN IS THIS OPTIMIZATION AND NORMALLY,
utt_0164 utt 864.49 873.32 -X WE JUST USE AUTOMATIC DIFFERENTIATION TO BACKPROPAGATE THROUGH THE OPTIMIZATION STEPS. THERE IS ACTUALLY A MUCH BETTER WAY TO DO THAT WITH IMPLICIT DIFFERENTIATION.
utt_0166 utt 874.12 887.44 -X IMPLICIT DIFFERENTIATION TELLS US THAT INSTEAD OF HAVING TO DIFFERENTIATE EVERY OPTIMIZATION STEP, WE CAN GET AWAY WITH ONLY DIFFERENTIATING THE LAST STEP AND SOLVING A TINY LINEAR SYSTEM. DOING THIS HAS A LOT OF BENEFITS:
utt_0169 utt 887.59 892.36 -X DIFFERENTIATING ONE STEP OBVIOUSLY USES LESS COMPUTATION THAN DIFFERENTIATING MANY STEPS,
utt_0170 utt 892.78 898.80 -X AND WE ALSO SAVE A HUGE AMOUNT OF MEMORY. WE NOW ONLY CARE ABOUT WHAT HAPPENS IN THAT LAST STEP,
utt_0171 utt 898.80 903.95 -X SO WE ALSO ONLY NEED TO KEEP THAT ONE LAST STEP IN MEMORY RATHER THAN THE WHOLE THING.
utt_0172 utt 903.95 914.57 -X SO, IN AN IMPLICIT DSPN, WE STILL USE AUTOMATIC DIFFERENTIATION FOR MOST THINGS, BUT WE SWITCH TO IMPLICIT DIFFERENTIATION WHEN BACKPROPAGATING THROUGH THE OPTIMIZATION PROCEDURE.
utt_0174 utt 914.57 929.07 -X THIS LETS US USE MORE OPTIMIZATION STEPS AND A BETTER OPTIMIZER (BOTH OF WHICH SHOULD LEAD TO BETTER RESULTS), FOR THE SAME MODEL SIZE, SAME TOTAL COMPUTATION AND LESS MEMORY. WE CAN EVEN SKIP SOLVING THE LINEAR SYSTEM THAT IS NORMALLY REQUIRED FOR IMPLICIT DIFFERENTIATION,
utt_0177 utt 929.07 933.23 -X MAKING IT FASTER TO RUN AND EASIER TO IMPLEMENT WITHOUT ANY LOSS IN PERFORMANCE.
utt_0178 utt 937.29 941.23 -X HOW WELL DOES THIS ACTUALLY WORK IN PRACTICE? IT TURNS OUT, EXTREMELY WELL.
utt_0179 utt 941.83 946.54 -X THERE IS THIS CLEVR OBJECT DATASET WHERE WE HAVE THESE IMAGES OF threeD SCENES AS INPUT,
utt_0180 utt 946.57 957.20 -X AND WE WANT TO PREDICT THE SET OF OBJECTS AND ALL THEIR ATTRIBUTES. FOR EXAMPLE, THIS OBJECT HERE WE WOULD WANT TO BE PREDICTED AS A LARGE PURPLE METAL CYLINDER AT THESE threeD COORDINATES.
utt_0182 utt 957.93 965.71 -X FOR AN ELEMENT IN THE PREDICTION TO BE CONSIDERED CORRECT, EVERY ATTRIBUTE HAS TO BE CORRECT AND THE threeD COORDINATES HAVE TO BE WITHIN SOME DISTANCE THRESHOLD.
utt_0184 utt 966.54 971.12 -X IF WE SET THIS THRESHOLD TO BE PRETTY STRICT, THE REGULAR DSPN GETS one% AVERAGE
utt_0185 utt 971.40 975.98 -X PRECISION AND THE PREVIOUS STATE-OF-THE-ART, THE SET-EQUIVARIANT SLOT ATTENTION, GETS eight%.
utt_0186 utt 977.29 983.41 -X AFTER ADDING IMPLICIT DIFFERENTIATION TO DSPN, A SET-EQUIVARIANT VERSION WITH SUM POOLING GETS forty-nine%,
utt_0187 utt 984.56 988.82 -X AND MAKING IT NOT SET-EQUIVARIANT BUT MULTISET-EQUIVARIANT BY USING SORTING,
utt_0188 utt 989.52 993.46 -X IT GETS UP TO seventy-seven%. THAT'S MASSIVELY BETTER THAN ANY EXISTING RESULT!
utt_0189 utt 994.64 1005.07 -X AND BEST OF ALL, WE DON'T NEED TO TRAIN NEARLY AS MUCH TO GET THERE: ON SIMILAR HARDWARE, WE ONLY NEED ABOUT six PERCENT THE NUMBER OF EFFECTIVE EPOCHS AND TRAINING TIME OF SLOT ATTENTION.
utt_0191 utt 1005.07 1014.51 -X AT LEAST TO ME, IT SEEMS LIKE FOR SET PREDICTION, THE INDUCTIVE BIAS WITH MULTISET-EQUIVARIANCE IS MUCH BETTER THAN WITH SET-EQUIVARIANCE. AND THAT KIND OF MAKES SENSE,
utt_0193 utt 1014.51 1018.45 -X SINCE SET PREDICTORS OPERATE ON LISTS THAT REPRESENT MULTISETS, NOT SETS.
utt_0194 utt 1024.11 1037.68 -X SO, TO SUMMARISE. WE STARTED WITH SEEING THAT SET PREDICTION MODELS OPERATE ON MULTISETS, BUT THEY ARE USUALLY SET-EQUIVARIANT, INCLUDING THE POPULAR SLOT ATTENTION AND TRANSFORMERS. THIS IS TOO RESTRICTIVE, THEY CAN'T SEPARATE EQUAL
utt_0197 utt 1037.68 1049.52 -X ELEMENTS OR HAVE DIFFICULTY SEPARATING SIMILAR ELEMENTS. SO, WE CAME UP WITH THIS NEW NOTION OF MULTISET-EQUIVARIANCE, WHICH DOESN'T HAVE THIS LIMITATION. WE FOUND THAT DSPN WITH A SPECIFIC
utt_0199 utt 1049.52 1059.28 -X ENCODER USING SORTING IS MULTISET-EQUIVARIANT AND NOT SET-EQUIVARIANT. LASTLY, WE SAW HOW WE CAN USE IMPLICIT DIFFERENTIATION TO MAKE IT FASTER AND MORE MEMORY-EFFICIENT.
utt_0201 utt 1059.92 1063.03 -X THIS LETS US IMPROVE THE STATE-OF-THE-ART BY A HUGE AMOUNT.
utt_0202 utt 1064.37 1067.95 -X OF COURSE, THIS ISN'T THE BE-ALL END-ALL SOLUTION TO SET PREDICTION.
utt_0203 utt 1068.21 1076.85 -X ONE MAJOR PROBLEM SO FAR IS THAT, IT'S A BIT DIFFICULT TO MAKE IT WORK WELL ON A REAL, LARGE-SCALE DATASET, SO THERE IS DEFINITELY PLENTY OF ROOM FOR IMPROVEMENT.
utt_0205 utt 1077.33 1082.19 -X AND IT WOULD BE NICE TO GET A BETTER THEORETICAL UNDERSTANDING OF MULTISET-EQUIVARIANCE,
utt_0206 utt 1082.19 1085.75 -X FOR EXAMPLE WHAT INGREDIENTS ARE NEEDED FOR UNIVERSAL APPROXIMATION.
utt_0207 utt 1086.61 1089.36 -X IN GENERAL, THESE KINDS OF SYMMETRIES ARE EVERYWHERE,
utt_0208 utt 1089.36 1094.26 -X AND IDENTIFYING THE RIGHT STRUCTURES (LIKE WE DID HERE WITH MULTISET-EQUIVARIANCE FOR MULTISETS)
utt_0209 utt 1094.42 1097.81 -X CAN REALLY HELP A MODEL BE MUCH MORE PERFORMANT AND DATA-EFFICIENT.
utt_0210 utt 1098.13 1102.26 -X IF YOU'RE CURIOUS ABOUT THE DETAILS OF THIS WORK, HAVE A LOOK AT THE DESCRIPTION,
utt_0211 utt 1102.26 1105.20 -X WITH THE PAPER, OPEN-SOURCE CODE, AND PRE-TRAINED MODELS AVAILABLE.
utt_0212 utt 1105.78 1109.72 -X THIS HAS BEEN WORK BY ME AND DAVID ZHANG AS EQUAL CONTRIBUTION FIRST AUTHORS,
utt_0213 utt 1109.81 1113.59 -X A COLLABORATION BETWEEN PEOPLE OF SAMSUNG SAIT AI LAB MONTREAL,
utt_0214 utt 1113.59 1117.52 -3.6828 UNIVERSITY OF AMSTERDAM, MILA, UNIVERSITY OF MONTREAL, AND TNO.
