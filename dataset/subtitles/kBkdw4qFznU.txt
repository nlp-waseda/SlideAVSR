utt_0000 utt 0.02 3.38 -X HELLO EVERYONE, WELCOME TO OUR TALK AT CVPR two thousand and twenty-one.
utt_0001 utt 4.52 11.92 -X WEâ€™LL TALK ABOUT OUR STUDY ON RADAR CAMERA FUSION VIA REPRESENTATION LEARNING IN AUTONOMOUS DRIVING.
utt_0003 utt 12.11 24.87 -X AS A FORMULATION OF THE RADAR CAMERA FUSION PROBLEM, WE ARE GIVEN A CAMERA IMAGE AND A BUNCH OF DETECTED VEHICLE twoD BOUNDING BOXES, AND A BUNCH OF RADAR PINS, OUR GOAL IS TO
utt_0005 utt 24.87 32.98 -X MAKE THE ASSOCIATIONS BETWEEN THE twoD BOUNDING BOXES AND THE RADAR PINS IF THEY CORRESPOND TO THE SAME VEHICLE IN THE SCENE.
utt_0007 utt 33.74 39.64 -X THE DESIRED ASSOCIATION RELATIONSHIPS ARE HIGHLIGHTED AS RED LINES IN THE IMAGE.
utt_0008 utt 39.73 42.77 -X THERE ARE THREE FEATURES TO NOTE ABOUT OUR METHOD.
utt_0009 utt 43.02 51.51 -X FIRST, OUR METHOD IS AN OBJECT-LEVEL FUSION METHOD, AS OPPOSED TO OTHER DATA-LEVEL FUSION OR FEATURE-LEVEL FUSION METHODS.
utt_0011 utt 52.05 56.72 -X HENCE OUR METHOD IS MORE AMENABLE TO REAL AUTONOMOUS DRIVING SYSTEMS.
utt_0012 utt 56.72 60.08 -X SECOND, OUR METHOD IS AN LEARNING-BASED METHOD.
utt_0013 utt 60.08 71.70 -X COMPARED TO TRADITIONAL RULE-BASED RADAR-CAMERA FUSION METHODS, OUR METHOD IS LESS SUSCEPTIBLE TO PERFORMANCE DEGRADATION IN CHALLENGING SCENARIOS OR FAILURE IN CORNER CASES.
utt_0015 utt 71.95 80.57 -X THIRD, MOST EXISTING LEARNING-BASED RADAR-CAMERA FUSION METHODS RELY ON LIDAR TO PROVIDE GROUND-TRUTH INFORMATION.
utt_0017 utt 80.57 90.90 -X YET, OUR METHOD ADOPTS A TRADITIONAL RADAR-CAMERA FUSION METHOD TO AUTOMATICALLY GENERATE ASSOCIATION GROUND-TRUTH, ENTIRELY ELIMINATING THE NEED FOR LIDAR.
utt_0019 utt 92.53 95.45 -X OUR METHOD IS BASED ON REPRESENTATION LEARNING.
utt_0020 utt 95.45 103.06 -X THE INPUT FEATURES INCLUDE THE RAW CAMERA RGB IMAGE, THE BOUNDING BOX FEATURES AND RADAR PIN FEATURES.
utt_0022 utt 103.06 109.08 -X WE AIM AT LEARNING REPRESENTATIONS OF EACH twoD BOUNDING BOX AND EACH RADAR PIN.
utt_0023 utt 110.20 112.47 -X HERE IS AN OVERVIEW OF OUR METHOD.
utt_0024 utt 112.47 121.66 -X WE FIRST USE THE BOUNDING BOXES AND THE RADAR PINS TO CREATE A PSEUDO-IMAGE, WHERE EACH FEATURE OCCUPIES ONE IMAGE CHANNEL.
utt_0026 utt 121.85 126.20 -X EACH BOUNDING BOX AND RADAR PIN HAS AN UNIQUE PIXEL LOCATION.
utt_0027 utt 126.23 138.39 -X THEN WE CONCATENATE THE PSEUDO-IMAGE WITH THE RAW CAMERA RGB IMAGE TO FORM A FINAL PSEUDO-IMAGE TO BE FED INTO A NEURAL NETWORK TO PERFORM REPRESENTATION LEARNING.
utt_0029 utt 138.39 147.48 -X AT THE LAST LAYER OF THE NETWORK FEATURE-MAP, WE EXTRACT THE LEARNED REPRESENTATIONS FOR EACH BOUNDING BOX AND EACH RADAR PIN AT ITS PIXEL POSITION.
utt_0031 utt 148.02 155.58 -X DURING THE TRAINING, IF ONE BOUNDING BOX AND ONE RADAR PIN ARE ASSOCIATED, WE PULL TOGETHER THEIR REPRESENTATIONS.
utt_0034 utt 155.58 158.84 -X OTHERWISE, WE PUSH APART THEIR REPRESENTATIONS.
utt_0035 utt 158.97 169.02 -X IN THIS WAY, THE ASSOCIATION RELATIONSHIPS BETWEEN BOUNDING BOXES AND RADAR PINS ARE ENCODED AS THE EUCLIDEAN DISTANCE BETWEEN THEIR REPRESENTATION VECTORS.
utt_0037 utt 169.05 178.53 -X DURING INFERENCE, WE FIRST CALCULATE THE AFFINITY MATRIX BETWEEN REPRESENTATIONS OF BOUNDING BOXES AND REPRESENTATIONS OF RADAR PINS.
utt_0039 utt 178.59 185.92 -X THEN WE SELECT THE BEST ASSOCIATED BOUNDING BOX FOR EACH RADAR PIN BY LOOKING FOR THE MINIMUM DISTANCE.
utt_0041 utt 187.26 191.20 -X WE ALSO DESIGNED TWO MECHANISMS TO ENHANCE PERFORMANCE.
utt_0042 utt 191.32 193.60 -X FIRST IS THE LOSS SAMPLING.
utt_0043 utt 193.60 200.70 -X TO MITIGATE THE LABELING NOISE, WE FIRST FILTER LOW-CONFIDENCE POSITIVE PAIRS IN GROUND-TRUTH
utt_0044 utt 200.73 205.15 -X AND SAMPLE THE NEGATIVE PAIRS FOR LOSS CALCULATION.
utt_0045 utt 205.15 207.07 -X SECOND IS THE ORDINAL LOSS.
utt_0046 utt 207.07 213.94 -X WE OBSERVE THAT THE RELATIVE ORDERING OF THE Y MAX OF BOUND BOXES REFLECTS THE ORDERING
utt_0047 utt 214.17 216.35 -X OF THE DEPTHS OF OBJECTS IN threeD WORLD.
utt_0048 utt 216.83 225.38 -X HENCE, WE CAN ENFORCE THE SELF-CONSISTENCY AMONG THE ORDERING OF BOUNDING BOXES AND THE ORDERING OF ASSOCIATED RADAR PINS.
utt_0050 utt 225.41 233.19 -X WE ENCODE THE SELF-CONSISTENCY AS THE ORDINAL LOSS AND ADD THE LOSS INTO THE FINAL LOSS FUNCTION DURING TRAINING.
utt_0052 utt 233.19 235.46 -X HERE ARE SOME QUANTITATIVE RESULTS.
utt_0053 utt 235.46 241.35 -X FOR LOSS SAMPLING, WE STUDIED DIFFERENT SAMPLE RATIO AND THE PERFORMANCE GAIN IS one point one PERCENT
utt_0054 utt 241.82 243.94 -X IN TERMS OF THE Fone SCORE.
utt_0056 utt 243.94 250.18 -X FOR THE ORDINAL LOSS, THE PERFORMANCE GAIN IS one point eight PERCENT IN TERM OF THE Fone SCORE.
utt_0057 utt 250.43 263.44 -X LASTLY, EVEN THOUGH WE USED A TRADITIONAL RULE-BASED ALGORITHM TO GENERATE GROUND-TRUTH LABEL DURING THE TRAINING, OUR PROPOSED LEARNING-BASED ALGORITHM SIGNIFICANTLY OUTPERFORMS THE RULE-BASED
utt_0059 utt 265.22 268.23 -X TEACHER BY eleven point six PERCENT IN TERMS OF THE Fone SCORE.
utt_0060 utt 269.60 273.19 -X HERE ARE TWO EXAMPLES OF THE PREDICTED ASSOCIATIONS.
utt_0061 utt 273.19 281.73 -X DESPITE MULTIPLE BIG TRUCKS PRESENT IN BOTH EXAMPLES, OUR PROPOSED ALGORITHM CORRECTLY PREDICTED THEIR ASSOCIATIONS.
utt_0063 utt 281.73 288.07 -X ON THE OTHER HAND, IN THE SECOND EXAMPLE, THERE ARE TWO BOUNDING BOXES INCORRECTLY ASSOCIATED.
utt_0064 utt 288.07 294.85 -X THE MISTAKES ARE LARGELY DUE TO THE SMALL SIZES OF THE OBJECTS IN THE CAMERA IMAGE AND ALSO THE HEAVY OCCLUSIONS.
utt_0066 utt 294.85 296.26 -X THANK YOU FOR YOUR ATTENTION.
utt_0067 utt 296.26 299.18 -2.0114 PLEASE REFER TO OUR PAPER FOR FURTHER DETAILS.
