utt_0000 utt 0.72 3.47 -X HELLO EVERYONE. I'M YIKAI WANG,
utt_0001 utt 4.20 8.02 -X A threeRD YEAR PHD STUDENT WITH SUPERVISOR, DR. YANWEI FU,
utt_0002 utt 8.02 11.31 -X FROM SCHOOL OF DATA SCIENCE, FUDAN UNIVERSITY.
utt_0003 utt 11.47 14.58 -X FOLLOWING YANWEI'S TALK, I WILL SPEND twenty MINUTES
utt_0004 utt 14.67 17.31 -X TO GIVE THE MORE DETAILED EXPLANATION
utt_0005 utt 17.42 20.75 -X ON SPARSE LEARNING FOR NOISY DATA/LABELS PART,
utt_0006 utt 21.29 24.88 -X MOSTLY FROM THE STATISTICAL PERSPECTIVE.
utt_0007 utt 24.88 34.16 -X I WILL INTRODUCE A SIMPLE YET EFFECTIVE FRAMEWORK FOR NOISE DATA DETECTION WITH APPLICATIONS IN DIFFERENT VISION TASKS.
utt_0009 utt 35.85 42.16 -X FIRST, I WILL INTRODUCE WHAT IS SPARSE LEARNING FOR NOISE DATA DETECTION.
utt_0010 utt 42.32 43.95 -X WHAT IS NOISE DATA?
utt_0011 utt 44.49 48.18 -X IT MAY BE THE RED APPLE SURROUNDED BY GREEN APPLES.
utt_0012 utt 49.04 53.30 -X IT MIGHT BE THE DATA FALL MORE THAN three STDS FROM THE MEAN VALUE.
utt_0013 utt 55.21 58.96 -X OR IT MIGHT BE THE POINT THAT FAR AWAY FROM THE CURVE.
utt_0014 utt 60.37 68.21 -X GENERALLY NOISE DATA OR OUTLIERS ARE THE IRREGULAR DATA COMPARED WITH THE MAJORITY OF THE DATASET.
utt_0016 utt 70.03 76.66 -X IN PARTICULAR, IN LABEL SPACE, NOISE DATA MAINLY REFERS TO THE MIS-LABELED DATA.
utt_0017 utt 77.30 81.59 -X THESE MISLABELED DATA MAY COME FROM MANY ASPECTS.
utt_0018 utt 82.19 85.84 -X FOR EXAMPLE, IF THE LABELS ARE RANDOMLY CORRUPTED,
utt_0019 utt 85.90 89.78 -X THEN SOME IRREGULAR LABELS WILL LIE IN THE LABEL SPACE.
utt_0020 utt 90.99 94.74 -X OUR HUMANS ARE INEVITABLE TO MAKE MISTAKES,
utt_0021 utt 94.74 97.56 -X SO OUR ANNOTATIONS ARE NOT SO CREDIBLE.
utt_0022 utt 99.02 101.08 -X IN SOME RESEARCH AREAS,
utt_0023 utt 101.14 104.82 -X PEOPLE WILL USE THE SEARCH ENGINE TO GET TRAINING DATA.
utt_0024 utt 104.98 109.75 -X HOWEVER, AS AN EXAMPLE, IF YOU SEARCH 'SHOGUN' IN GOOGLE,
utt_0025 utt 109.75 113.59 -X IT MAY RETURN A HISTORICAL FIGURE OR A GAME CHARACTER,
utt_0026 utt 113.59 117.33 -X BUT USUALLY NOT THE BOTH ARE EXPECTED.
utt_0027 utt 118.42 123.03 -X ALSO, WE MAY COLLECT SOME COMPLEX OR CONFUSING IMAGES.
utt_0028 utt 123.03 128.98 -X IT'S HARD TO DISTINGUISH, E.G., THIS IS A DOG BUT LOOKS LIKE A CAT.
utt_0029 utt 129.33 135.77 -X WHAT CAN WE DO TO IDENTIFY THESE NOISE DATA IN LABEL SPACE?
utt_0030 utt 135.77 139.54 -X HERE, TO IDENTIFY THESE NOISY LABELS FROM THE TRAINING SET,
utt_0031 utt 139.73 144.07 -X WE PROPOSE TO USE A SIMPLE LINEAR SYSTEM TO MODEL THE RELATION
utt_0032 utt 144.08 148.34 -X BETWEEN NETWORK FEATURES X AND ONE HOT LABELS Y.
utt_0033 utt 149.49 153.72 -X THIS LINEAR SYSTEM IS KNOWN TO BE SENSITIVE TO NOISY DATA.
utt_0034 utt 154.10 158.26 -X THAT IS, THE SOLUTION OF BETA IS NOT STABLE,
utt_0035 utt 158.26 161.72 -X AND IT MAY BE INFLUENCED BY EVEN A SINGLE OUTLIER.
utt_0036 utt 161.84 165.78 -X BUT ACTUALLY THIS IS EXACTLY WHAT WE WANT.
utt_0037 utt 167.54 170.17 -X WHY DO WE USE THIS LINEAR SYSTEM?
utt_0038 utt 170.45 172.79 -X FOR AN IDEALLY TRAINED NETWORK,
utt_0039 utt 172.88 179.03 -X WE HAVE A SOFT-MAX LINEAR RELATION BETWEEN FEATURE VECTOR AND ONE HOT LABELS.
utt_0041 utt 179.86 184.41 -X IF WE APPROXIMATE THIS BY DROPPING THE SOFT-MAX OPERATOR,
utt_0042 utt 184.41 186.87 -X WE CAN GET THE LINEAR SYSTEM.
utt_0043 utt 187.38 193.27 -X THIS IS A WEAK APPROXIMATION, BUT THIS IS HELPFUL FOR OUR TARGET.
utt_0044 utt 195.38 203.06 -X SPECIFICALLY, WE INTRODUCE A NOISE DATA INDICATOR GAMMA TO REPRESENT THE NOISY DATA EXPLICITLY.
utt_0046 utt 204.21 207.64 -X FOR CLEAN DATA, THE LINEAR RELATION IS STRONG.
utt_0047 utt 207.64 211.13 -X THUS WE SHOULD HAVE A SMALL OR EVEN ZERO GAMMA.
utt_0048 utt 211.32 222.26 -X IF THE DATA IS NOISY, A LARGE GAMMA WILL RETURNED BY THE LINEAR MODEL TO BETTER FIT THE NOISY FEATURE LABEL PAIRS.
utt_0050 utt 222.32 227.06 -X BASICALLY, GAMMA CAN BE REGARDED AS A RESIDUAL PREDICT ERROR.
utt_0051 utt 227.89 232.15 -X THIS IS A BASELINE METHOD TO IDENTIFY NOISY DATA IN THE TRAINING SET.
utt_0052 utt 233.33 241.43 -X HOWEVER, THESE ROW RESIDUALS FAIL TO DETECT OUTLIERS AT LEVERAGE POINTS.
utt_0053 utt 241.46 246.91 -X A BETTER WAY IS TO USE THE LEAVE-ONE-OUT EXTERNALLY STUDENTIZED RESIDUAL
utt_0055 utt 247.13 255.96 -X TO TEST WHETHER THE PREDICT RESIDUAL IS LARGE BASED ON THE OTHER OBSERVATIONS.
utt_0056 utt 255.96 261.82 -X THIS IS EQUIVALENT TO TEST WHETHER GAMMA EQUALS TO ZERO IN THIS EQUATION.
utt_0057 utt 262.97 267.71 -X HOWEVER, THIS CAN ONLY BE SOLVED WHEN THERE IS A SINGLE OUTLIER.
utt_0058 utt 268.76 274.36 -X WHEN THERE ARE MULTIPLE OUTLIERS, WE HAVE TWO ADDITIONAL PROBLEMS TO SOLVE.
utt_0059 utt 274.93 276.73 -X FIRST IS MASKING.
utt_0060 utt 276.85 282.27 -X MULTIPLE OUTLIERS MAY GROUPED TOGETHER TO MASK EACH OTHER TO BE UNDETECTED
utt_0061 utt 282.68 287.10 -X IF WE ONLY LEAVE ONE OF THEM IN THE STATISTICAL TESTING.
utt_0062 utt 288.44 291.07 -X THE OTHER IS SWAMPING.
utt_0063 utt 291.07 294.78 -X MULTIPLE OUTLIERS MAY RESULT IN A LARGE T FOR CLEAN DATA
utt_0064 utt 295.06 298.49 -X LEADING TO A FALSE DISCOVERY OF OUTLIERS.
utt_0065 utt 299.54 303.68 -X THUS, WE CONSIDER SOLVE THE WHOLE INDICATOR MATRIX
utt_0066 utt 303.80 309.15 -X FOR ALL TRAINING DATA AT THE SAME TIME, LEADING TO THIS EQUATION.
utt_0067 utt 311.06 314.04 -X OUR TARGET IS TO SOLVE GAMMA
utt_0068 utt 314.20 316.16 -X USING ALL THE TRAINING SET,
utt_0069 utt 316.44 320.41 -X AND IDENTIFY THE NOISY DATA BASED ON THE GAMMA.
utt_0070 utt 321.46 325.56 -X AS WE SAID ABOVE, THOSE NON-ZERO GAMMA
utt_0071 utt 325.98 329.92 -X IS REGARDED AS A NOISY DATA IN THE TRAINING SET.
utt_0072 utt 330.49 334.17 -X TO ENSURE THIS, WE ADD THE SPARSE PENALTY TO GAMMA,
utt_0073 utt 334.17 336.22 -X LEADING TO THIS OBJECTIVE.
utt_0074 utt 337.27 343.07 -X NOW, THE OBJECTIVE HAVE TWO UNKNOWN PARAMETERS, BETA AND GAMMA.
utt_0075 utt 344.22 347.90 -X BUT WE ONLY CARE ABOUT THE SOLUTION OF GAMMA,
utt_0076 utt 348.03 351.45 -X AND WE DON'T CARE ABOUT WHAT BETA IS.
utt_0077 utt 351.45 354.78 -X FURTHER, BETA IS KNOWN TO BE SENSITIVE TO OUTLIERS.
utt_0078 utt 355.29 360.54 -X SO WE FIRST SOLVE BETA WITH GAMMA FIXED,
utt_0079 utt 360.67 366.78 -X THEN WE CAN USE THE ESTIMATE OF BETA TO THE OBJECTIVE,
utt_0080 utt 366.78 370.72 -X LEADING TO THIS SIMPLIFIED OBJECTIVE.
utt_0081 utt 371.42 376.22 -X WITH FURTHER SIMPLIFICATION, WE GIVE SOME NOTATIONS,
utt_0082 utt 376.54 380.41 -X AND WE CAN TRANSLATE THE OBJECTIVE INTO THIS ONE,
utt_0083 utt 381.12 386.05 -X A STANDARD LINEAR REGRESSION.
utt_0084 utt 386.05 391.17 -X NOW, THE ONLY PROBLEM IS HOW TO CHOOSE THE SPARSE COEFFICIENT LAMBDA.
utt_0085 utt 391.17 397.09 -X A NUMBER OF METHODS FOR DETERMINING LAMBDA EXIST,
utt_0086 utt 397.21 400.26 -X BUT NONE IS SUITABLE FOR OUR FORMULATION.
utt_0087 utt 400.89 408.77 -X FOR EXAMPLE, SOME HEURISTICS RULES ON SETTING THE LAMBDA ARE POPULAR IN EXISTING MODELS,
utt_0089 utt 408.77 417.18 -X SUCH AS THE M-ESTIMATOR, WHERE SIGMA HAT IS A GAUSSIAN VARIANCE SET MANUALLY BASED ON HUMAN PRIOR KNOWLEDGE.
utt_0091 utt 417.63 424.77 -X HOWEVER, SETTING A CONSTANT LAMBDA VALUE INDEPENDENT OF DATASET IS FAR FROM OPTIMAL,
utt_0093 utt 424.86 430.78 -X BECAUSE THE RATIO OF OUTLIERS MAY VARY FOR DIFFERENT CROWDSOURCED DATASETS.
utt_0094 utt 433.18 437.31 -X CROSS VALIDATION IS ALSO NOT APPLICABLE HERE,
utt_0095 utt 437.31 442.15 -X BECAUSE IT CAN ONLY OPTIMIZE PART OF THE SPARSE VARIABLES,
utt_0096 utt 442.30 446.56 -X WHILE LEAVING THOSE HELD-OUT VALIDATION SET UNDETERMINED.
utt_0097 utt 446.88 450.75 -X BUT THIS VALIDATION SET IS ALSO NOISY.
utt_0098 utt 453.47 456.35 -X SOME DATA-ADAPTIVE TECHNIQUES,
utt_0099 utt 456.35 460.45 -X SUCH AS SCALED LASSO AND SQUARE-ROOT LASSO ,
utt_0100 utt 460.45 464.96 -X TYPICALLY GENERATE OVERESTIMATES ON THE SUPPORT SET OF OUTLIERS.
utt_0101 utt 465.66 470.75 -X MOREOVER, THEY RELY ON THE HOMOGENEOUS GAUSSIAN NOISE ASSUMPTION,
utt_0102 utt 470.75 475.20 -X WHICH IS OFTEN NOT VALID IN PRACTICE.
utt_0103 utt 475.29 483.65 -X THE OTHER ALTERNATIVES LIKE AIC AND BIC ARE OFTEN UNSTABLE IN OUTLIER DETECTION LASSO PROBLEMS.
utt_0105 utt 483.97 488.93 -X THUS IT IS VERY HARD TO CHOOSE A PROPER LAMBDA IN ADVANCE.
utt_0106 utt 489.66 499.75 -X THIS INSPIRES US TO SEQUENTIALLY CONSIDER ALL AVAILABLE SOLUTIONS FOR ALL SPARSE VARIABLES ALONG THE REGULARIZATION PATH,
utt_0108 utt 500.16 504.61 -X BY REGARDING THE SOLUTION OF GAMMA AS A FUNCTION OF LAMBDA.
utt_0109 utt 505.73 510.21 -X WHEN LAMBDA TURNS TO INFINITY, GAMMA WILL TURN TO ZERO.
utt_0110 utt 510.53 514.28 -X IN THIS PATH, WITH PROPER PENALTY, E.G.
utt_0111 utt 515.52 518.05 -X THE ROW-WISE Ltwo VECTOR NORM,
utt_0112 utt 518.05 523.52 -X GAMMA WILL VANISH ROW BY ROW, THAT IS, INSTANCE BY INSTANCE.
utt_0113 utt 524.29 529.09 -X THEN WE CAN USE LAMBDA, WHERE GAMMA TURNS INTO ZERO,
utt_0114 utt 529.09 535.11 -X TO RANK THE DATA AND DENOTE THE LAST VANISHED INSTANCES AS NOISY DATA.
utt_0115 utt 536.61 542.92 -X FORTUNATELY, THIS SOLUTION PATH CAN BE GENERATED BY THE GLM-NET ALGORITHM.
utt_0116 utt 544.99 547.72 -X THIS IS THE VISUALIZATION OF THE SOLUTION PATH.
utt_0117 utt 548.13 553.77 -X THE RED LINE IS THE MOST CREDIBLE DATA SUGGESTED BY OUR ALGORITHM.
utt_0118 utt 555.43 564.13 -X INTUITIVELY, WE ARE FINDING THE X-AXIS (WHERE) THE NORM OF GAMMA FINISHES.
utt_0119 utt 567.84 573.80 -X IN SUMMARY, OUR PURPOSE FRAMEWORK, INSTANCE CREDIBILITY INFERENCE,
utt_0120 utt 575.43 580.71 -X ASSUME THE LINEAR RELATION BETWEEN NETWORK FEATURES AND ONE-HOT LABELS,
utt_0121 utt 581.92 585.96 -X AND WE USE AN EXPLICIT NOISE DATA INDICATOR GAMMA,
utt_0122 utt 585.96 588.30 -X TO REPRESENT THE NOISY DATA.
utt_0123 utt 589.86 593.45 -X WE SIMPLIFY THIS LINEAR REGRESSION MODEL WITH GAMMA
utt_0124 utt 594.05 596.68 -X TO LINEAR REGRESSION MODEL OF GAMMA,
utt_0125 utt 597.96 604.68 -X AND SOLVE THE SOLUTION PATH OF GAMMA TO RANK THE DATA AND SELECT THE CLEAN DATA FOR TRAINING.
utt_0127 utt 604.68 609.13 -X OR, IN OTHER WORDS, IDENTIFY AND REMOVE THE NOISY DATA.
utt_0128 utt 611.59 613.67 -X AS WE ARE A STATISTICAL METHOD.
utt_0129 utt 613.83 618.60 -X WE HAVE VERY NICE THEORETICAL PROPERTIES TO IDENTIFY THE NOISE SUBSET.
utt_0130 utt 618.82 622.22 -X THAT IS THE NOISE SET RECOVERY.
utt_0131 utt 623.43 628.71 -X OUR PROBLEM IS, WHEN WILL THE MODEL IDENTIFY ALL THE OUTLIERS?
utt_0132 utt 629.32 634.09 -X OUR RESULTS BASED ON THE MODEL SELECTION CONSISTENCY OF LASSO MODEL.
utt_0133 utt 635.43 637.71 -X WE GIVE THREE CONDITIONS HERE,
utt_0134 utt 638.41 642.44 -X AND WE JUST TRANSLATE THESE CONDITIONS INTO ENGLISH.
utt_0135 utt 643.94 647.11 -X IF WE HAVE Cone, WE CAN GET A UNIQUE SOLUTION.
utt_0136 utt 648.74 656.11 -X IF Ctwo HOLDS, CLEAN DATA AND NOISY DATA ARE DIVERGED TO SOME EXTENT.
utt_0137 utt 656.11 662.51 -X IF WE HAVE Cthree, THE ERROR IS LARGE ENOUGH TO BE IDENTIFIED FROM RANDOM NOISE.
utt_0138 utt 662.79 664.49 -X WITH THESE CONDITIONS,
utt_0139 utt 664.87 668.14 -X WE CAN GET THE FOLLOWING THEOREM.
utt_0140 utt 669.48 671.85 -X IF WE HAVE A LARGE LAMBDA
utt_0141 utt 672.10 674.47 -X AND WE HAVE Cone AND Ctwo,
utt_0142 utt 675.01 682.76 -X THEN THE NOISE SET IDENTIFIED BY ICI IS A SUBSET OF GROUND-TRUTH NOISE SET.
utt_0143 utt 683.72 689.29 -X IF FURTHER WE HAVE Cthree, ICI WILL IDENTIFY ALL THE NOISY DATA.
utt_0144 utt 691.27 696.36 -X AS OUR THEORY MAINLY DESIGNED ON THE ASSUMPTION OF SUB-GAUSSIAN NOISE.
utt_0145 utt 698.12 701.23 -X WE TEST BY RUNNING two thousand RANDOM TASKS TO CHECK
utt_0146 utt 701.29 703.53 -X WHETHER THIS HOLD IN PRACTICE.
utt_0147 utt 704.94 711.08 -X THE SIMULATED DISTRIBUTION OF NOISE CAN BE ROUGHLY COMBINED WITH THREE GAUSSIAN DISTRIBUTIONS,
utt_0149 utt 711.30 716.11 -X WHICH VERIFY THAT OUR ASSUMPTION IS REASONABLE IN REALITY.
utt_0150 utt 719.34 724.14 -X NOW WE INTRODUCE SOME APPLICATIONS OF SPARSE LEARNING IN NOISY DATA.
utt_0151 utt 725.54 729.45 -X WE FIRST INTRODUCE SPARSE LEARNING IN FEW-SHOT LEARNING PROBLEM.
utt_0152 utt 731.56 739.05 -X FEW-SHOT LEARNING AIMS TO LEARN A ROBUST MODEL WITH ONLY A FEW TRAINING DATA AVAILABLE.
utt_0154 utt 739.05 742.06 -X IF WE CONSIDER THE ANNOTATION COST,
utt_0155 utt 742.25 747.53 -X FEW-SHOT LEARNING LIES BETWEEN UNSUPERVISED LEARNING AND LEARNING FROM NOISY LABELS,
utt_0157 utt 747.63 751.37 -X SINCE WE ONLY NEED TO ANNOTATE A FEW TRAINING EXAMPLES.
utt_0158 utt 751.95 756.27 -X BUT IT IS NOT JUST TRAINING WITH A FEW DATA.
utt_0159 utt 756.39 763.79 -X FOR EXAMPLE THE STANDARD BINARY CLASSIFICATION NEEDS A LOT OF LABELED DATA TO MODEL THE DECISION BOUNDARY.
utt_0161 utt 764.20 766.64 -X WHEN WE HAVE ONLY A FEW TRAINING DATA,
utt_0162 utt 766.64 769.36 -X THE DECISION BOUNDARY IS VERY DIFFERENT,
utt_0163 utt 769.36 773.97 -X AS WE ONLY HAVE ACCESS TO LITTLE INFORMATION ABOUT THE TWO CLASSES.
utt_0164 utt 775.27 783.76 -X BUT FEW-SHOT LEARNING AIMS TO GET DECISION BOUNDARY TRAINED WITH MANY LABELED DATA USING ONLY A FEW LABELED DATA.
utt_0166 utt 784.11 787.60 -X THIS REQUIRES THE ADDITIONAL PRIOR KNOWLEDGE,
utt_0167 utt 787.60 793.07 -X LIKE PRE-TRAIN WITH RELATED DATASETS, OR WITH NOISY UNLABELED DATA
utt_0168 utt 793.64 800.75 -X IN INDUSTRY, IT IS REASONABLE AND CHEAP TO COLLECT NOISY UNLABELED DATA TO SUPPORT THE LEARNING MODEL.
utt_0170 utt 802.22 806.99 -X IN THIS SCENARIO, WE CAN USE SELF-TAUGHT LEARNING PIPELINE.
utt_0171 utt 806.99 811.79 -X FIRST WE TRAIN THE FEW-SHOT MODEL WITH THE LABELED IMAGE-LABEL PAIRS.
utt_0172 utt 812.68 816.50 -X THEN WE INFERENCE THE PSEUDO-LABELS FOR UNLABELED IMAGES.
utt_0173 utt 817.20 821.42 -X WE CAN FURTHER TRAIN THE FEW-SHOT MODELS WITH THESE PSEUDO-LABELED IMAGES.
utt_0174 utt 822.16 825.30 -X AND THIS IS THE SELF-TAUGHT LEARNING PIPELINE.
utt_0175 utt 825.42 836.24 -X THE CORE PROBLEM IS HOW TO ENSURE THE TRAINING WITH PSEUDO-LABELED IMAGES IS TRUSTWORTHY THAT THE RE-TRAINING WILL NOT DAMAGE THE CAPACITY.
utt_0177 utt 836.81 842.48 -X HOWEVER, IT IS HARD TO DO, AS THE PSEUDO-LABELS ARE VERY NOISY.
utt_0178 utt 842.61 849.94 -X WE WILL USE THE PROPOSED ICI TO ENSURE THAT THIS SELECTION IS CREDIBLE.
utt_0179 utt 850.06 853.39 -X WE FIRST ENCODE THE IMAGES INTO FEATURES.
utt_0180 utt 853.84 859.19 -X THEN WE TRAIN THE LINEAR CLASSIFIER WITH THE LABELED IMAGE-LABEL PAIRS.
utt_0181 utt 860.72 864.66 -X THEN UNLABELED IMAGES ARE INFERRED BY THE TRAINED LINEAR MODEL
utt_0182 utt 865.36 869.46 -X TO SELECT THE CREDIBLE SUBSET TO EXPAND THE TRAINING SET.
utt_0183 utt 869.68 873.38 -X WE USE THE PURPOSED ICI TO IDENTIFY NOISY DATA
utt_0184 utt 873.45 877.01 -X AND REMOVE THEM FROM THE SELECTED SUBSET.
utt_0185 utt 877.10 881.46 -X THEN WE CAN RETRAIN THE CLASSIFIER WITH THESE PSEUDO-LABELED IMAGES,
utt_0186 utt 882.25 886.83 -X AND REPEAT THIS PROCESS UNTIL ALL THE DATA ARE SELECTED.
utt_0187 utt 888.97 890.93 -X AS WE INTRODUCED EARLIER,
utt_0188 utt 891.60 898.84 -X WE MODEL THE FEATURES AND ONE-HOT PSEUDO-LABELS WITH THE AFOREMENTIONED LINEAR SYSTEM WITH NOISY INDICATORS.
utt_0190 utt 900.50 902.29 -X THEN WE SOLVE THIS SYSTEM
utt_0191 utt 902.74 909.52 -X AND SIMPLIFY THIS TO A SINGLE STANDARD LINEAR REGRESSION OF NOISY INDICATOR.
utt_0192 utt 909.78 914.64 -X HOWEVER, POPULAR APPROACHES TO CHOOSE LAMBDA ARE NOT SUITABLE IN OUR CASE.
utt_0193 utt 914.99 919.67 -X WE THUS GENERATE A SOLUTION PATH TO BETTER IDENTIFY THE NOISY DATA,
utt_0194 utt 919.95 921.84 -X AS VISUALIZED IN THIS FIGURE.
utt_0195 utt 924.14 930.58 -X WE CAN FURTHER EXTEND THE SPARSE LEARNING FRAMEWORK TO LOGISTIC REGRESSION CASE.
utt_0196 utt 930.58 932.69 -X SIMILAR TO THE LINEAR REGRESSION CASE,
utt_0197 utt 932.69 936.79 -X WE INTRODUCE THE NOISE INDICATOR TO THE LINEAR PREDICTOR.
utt_0198 utt 937.36 945.30 -X THEN WITH SIMPLE TRANSFORM WE CAN GET A STANDARD LOGISTIC REGRESSION MODEL WITH EXTENDED COEFFICIENTS.
utt_0200 utt 946.42 951.67 -X THUS, WE CAN USE POPULAR OPTIMIZATION ALGORITHM TO SOLVE THIS MODEL.
utt_0201 utt 953.55 958.68 -X HERE, WE CHECK WHETHER OUR THEORY AND EMPIRICAL RESULTS ARE MATCH.
utt_0202 utt 959.38 961.85 -X IN MORE THAN HALF OF THE EXPERIMENTS,
utt_0203 utt 961.91 966.61 -X ASSUMPTIONS Cone AND Ctwo ARE SATISFIED.
utt_0204 utt 966.61 969.88 -X THIS IS THE KEY TO THE SUCCESS OF ICI,
utt_0205 utt 970.32 975.03 -X MOST OF THEM WILL ACHIEVE BETTER PERFORMANCE AFTER SELF-TAUGHT WITH ICI.
utt_0206 utt 977.33 979.70 -X WHEN ALL THE ASSUMPTIONS ARE SATISFIED,
utt_0207 utt 979.70 982.65 -X WE WILL GET BETTER PERFORMANCE IN A HIGH RATIO.
utt_0208 utt 984.88 987.80 -X EVEN IF Ctwo AND Cthree ARE NOT SATISFIED,
utt_0209 utt 987.83 994.07 -X WE STILL HAVE THE CHANCE OF IMPROVING THE PERFORMANCE.
utt_0210 utt 995.09 1001.82 -X WE CAN FURTHER USE THIS LINEAR SYSTEM IN THE PROBLEM OF LEARNING WITH NOISY LABELS.
utt_0212 utt 1002.48 1007.64 -X LEARNING WITH NOISE LABELS AIM TO LEARN A ROBUST MODEL FROM NOISY TRAINING SET.
utt_0214 utt 1007.64 1016.57 -X WE COMBINE THE AFOREMENTIONED LINEAR SYSTEM INTO LEARNING THE RECOGNITION MODEL WITH NOISY TRAINING SET
utt_0216 utt 1016.88 1022.42 -X BY ITERATIVELY DO THE FEATURE LEARNING AND SAMPLE SELECTION STAGES IN THE TRAINING TIME.
utt_0218 utt 1023.28 1030.42 -X IN THE FIRST STAGE, THE NETWORK IS TRAINED WITH CROSS-ENTROPY TO EXTRACT FEATURES AND CLASSIFY LABELS.
utt_0220 utt 1030.96 1032.89 -X AS THERE ARE NOISY LABELS,
utt_0221 utt 1032.95 1035.22 -X THE NETWORK IS NOT WELL TRAINED.
utt_0222 utt 1035.83 1037.56 -X TO SOLVE THIS PROBLEM,
utt_0223 utt 1037.59 1042.27 -X WE NEED TO IDENTIFY NOISY DATA AND REMOVE THEM FROM THE TRAINING SET.
utt_0224 utt 1043.48 1047.55 -X WE ASSUME A LINEAR RELATION BETWEEN THE FEATURES AND ONE-HOT LABELS,
utt_0225 utt 1047.55 1051.80 -X AND THOSE BREAK THIS ASSUMPTION WILL BE IDENTIFIED AS NOISY DATA.
utt_0226 utt 1052.89 1057.11 -X WE SOLVE THIS LINEAR MODEL WITH SPARSE PENALTY ON THE NOISY DATA INDICATORS
utt_0228 utt 1057.65 1059.35 -X WITH A SOLUTION PATH.
utt_0229 utt 1059.35 1062.01 -X WE RANK THE DATA BY THEIR SELECTING TIME
utt_0230 utt 1062.07 1064.57 -X AND REMOVE THE IDENTIFIED NOISY DATA.
utt_0231 utt 1065.27 1068.54 -X THIS ENDS THE SECOND STAGE, SAMPLE SELECTION STAGE.
utt_0232 utt 1069.11 1075.80 -X THEN WE RETURN TO THE FEATURE LEARNING STAGE AND REPEAT THE TWO STAGES ITERATIVELY UNTIL CONVERGE.
utt_0234 utt 1077.75 1082.23 -X TO MAKE THE LINEAR SYSTEM SCALABLE TO LARGE DATASETS,
utt_0235 utt 1082.23 1085.98 -X WE FIRST COMPUTE THE CLASS SIMILARITY MATRIX,
utt_0236 utt 1086.36 1090.33 -X AND DIVIDE THE MOST DIS-SIMILAR CLASSES INTO GROUPS.
utt_0237 utt 1090.33 1096.48 -X FOR EACH GROUP, WE SELECT A SMALL NUMBER OF TRAINING DATA TO CONSTRUCT SMALL PIECES OF DATA.
utt_0239 utt 1096.92 1100.54 -X THEN OUR MODEL IS RUNNING ON EACH PIECE IN PARALLEL.
utt_0240 utt 1102.97 1105.21 -X TO ENCOURAGE THE LINEAR RELATION,
utt_0241 utt 1105.21 1111.32 -X WE CAN APPEND A SPARSE PENALTY ON THE OUTPUT OF THE FC LAYER TO GET A SPARSE OUTPUT.
utt_0243 utt 1112.60 1116.22 -X TO FURTHER EXPLOIT THE SUPPORT OF NOISY DATA,
utt_0244 utt 1116.22 1124.28 -X WE COMBINE OUR ALGORITHM WITH CUTMIX AND REGARD THOSE DATA AS UNLABELED DATA TO GENERATE NEW TRAINING DATA.
utt_0246 utt 1127.06 1135.07 -X WE TEST LABEL PRECISION PERFORMANCE OF OUR ALGORITHM TO CHECK WHETHER WE CAN DISTINGUISH CLEAN DATA FROM NOISY DATA.
utt_0248 utt 1136.86 1140.25 -X IT CAN BE FOUND THAT IN DIFFERENT SETTINGS,
utt_0249 utt 1140.44 1143.61 -X WE CAN CONSISTENTLY ACHIEVE HIGH PRECISION,
utt_0250 utt 1143.61 1148.41 -X SHOWING THE CAPACITY OF NOISE DATA DETECTION OF OUR ALGORITHM.
utt_0251 utt 1148.41 1153.66 -2.9273 THAT'S ALL. THANKS FOR LISTENING TO THE TALK.
