utt_0001 utt 5.63 16.90 -X I COAUTHORED WITH MY PHD CO-ADVISOR ZACH PARDOS AT UC BERKELEY. THE TITLE OF OUR PAPER IS EVALUATING SOURCES OF COURSE INFORMATION AND MODELS OF REPRESENTATION
utt_0003 utt 16.91 28.82 -X ON A VARIETY OF INSTITUTIONAL PREDICTION TASKS. AS THE UTILITY OF INSTITUTIONAL DATA STARTS BECOME CLEAR, DATA FROM INSTITUTIONS OF HIGHER EDUCATION ARE
utt_0005 utt 28.82 43.57 -X QUICKLY COMING INTO FOCUS FOR EDUCATIONAL DATA MINING AND LEARNING ANALYTICS COMMUNITIES. RECENTLY, EDM HAS SEEN A MORE THAN DOUBLING YEAR-TO-YEAR IN PAPERS FOCUSED ON PREDICTION WITH LARGE INSTITUTIONAL ENROLLMENT SETS.
utt_0008 utt 44.46 54.23 -X ACTUALLY, THE USE OF INSTITUTIONAL DATA SETS DATES BACK TO THE FIRST EDM CONFERENCE WITH THE THEMES OF CLASSIFYING STUDENTS AND USING STUDENTS
utt_0010 utt 54.23 63.63 -X ENROLLMENT DATA TO ACQUIRE BACKGROUND KNOWLEDGE FOR ITS. HERE WE SUMMARIZED THIS BODY OF WORK IN TERMS OF THE MOST COMMON DATA SOURCES USED AND THE
utt_0012 utt 63.63 74.00 -X MOST COMMON PREDICTION TASKS. IT'S WORTH MENTIONING THAT THAT THE EMPTY CELLS HERE SHOW THAT THERE'S NO WORK USING THE COURSE CONTENT INFORMATION LIKE THE CATALOG
utt_0014 utt 74.00 85.11 -X DESCRIPTIONS FOR THE GREAT PREDICTION AND THE PREREQUISITE PREDICTION TASKS. SO THIS BECAME A GAP FOR US TO FILL IN THIS WORK. CONTEMPORARY APPROACHES TO DATA
utt_0016 utt 85.11 95.89 -X MINING INSTITUTIONAL DATA SETS IN HIGHER EDUCATION HAVE BEEN EXPLORING EFFECTIVE REPRESENTATIONAL METHODS THAT VECTORIZE, EMBED OR VECTORIZE COURSES INTO A
utt_0018 utt 95.89 109.83 -X SPACE, SUCH AS MATRIX FACTORIZATION, NEURAL EMBEDDING MODELS, GRAPH BASED MODELS, AND CONTENT BASED METHODS. SO IN OUR WORK WE SUMMARIZED AND EVALUATED INSTITUTIONAL
utt_0020 utt 109.83 116.76 -X PREDICTION TASKS IN EDM ACROSS THREE DIMENSIONS. THE FIRST DIMENSION IS DATA,
utt_0021 utt 116.76 122.84 -X BY DATA WE MEAN SOURCES OF INSTITUTIONAL DATA. THE SECOND DIMENSION IS METHODS,
utt_0022 utt 122.90 133.72 -X BASICALLY MODELS FOR PRESENTING STUDENTS AND COURSES. THE THIRD DIMENSION IS TASKS AND HERE WE MEAN INSTITUTIONAL RELEVANT PREDICTION TASKS.
utt_0024 utt 134.48 146.29 -X FIRST, LET ME INTRODUCE THE INSTITUTIONAL DATA SOURCES WE USE IN THIS WORK. THE FIRST DATA SOURCE WE USED IS ENROLLMENT HISTORIES AND GRADES, WHICH CONSIST OF
utt_0026 utt 146.29 157.40 -X ANONYMIZED STUDENTS ENROLLMENT HISTORIES AND GRADE HISTORIES AT UC BERKELEY FOR NINE YEARS. BESIDES THE STUDENT DATA, THERE ARE ALSO COURSES DATA WITH META-
utt_0028 utt 157.40 163.57 -X INFORMATION. THE OTHER TWO SOURCES OF DATA ARE BOTH COURSE CONTENT INFORMATION,
utt_0029 utt 163.57 173.91 -X WITH COURSE CATALOG A BRIEF DESCRIPTION OF THE COURSE AND COURSE SYLLABUS A MORE DETAILED CHRONOLOGICAL LIST OF SUBJECTS AND ASSIGNMENTS THAT A COURSE WILL COVER.
utt_0031 utt 175.12 186.45 -X NEXT, LET ME INTRODUCE THE SECOND DIMENSION - MODELS FOR REPRESENTING STUDENTS AND COURSES. IN MACHINE LEARNING, REPRESENTATION LEARNING IS A SET OF
utt_0033 utt 186.45 197.13 -X TECHNIQUES TO AUTOMATICALLY DISCOVER THE REPRESENTATIONS NEEDED FOR FEATURE DETECTION OR CLASSIFICATION FROM RAW DATA. HERE ARE THE FOUR MODELS WE EITHER
utt_0035 utt 197.13 207.05 -X PROPOSED OR USED IN OUR PREVIOUS WORK OR DESIGNED THIS WORK, AND THE DATA SOURCES EACH MODEL USED TO LEARN REPRESENTATIONS FOR COURSES, STUDENTS, AND
utt_0037 utt 207.05 218.49 -X EVEN COURSE GRADES. THIS WHOLE FIGURE ILLUSTRATED HOW EACH MODEL TAKES THEIR CORRESPONDING DATA AS INPUT AND GENERATE COURSE REPRESENTATIONS AS MODEL OUTPUT.
utt_0039 utt 220.37 232.09 -X LET'S START WITH BAG-OF-WORDS. IN NATURAL LANGUAGE PROCESSING, EACH DOCUMENT CAN BE REPRESENTED BY A VECTOR OF REAL NUMBERS, AND EACH NUMBER REPRESENTS A TERM WEIGHT,
utt_0041 utt 232.09 241.75 -X WHICH CAN BE TERM FREQUENCY, A BINARY VALUE OR TF-IDF WEIGHT. SO, WE CAN APPLY THESE BAG-OF-WORDS TECHNIQUES TO BOTH OUR COURSE CATALOG
utt_0043 utt 241.75 253.78 -X DESCRIPTIONS AND SYLLABUS TO GENERATE COURSE REPRESENTATIONS. THE SECOND REPRESENTATION MODEL IS MULTI-FACTOR COURSEtwoVEC. IN OUR PREVIOUS WORK, INSPIRED
utt_0045 utt 253.78 264.15 -X BY THE WORDtwoVEC MODEL IN NLP, WE PROPOSED A COURSEtwoVEC MODEL TO LEARN DISTRIBUTED REPRESENTATIONS OF COURSES FROM STUDENTS ENROLLMENT RECORDS THROUGHOUT
utt_0047 utt 264.15 275.10 -X SEMESTERS. THEN, WE INCORPORATED MORE FEATURES OF COURSES TO THE INPUT OF THE COURSEtwoVEC MODEL TO ENHANCE THE LEARNED COURSE REPRESENTATIONS. THE THIRD
utt_0049 utt 275.10 286.84 -X REPRESENTATION MODEL WE USED IS LSTM, A VARIANT OF RECURRENT NEURAL NETWORK. IN OUR PREVIOUS WORK, WE USED LSTM TO GENERATE COURSE REPRESENTATIONS TO
utt_0051 utt 286.84 297.50 -X STUDENTS FOR THE NEXT SEMESTER. THE INPUT IN EACH TIME SLICE IS A MULTI-HOT VECTOR REPRESENTING THE COURSES TAKEN IN THE CORRESPONDING SEMESTER AND THE OUTPUT
utt_0053 utt 297.50 307.94 -X PREDICTS COURSES A STUDENT WILL TAKE IN THE NEXT SEMESTER. THERE ARE FOUR WEIGHTS OF THE INPUT LEARNED BY THE LSTM AND WE COMBINE THEM TOGETHER TO FORM
utt_0055 utt 307.94 319.45 -X REPRESENTATIONS OF COURSES. THE LAST REPRESENTATION MODEL IS A NEWLY PROPOSED GRAPH BASED MODEL TO CONSTRUCT A STUDENT- COURSE GRAPH BASED ON BOTH ENROLLMENTS
utt_0057 utt 319.45 325.08 -X OF STUDENTS AND THEIR GRADES FOR EACH ENROLLED COURSE. FOR EXAMPLE, IN THIS FIGURE,
utt_0058 utt 325.08 336.02 -X WE HAVE FOUR STUDENTS AND FOUR COURSES. THERE'S AN EDGE BETWEEN A STUDENT AND A COURSE IF THE STUDENT HAS ENROLLED IN THE COURSE. IN ADDITION, EACH EDGE HAS A
utt_0060 utt 336.02 346.62 -X COLOR REPRESENTING THE GRADE THE STUDENT ACHIEVED FOR THE COURSE AND EACH COURSE HAS HIS CONTENT INFORMATION LIKE CATALOG DESCRIPTIONS AND SYLLABUS. THEREFORE, IF
utt_0062 utt 346.62 358.33 -X WE INCORPORATE ALL THESE TYPES OF DATA INTO THE GRAPH, THE NETWORK TURNS INTO AN ATTRIBUTED MULTIPLEX HETEROGENEOUS NETWORK(AMHEN), FOR SHORT, AMHEN. HERE ARE THE
utt_0064 utt 358.33 364.41 -X MAIN STEPS FOR TRAINING THE AMHEN MODEL. FIRST, WE SEPARATE A WHOLE NETWORK BY EDGE TYPE,
utt_0065 utt 364.41 375.02 -X WHICH IS GRADE TYPE. FOR EXAMPLE, WE SEGREGATE THE WHOLE NETWORK INTO FOUR SUB-GRAPHS. EACH SUB-GRAPH IS CONSTRUCTED BY STUDENTS AND COURSES WITH A CERTAIN
utt_0067 utt 375.02 386.57 -X GRADE TYPE. THEN, FOR EACH SUB-GRAPH, WE USE META-PATH-BASED RANDOM WORK TO GENERATE NODE SEQUENCES. NEXT, WE APPLY A SKIP-GRAM MODEL OVER THE NODE
utt_0069 utt 386.57 401.53 -X SEQUENCES TO LEARN THEIR EMBEDDINGS. FOR EACH ITERATION IN SKIP-GRAM TRAINING, THE GRADE EMBEDDING OF A COURSE NODE ON A GRADE TYPE IS AGGREGATED FROM INDIVIDUAL EMBEDDINGS OF STUDENTS THAT ARE ITS NEIGHBORS, AND THE INDIVIDUAL EMBEDDING
utt_0072 utt 401.53 415.39 -X OF A STUDENT NOTE IS AGGREGATED FROM GRADING BEDDINGS OF COURSES THAT ARE ITS NEIGHBORS. A SELF-ATTENTION TECHNIQUE IS ALSO LEVERAGED TO COMBINE GRADE EMBEDDINGS ON EACH GRADE TYPE. FINALLY, WE
utt_0075 utt 415.39 427.48 -X CALCULATE THE OVERALL EMBEDDINGS OF COURSE NODES FOR EACH GRADE TYPE AND THE OVERALL EMBEDDINGS OF STUDENTS. THE LAST DIMENSION WE EXPLORED IN THIS WORK
utt_0077 utt 427.48 433.18 -X IS TASKS, WHERE THESE FIVE INSTITUTIONAL RELEVANT PREDICTION TASKS WERE EVALUATED.
utt_0078 utt 434.42 444.06 -X COURSE SIMILARITY PREDICTION IS A WAY TO TEST WHETHER COURSE REPRESENTATIONS CONTAIN IMPORTANT FEATURES OF COURSES THAT COULD DIFFERENTIATE BETWEEN SIMILAR
utt_0080 utt 444.06 456.28 -X AND DISSIMILAR COURSES. FOR EXAMPLE, THESE TWO COURSES CAN BE DEEMED AS AN EQUIVALENCY PAIR BECAUSE OF THEIR VERY SIMILAR COURSE CONTENT. FOR EVALUATION, WE
utt_0082 utt 456.28 466.78 -X FIRST FIXED THE FIRST COURSE IN EACH PAIR AND RANKED ALL THE OTHER COURSES ACCORDING TO THEIR COSINE SIMILARITY TO THE FIRST COURSE IN DESCENDING ORDER.
utt_0084 utt 467.48 477.76 -X THEN, WE RECORDED THE RANK OF THE EXPECTED SECOND COURSE IN THE PAIR AND DESCRIBED THE PERFORMANCE OF EACH MODEL ON ALL VALIDATION PAIRS IN TERMS OF
utt_0086 utt 477.76 489.39 -X MEAN RANK, MEDIAN RANK, AND RECALL@ten. ENROLLMENT PREDICTION AIMS AT PREDICTING COURSES A STUDENT IS GOING TO TAKE IN THE NEXT SEMESTER BASED ON THEIR
utt_0088 utt 489.39 500.85 -X ENROLLMENT HISTORIES. IN OUR PREVIOUS WORK, WE USE A MULTI-HOT VECTOR TO REPRESENT THE COURSES TAKEN BY A STUDENT IN A SEMESTER AND FED THE MULTI-HOT
utt_0090 utt 501.53 506.05 -X INPUT TO LSTM TO PREDICT THE STUDENTS ENROLLMENTS FOR THE NEXT SEMESTER.
utt_0091 utt 508.41 521.73 -X INSTEAD OF MULTI-HOT, WE CAN ALSO USE THE SUM OF COURSE REPRESENTATIONS LEARNED BY OTHER MODELS, WHICH USUALLY HAVE LOWER DIMENSIONS THAN MULTI-HOT. GRADE
utt_0093 utt 521.73 534.81 -X PREDICTION AIMS AT PREDICTING GRADES A STUDENT WILL ACHIEVE FOR EACH ENROLLED COURSE IN THE NEXT SEMESTER BASED ON THEIR ENROLLMENT HISTORIES AND GRADE HISTORIES. IN OUR PREVIOUS WORK, WE ALSO FED A
utt_0096 utt 535.00 548.25 -X MULTI-HOT VECTOR TO THE INPUT OF AN LSTM WHERE THE POSITION OF GRADES STUDENTS RECEIVED FOR ENROLLED COURSES WERE SET ONE AND OTHER POSITIONS TO ZERO. WE CAN
utt_0098 utt 548.25 558.94 -X ALSO REPLACE MULTI-HOT VECTORS WITH THE SUM OF COURSE GRADE REPRESENTATIONS LEARNED BY THE STUDENT-COURSE AMHEN MODEL TO REDUCE THE DIMENSION OF MULTI-HOT TO
utt_0100 utt 558.94 569.38 -X A LARGE DEGREE. THE NEXT TASK IS PREREQUISITE PREDICTION, WHICH IS ESSENTIAL TO PREPARE STUDENTS WITH THE NECESSARY FOUNDATIONAL EXPERIENCE TO
utt_0102 utt 569.38 580.03 -X LEARN AND SUCCEED IN THE ADVANCED STAGES OF THEIR DEGREE. IN OUR PREVIOUS WORK, WE PROPOSED A METHOD TO INFER THE PREREQUISITE COURSES FOR ANY TARGET
utt_0104 utt 580.03 590.26 -X COURSE BASED ON THE LSTM GRADE PREDICTION MODEL WE JUST INTRODUCED. MEANWHILE, WE COULD ALSO USE A SIMPLE MULTINOMIAL LOGISTIC REGRESSION TO BUILD A
utt_0106 utt 590.30 600.26 -X SUPERVISED PREREQUISITE PREDICTION MODEL, WHERE THE INPUT CAN BE ANY REPRESENTATION OF A TARGET COURSE AND OUTPUT IS A MULTI-HOT OF ITS
utt_0108 utt 600.26 611.63 -X PREREQUISITE COURSES. IN ADDITION, WE WERE ALSO CURIOUS ABOUT WHETHER COURSE REPRESENTATIONS LEARNED BY DIFFERENT METHODS COULD ENCODE COURSE
utt_0110 utt 611.63 621.81 -X POPULARITY INFORMATION. SO WE USE A SINGLE MULTI-LAYER PERCEPTRON TO PREDICT AVERAGE ENROLLMENT SIZE PER COURSE USING THE DIFFERENT TYPES OF COURSE EMBEDDINGS
utt_0112 utt 621.81 632.22 -X WE INTRODUCED IN OUR METHOD SECTION. THIS WILL HELP TO ANTICIPATE INCREASES IN THE COURSE DEMAND AND ALLOW INSTITUTIONS TO BETTER PLAN ROOM AND TEACHING STAFF
utt_0114 utt 632.22 643.17 -X ALLOCATIONS. BEFORE WE MOVE ON, HERE WE SUMMARIZED EACH METHOD WITH ITS CORRESPONDING DATA SOURCE LEVERAGED AND THE EMBEDDINGS LEARNED FOR EACH TASK.
utt_0116 utt 648.99 662.15 -X NOW LET'S GET INTO THE EXPERIMENTAL RESULTS. ON THE TASK OF COURSE SIMILARITY PREDICTION, A SINGLE BAG-OF-WORDS REPRESENTATION OF THE COURSE CATALOG DESCRIPTION PERFORMED THE BEST IN TERMS OF MEDIAN RANK AND RECALL@ten.
utt_0119 utt 663.45 671.65 -X ENROLLMENT HISTORIES PROVIDED A SECOND BEST PERFORMING SCORE USING STUDENT COURSE AMHEN NETWORK-BASED EMBEDDINGS, FOLLOWED BY
utt_0121 utt 671.65 679.11 -X MULTI-COURSEtwoVEC. FOR THE TASK OF ENROLLMENT PREDICTION, LSTM WITH A MULTI-
utt_0122 utt 679.11 685.19 -X HOT INPUT PROVIDED THE BEST PERFORMANCE IN TERMS OF BOTH METRICS. IN THIS TASK,
utt_0123 utt 685.19 699.99 -X USING PRE-TRAINED EMBEDDINGS FROM THE NETWORK-BASED OR A MULTI-COURSEtwoVEC APPROACH WORKED LESS WELL THAN THE MULTI-HOT, FOLLOWED BY USING THE CONTENT-BASED REPRESENTATIONS AS INPUTS. IN GRADE PREDICTION, THE NETWORK-BASED
utt_0126 utt 699.99 714.96 -X METHOD PERFORMED SLIGHTLY BETTER THAN THE PREVIOUS LSTM. ON THE TASK OF PREREQUISITE PREDICTION, THE NETWORK- BASED APPROACH PERFORMED THE BEST IN RECOVERING THE GROUND TRUTH PREREQUISITE RELATIONSHIPS FOUND IN OUR INSTITUTIONAL
utt_0129 utt 714.96 719.39 -X DATA. THE MULTI-COURSEtwoVEC APPROACH WAS NOT FAR BEHIND.
utt_0130 utt 720.19 724.39 -X FINALLY, ON THE TASK OF PREDICTING THE AVERAGE ENROLLMENT OF A COURSE,
utt_0131 utt 724.39 736.13 -X MULTI-COURSEtwoVEC PROVIDED THE LOWEST RMSE. IN ADDITION, FOR COURSE SIMILARITY PREDICTION, WE FOUND CONCATENATIONS OF THE BAG-OF-WORDS-BASED METHODS AND
utt_0133 utt 736.13 745.24 -X COURSEtwoVEC-BASED METHOD PERFORMED THE BEST. THE COURSE ATTRIBUTE REPRESENTATIONS SOURCED FROM THE STUDENT- COURSE AMHEN MODEL PERFORMED THE SECOND
utt_0135 utt 745.24 755.09 -X BEST AMONG ALL SINGLE REPRESENTATION MODELS. FOR GRADE PREDICTION, WE FOUND THE PROPOSED STUDENT-COURSE AMHEN MODEL, THOUGH STATIC
utt_0137 utt 755.09 765.54 -X ITSELF, COULD MAP THE KNOWLEDGE LEVELS OF STUDENTS ON THE FEATURES OF COURSES WITH GRADE TYPES TO A CERTAIN DEGREE. BESIDES, THE SEQUENTIAL INFORMATION OF
utt_0139 utt 765.54 770.18 -X STUDENTS GRADES BY SEMESTERS EXHIBITED SUBSTANTIAL IMPORTANCE.
utt_0140 utt 770.21 783.72 -X LASTLY, THE COURSE EMBEDDINGS WITH DIFFERENT GRADE TYPES LEARNED FROM THE STUDENT-COURSE AMHEN MODEL HELPED INCREASE THE ACCURACY OF GREAT PREDICTION OVER THE MULTI-HOT INPUT NO MATTER WHAT THE GREAT CUTOFF IS.
utt_0143 utt 784.67 799.00 -X FOR PREREQUISITE PREDICTION, WE FOUND ALL THE SUPERVISED MODELS THAT LEVERAGED THE PREREQUISITE INFORMATION PERFORMED BETTER THAN THE PREVIOUS STATE-OF-THE-ART INFERENCE MODEL, WHICH MAKES SENSE. AND
utt_0146 utt 799.00 811.91 -X AMONG ALL THE PREDICTION MODELS, THE COURSE EMBEDDINGS AND GREAT EMBEDDINGS LEARNED FROM THE STUDENT-COURSE AMHEN MODEL PERFORMED THE BEST. IN CONCLUSION, IN THIS WORK, WE PROVIDED A
utt_0149 utt 811.91 821.70 -X COMPREHENSIVE EVALUATION ON THE PERFORMANCE OF DIFFERENT COMBINATIONS OF DATA AND MODELS ON FIVE INSTITUTIONAL PREDICTION TASKS. IN TERMS OF
utt_0151 utt 821.70 827.14 -X INSTITUTIONAL DATA SOURCES, WE INTRODUCED A LARGE-SCALE SYLLABUS DATA AS
utt_0152 utt 827.97 838.63 -X A NOVEL SOURCE OF INFORMATION ABOUT COURSES. WE FOUND SYLLABUS DATA SHOWED BENEFIT OVER CATALOG DESCRIPTIONS ON PREREQUISITE PREDICTIONS, GRADE PREDICTION,
utt_0154 utt 838.85 851.78 -X AND AVERAGE ENROLLMENT PREDICTION. IT ALSO COMPLEMENTS THE CATALOG DESCRIPTION AND ENROLLMENT DATA ON THE COURSE SIMILARITY TASK. WITH REGARD TO METHODS, WE PROVIDED
utt_0156 utt 851.78 861.54 -X A NOVEL APPLICATION OF A NASCENT GRAPH- BASED APPROACH FOR PRESENTING COURSES AND THEIR GRADES, WHICH IS THE FIRST TIME THAT COURSE GRADES COULD BE
utt_0158 utt 861.54 867.91 -X EMBEDDED AS DISTRIBUTED REPRESENTATIONS. FOR TASKS, WE FOUND REPLACING MULTI-
utt_0159 utt 867.91 872.04 -X HOTS WITH PRE-TRAINED COURSE REPRESENTATIONS IN THE LSTM INPUT DID
utt_0160 utt 872.26 878.57 -X NOT IMPROVE THE ENROLLMENT PREDICTION PERFORMANCE. HOWEVER, REPLACING MULTI-
utt_0161 utt 878.57 888.17 -X HOTS WITH PRE-TRAINED COURSE GRADE REPRESENTATIONS FROM THE STUDENT-COURSE AMHEN MODEL PROVIDED A SMALL IMPROVEMENT IN THE GRADE PREDICTION TASK. IN THE
utt_0163 utt 888.17 899.05 -X FUTURE, WE WILL TRY TO SOLVE SOME LIMITATIONS OF THIS WORK. FIRST, WE AIM TO EVALUATE MULTIPLE INSTITUTIONS BESIDES OUR CURRENT DATA TO EXAMINE THE
utt_0165 utt 899.05 908.98 -X GENERALIZABILITY OF THE PROPOSED APPROACHES. IN ADDITION, WE WILL EVALUATE OTHER CLASSICAL MODELS FOR THESE TASKS AND EVALUATE THE LEARNED REPRESENTATIONS
utt_0167 utt 908.98 919.59 -X ON OTHER INSTITUTIONAL TASKS IN THE FUTURE. LASTLY, WE WILL TRY TO INCORPORATE MORE VARIETIES OF DATA, SUCH AS THE CLICKSTREAM DATA IN LEARNING MANAGEMENT
utt_0169 utt 919.59 931.37 -X SYSTEM, TO ENHANCE COURSE REPRESENTATIONS. THAT'S ALL FOR THIS PRESENTATION. THANK YOU FOR LISTENING! IF YOU HAVE ANY QUESTIONS OR COMMENTS, PLEASE DO NOT HESITATE TO CONTACT US.
