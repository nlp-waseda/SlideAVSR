utt_0000 utt 0.05 7.60 -X HELLO, I'M SHRIMAI PRABHUMOYE AND TODAY, I'M GOING TO TALK ABOUT OUR WORK ON EXPLORING CONTROLLABLE TEXT GENERATION TECHNIQUES.
utt_0002 utt 8.27 20.75 -X CONTROLLABLE TEXT GENERATION ALLOWS US TO ADD KNOBS TO CONTROL THE ATTRIBUTES OF THE TEXT TO BE GENERATED. FOR EXAMPLE, IF THE USER ASKS, WHO DID YOU LIKE THE BEST IN AVENGERS.
utt_0004 utt 20.75 35.57 -X NOW THE BOT CAN DECIDE TO HALLUCINATE THIS INFORMATION. OR WE CAN PROVIDE THE CONTROL ATTRIBUTES LIKE ROBERT DOWNEY JR OR SCARLETT JOHANSSON TO THE BOT SO THAT THE BOT CAN RESPOND BY SAYING I LIKED ROBERT DOWNEY JR
utt_0007 utt 35.57 45.06 -X AND SCARLETT JOHANSSON IN THE MOVIE. SIMILARLY, IF THE USER ASKS, DID YOU LIKE THE MOVIE? NOW THE BOT CAN TAILOR ITS RESPONSES ACCORDING TO THE
utt_0009 utt 45.10 55.44 -X PREFERENCES OF THE USER. FOR EXAMPLE, IF THE BOT KNOWS THAT THE USER LIKE SUPERHERO MOVIES, THEN THE BOT CAN RESPOND WITH A POSITIVE SENTIMENT
utt_0011 utt 55.44 67.60 -X SENTENCE LIKE, YEAH, I LOVE THE MOVIE. AND IF THE BOR KNOWS THAT THE USER DOES NOT LIKE SUCH MOVIES. THEN IT CAN RESPOND WITH A NEGATIVE SENTIMENT SENTENCE BY SAYING NO, I HATED IT.
utt_0014 utt 67.82 81.65 -X THERE ARE MANY APPLICATIONS AND TASKS UNDER THE UMBRELLA OF CONTROLLABLE TEXT GENERATION ON HERE. I'M GOING TO DESCRIBE SOME OF THEM SO AS DESCRIBED IN THE PREVIOUS LIGHT. YOU CAN CONTROL VARIOUS ASPECTS OF DIALOGUE SYSTEMS.
utt_0017 utt 81.65 93.75 -X YOU CAN CONTROL THE PERSONA OF THE DIALOGUE SYSTEM, THE STYLE OF THE RESPONSES, LIKE WHETHER YOU SAY SOMETHING IN A POLITE MANNER OR YOU SHOW AUTHORITY IN YOUR RESPONSES. YOU CAN CONTROL THE CONTENT OF THE
utt_0020 utt 93.75 101.84 -X RESPONSES, AND YOU CAN CONTROL THE TOPIC OF THE CONVERSATION. YOU CAN RECOMMEND POLITE EMAILS IN AN EMAIL WRITING APP.
utt_0022 utt 104.01 114.80 -X YOU CAN CONTROL VARIOUS ASPECTS OF STORY IN THE STORY GENERATION TASK LIKE YOU CAN CONTROL THE PLOT OF THE STORY, THE ENDING OF THE STORY, THE SENTIMENT OF
utt_0024 utt 114.80 125.11 -X THE STORY OR THE SEQUENCE OF TOPICS IN THE STORY. AND YOU CAN EVEN CONTROL THE PERSONA OFF THE NARRATOR OF THE STORY ON YOU CAN IN GENERAL, GENERATE ENTIRE
utt_0026 utt 125.26 134.00 -X WEBSITE OR WIKIPEDIA ARTICLES BY PERFORMING CONTENT GROUNDED GENERATION.
utt_0027 utt 134.00 143.84 -X THERE IS A LARGE BODY OF WORK IN DIFFERENT DOMAINS AND TASKS AND IT IS VERY HARD TO COMPREHEND THESE COMPLEX ARCHITECTURES AND HOW THEY'RE CONNECTED
utt_0029 utt 143.84 154.34 -X WITH EACH OTHER. THERE IS NO UNIFYING THEME TO DESCRIBE THIS. HENCE WE NEED A SCHEMA FOR CONTROLLABLE TEXT GENERATION MODELS, AND THAT IS WHAT THIS PAPER IS
utt_0031 utt 154.34 168.76 -X ALL ABOUT. PRIOR WORK IN CONTROLLABLE TEXT GENERATION HAS BEEN DONE INDEPENDENTLY. OUR GOAL IS TO ORGANIZE THE VARIOUS TECHNIQUES AND TASKS IN CONTROLLABLE TEXT GENERATION AND CONNECT THEM TOGETHER USING A SCHEMA.
utt_0034 utt 169.04 182.64 -X OUR SCHEMA CONTAINS FIVE MODULES, WHICH I WILL DESCRIBE SHORTLY. YOU CAN USE THE SCHEMA TO IDENTIFY ANY ARCHITECTURE AS BELONGING TO ONE OF THE FIVE MODULES. AND VERY IMPORTANTLY, YOU CAN USE THE SCHEME
utt_0037 utt 182.64 196.37 -X WITH ANY ALGORITHMIC PARADIGM LIKE THE SEQUENCE TO SEQUENCE PARADIGM OR THE REINFORCEMENT LEARNING PARADIGM AND SO ON. WE WANTED TO COLLATE KNOWLEDGE ABOUT THE DIFFERENT TECHNIQUES THAT ARE USED. TO ADD CONTROL IN EACH OF THESE
utt_0040 utt 196.37 211.22 -X FIVE MODELS OF THE SCHEMA, WE PROVIDE QUALITATIVE ANALYSIS OF THESE TECHNIQUES AND PAY WAY FOR NEW ARCHITECTURES. THIS OVERVIEW ALSO PROVIDES AN EASY ACCESS FOR COMPARISON. SO IF YOU'RE GOING TO, PROVIDE A NEW
utt_0043 utt 211.22 221.75 -X ARCHITECTURE OR PROPOSE A NEW MODEL, THEN YOU CAN EASILY LOOK UP IN THIS PARTICULAR WORK AND SEE WHICH OTHER PRIOR WORKS YOU WANT TO COMPARE YOURSELF WITH.
utt_0046 utt 221.75 236.24 -X I WILL FIRST BRIEFLY DESCRIBE A STANDARD GENERATION PROCESS WHICH USES RECURRENT NEURAL NETWORK AND THEN I WILL TALK ABOUT THE PROPOSED SCHEMA. SO THE ENCODER ENCODES THE SOURCE SENTENCE BY CONSUMING ONE WORD
utt_0049 utt 236.24 248.05 -X AT EACH TIME STEP AND CREATES A FIXED SIZED VECTOR. H_E, WHICH REPRESENTS THE WHOLE OF THE INPUT SENTENCE. IN THE STANDARD GENERATION PROCESS, H_E IS
utt_0051 utt 248.05 252.34 -X PASSED AS AN EXTERNAL INPUT TO THE DECODER AND THE GENERATION PROCESS
utt_0052 utt 252.37 266.42 -X BEGINS. THE DECODER IS GIVEN START TOKEN AT THE FIRST TIME STEP AS AN INPUT AND THE GENERATOR PRODUCES AN OUTPUT STATE O_one. O_one IS THEN PROJECTED OVER THE VOCABULARY SPACE FOLLOWED BY
utt_0055 utt 270.00 274.61 -X THE SOFT MAX FUNCTION AND THE ARG MAX FUNCTION TO GENERATE THE TOKEN 'YES'.
utt_0056 utt 274.87 280.73 -X THE WORD 'YES' ALONG WITH THE HIDDEN STATE THAT WAS CREATED AT THE
utt_0057 utt 281.17 285.30 -X PREVIOUS TIME H_one BOTH OF THESE YES AND H ONE ARE PASSED TO THE
utt_0058 utt 287.25 297.37 -X NEXT TIME STEP SO THAT THE GENERATOR PRODUCES THE OUTPUT STATE O_two AND THIS PROCESS CONTINUES UNTIL THE DECODER GENERATES THE END TOKEN.
utt_0060 utt 298.80 307.99 -X NOW LET'S LOOK AT THE DECODER CLOSELY AND SEE WHERE CAN WE POTENTIALLY ADD THE CONTROL ATTRIBUTE. IN THIS GENERATION PROCESS WHAT CAN BE MODIFIED
utt_0062 utt 308.02 317.65 -X TO INCLUDE THE CONTROL ATTRIBUTES. WE CAN FIRST OF ALL, POTENTIALLY MODIFY THE EXTERNAL INPUT AND ADD CONTROL ATTRIBUTE TO H ZERO. WE CAN ALSO ADD
utt_0064 utt 320.63 332.31 -X CONTROL ATTRIBUTES TO THE SEQUENTIAL INPUTS AT EACH TIME STEP OF THE GENERATION. WE CAN MAKE CHANGES TO THE GENERATOR OPERATIONS ITSELF SO THAT WE
utt_0066 utt 332.40 346.55 -X CAN ADD CONTROL ATTRIBUTES SPECIFIC PARAMETERS TO THEM. WE CAN ALSO MODIFY THE OUTPUT STATE SUCH THAT WHEN YOU GENERATE THE OUTPUT STATE ACTUALLY HAS THE CONTROL ATTRIBUTE THAT YOU'RE LOOKING FOR.
utt_0069 utt 347.57 354.97 -X AND FINALLY, WE CAN ALSO ADD LOSS OBJECTIVES TO ENSURE THAT THE TEXT BELONGS TO THE DESIRED CONTROL ATTRIBUTES.
utt_0071 utt 356.40 367.32 -X SO FINALLY, HERE IS THE SCHEMA THAT IS PROPOSED IN THIS PAPER. WE FIRST HAVE THE EXTERNAL INPUT H ZERO, WHICH CAN BE CHANGED. NEXT, WE HAVE THE
utt_0073 utt 367.32 378.23 -X SEQUENTIAL INPUTS X T AT EACH TIME STEP. IF YOU WANT TO SEE THE EFFECT OF YOUR CONTROL ATTRIBUTE AT EACH TIME STEP OF THE GENERATION, YOU CAN ALSO MAKE
utt_0075 utt 378.23 389.32 -X CHANGES TO THE OPERATIONS OF THE GENERATOR ITSELF. AND THAT IS THE THIRD MODULE. FOURTH MODULE IS THE OUTPUT SPACE. AND ONCE THE OUTPUT STATE IS
utt_0077 utt 389.32 394.20 -X PROJECTED OVER THE VOCABULARY SPACE TO GENERATE THE TOKEN X HAT T, IT
utt_0078 utt 394.93 400.34 -X IS COMPARED WITH Y T. WE CAN CHOOSE HOW WE WANT TO COMPARE THESE TWO.
utt_0079 utt 400.34 410.86 -X SO THE FIFTH MODULE IS THE TRAINING OBJECTIVE MODULE. THIS IS THE MAIN CONTRIBUTION OF THIS WORK. IN THIS PAPER, WE DESCRIBE THESE FIVE MODULES
utt_0081 utt 410.86 415.54 -X OF THE SCHEMA AND THE VARIOUS TECHNIQUES THAT ARE USED TO MODIFY THEM.
utt_0082 utt 415.54 422.55 -X THIS SCHEMA ORGANIZES THE VAST AMOUNT OF PRIOR WORK IN DIFFERENT TASKS AND DOMAINS.
utt_0084 utt 423.41 428.02 -X SO NOW I WILL BRIEFLY DESCRIBE ONLY SOME OF THE TECHNIQUES THAT ARE USED,
utt_0085 utt 428.85 441.82 -X UH, THAT ARE ACTUALLY DESCRIBED IN THE PAPER TO MODIFY EACH OF THESE FIVE MODULES. THESE ARE NOT NEW TECHNIQUES, BUT THIS IS A NEW CATEGORIZATION OF THESE TECHNIQUES. SO FIRST OF ALL, YOU CAN USE THE DECOMPOSE TECHNIQUE TO
utt_0088 utt 441.82 452.09 -X MODIFY THE EXTERNAL INPUT MODULE. THE ENCODER REPRESENTATION H E CAN BE DECOMPOSED INTO MULTIPLE SUBSPACES EACH OF WHICH REPRESENTS A PARTICULAR
utt_0090 utt 452.09 466.71 -X CONTROL ATTRIBUTE. FOR EXAMPLE, IF YOU'RE USING THIS TECHNIQUE FOR STYLE TRANSFER TASK, THEN H E CAN BE DECOMPOSED INTO THE MEANING SUBSPACE AND THE STYLE SUBSPACE. THEY CAN THEN EITHER BE COMBINED TOGETHER AND PASSED
utt_0093 utt 466.71 475.00 -X TO THE DECODER. OR YOU CAN CHOOSE TO JUST PASS ONE OF THEM, LIKE IN CASE OF STYLE TRANSFER. YOU CAN JUST PASS THE MEANING VECTOR, THE MEANING. SUBSPACE.
utt_0095 utt 475.38 490.04 -X THE ADVANTAGE OF THIS TECHNIQUE IS THAT YOU GET INTERPRETABLE REPRESENTATION BECAUSE YOU CAN ACTUALLY QUERY THIS SUBSPACE TO SEE IF IT ACTUALLY CAPTURES THE DESIRED CONTROL ATTRIBUTE OR NOT. ONE OF THE CAVEAT
utt_0098 utt 490.04 503.98 -X OF THIS TECHNIQUE IS THAT THE INPUT SHOULD CONTAIN THE SIGNAL OF THE CONTROL ATTRIBUTE THAT YOU'RE LOOKING FOR IN A SENSE THAT IN THIS CASE, FOR STYLE TRANSFER, LET'S SAY YOU ARE PERFORMING SENTIMENT MODIFICATION. AND
utt_0101 utt 503.98 508.89 -X NOW YOUR INPUT SENTENCE IS A POSITIVE SENTIMENT SENTENCE. THEN YOU
utt_0102 utt 509.30 518.27 -X NEED THE INPUT SENTENCE TO CONTAIN THAT POSITIVE SENTIMENT. UNLESS YOU HAVE THAT, YOU CANNOT DECOMPOSE IT INTO AH, POSITIVE SENTIMENT SUBSPACE.
utt_0104 utt 519.45 528.03 -X THIS TECHNIQUE WORKS REALLY WELL WITH SUPERVISION ON THIS DECOMPOSED SUBSPACE BECAUSE YOU WANT TO MAKE SURE THAT THESE DECOMPOSED SUBSPACES ACTUALLY
utt_0106 utt 529.05 531.90 -X LEARN THE CONTROL ATTRIBUTES THAT YOU ARE LOOKING FOR.
utt_0107 utt 533.85 544.95 -X SECOND IS THE EXTERNAL FEEDBACK WHICH CAN BE USED TO MODIFY THE EXTERNAL INPUT MODULE. IN THIS CASE, YOU CAN HAVE A REGULARIZER TO CONTROL H E.
utt_0109 utt 545.40 560.07 -X H IS NOT ONLY PASSED TO THE GENERATOR, BUT IT IS ALSO PASSED TO A DISCRIMINATOR AND THE DISCRIMINATOR GIVES FEEDBACK TO H E WHETHER IT ACTUALLY CONTAINS THE DESIRED CONTROL ATTRIBUTE OR NOT. IN THIS CASE, THOUGH, DISCRIMINATOR MUST BE
utt_0112 utt 560.07 570.04 -X JOINTLY TRAINED WITH THE GENERATOR AND THE ENCODER AND THIS TECHNIQUE IS VERY USEFUL IF YOU COMBINED WITH THE DECOMPOSE TECHNIQUE BECAUSE YOU CAN
utt_0114 utt 570.04 583.39 -X USE THE DISCRIMINATOR TO, MAKE SURE THAT THE DECOMPOSED SUBSPACES ACTUALLY CAPTURE THE CONTROL ATTRIBUTE OF YOUR CHOICE SO YOU CAN USE IT FOR SUPERVISION. THERE ARE MORE TECHNIQUES DESCRIBED IN THE PAPER IN DETAIL, LIKE
utt_0117 utt 583.39 596.87 -X THE ARITHMETIC OR LINEAR TRANSFORM TECHNIQUE OR THE STOCHASTIC CHANGES TECHNIQUE. SO NOW LET'S MOVE TO THE SECOND MODULE, WHICH IS THE SEQUENTIAL INPUT MODULE SIMILAR TO THE PREVIOUS MODEL, YOU CAN CONCATENATE THE
utt_0120 utt 596.87 603.00 -X ATTRIBUTES VECTOR S OR ADDED TO THE WORD EMBEDDING AT EACH TIME STEP T.
utt_0121 utt 603.29 616.73 -X KNOW THAT IN THIS CASE YOU ARE MAKING CHANGES TO THE INPUT OF THE GENERATOR ITSELF AND NOT THE CONTEXT THAT FLOWS THROUGH AT EACH TIME STEP. THIS TECHNIQUE HAS NOT SHOWN PROMISING RESULTS SO FAR.
utt_0124 utt 617.02 631.96 -X NOW LET'S TALK ABOUT THE THIRD MODULE, WHICH IS THE GENERATOR OPERATIONS ITSELF, SO YOU CAN MAKE CHANGES TO THE GENERATOR OPERATIONS ITSELF. TO ADD FOR YOUR CONTROL ATTRIBUTES. SO HERE I SHOW TWO EXAMPLES. THE FIRST IS AN EXAMPLE
utt_0127 utt 631.96 641.72 -X OF MODIFYING THE LSTM WHERE, IN ADDITION TO COMPUTING, HOW MUCH OF THE PRIOR CONTEXT TO FORGET AND HOW MUCH OF IT TO PASS TO THE NEXT TIME STEP.
utt_0129 utt 641.72 649.88 -X YOU ALSO CONSIDER HOW MUCH OF THE DIALOGUE ACT AT THE CURRENT TIME STEP SHOULD AFFECT THE GENERATION OF THE TOKEN AT THIS TIME STEP.
utt_0131 utt 650.49 654.81 -X IN THE SECOND EXAMPLE, THERE ARE CHANGES THAT ARE MADE TO THE GRU CELL, WHERE TWO
utt_0132 utt 655.13 666.04 -X ADDITIONAL GATES ARE ADDED. THE GOAL SELECT GATE CONTROLS WHEN THE GOAL OF THE RECIPE SHOULD BE TAKEN INTO ACCOUNT AND THE ITEM SELECT GATE CONTROLS WHEN
utt_0134 utt 666.04 671.07 -X THE AGENDA RECIPE ITEMS SHOULD BE TAKEN INTO ACCOUNT IN THE RECIPE GENERATION TASK.
utt_0136 utt 673.50 678.97 -X WE ALSO DESCRIBE IN DETAIL THE STANDARD GENERATORS LIKE RECURRENT NEURAL NETWORKS,
utt_0137 utt 678.97 682.36 -X TRANSFORMERS, AND WE ALSO TALK ABOUT PRE TRAINED LANGUAGE MODELS.
utt_0138 utt 685.08 693.88 -X NOW LET'S LOOK AT THE FOURTH MODULE, WHICH IS THE OUTPUT MODULE, THE MOST POPULAR WAY OF MODIFYING THE OUTPUT SPACE IS USING ATTENTION MECHANISM.
utt_0140 utt 694.10 708.32 -X ATTENTION LET'S YOU FOCUS ON PARTICULAR TOKENS OF THE SOURCE SEQUENCE. THERE ARE MANY DIFFERENT TYPES OFF ATTENTION, LIKE GLOBAL ATTENTION, LOCAL ATTENTION AND MULTI HEADED ATTENTION. BUT I WILL BRIEFLY TALK ABOUT THE GLOBAL ATTENTION HERE.
utt_0144 utt 709.56 720.25 -X SO WHEN YOU'RE GENERATING A TOKEN, LET'S SAY AT TIME STEP two, THE GENERATOR BUILDS THE OUTPUT STATE O TWO, AND NOW WE'RE GOING TO MODIFY THIS OUTPUT STATE
utt_0146 utt 720.41 731.61 -X SO THAT IT PAYS ATTENTION TO THE SOURCE SEQUENCE. SO WE CREATE AN ALIGNMENT MATRIX A_two WHICH GIVES US SCORES TO EACH HIDDEN STATE OF THE
utt_0148 utt 731.61 743.87 -X SOURCE SEQUENCE BASED ON THE CURRENT STATE O TWO. WE THEN GET A CONTEXT VECTOR C TWO BASED ON THESE SCORES AND FINALLY WE GET O HAT TWO BY CONCATENATING
utt_0150 utt 743.87 752.89 -X C TWO AND O TWO. SO NOW THIS MODIFIED STATE O HAT TWO IS WHAT IS GOING TO BE USED TO GENERATE THE TOKEN LEGS.
utt_0152 utt 755.26 765.53 -X ATTENTION MECHANISM HAS PROVED TO BE MOST EFFECTIVE, ESPECIALLY THE SELF AND CROSS ATTENTION USED IN TRANSFORMERS IN CONTROLLABLE TEXT GENERATION. THIS IS
utt_0154 utt 765.53 776.49 -X MOSTLY USED BY ADDING CONTROL ATTRIBUTE TOKENS TO THE SOURCE SEQUENCE SO THAT THE SELF ATTENTION CAN CAPTURE IT. BUT THIS AREA IS UNDER EXPLORED AND IT HAS
utt_0156 utt 776.49 782.21 -X A LOT OF POTENTIAL, ESPECIALLY IN THE SPACE OF CONTENT GROUNDED GENERATION.
utt_0157 utt 782.65 796.22 -X IN THE PAPER WE TALK ABOUT MORE TECHNIQUES LIKE EXTERNAL FEEDBACK TECHNIQUE AND THE ARITHMETIC OR LINEAR TRANSFORME TECHNIQUE, WHICH CAN ALSO BE USED TO MODIFY THE OUTPUT SPACE. AND FINALLY, LET'S TALK ABOUT THE TRAINING
utt_0160 utt 796.22 807.81 -X OBJECTIVE MODULE. SO TYPICALLY YOU, YOU USE THE CROSS ENTROPY LOSS FOR TRAINING YOUR DECODER AND THIS IS A GENERAL LOSS. THERE ARE OTHER GENERAL LOSSES, LIKE
utt_0162 utt 807.81 818.94 -X THE UNLIKELIHOOD LOSS, WHICH BASICALLY MAKE SURE THAT YOU DON'T REPEAT THE TOKENS IN YOUR GENERATION AND THESE LOSSES CAN BE USED WITH ANY TASK, ANY
utt_0164 utt 818.94 824.42 -X GENERATION TASK. YOU CAN ALSO USE DECODING STRATEGIES LIKE BEAM SEARCH,
utt_0165 utt 824.44 834.40 -X BUT YOU CAN ALSO ADD ADDITIONAL LOSS TERMS THAT ARE SPECIFIC TO YOUR TASK OR WHAT YOU WANT TO DO OR THE ATTRIBUTES THAT YOU WANT TO CONTROL. FOR EXAMPLE,
utt_0167 utt 834.40 839.42 -X YOU CAN USE CLASSIFIER LOSS WHERE YOU CAN DESIGN A CLASSIFIER TO MAKE SURE
utt_0168 utt 839.55 852.03 -X WHICH GIVES THE FEEDBACK TO THE DECODER THROUGH A LOST TERM THAT WHETHER THE DECODER GENERATED SENTENCE BELONGS TO THE TARGET ATTRIBUTE OR NOT.
utt_0171 utt 852.03 866.27 -X WE DESCRIBE MORE LOSS TERMS IN THE PAPER KL DIVERGENCE LOSS OR TASK SPECIFIC LOSSES LIKE THE COVERAGE LOSS AND STRUCTURE LOSS AND THE FUTURE WORK FOR THIS PAPER IS EMPIRICAL EVALUATION OF THE SCHEMA.
utt_0174 utt 866.27 880.35 -X THIS IS TO UNDERSTAND QUANTITATIVELY, WHICH MODULES ARE MORE EFFECTIVE IN CONTROLLING ATTRIBUTES. WE ALSO WANT TO STUDY TASK RELATED ARCHITECTURES, SO SOME ARCHITECTURES MAY BE MORE BENEFICIAL FOR CERTAIN KINDS OF TASKS,
utt_0177 utt 880.35 890.88 -X AND SOME MAY NOT BE. WE ALSO WANT TO ADD THESE CONTROL TECHNIQUES TO PRE TRAINED MODELS LIKE BART Tfive. MOST OF THESE TECHNIQUES HAVE BEEN STUDIED IN
utt_0179 utt 890.88 900.48 -X THE RECURRENT NEURAL NETWORK FRAMEWORK, BUT WE WANT TO, THIS IS A NEW SPACE TO EXPLORE WHERE YOU CAN SEE HOW TO ADD THESE TECHNIQUES TO BART Tfive.
utt_0181 utt 900.48 901.98 -0.5672 THANK YOU.
