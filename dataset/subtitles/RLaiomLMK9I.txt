utt_0000 utt 0.62 12.37 -X HELLO AND WELCOME TO THIS UNUSUAL AI COFFEE BREAK. TODAY WE’LL GIVE YOU A TEASER ABOUT SOME OF OUR OWN WORK, WHICH WAS ACCEPTED AT ACL two thousand and twenty-three. WE WILL PRESENT IT IN JULY IN TORONTO, SO
utt_0002 utt 13.13 19.06 -X IF YOU HAPPEN TO BE AT ACL IN TORONTO THIS YEAR, DO NOT HESITATE TO SAY HI! SO HERE,
utt_0003 utt 19.06 23.06 -X WE’LL LEAVE YOU WITH THE BIT WE HAVE RECORDED TO PRESENT OUR PAPER.
utt_0004 utt 24.05 31.52 -X HELLO! WE ARE HAPPY TO PRESENT „MM-SHAP” A RELIABLE WAY TO MEASURE RELATIVE CONTRIBUTIONS
utt_0005 utt 31.52 39.57 -X OF INDIVIDUAL MODALITIES IN MULTIMODAL MODELS. WHY IS MEASURING MULTIMODAL CONTRIBUTIONS IMPORTANT?
utt_0006 utt 39.57 52.07 -X VISION AND LANGUAGE MODELS HAVE BETTER AND BETTER PERFORMANCE ACROSS ALL POSSIBLE VL TASKS. BUT CAN WE TRUST THEY DO THIS FOR THE RIGHT REASONS? FUNDAMENTALLY, THESE MODELS: ARE TRANSFORMERS
utt_0008 utt 52.07 64.68 -X THAT WORK ON TEXT AND IMAGE INPUT. THEY ARE TRAINED, FOR EXAMPLE, WITH THE IMAGE-SENTENCE ALIGNMENT OBJECTIVE TO SAY WHETHER A PAIR OF TEXT AND IMAGES MATCH OR MISMATCH. NOW, THERE IS REASON
utt_0010 utt 64.68 77.82 -X TO BELIEVE THAT NOT ALL MODALITIES MATTER FOR THESE MODELS EQUALLY, SINCE THEY CAN SUFFER FROM UNIMODAL COLLAPSE. THIS MEANS, EVEN IF TRAINED MULTIMODALLY, A MODEL CAN EXPLOIT ONE MODALITY
utt_0012 utt 77.82 91.10 -X FAR MORE THAN THE OTHER, AND INDEED WE OBSERVE THAT ON MULTIMODAL TASKS, UNIMODAL BASELINES CAN BE CLOSE IN ACCURACY TO MULTIMODAL MODELS. THE REASON ARE OFTEN DATASET BIASES: FOR EXAMPLE,
utt_0014 utt 91.10 102.96 -X “HOW MANY” QUESTIONS CAN BE CONFIDENTLY ANSWERED WITH “TWO” IF “TWO” IS THE MOST FREQUENT ANSWER IN THE DATASET. OR IF QUESTIONS ABOUT EXISTENCE OF AN OBJECT CAN BE TYPICALLY ANSWERED WITH “YES”, DUE
utt_0016 utt 102.96 116.06 -X TO SO-CALLED PLAUSIBILITY BIAS. PREVIOUS WORK HAS USED MODEL PERFORMANCE TO ASSESS A MODEL’S FOCUS ON A MODALITY, FOR EXAMPLE BY TESTING WHETHER IT IS SENSITIVE TO CHANGING MATCHING IMAGE-CAPTION
utt_0018 utt 116.06 129.73 -X PAIRS INTO MISMATCHING ONES, OR BY MEASURING WHETHER THEIR ACCURACY CHANGES WHEN EXCHANGING EXISTING IMAGES WITH RANDOM ONES AND QUESTIONS FOR VQA WITH RANDOM QUESTIONS. OR BY DELETING IMAGE
utt_0020 utt 129.73 143.14 -X REGIONS OR TEXT TOKENS AND MEASURING WHETHER MODEL PERFORMANCE CHANGES. BUT ACCURACY-BASED MEASURES ARE NOT IDEAL. FOR EXAMPLE, THE VISUAL IMPORTANCE CONSIDERS THE DIFFERENCE IN MODEL ACCURACY WHEN IT
utt_0022 utt 143.14 156.32 -X USES BOTH VISION AND TEXT AND WITH MISSING VISUAL INFORMATION. THIS IS PROBLEMATIC FOR WRONG MODEL PREDICTIONS, SINCE IN THESE CASES THE FIRST TERM IS zero AND WE EXPECT THE SECOND TERM TO BE ZERO TOO
utt_0024 utt 156.38 169.03 -X AND THIS RESULTS IN A VISUAL IMPORTANCE OF ZERO. BUT THE MODEL MAY WELL HAVE RELIED ON THE VISUAL MODALITY, BUT INCORRECTLY. SO, WE PROPOSE A PERFORMANCE-AGNOSTIC MEASURE OF MULTIMODAL
utt_0026 utt 169.03 181.12 -X CONTRIBUTIONS. FIRST, WE RELY ON SHAPLEY VALUES FROM GAME THEORY. SHAPLEY VALUES COMPUTE A FAIR PAY-OUT FOR EACH PLAYER BASED ON THEIR CONTRIBUTION TO A GAME’S OUTCOME. FOR EXAMPLE,
utt_0028 utt 181.12 192.96 -X IF MACHINE LEARNING WERE A SOCCER GAME, WE WOULD FIRST ASSES THE BASE VALUE, FOR THE GAME OUTCOME WHEN NO PLAYER IS PLAYING. THEN WE GRADUALLY LET THEM PLAY AND MEASURE IN ALL POSSIBLE COMBINATIONS
utt_0030 utt 192.96 204.17 -X OF TEAMS FROM OUR AVAILABLE PLAYERS, WHAT THE GAME OUTCOMES ARE AND WE DETERMINE HOW MUCH PAYMENT THEY DESERVE. FOR TRANSFORMERS, THE PLAYERS ARE TOKENS AND THEY RECEIVE PAY-OUTS FROM THEIR
utt_0032 utt 204.17 214.95 -X CONTRIBUTION TOWARDS THE MODEL PREDICTION (FOR EXAMPLE THE PROBABILITY OR ACTIVATION VALUE IT COMPUTED FOR A SAMPLE) IRRESPECTIVE OF WHETHER ITS PREDICTION IS CORRECT OR NOT. WE TAKE THE
utt_0034 utt 214.95 226.92 -X DIFFERENCE BETWEEN THE MODEL OUTPUT WITH A TOKEN BEING ACTIVE AND WITH IT INACTIVE, AND WE DO THIS FOR ALL POSSIBLE COMBINATIONS OF TOKENS SWITCHED ON AND OFF. TO GET THE CONTRIBUTIONS FOR A TOKEN
utt_0036 utt 226.92 239.34 -X (WHICH CAN BE POSITIVE OR NEGATIVE), WE SUM UP AND NORMALIZE OVER ALL MARGINAL CONTRIBUTIONS. FOR VISION AND LANGUAGE TRANSFORMERS, WE HAVE IMAGE TOKENS AND TEXT TOKENS AND WE CAN COMPUTE THEIR
utt_0038 utt 239.34 245.51 -X CONTRIBUTION TOWARDS THE MODEL PREDICTION, SUCH AS THE IMAGE-SENTENCE-ALIGNMENT SCORE. AND LUCKILY,
utt_0039 utt 245.51 256.94 -X NONE OF THE COMPUTATIONS ARE ACCURACY-BASED. WE DEFINE MM-SHAP BY CALCULATING THE CONTRIBUTION PERCENTAGE OF TEXT TOKENS AND COMPARE IT TO THE CONTRIBUTION PERCENTAGE OF IMAGE TOKENS,
utt_0041 utt 256.94 269.37 -X WHICH CAN BE EXTENDED TO MULTIPLE MODALITIES. FOR IMAGE-SENTENCE ALIGNMENT, WE HYPOTHESIZE THAT IMAGE AND TEXT CONTRIBUTE EQUALLY ON AVERAGE AT CORPUS-LEVEL BECAUSE IT IS USEFUL WHEN COMPARING
utt_0043 utt 269.37 282.33 -X DIFFERENT MODELS ON THE SAME TASK AND DATA. USING MM-SHAP ON SAMPLE-LEVEL, WE SEE HOW IMAGE AND TEXT TOKENS CONTRIBUTE TOWARDS THE PREDICTED ninety-nine% IMAGE-SENTENCE ALIGNMENT SCORE IN BLUE AND TOKENS
utt_0045 utt 282.33 293.42 -X THAT CONTRIBUTED AGAINST, IN RED. INTRODUCING AN ERROR IN THE CAPTION DECREASES THE ALIGNMENT SCORE AND THE TOKEN CONTRIBUTIONS REFLECT THE MISALIGNMENT, WITH THE TOKEN “KEYBOARD” SHOWING
utt_0047 utt 293.42 305.33 -X NEGATIVE CONTRIBUTION. AT DATASET- AND TASK-LEVEL WE SEE THAT MODELS SUCH AS CLIP ARE MULTIMODALLY BALANCED, WHILE ALBEF MODELS ARE MORE TEXTUALLY FOCUSED. THIS TREND HOLDS OVER VQA DATA AS WELL.
utt_0049 utt 305.33 316.08 -X ON THE VALSE BENCHMARK, TESTING MODELS WITH FINE-GRAINED MISMATCHES, WE SEE EVEN BETTER THAT CLIP IS BALANCED, LXMERT HAS A HIGHER VISUAL PREFERENCE, WHILE ALBEF MODELS ARE MORE TEXTUAL,
utt_0051 utt 316.08 329.01 -X SHOWING THAT DIFFERENT VL MODELS BEHAVE DIFFERENTLY ON THE SAME TASK. FINETUNING ALSO AFFECTS MULTIMODAL CONTRIBUTIONS AND WITH MM-SHAP, WE CAN MEASURE HOW MUCH, EVEN IN CASES
utt_0053 utt 329.01 335.28 -X WHERE TASK ACCURACY IS VERY LOW AND ACCURACY-BASED METRICS WOULD FAIL. THROUGHOUT OUR ANALYSIS,
utt_0054 utt 335.28 348.37 -X WE CONSISTENTLY PLOTTED BOTH ACCURACY AND MM-SHAP SCORES, BECAUSE WE ENVISION MM-SHAP TO COMPLEMENT ACCURACY, NOT REPLACE IT, SINCE WE ARE OF COURSE INTERESTED IN HOW WELL MODELS SOLVE A TASK
utt_0056 utt 348.37 359.99 -X BUT IT’S ONLY BY USING ACCURACY-AGNOSTIC METRICS SUCH AS MM-SHAP THAT WE CAN MEASURE HOW MUCH MODELS RELY ON EACH MODALITY. WE INVITE YOU TO READ OUR PAPER FOR MORE DETAILS.
utt_0058 utt 360.63 370.36 -X THANKS FOR STICKING UNTIL THE VERY END OF THIS, IT’S GREAT THAT SOMEONE IS INTERESTED IN THE WORK I DO FOR MY PHD. YOU KNOW, IT’S NOT MUCH, BUT IT IS HONEST WORK. IF YOU LIKED THIS,
utt_0060 utt 370.36 379.82 -X MAYBE YOU WANT TO WATCH OTHER VIDEOS ON OUR CHANNEL. IF YOU LIKE WORK FROM LARGE COMPANIES WITH ENORMOUS COLLABORATIONS, MAYBE YOU WANT TO CHECK OUT VELO TO SEE HOW NEURAL NETWORKS
utt_0062 utt 379.82 386.20 -3.7979 CAN LEARN TO ACT AS OPTIMIZERS FOR OTHER NETWORKS – POSSIBLY REPLACING ADAM. OR IF YOU WOULD RATHER
