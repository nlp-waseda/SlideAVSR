utt_0000 utt 1.45 2.03 -X HI!
utt_0001 utt 2.12 8.05 -X I'M LIMING JIANG, A PHD STUDENT FROM S-LAB, NANYANG TECHNOLOGICAL UNIVERSITY.
utt_0002 utt 8.33 10.13 -X I'LL PRESENT OUR PAPER:
utt_0003 utt 10.22 17.58 -X DECEIVE D: ADAPTIVE PSEUDO AUGMENTATION FOR GAN TRAINING WITH LIMITED DATA.
utt_0004 utt 19.40 32.72 -X WHILE STATE-OF-THE-ART GANS LIKE STYLEGANtwo ARE CONSTANTLY PUSHING FORWARD THE FIDELITY AND RESOLUTION OF SYNTHESIZED IMAGES, THEY USUALLY REQUIRE A LARGE AMOUNT OF TRAINING DATA TO FULLY UNLEASH THEIR POWER.
utt_0007 utt 33.33 38.26 -X TRAINING GANS WITH INSUFFICIENT DATA TEND TO GENERATE POOR-QUALITY IMAGES.
utt_0008 utt 38.86 52.11 -X AS SHOWN IN THIS FIGURE, STYLEGANtwo SYNTHESIZED RESULTS DETERIORATE GIVEN ONLY THOUSANDS OF IMAGES ON VARIOUS DATASETS, SUCH AS HUMAN FACES, CAT FACES, AND ANIME.
utt_0010 utt 52.11 59.41 -X COLLECTING SUFFICIENT DATA SAMPLES FOR CONTEMPORARY GANS IS SOMETIMES INFEASIBLE,
utt_0011 utt 59.41 64.08 -X ESPECIALLY IN DOMAINS WHERE DATA ARE SPARSE AND PRIVACY-SENSITIVE.
utt_0012 utt 64.88 77.81 -X TO EASE THE PRACTICAL DEPLOYMENT OF POWERFUL GANS, IT IS NECESSARY TO DEVISE NEW STRATEGIES FOR TRAINING GANS WITH LIMITED DATA WHILE PRESERVING THE QUALITY OF SYNTHESIS.
utt_0014 utt 82.19 91.76 -X WE ANALYZE THE UNDERLYING CAUSE THAT IMPEDES EFFECTIVE GAN TRAINING ON LIMITED DATA AND DRAW CONSISTENT CONCLUSIONS WITH SOME RECENT STUDIES.
utt_0016 utt 92.43 99.19 -X IN THIS FIGURE, WE SHOW THE TRAINING SNAPSHOTS OF TWO STYLEGANtwo MODELS ON THE FFHQ DATASET.
utt_0017 utt 99.44 105.36 -X THE SETTINGS OF THESE TWO MODELS DIFFER ONLY BY THE AMOUNT OF TRAINING DATA AVAILABLE TO THEM.
utt_0018 utt 105.87 118.71 -X AS CAN BE OBSERVED, BOTH TRAINING PROCESSES START SMOOTHLY, AND DISTRIBUTIONS OF DISCRIMINATOR OUTPUTS FOR THE REAL AND GENERATED IMAGES OVERLAP AT THE EARLY STAGE.
utt_0020 utt 119.73 125.72 -X AS THE TRAINING PROGRESSES, THE DISCRIMINATOR, WHICH ONLY HAS ACCESS TO LIMITED DATA,
utt_0021 utt 125.94 130.42 -X EXPERIENCES DIVERGED PREDICTIONS MUCH MORE RAPIDLY,
utt_0022 utt 131.76 136.09 -X AND THE AVERAGE SIGN BOUNDARY TURNS OUT TO BE MORE APPARENT.
utt_0023 utt 136.85 146.81 -X THIS DIVERGENCE IN PREDICTION SHOWS THAT THE DISCRIMINATOR BECOMES INCREASINGLY CONFIDENT IN CLASSIFYING REAL AND FAKE SAMPLES.
utt_0025 utt 148.31 156.89 -X MEANWHILE, THE EVALUATION FID SCORES DETERIORATE, CONSISTENT WITH THE DIVERGENCE OF DISCRIMINATOR PREDICTIONS.
utt_0026 utt 158.26 165.78 -X THESE PHENOMENA DEMONSTRATE HOW A DISCRIMINATOR GETS OVERFITTED QUICKLY UNDER LIMITED DATA.
utt_0027 utt 165.78 170.42 -X THE OVERFITTING OF DISCRIMINATOR IMPEDES THE GENERATOR'S CONVERGENCE,
utt_0028 utt 170.93 174.42 -X RENDERING SEVERE INSTABILITY OF TRAINING DYNAMICS.
utt_0029 utt 175.28 179.06 -X ITS FEEDBACK TO THE GENERATOR BECOMES LESS INFORMATIVE.
utt_0030 utt 180.11 188.66 -X CONSEQUENTLY, THE GENERATOR CONVERGES TO AN INFERIOR POINT, COMPROMISING THE QUALITY OF SYNTHESIZED IMAGES.
utt_0031 utt 192.69 203.26 -X MANY RECENT STUDIES PROPOSE TO APPLY STANDARD DATA AUGMENTATIONS FOR GAN TRAINING TO ENRICH THE DATASET DIVERSITY TO MITIGATE THE OVERFITTING OF THE DISCRIMINATOR.
utt_0033 utt 203.67 210.41 -X FOR INSTANCE, ADAPTIVE DISCRIMINATOR AUGMENTATION (ADA) ADOPTS DATA AUGMENTATIONS
utt_0034 utt 210.45 212.95 -X TO BOTH REAL AND GENERATED IMAGES,
utt_0035 utt 213.43 220.50 -X AND IT FURTHER DEVISES AN ADAPTIVE APPROACH TO CONTROL THE STRENGTH OF AUGMENTATIONS.
utt_0036 utt 221.04 225.69 -X OUR WORK EXTENDS THE STUDY OF SUCH AN ADAPTIVE APPROACH.
utt_0037 utt 226.55 234.65 -X MEANWHILE, OUR METHOD DOES NOT RELY ON ANY EXTERNAL AUGMENTATIONS THAT REQUIRE CERTAIN COMPUTATIONAL COST.
utt_0038 utt 236.02 239.39 -X ANOTHER TYPE OF SOLUTION IS MODEL REGULARIZATION.
utt_0039 utt 240.18 245.66 -X PREVIOUS RELEVANT STRATEGIES ON REGULARIZING GANS ARE DESIGNED FOR VARIOUS GOALS,
utt_0040 utt 245.66 249.31 -X EITHER FOR STABILIZING TRAINING OR PREVENTING MODE COLLAPSE.
utt_0041 utt 250.23 256.03 -X UNDER THE LIMITED DATA SETTING, A VERY RECENT STUDY PROPOSES AN LC-REGULARIZATION TERM
utt_0042 utt 256.41 262.91 -X TO REGULATE THE DISCRIMINATOR PREDICTIONS USING TWO EXPONENTIAL MOVING AVERAGE VARIABLES
utt_0043 utt 263.61 268.31 -X THAT TRACK THE DISCRIMINATOR OUTPUTS THROUGHOUT TRAINING.
utt_0044 utt 268.37 276.76 -X COMPARED TO MODEL REGULARIZATION, OUR METHOD IS MORE ADAPTIVE TO FIT DIFFERENT SETTINGS AND TRAINING STATUS WITHOUT MANUAL TUNING.
utt_0046 utt 278.49 282.75 -X ADDRESSING THE DISCRIMINATOR OVERFITTING IS STILL AN OPEN PROBLEM.
utt_0047 utt 283.09 293.98 -X WE THUS PROVIDE AN ALTERNATIVE WAY FROM A DIFFERENT PERSPECTIVE, WHICH IS ALSO COMPLEMENTARY TO STANDARD DATA AUGMENTATIONS FOR GAINING A FURTHER PERFORMANCE BOOST.
utt_0049 utt 297.24 303.52 -X WE PROPOSE A SIMPLE YET EFFECTIVE METHOD CALLED ADAPTIVE PSEUDO AUGMENTATION (APA).
utt_0050 utt 305.82 311.74 -X IN GAN'S ADVERSARIAL TRAINING, THE GOAL OF THE GENERATOR G IS TO DECEIVE THE DISCRIMINATOR D
utt_0051 utt 311.80 316.48 -X AND MAXIMIZE THE PROBABILITY THAT D MAKES A WRONG JUDGMENT.
utt_0052 utt 317.02 323.80 -X THEREFORE, G KEEPS REFINING ITS GENERATED SAMPLES TO BETTER DECEIVE D OVER TIME.
utt_0053 utt 325.34 332.32 -X WHEN THE TRAINING ONLY ACCESSES A LIMITED AMOUNT OF DATA, ONE WOULD OBSERVE THAT
utt_0054 utt 332.50 337.37 -X D TURNS OUT TO BE OVERLY CONFIDENT AND HARDLY MAKES ANY MISTAKE,
utt_0055 utt 337.75 341.63 -X CAUSING ITS FEEDBACK TO G TO BECOME MEANINGLESS.
utt_0056 utt 342.52 351.26 -X WE FIND THAT THE GENERATOR ITSELF NATURALLY POSSESSES THE CAPABILITY TO COUNTERACT DISCRIMINATOR OVERFITTING.
utt_0057 utt 353.56 358.08 -X WE EMPLOY A GAN TO AUGMENT ITSELF USING THE GENERATED SAMPLES.
utt_0058 utt 358.52 371.07 -X SPECIFICALLY, OUR METHOD TAKES THE FAKE/PSEUDO SAMPLES SYNTHESIZED BY G AND MODERATELY FEEDS THEM INTO THE LIMITED REAL DATA.
utt_0060 utt 371.07 375.61 -X SUCH PSEUDO DATA ARE PRESENTED TO D AS THE REAL INSTANCES.
utt_0061 utt 376.86 387.84 -X THE GOAL OF THIS PSEUDO AUGMENTATION IS NOT TO ENLARGE THE REAL DATASET BUT TO SUPPRESS THE DISCRIMINATOR'S CONFIDENCE IN DISTINGUISHING REAL AND FAKE DISTRIBUTIONS.
utt_0063 utt 388.99 399.39 -X HOWEVER, BLINDLY PRESENTING THE FAKE IMAGES AS REALS TO D MAY WEAKEN THE FUNDAMENTAL ABILITY OF D IN ADVERSARIAL TRAINING.
utt_0065 utt 401.60 408.58 -X TO MODERATE THE DECEPTION, WE PERFORM THE PSEUDO AUGMENTATION BASED ON A PROBABILITY P
utt_0066 utt 408.70 411.46 -X THAT QUANTIFIES THE DECEPTION STRENGTH.
utt_0067 utt 412.92 418.24 -X WE NOTE THAT THE OVERFITTING STATE OF D IS DYNAMIC DURING TRAINING.
utt_0068 utt 418.78 426.30 -X IT IS INTUITIVE TO LET THE DECEPTION PROBABILITY P BE ADJUSTED ADAPTIVELY BASED ON THE DEGREE OF OVERFITTING.
utt_0069 utt 428.28 438.43 -X TO QUANTIFY OVERFITTING, WE STUDY A SERIES OF OVERFITTING HEURISTICS ùúÜ DERIVED FROM THE DISCRIMINATOR RAW OUTPUT LOGITS.
utt_0071 utt 439.29 450.66 -X APPLYING THE OVERFITTING HEURISTIC, THE DECEPTION STRENGTH P CAN BE ADAPTIVELY CONTROLLED WITHOUT MANUAL TUNING REGARDLESS OF DATA SCALES AND PROPERTIES.
utt_0073 utt 454.53 460.10 -X WE PROVIDE THREE PLAUSIBLE VARIANTS OF THE OVERFITTING HEURISTICS ùúÜ.
utt_0074 utt 462.65 469.95 -X HERE, ùúÜ_R ESTIMATES THE PORTION OF REAL IMAGES THAT OBTAIN POSITIVE LOGIT PREDICTIONS BY D,
utt_0075 utt 470.81 474.34 -X AND THAT OF GENERATED IMAGES IS CAPTURED BY ùúÜ_F.
utt_0076 utt 476.06 485.28 -X BESIDES, ùúÜ_RF INDICATES HALF OF THE DISTANCE BETWEEN THE SIGNS OF THE REAL AND FAKE LOGITS.
utt_0077 utt 487.68 495.27 -X FOR ALL THESE HEURISTICS, ùúÜ=zero REPRESENTS NO OVERFITTING, AND ùúÜ=one MEANS COMPLETE OVERFITTING.
utt_0078 utt 496.29 499.08 -X WE USE ùúÜ_R BY DEFAULT.
utt_0079 utt 500.54 505.35 -X DURING TRAINING, P IS ADJUSTED ADAPTIVELY ACCORDING TO ùúÜ.
utt_0080 utt 506.50 510.40 -X WE INITIALIZE P TO ZERO AND SET A THRESHOLD VALUE FOR ùúÜ.
utt_0081 utt 512.10 516.80 -X IF ùúÜ SIGNIFIES TOO MUCH/LITTLE OVERFITTING REGARDING THE THRESHOLD,
utt_0082 utt 517.02 520.93 -X THAT IS TO SAY, LARGER/SMALLER THAN THE THRESHOLD,
utt_0083 utt 521.41 528.26 -X WE WILL INCREMENT/DECREMENT P BY ONE FIXED STEP AND CLAMP IT FROM BELOW TO ZERO.
utt_0084 utt 529.44 541.12 -X WE ALSO PROVIDE A THEORETICAL ANALYSIS OF THE PROPOSED APA AND CONNECT IT WITH MINIMIZING THE JS DIVERGENCE BETWEEN THE SMOOTHED DATA DISTRIBUTION AND GENERATED DISTRIBUTION,
utt_0086 utt 541.22 543.84 -X PROVING ITS CONVERGENCE AND RATIONALITY.
utt_0087 utt 544.32 547.08 -X DETAILS CAN BE FOUND IN OUR PAPER.
utt_0088 utt 551.87 555.97 -X THEN, LET'S BRIEFLY GO THROUGH SOME TYPICAL EXPERIMENTAL RESULTS.
utt_0089 utt 556.74 566.76 -X WE SHOW THE EFFECTIVENESS OF APA ON VARIOUS DATASETS WITH LIMITED DATA AMOUNTS AND THE COMPARATIVE RESULTS BY STATE-OF-THE-ART STYLEGANtwo.
utt_0091 utt 568.07 573.73 -X THE QUALITY OF IMAGES SYNTHESIZED BY STYLEGANtwo DEGRADES UNDER LIMITED DATA.
utt_0092 utt 574.50 578.76 -X RIPPLE ARTIFACTS APPEAR ON THE CAT FACES AND HUMAN FACES,
utt_0093 utt 578.88 583.11 -X AND THE FACIAL FEATURES OF THE ANIME FACES ARE MISPLACED.
utt_0094 utt 584.07 587.11 -X ON THE BIRD DATASET WITH HEAVY BACKGROUND CLUTTER,
utt_0095 utt 587.20 592.36 -X THE GENERATED IMAGES ARE COMPLETELY DISTORTED ALTHOUGH TRAINED WITH MORE DATA.
utt_0096 utt 592.74 599.94 -X THE PROPOSED APA SIGNIFICANTLY AMELIORATES IMAGE QUALITY ON ALL THESE DATASETS,
utt_0097 utt 599.94 602.92 -X PRODUCING MUCH MORE PHOTOREALISTIC RESULTS.
utt_0099 utt 610.57 613.13 -X APA IMPROVES THE IMAGE QUALITY IN ALL CASES.
utt_0100 utt 613.13 619.24 -X NOTABLY, THE QUALITY OF SYNTHESIZED IMAGES BY APA ON fiveK AND sevenK DATA IS VISUALLY
utt_0101 utt 619.49 626.06 -X CLOSE TO STYLEGANtwo RESULTS ON THE FULL DATASET WHILE WITH AN ORDER OF MAGNITUDE FEWER TRAINING SAMPLES.
utt_0103 utt 630.02 634.89 -X THE DIVERGENCE OF STYLEGANtwo DISCRIMINATOR PREDICTIONS CAN BE EFFECTIVELY RESTRICTED
utt_0107 utt 646.56 651.78 -X BESIDES, APA IMPROVES THE TRAINING CONVERGENCE OF STYLEGANtwo ON LIMITED DATA,
utt_0108 utt 651.78 653.06 -X SHOWN BY THE FID CURVES.
utt_0109 utt 653.06 660.17 -X WE COMPARE THE PROPOSED APA WITH OTHER STATE-OF-THE-ART SOLUTIONS DESIGNED FOR GAN TRAINING WITH LIMITED DATA.
utt_0111 utt 660.17 668.91 -X LC-REGULARIZATION SLIGHTLY IMPROVES THE SYNTHESIS QUALITY OF STYLEGANtwo BY REDUCING DISTORTIONS ON THE IMAGES.
utt_0113 utt 668.91 672.36 -X THE AMELIORATION OF VISUAL QUALITY IS MORE EVIDENT BY APPLYING ADA,
utt_0114 utt 672.36 677.96 -X WHERE THE RIPPLE ARTIFACTS ARE CLEARLY SUBSIDED WHILE SOME MINOR ARTIFACTS EXIST ON THE HAIR AND BEARD.
utt_0116 utt 682.99 686.89 -X NOTABLY, APA IS ALSO COMPLEMENTARY TO ADA FOR GAINING A FURTHER PERFORMANCE BOOST,
utt_0118 utt 694.25 697.29 -X WE ALSO COMPARE THE COMPUTATIONAL COST OF APA AGAINST ADA.
utt_0119 utt 697.29 701.58 -X THERE IS NO PARAMETER OR MEMORY INCREMENT FOR BOTH METHODS.
utt_0120 utt 701.58 709.13 -X THE AVERAGE TRAINING TIME WE PRESENT HERE INDICATES THE ADVANTAGE OF THE PROPOSED APA ON NEGLIGIBLE COMPUTATIONAL COST.
utt_0122 utt 709.42 718.00 -X HERE ARE SOME HIGHER-RESOLUTION EXAMPLES TO FURTHER ILLUSTRATE THE EFFECTIVENESS OF OUR APPROACH IN IMPROVING STYLEGANtwo WITH LIMITED TRAINING DATA.
utt_0124 utt 718.00 722.76 -X FINALLY, WE PRESENT ADDITIONAL TRAINING CONVERGENCE VISUALIZATIONS ON FFHQminus sevenK.
utt_0125 utt 723.72 729.77 -X THE PROPOSED APA EFFECTIVELY IMPROVES THE TRAINING CONVERGENCE OF STYLEGANtwo ON LIMITED DATA.
utt_0126 utt 729.77 737.71 -X FOR MORE DETAILS SUCH AS QUANTITATIVE RESULTS, PERFORMANCE ON BIGGAN, AND ABLATION STUDIES,
utt_0127 utt 737.71 741.26 -X PLEASE REFER TO OUR PAPER AND SUPPLEMENTARY MATERIAL.
utt_0128 utt 741.26 743.63 -X OUR CODE WILL BE MADE PUBLICLY AVAILABLE.
utt_0129 utt 743.63 744.59 -5.6795 THANK YOU FOR WATCHING!
