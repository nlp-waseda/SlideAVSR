utt_0000 utt 0.10 8.77 -X HELLO EVERYONE, TODAY I'M GOING TO TALK TO YOU ABOUT OUR WORK CONTINUAL SELF-TRAINING WITH BOOTSTRAPPED REMIXING FOR SPEECH ENHANCEMENT PROBLEMS
utt_0002 utt 9.04 16.85 -X THIS IS A JOINT WORK WITH YOSSI ADI, VAMSI ITHAPU, BUYE XU AND ANURAG KUMAR.
utt_0003 utt 17.36 28.87 -X SO, SINGLE CHANNEL SPEECH ENHANCEMENT IS EFFECTIVELY A SOURCE SEPARATION PROBLEM WHERE ONE WANTS TO EXTRACT THE SPEECH TARGET FROM A MIXTURE OF SOUNDS. STATE-OF-THE-ART SYSTEMS
utt_0005 utt 28.87 41.81 -X SOURCE SEPARATION SYSTEMS LOOK LIKE THIS. WHERE THEY OPERATE ON SPECTROGRAM-LIKE REPRESENTATIONS AND THEY OPTIMIZE A TIME-DOMAIN LOSS. SO, THE PROBLEM WITH ALL THOSE SYSTEMS IS THAT THEY
utt_0007 utt 41.81 47.76 -X NEED LOTS AND LOTS OF SUPERVISED IN-DOMAIN DATA TO TRAIN ON AND OF COURSE AS YOU CAN IMAGINE,
utt_0008 utt 47.95 60.50 -X IT'S ALMOST IMPOSSIBLE TO GATHER CLEAN GROUND TRUTH SOURCE WAVEFORMS FOR REAL-WORLD MIXTURES SO, THE GOAL IS TO LEVERAGE IN THE WILD MIXTURE DATA IN ORDER TO BE ABLE TO SCALE TO MISMATCHED
utt_0010 utt 60.50 73.40 -X ENVIRONMENTAL CONDITIONS SUCH AS DIFFERENT INPUT SIGNAL-TO-NOISE RATIOS (SNRS), DIFFERENT NOISE SOURCES, SPEAKERS ETC AND WE NEED ALL THAT IN ORDER TO MAKE SPEECH DENOISING
utt_0012 utt 73.87 87.03 -X SYSTEMS UNIVERSAL IN ORDER TO WORK WITH ANY HEADSET AND ANY PHONE AND ANY ENVIRONMENT AND ALSO MAKE REALISTIC AND MORE IMMERSIVE AUDIO VIRTUAL- REALITY ENVIRONMENTS. IN A CONVENTIONAL SUPERVISED
utt_0014 utt 87.03 93.62 -X TRAINING RECIPE, WE SYNTHETICALLY GENERATE MIXTURES BY MIXING A CLEAN SPEED SAMPLE AND A CLEAN
utt_0015 utt 94.00 106.55 -X NOISE WAVEFORM WE LET THE SUPERVISED MODEL ESTIMATE THE SOURCES HERE AND REGRESS OVER THE TARGETS. MIXTURE INVARIANT TRAINING IS A SELF-SUPERVISED APPROACH FOR SOURCE SEPARATION WHERE
utt_0017 utt 106.55 111.73 -X IT SYNTHETICALLY GENERATES MIXTURES OF MIXTURES IN THE SPEECH ENHANCING CASE, WE
utt_0018 utt 113.94 121.91 -X MIX A NOISY SPEECH SAMPLE WITH THE CLEAN NOISE WAVEFORM AND WE LET THE MODEL ESTIMATE ALL OF
utt_0019 utt 121.91 130.12 -X THE SOURCES HERE AND WE ASSIGN THE SOURCES BACK TO THE INPUT MIXTURES. HERE WE CAN ALSO
utt_0020 utt 130.13 138.13 -X OPTIONALLY INJECT SOME EXTRA NOISE BUT THIS, AS WE WILL SEE, LEADS TO POOR OUT OF DOMAIN
utt_0021 utt 138.13 147.03 -X GENERALIZATION ON UNSEN MIXTURES. THIS PROBLEM COMES IN BECAUSE EFFECTIVELY WE DON'T KNOW
utt_0022 utt 147.35 154.29 -X IF THE NOISE SOURCES, THE NOISE DATA DATASET IN-HAND CAN CLOSELY CAPTURE THE REAL NOISE DISTRIBUTION
utt_0023 utt 154.67 162.33 -X OVER HERE AND ALSO THIS ARTIFICIAL MIXING PROCESS IT IS ARTIFICIALLY CHANGING
utt_0024 utt 162.33 176.44 -X THE INPUT MIXTURE DISTRIBUTION. AND WHAT YOU SEE HERE IS THAT AS LONG AS YOU HAVE IN-DOMAIN OR SIMILAR TO IN-DOMAIN NOISE, YOU DO QUITE WELL. CLOSELY MATCHING THE SUPERVISED
utt_0026 utt 176.44 189.08 -X PERFORMANCE. BUT ONCE YOU GO TO A SLIGHTLY MISMATCHED OUT OF DOMAIN DATASET THE PERFORMANCE DROPS SIGNIFICANTLY. SO, WHAT WE PROPOSE, WE CALL OUR METHOD REMIXIT, IS THAT WE
utt_0028 utt 189.11 203.59 -X BOOTSTRAP THE MIXING PROCESS USING A STUDENT-TEACHER FRAMEWORK AND WE CONTINUALLY REFINE THE TEACHER ESTIMATES TO MAKE THE STUDENT EVEN BETTER WITHOUT THE NEED OF IN-DOMAIN WAVEFORMS
utt_0030 utt 203.59 212.54 -X NEITHER SPEECH OR NOISE. SO REMIXIT IN A NUTSHELL STARTS FROM A NOISY SPEECH DATASET WHERE WE GRAB
utt_0031 utt 213.11 226.73 -X A BATCH OF MIXTURES, AS WE SEE HERE, NOISY MIXTURES AND WE HAVE A PRE-TRAINED TEACHER ON SOME KIND OF OUT-OF-DOMAIN DATA. COULD BE MIX-IT OR SUPERVISED ON OUT OF DOMAIN DATA
utt_0033 utt 226.81 240.81 -X AND WE ESTIMATE THE TEACHER NOISE ESTIMATES AND THE SPEECH ESTIMATES AND BY THE WAY WE CAN ALSO OPTIMIZE FINE-TUNE THIS ALONGSIDE THE WHOLE NETWORK BUT WE SAW THAT IN OUR EXPERIMENTS THAT
utt_0035 utt 240.92 246.47 -X IT'S ENOUGH TO JUST KEEP IT FROZEN AND WE USE THE TEACHER SPEECH ESTIMATES
utt_0036 utt 247.41 254.52 -X ALONGSIDE THE PERMUTED NOISE ESTIMATES NOW SO WE TAKE THE NOISE ESTIMATES AND WE PERMUTE THEM WE
utt_0037 utt 255.09 265.82 -X GENERATE NEW BOOTSTRAPPED MIXTURES BY REMIXING THEM TOGETHER AS WE SEE HERE AND WE FEED THOSE BOOTSTRAPPED MIXTURES TO THE STUDENT MODEL THAT ESTIMATES SOME NOISE ESTIMATES AND SOME SPEECH ESTIMATES
utt_0039 utt 265.98 275.39 -X AND USES THE TEACHER'S PERMUTED SOURCES AS PSEUDO TARGETS ONE MORE ASPECT IS THAT WE CAN
utt_0040 utt 275.39 288.75 -X REFINE THE TEACHER ESTIMATES OVER DIFFERENT ROUNDS OVER DIFFERENT OPTIMIZATION STEPS BY USING A SEQUENTIAL UPDATED TEACHER OR RUNNING MEAN UPDATED TEACHER PROTOCOL USING THE LATEST STUDENTS WEIGHTS
utt_0042 utt 289.98 295.90 -X IN OUR EXPERIMENTS WE USE A VARIETY OF DATA COLLECTIONS INCLUDING THE DNS CHALLENGE AND
utt_0043 utt 296.18 307.68 -X LIBRIFSDfiftyK WHICH ARE FAIRLY LARGE AND DIVERSE WE ALSO USE WHAM! AS AN EXAMPLE OF A REALLY MISMATCHED SCENARIO WHERE WE HAVE TWO NOISE SOURCES PER MIXTURE AND
utt_0045 utt 307.74 314.62 -X A VERY RESTRICTIVE SET OF NOISE CLASSES OF URBAN SOUNDS AND ALSO WE USE VCTK
utt_0046 utt 318.23 330.29 -X SO, WE SEE HERE THAT WE USE AS A SEPARATION MODEL THE SUDO RM -RF WHICH IS ABLE TO OBTAIN STATE-OF-THE-ART RESULTS FOR SOURCE SEPARATION PROBLEMS WITH MUCH LESS MEMORY REQUIREMENTS
utt_0048 utt 330.29 342.65 -X AND MUCH LESS LATENCY BY USING ALSO MIXTURE CONSISTENCY AT THE OUTPUT AS A TIME DOMAIN LOSS WE USE THE SCALE INVARIANT SIGNAL TO DISTORTION RATIO
utt_0050 utt 343.80 350.73 -X WITH EQUAL WEIGHT FOR THE NOISE AND THE SPEECH SLOTS. IN OUR EXPERIMENTS WE
utt_0051 utt 350.97 363.17 -X WE SEE THAT WE CAN PERFORM SELF-SUPERVISED TRAINING STARTING FROM A PRE-TRAINED MIXIT MODEL TRAINED ON SOME KIND OF OUT OF DOMAIN DATA AND IN THIS CASE IT'S IMPORTANT TO UNDERSTAND THAT
utt_0053 utt 363.20 373.44 -X WE USE NO IN-DOMAIN CLEAN NOISE RECORDINGS WE CAN ALSO USE THE SAME OUR METHOD FOR PERFORMING
utt_0054 utt 373.44 386.66 -X SEMI-SUPERVISED DOMAIN ADAPTATION WHERE WE START FROM ANOTHER DOMAIN SUPERVISED TEACHER BUT ON SOME OTHER DATA AND WE CAN UPDATE THIS TEACHER GRADUALLY AND REFINE ITS
utt_0056 utt 388.41 395.14 -X ITS ESTIMATES AND AS THE LAST EXPERIMENT WE CAN PERFORM ZERO-SHOT DOMAIN ADAPTATION USING
utt_0057 utt 396.03 399.74 -X AN EXPONENTIAL MOVING AVERAGE TEACHER WHERE THE GOAL IS NOW IS
utt_0058 utt 400.22 407.52 -X IS TO FINE-TUNE OUR MODEL, OUR STUDENT MODEL, BUT WE DON'T HAVE LOTS OF MIXTURES TO
utt_0059 utt 407.52 415.35 -X LEVERAGE SO WE WANT A SLIGHTLY DIFFERENT APPROACH FOR LOW-RESOURCE DATA SETS
utt_0060 utt 419.10 431.81 -X SOME EXPERIMENTS SHOW THAT WE HAVE A CONSISTENT IMPROVEMENT WHEN WE USE REMIXIT OVER THE TEACHER MODEL AS WE SEE HERE AND THIS HOLDS FOR SUPERVISED TEACHERS
utt_0062 utt 432.73 441.22 -X AND THE MIXIT TEACHERS. WHEN WE GO FROM THE LIBRIFSDfiftyK TO THE DNS CHALLENGE
utt_0063 utt 444.45 458.18 -X ANOTHER IMPORTANT ASPECT IS THAT WE CLOSE THE PERFORMANCE GAP WHEN WE USE OUR METHOD BETWEEN THE SELF-SUPERVISED METHODS AND THE STATE-OF-THE-ART
utt_0065 utt 460.51 466.12 -X NUMBER WHICH IS OBTAINED BY USING SUPERVISED IN-DOMAIN TRAINING
utt_0066 utt 467.46 474.95 -X AND GOING BACK TO THE COMPARISON WITH MIXIT WE SAY THAT ALTHOUGH REMIXIT DOES NOT USE ANY
utt_0067 utt 474.95 483.78 -X IN-DOMAIN DATA AS WE AS WE SEE HERE SO IT DOES NOT USE ANY IN DOMAIN DNS DATA IT IS ABLE TO
utt_0068 utt 484.54 491.14 -X USE TO YIELD A SIGNIFICANT IMPROVEMENT OVER THE PREVIOUS STATE-OF-THE-ART SELF-SUPERVISED METHOD
utt_0069 utt 493.31 503.88 -X ANOTHER INTERESTING ASPECT IS THAT BY USING THIS KIND OF SEQUENTIAL TEACHER UPDATE WE CAN ALSO MAKE THE STUDENT GRADUALLY DEEPER AND DEEPER
utt_0071 utt 504.22 511.13 -X SO EVEN MORE EXPRESSIVE AND WE SEE THAT WE CAN CONTINUE TO LEARN BETTER
utt_0072 utt 512.32 516.97 -X INSTEAD OF HAVING A STATIC TEACHER AND STUDENT WITH A STATIC
utt_0073 utt 518.15 532.33 -X NUMBER OF PROCESSING BLOCKS. SO, REPLACING THE OLD TEACHER WITH THE LATEST STUDENT WEIGHTS IS REALLY IMPORTANT TO OBTAIN BETTER RESULTS BY DOING COMPLETELY SELF-SUPERVISED TRAINING
utt_0075 utt 532.77 539.75 -X METHOD. FOR THE LAST EXPERIMENTS WE SEE THAT WE CAN PERFORM ZEROSHOT DOMAIN ADAPTATION AT DIFFERENT
utt_0076 utt 539.94 547.17 -X ADAPTATION TEST DATA SETS AND WE SEE THAT REMIXIT SHOWS SIGNIFICANT IMPROVEMENTS AGAINST ALL OF THE
utt_0077 utt 548.42 553.46 -X TEACHER MODELS THAT WE USE HERE WHICH IS LIKE WHICH IS LIKE CROSS DOMAIN
utt_0078 utt 554.40 561.83 -X AND THIS ALSO HOLDS FOR REALLY CHALLENGING SETUPS SUCH AS THE DNS CHALLENGE TEST SET
utt_0079 utt 563.49 572.58 -X WHERE WE ONLY HAVE LIKE one hundred and fifty MIXTURES SO A REALLY LOW RESEARCH DATA SET AND ALSO IN CASES WHERE WE HAVE
utt_0080 utt 572.58 580.07 -X A GREAT MISMATCH BETWEEN TRAINING AND THE ADAPTATION SUCH AS WHAM!
utt_0081 utt 580.26 584.58 -X SOME MORE EXPERIMENTS WHICH WE INVITE YOU TO
utt_0082 utt 584.61 589.70 -X SEE IN OUR LONGER VERSION OF THE PAPER WHICH IS AVAILABLE ONLINE
utt_0083 utt 591.36 605.39 -X IS HOW WELL WE CAN USE SELF-SUPERVISION FOR PERFORMING TARGET DOMAIN SPEECH ENHANCEMENT AND ALSO CROSS-DOMAIN GENERALIZATION AND WE SEE HERE WHEN WE GO FROM THE LIBRIFSDfiftyK
utt_0085 utt 606.02 615.32 -X TO THE DNS CHALLENGE. REMIXIT YIELDS AN IMPROVEMENT OF AROUND one point five DB FOR YOUR TARGET
utt_0086 utt 615.49 625.48 -X DOMAIN NUMBERS BUT IT IS QUITE SIGNIFICANT THAT WE CAN OBTAIN A REALLY REALLY GOOD six point three
utt_0087 utt 628.36 634.23 -X IMPROVEMENT ON CROSS-DOMAIN GENERALIZATION EVEN WITH SLIGHTLY MISMATCHED NOISE DISTRIBUTIONS
utt_0088 utt 634.28 648.65 -X BETWEEN UH THE LIBRIFSDfiftyK AND THE DNS CHALLENGE SO THIS PROBLEM BECOMES EVEN HARDER FOR MIXIT UH WHEN WHEN WE USE SEVERE NOISE MISMATCH CASES SUCH AS GOING FROM LIBRIFSDfiftyK TO WHAM!
utt_0090 utt 649.83 654.86 -X UH WHICH HAS A DIFFERENT SET OF NOISE SOURCES ACTIVE AND ALSO A DIFFERENT SET
utt_0091 utt 656.74 666.00 -X OF TYPES OF NOISES AND WE SEE HERE THAT REMIXIT OBTAINS ALMOST SIMILAR RESULTS AS BEFORE LIKE WITH
utt_0092 utt 666.00 672.43 -X SOME PERFORMANCE HIT BUT EVEN BETTER IN MOST OF THE CASES COMPARED TO IN-DOMAIN
utt_0093 utt 675.82 682.35 -X SELF-SUPERVISED TRAINING OF MIXIT WHILE MIXIT TOTALLY FAILS TO GENERALIZE
utt_0094 utt 683.02 691.05 -X TO SEVERE MISMATCHED DATA AND THERE IS A REASON FOR ALL THAT BECAUSE REMIXIT IS ABLE TO
utt_0095 utt 691.40 705.33 -X LEARN ROBUSTLY UNDER EXPECTATION. IF YOU TAKE A LOSS WHICH MINIMIZES THE Ltwo NORM OF THE ERROR YOU CAN SEE THAT YOU END UP HAVING LIKE THREE TERMS WHICH IS EFFECTIVELY THE
utt_0097 utt 705.33 711.34 -X SUPERVISED LOSS OF THE STUDENT ASSUMING THAT WE HAD ACCESS TO THE CLEAN SPEECH, WHICH WE DON'T HAVE,
utt_0098 utt 711.34 716.27 -X SOMETHING WHICH IS CONSTANT AND WE DON'T CARE ABOUT AND ALSO AN ERRORS' CORRELATION TERM BETWEEN
utt_0099 utt 716.39 721.61 -X THE STUDENT AND THE TEACHER ESTIMATES WITH RESPECT TO THE SAME GROUND TRUTH SOURCE
utt_0100 utt 721.96 729.20 -X SO IF YOU ANALYZE THE LATTER TERM, YOU END UP HAVING AN EMPIRICAL MEAN
utt_0101 utt 729.20 737.29 -X STUDENT ERROR HERE SO EFFECTIVELY IS THAT YOU MIX ALMOST THE SAME CLEAN SOURCE UNDER A DIFFERENT
utt_0102 utt 737.96 744.62 -X NOISE SOURCE DISTRIBUTION SO BY CREATING THIS THIS BOOTSTRAPPED MIXTURES. SO, WE SEE HERE FROM one
utt_0103 utt 747.15 754.02 -X TO sixty-four. WE SEE HOW WE OBTAIN A REALLY BIG IMPROVEMENT EVEN IN CASES WHERE YOUR TEACHER
utt_0104 utt 755.34 761.71 -X DOES NOT PERFORM A GOOD ESTIMATE AT ALL SO WE CAN PERFORM BETTER AND BETTER
utt_0105 utt 764.27 772.14 -X A FINAL COOL ASPECT FOR ALL THAT IS THAT YOU CAN ALSO PERFORM REMIXIT WITH AROUND
utt_0106 utt 772.14 779.92 -X thirty LINES OF CODE WHICH IS ACTUALLY THAT I FIT IT HERE. SO, THIS IS A PERFECTLY WORKING
utt_0107 utt 781.80 786.74 -X PYTORCH LOOP THAT YOU CAN USE FOR YOUR EXPERIMENTS AND MAKE YOUR SPEECH ENHANCEMENT
utt_0108 utt 786.89 800.40 -X MODELS BETTER IF YOU'RE INTERESTED IN. IN ORDER TO CONCLUDE REMIX IT IS THE FIRST SPEECH ENHANCEMENT METHOD WHICH WORKS COMPLETELY SELF-SUPERVISED WITHOUT THE NEED OF A SINGLE IN DOMAIN WAVEFORM
utt_0110 utt 800.78 814.74 -X IT ALSO OUTPERFORMS PREVIOUS STATE-OF-THE-ART METHODS WHICH REQUIRE IN DOMAIN DATA AND HELPS SIGNIFICANTLY WITH CROSS-DOMAIN GENERALIZATION WHICH IS REALLY IMPORTANT. ALSO,
utt_0112 utt 815.37 822.61 -X CAN BE USED SEAMLESSLY FOR SEMI-SUPERVISED DOMAIN ADAPTATION AND ZERO-SHOT LEARNING, WHEN WE HAVE
utt_0113 utt 822.61 831.60 -X LIKE A LOW RESOURCE DATA SET. REMIXIT TREATS THE TRAINING METHOD AS A LIFELONG PROCESS
utt_0114 utt 831.60 843.79 -X AND IS BASED ON BOOTSTRAPPING THE MIXING PROCESS LIKE HERE. ITERATIVELY REFINES THE ESTIMATES OF THE TEACHER AND ROBUSTLY LEARNS UNDER REALLY NOISY ESTIMATES
utt_0116 utt 844.78 851.81 -X REMIXIT CAN ALSO BE PAIRED, AS WE SAID, WITH THE END-TO-END TRAINING OF THE TEACHER AND
utt_0117 utt 855.02 867.56 -X PEOPLE HAVE USED REMIXIT FOR MORE GENERAL SOURCE OPERATION PROBLEMS AND WE CAN ALSO MAKE REMIXIT BETTER BY USING A CONFIDENCE-BASED MEASURE TO REFINE THE TEACHER
utt_0119 utt 867.56 876.28 -X ESTIMATES. SO, I WOULD LIKE TO THANK YOU ALL FOR ATTENDING THIS TALK AND WATCHING THE VIDEO
utt_0120 utt 876.98 883.76 -X WE'RE LOOKING FORWARD TO MEETING EVERYBODY IN THE Q&AMPA SESSION AND WE WELCOME YOU TO
utt_0121 utt 884.34 889.78 -4.5404 TO SEE THE LONGER VERSION OF OUR PAPER WHICH IS AVAILABLE ONLINE. THANK YOU VERY MUCH!
