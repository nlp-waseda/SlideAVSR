utt_0000 utt 4.72 5.42 -X HELLO,
utt_0001 utt 5.42 5.94 -X EVERYONE.
utt_0002 utt 5.94 11.06 -X IT IS MY GREAT PLEASURE TODAY TO TALK TO YOU ABOUT OUR RECENT WORK ON DISTILLING MASSIVE,
utt_0003 utt 11.06 12.05 -X MULTILINGUAL MODELS.
utt_0004 utt 12.52 14.26 -X MY NAME IS SUBHABRATA MUKHERJEE.
utt_0005 utt 14.26 17.58 -X THIS IS A JOINT WORK WITH AHMED HASAN AT MICROSOFT RESEARCH.
utt_0006 utt 20.56 21.94 -X IN RECENT TIMES,
utt_0007 utt 21.94 29.52 -X LANGUAGE MODEL PRE-TRAINING HAS BEEN ADOPTED FOR A NUMBER OF NATURAL LANGUAGE UNDERSTANDING TASKS WHERE THEY HAVE BEEN SHOWN TO HAVE SUPERIOR PERFORMANCE.
utt_0009 utt 29.74 38.58 -X IN THIS DEEP AND HUGE NEURAL NETWORKS ARE TRAINED ON LARGE AMOUNTS OF UNLABELED DATA WITH SELF-SUPERVISION OBJECTIVES.
utt_0012 utt 38.58 39.19 -X THEREAFTER,
utt_0013 utt 39.19 44.15 -X THE REPRESENTATIONS ARE FINE-TUNED ON TASK-SPECIFIC LABELED DATA FOR DOWNSTREAM TASKS.
utt_0015 utt 44.91 46.35 -X WITH THIS PRINCIPLE,
utt_0016 utt 46.35 48.85 -X A NUMBER OF LANGUAGE MODELS HAVE BEEN DEVELOPED,
utt_0017 utt 49.17 52.75 -X RANGING FROM GPT AND OUR IN-HOUSE TURING-NLG.
utt_0018 utt 53.93 58.77 -X WHILE THESE MODELS HAVE BEEN SUCCESSFUL IN OBTAINING SUPERIOR PERFORMANCE FOR SO MANY TASKS,
utt_0019 utt 58.77 62.58 -X THEIR HUGE MODEL SIZE IS A BIG CONCERN TO USE THEM IN PRACTICE.
utt_0020 utt 62.70 63.79 -X FOR INSTANCE,
utt_0021 utt 63.79 67.92 -X THE MOST RECENTLY RELEASED GPTminus three BOASTS OF A MASSIVE one hundred and seventy-five BILLION
utt_0022 utt 68.11 69.17 -X PARAMETERS.
utt_0023 utt 72.40 77.68 -X PRIOR WORK IN THE VISION COMMUNITY HAS ADDRESSED THIS SHORTCOMING USING A TECHNIQUE CALLED KNOWLEDGE DISTILLATION.
utt_0024 utt 78.45 81.01 -X THE OBJECTIVE HERE IS AS FOLLOWS. GIVEN A TEACHER MODEL,
utt_0025 utt 81.01 84.95 -X WHICH IS A HUGE PRE-TRAINED MODEL, WE WANT TO TRANSFER THE KNOWLEDGE TO A SHALLOW STUDENT MODEL.
utt_0026 utt 85.07 87.89 -X THE KNOWLEDGE TRANSFER TAKES PLACE OVER UNLABELED DATA,
utt_0027 utt 87.89 89.65 -X WHICH ACTS AS THE TRANSFER SET.
utt_0028 utt 90.19 95.16 -X NOW WE CAN USE THE TEACHER'S INTERNAL REPRESENTATIONS AS WELL AS TASK-SPECIFIC SOFT LOGITS
utt_0029 utt 95.25 97.08 -X TO GUIDE THE DISTILLATION PROCESS.
utt_0030 utt 99.02 99.89 -X GENERALLY,
utt_0031 utt 99.89 102.26 -X KNOWLEDGE DISTILLATION CAN HAVE THE FOLLOWING INFORMATION FLOW.
utt_0032 utt 102.51 110.74 -X YOU HAVE THE TEACHER. NOW YOU CAN CONTINUE PRE-TRAINING THE TEACHER MODEL FOR LANGUAGE MODEL FINE-TUNING ON TASK-SPECIFIC LABELED DATA.
utt_0035 utt 110.80 116.21 -X THIS MAKES IT AWARE OF THE DOMAIN SEMANTICS AND VOCABULARY FOR THE GIVEN TASK. NOW,
utt_0036 utt 116.21 125.27 -X GIVEN SOME AMOUNT OF LABELED DATA YOU CAN FINE-TUNE THE TEACHER MODEL FOR THE DOWNSTREAM TASK AND USE IT TO PRODUCE LOGITS AND REPRESENTATIONS ON THE UNLABELED DATA.
utt_0038 utt 125.27 129.62 -X THIS GIVES US THE AUGMENTED DATA THAT WE CAN USE TO TRANSFER KNOWLEDGE TO THE STUDENT MODEL
utt_0039 utt 129.87 131.38 -X VIA SOFT TRAINING.
utt_0040 utt 134.10 135.19 -X IN THIS WORK,
utt_0041 utt 135.19 137.01 -X WE FOCUS ON MULTI LINGUAL NER.
utt_0043 utt 137.01 142.55 -X AS OUR PRIMARY TASK, WE ARE GIVEN A NUMBER OF SENTENCES IN DIFFERENT LANGUAGES.
utt_0046 utt 142.87 151.99 -X EACH SENTENCE IS A SEQUENCE OF TOKENS. WE WANT TO ASSOCIATE A TAG TO EACH TOKEN. WE POSE IT AS A SEQUENCE LABELING TASK OR A SPAN DETECTION TASK.
utt_0048 utt 152.31 161.43 -X IN THIS WE WANT TO ASSOCIATE A SPAN BOUNDARY IN TERMS OF MARKERS LIKE BEGINNING OF THE SPAN, INTERMEDIATE TOKEN IN THE SPAN OR OUT-OF-SPAN BOUNDARY.
utt_0052 utt 161.59 166.84 -X AND WE WANT TO DEVELOP A SINGLE MODEL TO DO THIS TASK THAT WORKS SEAMLESSLY ACROSS ALL THE DIFFERENT LANGUAGES.
utt_0054 utt 167.54 168.31 -X FOR THIS,
utt_0055 utt 168.31 169.85 -X WE NEED A MULTILINGUAL ENCODER.
utt_0056 utt 169.97 170.97 -X IN THIS WORK,
utt_0057 utt 170.97 180.98 -X WE ADOPT MULTILINGUAL BERT AS A TEACHER MODEL THAT IS PRE-TRAINED ON one hundred and four LANGUAGES IN WIKIPEDIA. ALSO NOTE THAT MULTILINGUAL BERT DOES NOT USE ANY LANGUAGE ID FOR PRE-TRAINING.
utt_0061 utt 184.34 188.70 -X KNOWLEDGE DISTILLATION IN OUR CASE WORKS ON TWO DIFFERENT SEGMENTS OF THE DATA.
utt_0063 utt 188.70 189.08 -X FIRST,
utt_0064 utt 189.08 192.76 -X YOU HAVE TASK SPECIFIC LABELED DATA ON WHICH YOU CAN FINE-TUNE THE STUDENT MODEL.
utt_0065 utt 192.95 199.48 -X THE STUDENT IS A three-LAYER NEURAL NETWORK CONSISTING OF THE WORD EMBEDDING, THE RNN LAYER AND THE FINAL FEED-FORWARD NEURAL NETWORK LAYER.
utt_0067 utt 200.15 201.72 -X INSTEAD OF THE RNN STUDENT,
utt_0068 utt 201.72 205.05 -X YOU CAN ALSO USE A TRANSFORMER AS A STUDENT AND WE HAVE EXPERIMENTS IN THE PAPER.
utt_0069 utt 206.52 218.43 -X THE OTHER PART OF KNOWLEDGE DISTILLATION OPERATES OVER THE TASK-SPECIFIC UNLABELED DATA. FOR EACH SEQUENCE, FOR EACH TOKEN, WE WANT TO MAKE THE STUDENT MODEL PRODUCE LOGITS THAT MATCH THE TEACHER'S PREDICTIONS.
utt_0072 utt 218.43 223.77 -X SO WE WANT TO OPTIMIZE FOR THE MEAN-SQUARED ERROR LOSS BETWEEN THE CORRESPONDING LOGITS OF THE TEACHER AND THE STUDENT.
utt_0074 utt 224.02 229.11 -X WE ALSO WANT TO LEVERAGE THE TEACHER'S INTERNAL HIDDEN REPRESENTATIONS TO GUIDE THE STUDENT MODEL.
utt_0075 utt 229.40 230.49 -X TO THIS END,
utt_0076 utt 230.49 233.43 -X WE WANT TO OPTIMIZE FOR THE REPRESENTATION LOSS OF THE STUDENT MODEL
utt_0077 utt 233.85 240.28 -X COMING FROM THE FINAL LAYER AS WELL AS THE REPRESENTATION COMING FROM SOME HIDDEN INTERNAL LAYER OF THE TEACHER MODEL.
utt_0079 utt 241.91 244.09 -X SO WE HAVE THREE DIFFERENT LOSS FUNCTIONS.
utt_0080 utt 244.12 251.29 -X WE HAVE THE CROSS-ENTROPY LOSS DEFINED OVER THE LABELED DATA, AND REPRESENTATION LOSS AND LOGIT LOSS COMING FROM THE UNLABELED DATA.
utt_0082 utt 251.86 254.46 -X NOW WE CAN USE HYPER-PARAMETERS TO COMBINE THEM.
utt_0083 utt 254.61 255.35 -X HOWEVER,
utt_0084 utt 255.35 259.67 -X THIS IS SUSCEPTIBLE TO THE CHOICE OF THE HYPER-PARAMETERS WHICH REQUIRES AN EXTENSIVE SEARCH.
utt_0085 utt 259.67 272.44 -X WE CAN ALSO ADOPT A STAGE-WISE OPTIMIZATION SCHEME, IN WHICH WE FIRST OPTIMIZE FOR THE MORE GENERAL REPRESENTATION LOSS AND GRADUALLY MOVE TOWARDS OPTIMIZING FOR THE MORE TASK-SPECIFIC LOSS.
utt_0088 utt 272.44 275.52 -X HOWEVER,
utt_0089 utt 275.52 282.59 -X THERE ARE SOME CHALLENGES RELATED TO CATASTROPHIC FORGETTING WHERE THE MODEL FORGETS INFORMATION THAT IT LEARNED IN THE EARLIER STAGES FOR THE TASK.
utt_0091 utt 282.59 287.80 -X THERE ARE SEVERAL APPROACHES THAT HAVE BEEN PROPOSED TO MITIGATE THIS,
utt_0092 utt 287.83 291.10 -X RANGING FROM PROGRESSIVELY FREEZING DIFFERENT LAYERS,
utt_0093 utt 291.29 296.67 -X LOWERING THE LEARNING RATE FOR CERTAIN LAYERS IN THE NEURAL NETWORK AND OTHER REGULARIZATION TECHNIQUES.
utt_0095 utt 297.27 298.36 -X IN THIS WORK,
utt_0096 utt 298.36 306.04 -X WE USE A COMBINATION OF FREEZING AND UNFREEZING OF THE NEURAL NETWORK LAYERS WITH A COSINE LEARNING RATE SCHEDULER TO GIVE US A BETTER OPTIMIZATION SCHEDULE.
utt_0099 utt 309.43 313.73 -X WE PERFORM OUR EXPERIMENTS ON THE WIKIANN DATASET OVER forty-one LANGUAGES
utt_0100 utt 314.01 317.50 -X FOR IDENTIFYING NAMED ENTITIES LIKE PERSON,
utt_0101 utt 317.50 318.72 -X ORGANIZATION AND LOCATION.
utt_0102 utt 319.39 322.37 -X WE ADOPT MULTILINGUAL BERT AS OUR TEACHER MODEL,
utt_0103 utt 322.37 325.50 -X WHICH IS JOINTLY FINE-TUNED FROM LABELED DATA FROM ALL THE LANGUAGES.
utt_0104 utt 326.81 333.18 -X MBERT-SINGLE IS FINE-TUNED FROM THE LABELED DATA FOR EACH LANGUAGE INDEPENDENTLY.
utt_0106 utt 333.18 334.88 -X XTREMEDISTIL IS OUR FRAMEWORK,
utt_0107 utt 334.88 339.45 -X AND WE ALSO COMPARE AGAINST THE MMNER WORK FROM LAST YEAR'S ACL.
utt_0108 utt 341.66 354.65 -X HERE, WE OBSERVE THAT THE DIFFERENT MODELS OBTAIN THE SAME PERFORMANCE TRAJECTORY IN TERMS OF THE Fone-SCORE FOR DIFFERENT LANGUAGES. THIS IS INTERESTING BECAUSE THESE MODELS USE DIFFERENT ENCODERS IN TERMS OF TRANSFORMERS AND RNNS.
utt_0112 utt 354.65 358.94 -X WE ALSO HAVE A LOT OF EXPERIMENTS RELATED TO CLASSIFICATION.
utt_0114 utt 359.39 362.18 -X PLEASE REFER TO OUR PAPER FOR DETAILS.
utt_0115 utt 363.87 373.86 -X TO SUMMARIZE PERFORMANCE FOR XTREMEDISTIL FOR NER, WE OBSERVE THAT IT HAS ninety-five% PERFORMANCE MATCH WITH MBERT WHILE OBTAINING thirty-fiveX PARAMETER COMPRESSION AND OVER fifty-oneX LATENCY
utt_0117 utt 373.88 374.69 -X SPEEDUP.
utt_0118 utt 374.88 375.81 -X MOREOVER,
utt_0119 utt 375.81 380.22 -X WE HAVE A SINGLE MODEL THAT OPERATES ACROSS ALL THE DIFFERENT LANGUAGES, AND AS I MENTIONED BEFORE,
utt_0120 utt 380.22 384.00 -X WE DO NOT USE ANY LANGUAGE ID EITHER DURING THE DISTILLATION OR THE PRE-TRAINING PHASE,
utt_0122 utt 384.03 388.29 -X AND IT CAN BE EASILY EXTENDED TO OTHER LANGUAGES, AS LONG AS YOU HAVE UNLABELED DATA FOR THE SAME.
utt_0125 utt 391.52 394.91 -X HERE WE SHOW THE TRADE-OFF BETWEEN PARAMETER COMPRESSION AND Fone-SCORE.
utt_0126 utt 395.48 396.32 -X OBVIOUSLY,
utt_0127 utt 396.32 398.14 -X AS YOU COMPRESS A MODEL MORE AND MORE,
utt_0128 utt 398.14 400.29 -X YOU LOSE THE PERFORMANCE IN TERMS OF Fone-SCORE.
utt_0129 utt 400.99 403.33 -X IN TERMS OF REGULATING THE PARAMETER COMPRESSION,
utt_0130 utt 403.33 407.68 -X YOU CAN EITHER REDUCE THE WORD EMBEDDING DIMENSION OR THE NUMBER OF HIDDEN STATES OF THE RNN.
utt_0131 utt 408.22 408.87 -X HERE,
utt_0132 utt 408.87 411.55 -X WE CHOOSE A CONFIGURATION WHICH GIVES US THE ninety-five% PERFORMANCE MATCH.
utt_0133 utt 412.89 417.22 -X ONE THING TO NOTE HERE IS, AS YOU REDUCE THE WORD EMBEDDING DIMENSION,
utt_0134 utt 417.22 419.30 -X IT CAN LEAD TO MASSIVE PARAMETER COMPRESSION.
utt_0135 utt 419.33 420.29 -X HOWEVER,
utt_0136 utt 420.29 423.49 -X IT DOESN'T NECESSARILY TRANSLATE TO AN INFERENCE SPEEDUP.
utt_0137 utt 423.49 428.13 -X THE LATENCY IS LARGELY CONTRIBUTED BY THE NUMBER OF HIDDEN STATES IN THE RNN,
utt_0139 utt 428.45 432.23 -X OR THE NUMBER OF LAYERS WHEN YOU ARE USING TRANSFORMER AS A STUDENT MODEL.
utt_0140 utt 432.80 438.11 -X AGAIN, YOU WOULD WANT TO CHOOSE A CONFIGURATION HERE THAT GIVES THE BEST LATENCY SPEEDUP WITH
utt_0141 utt 440.03 444.51 -X THE PARAMETER COMPRESSION FOR REDUCED MEMORY FOOTPRINT FOR THE CORRESPONDING Fone-
utt_0143 utt 444.51 445.86 -X SCORE MATCH THAT YOU WANT.
utt_0144 utt 445.86 455.97 -X HERE, WE SHOW THE PROGRESSIVE IMPROVEMENT IN OUR MODEL AS YOU INCORPORATE MORE SOPHISTICATED
utt_0145 utt 455.97 461.64 -X FEATURES IN TERMS OF THE LOGITS AS WELL AS THE INTERNAL REPRESENTATIONS OF THE TEACHER,
utt_0147 utt 461.98 466.69 -X FOLLOWED BY GRADUAL UNFREEZING OF THE NEURAL NETWORK LAYERS FOR BETTER OPTIMIZATION.
utt_0148 utt 467.68 475.62 -X WE ALSO OBSERVE THAT AS WE INCREASE THE AMOUNT OF UNLABELED TRANSFER DATA, THE STUDENT MODEL PERFORMANCE IMPROVES TILL A CERTAIN POINT,
utt_0150 utt 475.78 480.74 -X AFTER WHICH THERE IS DIMINISHING RETURN AS YOU ADD MORE DATA. IN GENERAL,
utt_0151 utt 480.74 490.28 -X WE OBSERVE THAT UNLABELED TRANSFER DATA IS THE KEY TO MINIMIZING THE PERFORMANCE GAP BETWEEN THE STUDENT AND THE TEACHER MODEL AND OBTAIN A GREAT DISTILLATION PERFORMANCE.
utt_0154 utt 495.17 501.51 -X WE NOTE THAT AS WE USE HIGHER LAYERS OF THE TEACHER TO GUIDE THE STUDENT MODEL, IT IS INCREASINGLY DIFFICULT
utt_0156 utt 501.83 505.45 -X FOR A SHALLOW STUDENT MODEL TO MIMIC THE DEEP
utt_0157 utt 506.02 508.52 -X LAYERS OF THE TEACHER. IN GENERAL,
utt_0158 utt 508.52 512.55 -X IF YOU'RE USING A twelve-LAYER TRANSFORMER, THEN THE INFORMATION COMING FROM CENTRAL LAYERS, FOR EXAMPLE,
utt_0159 utt 512.55 514.63 -X sixTH OR sevenTH LAYER WORKS QUITE WELL IN PRACTICE
utt_0160 utt 514.79 517.03 -X TO GUIDE THE STUDENT MODEL FOR DISTILLATION.
utt_0161 utt 519.68 524.90 -X WE ALSO OBSERVE THAT RANDOM INITIALIZATION OF THE WORD EMBEDDING MATRIX WORKS QUITE WELL IN PRACTICE.
utt_0163 utt 525.03 528.95 -X YOU CAN OF COURSE DO MORE BY USING GLOVE AND FASTTEXT EMBEDDINGS
utt_0166 utt 530.72 532.81 -X IN TERMS OF PRE-TRAINED WORD EMBEDDINGS.
utt_0167 utt 533.41 541.93 -X YOU CANNOT DIRECTLY USE THE FINE-TUNED MULTILINGUAL BERT WORD EMBEDDINGS BECAUSE OF LARGE NUMBER OF PARAMETERS COMING FROM THE SEVEN HUNDRED DIMENSIONAL WORD EMBEDDINGS.
utt_0169 utt 543.14 544.04 -X HOWEVER,
utt_0170 utt 544.04 548.15 -X WE CAN USE SINGULAR VALUE DECOMPOSITION FOR DIMENSIONALITY REDUCTION
utt_0171 utt 548.87 551.43 -X AND STILL USE THE FINE-TUNED MBERT WORD EMBEDDINGS.
utt_0173 utt 551.43 555.18 -X THIS GIVES US SOMEWHAT PERFORMANCE BOOST BY REDUCING THE NOISE.
utt_0174 utt 560.23 562.09 -X TO SUMMARIZE, IN THIS WORK,
utt_0175 utt 562.09 563.85 -X WE STUDY VARIOUS DISTILLATION ASPECTS,
utt_0176 utt 564.00 568.33 -X RANGING FROM DIFFERENT KINDS OF FEATURES IN TERMS OF SOFT LOGITS AND INTERNAL REPRESENTATIONS,
utt_0177 utt 568.33 572.90 -X AS WELL AS THE TRADE-OFF BETWEEN PARAMETER COMPRESSION, LATENCY SPEEDUP AND THE Fone-SCORE.
utt_0178 utt 573.83 582.28 -X WE SHOWCASE OUR MODEL PERFORMANCE CONFIGURATION WITH ninety-five% PERFORMANCE MATCH AND thirty-fiveX PARAMETER COMPRESSION WITH fifty-oneX LATENCY SPEEDUP.
utt_0181 utt 582.69 585.19 -X WE ALSO HAVE A SINGLE MODEL ACROSS ALL THE LANGUAGES,
utt_0183 utt 585.19 590.38 -X AND CAN BE EASILY EXTENDED TO OTHER LANGUAGES SINCE WE DO NOT USE LANGUAGE ID EITHER FOR DISTILLATION OR FOR THE PRE-TRAINING PHASE.
utt_0185 utt 592.71 597.64 -X THIS MODEL IS ALREADY IN PRODUCTION FOR SEVERAL SCENARIOS IN MICROSOFT.
utt_0186 utt 598.69 604.14 -X TO GIVE YOU SOME INSIGHT AS TO WHY SUCH A FRAMEWORK WOULD BE HUGELY USEFUL IN THE DEPLOYMENT ENVIRONMENT,
utt_0188 utt 604.14 608.84 -X CONSIDER THE FOLLOWING HYPOTHETICAL ENTERPRISE SCENARIO WHERE YOU HAVE one hundred MILLION USERS.
utt_0189 utt 610.47 612.94 -X EACH USER CONTRIBUTES ten QUERIES PER DAY.
utt_0190 utt 614.18 629.02 -X IN A fifty-two-WEEK YEAR, YOU HAVE five WORK-DAYS PER WEEK. YOU CAN MULTIPLY ALL OF THESE NUMBERS TO DERIVE THE EXPECTED COST OF DEPLOYING SUCH A MODEL, AFTER FACTORING IN THE GPU COST PER HOUR AND THE one hundred MILLISECONDS LATENCY FOR QUERY PROCESSING
utt_0194 utt 629.02 629.87 -X BY THE BERT MODEL.
utt_0195 utt 631.27 636.14 -X OUR MODEL REDUCES THIS LATENCY TO two MILLISECONDS FOR THE QUERY PROCESSING,
utt_0196 utt 636.36 642.60 -X AND THEREFORE THIS CONTRIBUTES TO fifteen MILLION DOLLARS PER YEAR SAVINGS FOR DEPLOYMENT.
utt_0197 utt 642.69 647.95 -X THIS COMES WITH THIS ninety-five% PERFORMANCE MATCH AND THE REDUCED COMPRESSION FOR MEMORY FOOTPRINT,
utt_0199 utt 648.23 651.60 -X AS WELL AS THE LATENCY SPEEDUP FOR FAST INFERENCE.
utt_0200 utt 653.32 658.54 -3.6314 OUR CODE AND RESOURCES ARE AVAILABLE IN THE FOLLOWING LINK. THANKS A LOT FOR YOUR ATTENTION.
