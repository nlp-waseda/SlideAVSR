utt_0000 utt 0.94 2.13 -X HELLO EVERYONE
utt_0001 utt 2.28 12.05 -X MY NAME IS RICHARD DROSTE AND I AM PRESENTING UNIFIED IMAGE AND VIDEO SALIENCY MODELING ON BEHALF OF MY CO-AUTHORS JIANBO JIAO AND ALISON NOBLE.
utt_0004 utt 12.05 18.55 -X WHEN HUMANS PROCESS VISUAL INPUTS THEY NATURALLY FIXATE THE MOST IMPORTANT INFORMATION.
utt_0006 utt 18.99 24.37 -X PREDICTING THE DISTRIBUTION OF FIXATIONS IS CALLED “VISUAL SALIENCY MODELING”.
utt_0007 utt 25.01 28.18 -X THIS IS USEFUL FOR APPLICATIONS LIKE IMAGE COMPRESSION
utt_0008 utt 28.40 29.81 -X ACTION RECOGNITION
utt_0009 utt 29.84 33.39 -X CAPTIONING VISUAL QUESTION ANSWERING AND MANY MORE.
utt_0012 utt 33.97 37.63 -X PRIOR WORK HAS FOCUSED ON PREDICTING SALIENCY FOR IMAGES
utt_0013 utt 37.84 40.18 -X OR FOR VIDEOS.
utt_0014 utt 40.18 46.10 -X HERE, WE PROPOSE THE FIRST UNIFIED SALIENCY MODEL THAT PREDICTS SALIENCY FOR BOTH MODALITIES.
utt_0016 utt 46.83 51.19 -X FURTHER, WE SHOW THAT THIS RESULTS IN BETTER OVERALL PERFORMANCE
utt_0017 utt 51.22 56.69 -X AND A SIMPLER, SMALLER AND FASTER MODEL WITH ONE SET OF PARAMETERS FOR ALL INPUTS.
utt_0019 utt 58.26 64.95 -X LET’S HAVE A LOOK WHY JOINT MODELING OF VIDEO AND IMAGE DATA MIGHT BE CHALLENGING.
utt_0020 utt 65.36 69.08 -X CONSIDER, FOR EXAMPLE, THIS SCENE OF A SNOWY MOUNTAIN.
utt_0021 utt 69.08 75.16 -X A STATIC IMAGE SALIENCY MODEL WILL PREDICT FIXATIONS ON THE TREES AND IN THE DISTANCE.
utt_0022 utt 75.16 81.62 -X HOWEVER, THE SCENE IS FROM A VIDEO CLIP AND HUMANS FIXATE THE SNOWBOARDER THAT IS OCCLUDED IN THE EXAMPLE SCENE.
utt_0024 utt 81.97 88.21 -X THIS CAN ONLY BE PREDICTED VIA A DYNAMIC MODEL WHICH HAS SPATIO-TEMPORAL FEATURES.
utt_0026 utt 88.75 91.45 -X ON THE OTHER HAND, CONSIDER THIS SCENE.
utt_0027 utt 92.08 99.06 -X THE STATIC MODEL PREDICTS THE HIGHEST FIXATION PROBABILITY FOR THE TEXT THAT INFORMS US ABOUT THE DESTINATION OF THE BUS.
utt_0029 utt 99.70 106.33 -X THIS IS IN ACCORDANCE WITH THE GROUND TRUTH SINCE THIS IS AN IMAGE THAT IS VIEWED FOR SEVERAL SECONDS BY THE HUMAN SUBJECTS.
utt_0031 utt 107.12 112.60 -X THE DYNAMIC MODEL, IN CONTRAST, ASSIGNS THE HIGHEST PROBABILITY TO THE PEOPLE ENTERING THE BUS
utt_0032 utt 112.66 115.90 -X WHICH IS WHERE THE ACTION WOULD BE HAPPENING IF THIS WERE A VIDEO.
utt_0033 utt 116.85 123.64 -X THESE EXAMPLES INDICATE THAT HUMANS VIEW THE SAME SCENE DIFFERENTLY DEPENDING ON WHETHER IT’S AN IMAGE OR A VIDEO.
utt_0035 utt 124.85 129.50 -X THEREFORE, A UNIFIED MODEL NEEDS A WAY TO REPRESENT THESE DIFFERENCES.
utt_0036 utt 130.29 135.45 -X BEFORE I EXPLAIN OUR UNIFIED MODEL, LET ME DESCRIBE THE DATASETS THAT WE CONSIDER.
utt_0037 utt 136.12 141.75 -X SALICON IS A LARGE IMAGE SALIENCY DATASET THAT IS BASED ON MICROSOFT COCO
utt_0038 utt 142.07 145.47 -X AND THEREFORE FEATURES EVERYDAY SCENES WITH COMMON OBJECTS.
utt_0039 utt 146.77 153.05 -X THE HUMAN FIXATIONS FOR EACH IMAGE ARE RECORDED DURING FIVE SECONDS OF FREE VIEWING.
utt_0040 utt 153.30 159.03 -X DHoneFK IS A LARGE VIDEO SALIENCY DATASET WITH DIVERSE CONTENT THAT IS SOURCED FROM YOUTUBE.
utt_0041 utt 159.29 163.61 -X THE FIXATIONS ARE RECORDED WHILE THE SUBJECTS FREELY VIEW THE VIDEOS.
utt_0042 utt 164.34 169.15 -X FOR HOLLYWOODminus two, IN CONTRAST WHICH CONTAINS SCENES FROM MOVIES AND TELEVISION
utt_0044 utt 169.43 175.45 -X THE VIDEOS ARE LABELED WITH ACTIONS AND SOME OF THE VIEWERS WERE INSTRUCTED TO IDENTIFY THE ACTION LABEL.
utt_0046 utt 175.96 182.11 -X THE SAME APPLIES TO UCF SPORTS WHICH CONTAINS SCENES FROM SPORTS BROADCAST.
utt_0048 utt 182.11 185.76 -X IN ADDITION WE USE THE MITone thousand and three AND MITthree hundred DATASETS FOR EVALUATION.
utt_0049 utt 189.85 195.04 -X NOW, WE APPROACH UNIFIED MODELING VIA DOMAIN ADAPTATION.
utt_0050 utt 195.04 198.65 -X THROUGH A SERIES OF EXPERIMENTS, WE IDENTIFY COVARIATE SHIFT
utt_0051 utt 198.75 200.08 -X SPATIAL BIAS
utt_0052 utt 200.19 201.66 -X FEATURE IMPORTANCE
utt_0053 utt 201.69 203.92 -X AND SALIENCY MAP SMOOTHNESS
utt_0054 utt 203.96 208.67 -X AS THE MAIN SOURCES OF DOMAIN SHIFT BETWEEN IMAGE AND VIDEO SALIENCY DATASETS.
utt_0055 utt 209.37 215.61 -X CONSEQUENTLY, WE PROPOSE CORRESPONDING DOMAIN ADAPTATION MODULES THAT ENABLE JOINT MODELLING.
utt_0056 utt 215.80 218.14 -X I WILL EXPLAIN THESE ON THE NEXT SLIDES.
utt_0057 utt 218.87 223.81 -X THE FIRST DOMAIN ADAPTATION MODULE ADDRESSES THE COVARIATE SHIFT BETWEEN THE DATASETS
utt_0058 utt 224.00 228.61 -X OR IN OTHER WORDS THE SHIFT IN NETWORK ACTIVATIONS BETWEEN DATASETS.
utt_0060 utt 229.05 233.12 -X TO ILLUSTRATE THE SHIFT, WE CONSTRUCT A SIMPLE PRELIMINARY EXPERIMENT.
utt_0061 utt 233.72 238.50 -X WE COMPUTE THE AVERAGE-POOLED OUTPUT FEATURES OF A MOBILENET-Vtwo MODEL
utt_0062 utt 238.56 241.47 -X FOR RANDOMLY SELECTED IMAGES AND VIDEO FRAMES.
utt_0063 utt 242.17 245.57 -X NEXT, WE VISUALIZE THE FEATURE SPACE THOUGH T-SNE
utt_0064 utt 246.36 249.79 -X AFTER NORMALIZING THE FEATURES TO ZERO-MEAN AND UNIT-VARIANCE.
utt_0065 utt 250.75 254.66 -X THIS IS THE RESULT WHEN NORMALIZING THE FEATURES ACROSS ALL DATASETS.
utt_0066 utt 255.10 261.51 -X IN CONTRAST, THIS IS THE RESULT WHEN NORMALIZING ACROSS THE FEATURES OF EACH DATASET SEPARATELY.
utt_0067 utt 261.95 266.66 -X WE CAN OBSERVE THAT FIRST THERE IS A VISIBLE DOMAIN SHIFT
utt_0068 utt 266.85 271.91 -X AND THAT IT IS RESOLVED THROUGH THE PER-DATASET NORMALIZATION.
utt_0069 utt 271.91 281.06 -X THEREFORE, WE USE DOMAIN-ADAPTIVE BATCH NORMALIZATION WHICH MEANS A SEPARATE SET OF BATCH NORM STATISTICS AND PARAMETERS FOR EACH DATASET.
utt_0071 utt 282.43 286.47 -X NEXT, WE ADDRESS THE DOMAIN SHIFT RELATED TO SPATIAL BIAS.
utt_0072 utt 287.39 291.75 -X HERE, WE PLOT THE AVERAGE GROUND TRUTH SALIENCY MAPS FOR EACH DATASET.
utt_0073 utt 292.61 298.21 -X IN GENERAL THE CENTER IS FIXATED MOST FREQUENTLY WHICH IS KNOWN AS VISUAL SALIENCY
utt_0075 utt 298.59 299.36 -X SORRY
utt_0076 utt 299.68 304.77 -X WHICH IS KNOWN AS CENTER BIAS IN VISUAL SALIENCY RESEARCH.
utt_0077 utt 304.77 311.88 -X IN ADDITION, WE CAN OBSERVE THAT THE CENTER BIAS IS MUCH STRONGER FOR THE VIDEO SALIENCY DATASETS COMPARED TO SALICON.
utt_0079 utt 312.55 317.57 -X CONSIDER THIS DHFoneK VIDEO FRAME AND SALICON IMAGE.
utt_0080 utt 317.89 324.39 -X BOTH SHOW BIRDS, BUT FOR THE VIDEO FRAME, ALL FIXATIONS ARE AT ONE LOCATION NEAR THE CENTER.
utt_0081 utt 324.74 328.81 -X FOR THE IMAGE, IN CONTRAST, DIFFERENT BIRDS ACROSS THE IMAGE ARE FIXATED
utt_0082 utt 329.76 333.94 -X TO MODEL THESE DIFFERENCES, WE INTRODUCE DOMAIN-ADAPTIVE PRIORS
utt_0083 utt 333.99 338.31 -X WHICH ARE GAUSSIAN PRIOR MAPS WITH A SEPARATE SET OF PARAMETERS FOR EACH DATASET.
utt_0084 utt 339.59 348.65 -X IN PARTICULAR, THE PRIOR MAPS ARE BIVARIATE GAUSSIANS WHICH ARE CONSTRUCTED FROM MEANS AND STANDARD DEVIATIONS THAT ARE LEARNED DURING TRAINING.
utt_0086 utt 349.25 355.69 -X THE PARAMETERS ARE INITIALIZED TO COVER A BROAD SPECTRUM OF POSSIBLE PRIORS.
utt_0087 utt 355.69 360.33 -X NEXT, WE ADDRESS THE DOMAIN SHIFT CAUSED BY DIFFERENT FEATURE IMPORTANCES.
utt_0088 utt 360.33 365.82 -X TO STUDY THIS, WE CONSTRUCT A SIMPLIFIED SALIENCY PREDICTOR FROM A MOBILENET Vtwo ENCODER
utt_0089 utt 366.02 370.71 -X A DOMAIN INVARIANT FUSION WHICH IS A WEIGHTED SUM OF THE MOBILENET FEATURE MAPS
utt_0091 utt 371.01 373.96 -X AND FINALLY A SIMPLE BILINEAR UPSAMPLING.
utt_0092 utt 374.53 384.94 -X WE INITIALIZE THE MOBILENET WITH IMAGENET WEIGHTS AND TRAIN THE FUSION LAYER UNTIL CONVERGENCE WHICH RESULTS IN THE PER-DATASET VALIDATION LOSSES AS SHOWN.
utt_0095 utt 386.73 389.39 -X NOW, IF WE MAKE THE FUSION LAYER DOMAIN-ADAPTIVE
utt_0096 utt 389.61 392.04 -X WHICH MEANS DIFFERENT WEIGHTS FOR EACH DATASET
utt_0097 utt 392.23 395.53 -X THEN THE VALIDATION LOSS IS SIGNIFICANTLY DECREASED.
utt_0098 utt 395.88 406.32 -X THIS INDICATES THAT THE IMPORTANCE OF EACH MOBILENET OUTPUT FEATURE MAP VARIES BETWEEN THE DATASETS.
utt_0099 utt 406.32 410.64 -X THEREFORE, WE INTRODUCE DOMAIN ADAPTIVE FUSION INTO OUR FRAMEWORK.
utt_0100 utt 412.17 416.88 -X FINALLY, WE EXAMINE THE DOMAIN SHIFT CAUSED BY SALIENCY MAP SMOOTHNESS.
utt_0101 utt 417.70 422.60 -X HERE WE CAN SEE EXEMPLARY SALIENCY MAPS FROM DIFFERENT DATASETS.
utt_0102 utt 423.62 430.00 -X EACH OF THESE IS COMPUTED BY SMOOTHING A BINARY FIXATION MAP WITH A FIXED-SIZED GAUSSIAN KERNEL.
utt_0103 utt 430.57 436.37 -X IT IS APPARENT THAT THE SALIENCY MAPS HAVE DIFFERENT SMOOTHNESS WHEN RESIZED TO THE SAME WIDTH.
utt_0104 utt 437.16 441.90 -X THIS CAN BE QUANTIFIED BY COMPUTING THE MAXIMUM GRADIENT MAGNITUDE FOR EACH IMAGE
utt_0105 utt 442.54 448.08 -X AND PLOTTING THE HISTOGRAM FOR EACH DATASET WHICH CONFIRMS THE INITIAL OBSERVATION.
utt_0107 utt 449.51 454.16 -X CONSEQUENTLY, WE INTRODUCE A SEPARATE LEARNED SMOOTHING KERNEL FOR EACH DATASET
utt_0108 utt 454.28 457.30 -X WHICH WE REFER TO AS DOMAIN-ADAPTIVE SMOOTHING.
utt_0109 utt 459.40 462.48 -X WE DESIGN A SIMPLE ENCODER-DECODER MODEL
utt_0110 utt 462.51 466.32 -X IN WHICH WE INTEGRATE THE PROPOSED DOMAIN ADAPTATION MODULES.
utt_0111 utt 467.05 468.27 -X IN PARTICULAR
utt_0112 utt 468.33 472.75 -X THE DOMAIN-ADAPTIVE PRIORS ARE CONCATENATED WITH THE FEATURE MAPS AT THE BOTTLENECK
utt_0113 utt 472.84 478.54 -X AND THE DECODER OUTPUT IS PASSED THROUGH DOMAIN ADAPTIVE FUSION AND SMOOTHING.
utt_0114 utt 478.54 482.35 -X DOMAIN-ADAPTIVE BATCH NORMALIZATION IS IMPLEMENTED THROUGHOUT THE DECODER.
utt_0115 utt 482.96 491.97 -X IN ADDITION, TEMPORAL FEATURES ARE EXTRACTED FROM THE VIDEOS THROUGH A RESIDUAL RNN THAT IS AUTOMATICALLY SWITCHED ON FOR VIDEO INPUTS
utt_0117 utt 492.46 494.96 -X WHICH WE REFER TO AS BYPASS-RNN.
utt_0118 utt 496.46 502.32 -X WITH ONE SET OF PARAMETERS, WE ACHIEVE STATE-OF-THE-ART PERFORMANCE IN ALL TESTED VIDEO SALIENCY DATASETS
utt_0119 utt 502.57 505.94 -X AND COMPETITIVE PERFORMANCE FOR IMAGE SALIENCY MODELING
utt_0120 utt 505.97 512.37 -X IN ADDITION TO FASTER RUNTIME AND A FIVE TO TWENTY-FOLD REDUCTION OF MODEL SIZE.
utt_0121 utt 512.46 515.25 -X HERE WE SHOW THE RESULTS FOR THE DHFoneK VIDEO SALIENCY
utt_0122 utt 516.34 520.18 -X AND SALICON IMAGE SALIENCY BENCHMARKS.
utt_0123 utt 520.18 523.35 -X THE X-AXIS SHOWS THE AUC-J PERFORMANCE METRIC
utt_0124 utt 523.50 526.28 -X AND THE Y-AXIS THE LOGARITHMIC MODEL SIZE
utt_0125 utt 526.29 528.31 -X WITH SMALLER SIZES FURTHER UP.
utt_0126 utt 529.94 537.24 -X ALSO, FOR DHFoneK, MODEL RUN TIME INFORMATION WAS AVAILABLE FOR THE COMPARED MODELS WHICH IS INDICATED BY THE CIRCLE SIZE.
utt_0128 utt 538.19 540.50 -X STATIC MODELS ARE SHOWN IN ORANGE
utt_0129 utt 542.10 544.04 -X AND DYNAMIC MODELS IN BLUE
utt_0130 utt 544.14 546.81 -X AND OUR UNIFIED UNISAL MODEL IN PINK.
utt_0131 utt 547.70 553.59 -X HERE ARE THE RESULTS FOR THE HOLLYWOODminus two AND UCF SPORTS VIDEO SALIENCY BENCHMARKS.
utt_0132 utt 555.25 561.01 -X PLEASE REFER TO THE PAPER FOR MORE QUANTITATIVE AND QUALITATIVE RESULTS AND ABLATION STUDIES.
utt_0133 utt 561.97 566.26 -3.6257 THANK YOU FOR WATCHING AND FEEL FREE TO CHECK OUT THE PROVIDED PAPER AND CODE.
