utt_0000 utt 4.46 16.79 -X HEY I'M ANTOINE YANG AND I'M EXCITED TO PRESENT YOU OUR WORK DONE ON GOOGLE: VIDtwoSEQ: LARGE-SCALE PRETRAINING OF A VISUAL LANGUAGE MODEL FOR DENSE VIDEO CAPTIONING OUR CODE IS AVAILABLE ON THE
utt_0002 utt 16.79 28.80 -X PROJECT WEB PAGE IN A NUTSHELL VIDtwoSEQ IS A VISUAL LANGUAGE MODEL THAT PREDICTS TEMPORAL BOUNDARIES AND CAPTIONS FOR ALL EVENTS IN AN UNTRIMMED VIDEO BY SIMPLY GENERATING A SINGLE SEQUENCE OF TOKENS
utt_0004 utt 29.68 40.72 -X SUCH A UNIFIED MODEL REQUIRES LARGE-SCALE TRAINING DATA WHICH IS NOT AVAILABLE IN CURRENT ANNOTATED DATASETS HENCE WE PRETRAIN VIDtwoSEQ ON MILLIONS OF UNLABELED NARRATIVE VIDEOS
utt_0006 utt 41.07 46.23 -X BY REFORMULATING TRANSCRIBED SPEECH SENTENCES AND THEIR CORRESPONDING TIMESTAMPS
utt_0007 utt 46.61 57.21 -X AS PSEUDO EVENT CAPTIONS AND BOUNDARIES AFTER FINETUNING THE PRETRAINED VIDtwoSEQ MODEL ACHIEVES STATE-OF-THE-ART RESULTS ON VARIOUS BENCHMARKS OF DENSE VIDEO CAPTIONING
utt_0009 utt 57.27 62.87 -X VIDEO PARAGRAPH CAPTIONING VIDEO CLIP CAPTIONING AND ALSO GENERALIZES WELL TO FEW-SHOT SETTINGS
utt_0010 utt 67.31 71.72 -X NOW LET'S DIVE INTO THE DETAILS STARTING WITH A LITTLE BIT OF BACKGROUND
utt_0011 utt 72.27 76.74 -X MOST VIDEO CAPTURING SYSTEMS CAN ONLY HANDLE SHORT TRIMMED VIDEOS
utt_0012 utt 76.92 89.21 -X BUT NATURAL VIDEOS MAY CONTAIN NUMEROUS EVENTS SO WE FOCUS ON THE DENSE VIDEO CAPTIONING TASK WHICH REQUIRES TEMPORARY LOCALIZING AND CAPTIONING ALL EVENTS IN UNTRIMMING MINUTES LONG VIDEOS
utt_0014 utt 90.10 95.70 -X PRIOR DENSE VIDEO CAPTIONING APPROACHES CONTAIN VARIOUS TASK SPECIFIC COMPONENTS
utt_0015 utt 95.89 100.97 -X LIKE EVENT COUNTERS WHICH MAKES IT HARD TO INTEGRATE THEM INTO POWERFUL FOUNDATION MODELS
utt_0016 utt 101.69 108.95 -X IN CONTRAST WE TAKE INSPIRATION FROM UNIFIED FRAMEWORKS LIKE PIXtwoSEQ AND FOLLOW UP WORKS
utt_0017 utt 109.33 120.67 -X THAT VIEW SPATIAL LOCALIZATION TASKS LIKE OBJECT DETECTION AS SINGLE SEQUENCE GENERATION TASKS THIS IS ACHIEVED BY QUANTIZING AND TOKENIZING SPATIAL COORDINATES
utt_0019 utt 121.46 132.01 -X HOWEVER IN DENSE VIDEO CAPTIONING WE DEAL WITH THE TEMPORAL LOCALIZATION TASK WE ALSO NEED TO PREDICT A CAPTION FOR EACH EVENT AND WE HAVE FEWER MANUALLY ANNOTATED DATA FOR TRAINING
utt_0021 utt 133.56 144.93 -X HENCE WE DESIGN VIDtwoSEQ A VISUAL LANGUAGE MODEL THAT VIEWS DENSE VIDEO CAPTIONING AS A SEQUENCE TO SEQUENCE PROBLEM TO ACHIEVE THIS WE QUANTIZE AND TOKENIZE TIME JOINTLY WITH TEXT
utt_0023 utt 145.56 151.49 -X THIS ENABLES VIDtwoSEQ TO SEAMLESSLY UNDERSTAND AND GENERATE SEQUENCES OF TEXT SENTENCES
utt_0024 utt 151.83 156.33 -X INTERLEAVED WITH SPECIAL TIME TOKENS THAT TEMPORALLY GROUND THESE SENTENCES IN THE VIDEO
utt_0025 utt 157.08 163.76 -X IN DETAIL INPUT VIDEO FRAMES AND THE TRANSCRIBED SPEECH SEQUENCE ARE ENCODED BY A VISUAL ENCODER
utt_0026 utt 163.77 174.24 -X AND A TEXT ENCODER RESPECTIVELY THE TEXT DECODER CROSS ATTENDS TO THESE MULTIMODAL EMBEDDINGS TO AUTOREGRESSIVELY GENERATE THE OUTPUT DENSE EVENT CAPTIONING SEQUENCE
utt_0028 utt 175.45 180.51 -X THE MODEL WEIGHTS ARE INITIALIZED WITH A CLIP VISUAL BACKBONE AND A Tfive LANGUAGE MODEL
utt_0029 utt 181.88 189.25 -X THIS UNIFIED VIDtwoSEQ ARCHITECTURE REQUIRES LARGE-SCALE TRAINING DATA WHICH IS NOT AVAILABLE IN CURRENT DENSE VIDEO CAPTIONING DATA SETS
utt_0031 utt 190.40 203.91 -X HENCE WE PROPOSE TO PRETRAIN VIDtwoSEQ ON UNLABELED NARRATIVE VIDEOS WHICH ARE READILY AVAILABLE AT SCALE FOR INSTANCE THE YT-TEMPORALminus oneB DATASET THAT WE USE CONTAINS eighteen MILLION NARRATIVE VIDEOS
utt_0033 utt 204.54 210.36 -X FOR PRETRAINING WE USE THE TRANSCRIBED SPEECH SENTENCES AND THEIR CORRESPONDING TIMESTAMPS
utt_0034 utt 210.43 222.23 -X AS PSEUDO DENSE VIDEO CAPTIONING SUPERVISION NOTE THAT SPEECH SENTENCES CONSIDERABLY DIFFER FROM DENSE EVENT CAPTIONS FOR INSTANCE IN TERMS OF SEMANTIC CONTENT OR DURATION IN THE VIDEO
utt_0036 utt 223.52 236.20 -X HOWEVER WE WILL SHOW IN THE EXPERIMENTS THAT THIS PRETRAINING IS HIGHLY BENEFICIAL FOR VIDtwoSEQ'S DOWNSTREAM PERFORMANCE WE PRETRAIN VIDtwoSEQ WITH A GENERATIVE AND A DENOISING OBJECTIVE
utt_0038 utt 237.19 248.77 -X THE GENERATIVE OBJECTIVE TEACHES THE DECODER TO PREDICT THE SPEECH SEQUENCE GIVEN VISUAL INPUTS ONLY THE DENOISING OBJECTIVE ENCOURAGES MULTIMODAL LEARNING BY REQUIRING THE
utt_0040 utt 248.77 253.66 -X MODEL TO PREDICT MASKED TOKEN GIVEN THE NOISY SPEECH SEQUENCE AND VISUAL INPUTS
utt_0041 utt 255.91 267.45 -X AFTER PRETRAINING VIDtwoSEQ CAN BE FINETUNED ON VARIOUS VIDEO CAPTIONING TASKS WITH A SIMPLE LANGUAGE MODELING OBJECTIVE NOTABLY VIDtwoSEQ ACHIEVES STATE-OF-THE-ART RESULTS ON
utt_0043 utt 267.45 272.83 -X THREE DENSE VIDEO CAPTIONING BENCHMARKS YOUCOOKtwo VITT AND ACTIVITYNET-CAPTIONS
utt_0044 utt 273.80 277.85 -X VIDtwoSEQ ALSO IMPROVES OVER ALL PRIOR VIDEO PARAGRAPH CAPTIONING METHODS
utt_0045 utt 278.25 290.38 -X INCLUDING THOSE THAT USE GROUND TRUTH EVENT PROPOSAL AT INFERENCE TIME ON BOTH THE YOUCOOKtwo AND ACTIVITYNET-CAPTIONS DATASETS FINALLY VIDtwoSEQ ALSO GENERATES WELL TO THE STANDARD VIDEO CLIP
utt_0047 utt 290.38 296.16 -X CAPTIONING TASK AS IT ACHIEVES STATE-OF-THE-ART RESULTS ON BOTH THE MSR-VTT AND MSVD DATASETS
utt_0048 utt 297.45 306.64 -X WE ALSO FIND THAT THE PRETRAINED VIDtwoSEQ MODEL GENERALIZED WELL TO FEW-SHOT SETTINGS WHEN FINETUNED ONLY USING A SMALL FRACTION OF THE DOWNSTREAM TRAINING DATASET
utt_0050 utt 307.46 311.98 -X IN OUR PAPER WE FURTHER SHOW THAT PRETRAINING IS CRUCIAL FOR THE FEW-SHOT PERFORMANCE
utt_0051 utt 314.73 326.69 -X OUR ABLATION STUDIES DEMONSTRATE THE BENEFIT OF PRETRAINING ON UNTRIMMED NARRATIVE VIDEOS BY EXPLOITING TRANSCRIBED SPEECH TIMESTAMPS VIA THE TIME TOKENS THIS IS UNLIKE PRIOR VIDEO CAPTIONING
utt_0053 utt 326.69 337.55 -X PRETRAINING FRAMEWORKS WHICH MAKE USE OF SHORT TRIMMED VIDEO-SPEECH SEGMENTS WE ALSO ANALYZE THE IMPORTANCE OF EACH PRETRAINING OBJECTIVE WE FIND THAT THE MODEL WITH VISUAL INPUTS ONLY
utt_0055 utt 337.61 348.94 -X BENEFITS CONSIDERABLY FROM PRETRAINING ON WITH THE GENERATIVE OPERATIVE FURTHERMORE THE DENOISING OBJECTIVE SIGNIFICANTLY IMPROVES THE PERFORMANCE OF THE MODEL WITH BOTH VISUAL AND SPEECH INPUTS
utt_0060 utt 364.02 367.63 -X THEIR SEMANTIC CONTENT DURING PRETRAINING IN ADDITION VIDtwoSEQ BENEFITS FROM LARGER LANGUAGE
utt_0074 utt 407.97 410.97 -6.9428 WELL TO FEW-SHOT SETTINGS MORE DETAILS AND INSIGHTS ARE INCLUDED IN OUR PAPER
