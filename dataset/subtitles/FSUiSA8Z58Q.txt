utt_0000 utt 1.42 2.96 -X I AM HAYATA YAMASAKI.
utt_0001 utt 3.12 13.10 -X WE SHOW A FRAMEWORK OF LEARNING WITH OPTIMIZED RANDOM FEATURES THAT USES AN EXPONENTIAL SPEEDUP BY QUANTUM MACHINE LEARNING WITHOUT SPARSITY AND LOW-RANK ASSUMPTIONS.
utt_0003 utt 14.28 20.37 -X QUANTUM TECHNOLOGIES TOWARD REALIZING QUANTUM COMPUTATION ARE ATTRACTING MORE AND MORE ATTENTION.
utt_0004 utt 20.37 30.64 -X THIS IS BECAUSE THE QUANTUM COMPUTATION MAY ACHIEVE LARGE SPEEDUPS IN SOLVING PROBLEMS SUCH AS INTEGER FACTORING, COMPARED TO CONVENTIONAL CLASSICAL COMPUTATION.
utt_0006 utt 30.64 40.31 -X BUT IT IS IMPORTANT TO ESTABLISH FURTHER SOLID MOTIVATIONS BY CLARIFYING ITS ADVANTAGE IN SOLVING THE PROBLEMS THAT MAY HAVE GREATER SOCIAL IMPACTS.
utt_0008 utt 41.23 50.03 -X IN OUR INFORMATION-TECHNOLOGY SOCIETY, DATA PROCESSING IN OUR DAILY LIFE IS SUPPORTED BY MACHINE-LEARNING TECHNOLOGIES WORKING BEHIND IT.
utt_0010 utt 51.21 55.47 -X QUANTUM MACHINE LEARNING EMERGES AS A RAPIDLY GROWING FIELD OF RESEARCH
utt_0011 utt 55.60 61.68 -X TO SPEED UP AND SCALE UP MACHINE LEARNING BY TAKING ADVANTAGE OF QUANTUM COMPUTATION.
utt_0012 utt 62.22 75.31 -X HOWEVER, TO ATTAIN LARGE SPEEDUPS, EXISTING QUANTUM MACHINE LEARNING ALGORITHMS REQUIRE RESTRICTIVE ASSUMPTIONS ON USING A VERY SPARSE MATRIX OR A VERY LOW-RANK MATRIX.
utt_0014 utt 75.79 81.68 -X SO CAREFUL JUSTIFICATIONS OF THESE EXTREME ASSUMPTIONS WERE NEEDED IN THESE ALGORITHMS.
utt_0015 utt 82.67 96.05 -X IN CONTRAST, WE SHOW A FRAMEWORK OF QUANTUM MACHINE LEARNING USING AN EXPONENTIAL SPEEDUP WITHOUT REQUIRING THESE ASSUMPTIONS AND HENCE BROADEN APPLICABILITY OF QUANTUM MACHINE LEARNING.
utt_0017 utt 96.24 104.02 -X IN OUR WORK, WE CONSIDER A COMMON TASK IN THE FIELD OF MACHINE LEARNING, CALLED SUPERVISED LEARNING.
utt_0018 utt 104.02 109.36 -X SUPERVISED LEARNING DEALS WITH A PROBLEM OF ESTIMATING AN UNKNOWN FUNCTION TO BE LEARNED
utt_0019 utt 109.84 115.96 -X FROM GIVEN N INPUT-OUTPUT PAIRS OF EXAMPLES OF DATA.
utt_0020 utt 115.96 121.88 -X THESE EXAMPLES ARE GIVEN ACCORDING TO SOME DATA PROBABILITY DISTRIBUTION.
utt_0021 utt 121.88 132.89 -X WE WANT TO LEARN AN ESTIMATE OF THE FUNCTION TO BE LEARNED WITHIN A FIXED SPACE CALLED A MODEL, SO THAT THE ERROR ON AVERAGE SHOULD BE MINIMIZED TO A DESIRED ACCURACY.
utt_0023 utt 134.83 146.17 -X RANDOM FEATURES PROVIDE SCALABLE LEARNING ALGORITHMS BASED ON WELL-ESTABLISHED LEARNING METHODS CALLED KERNEL METHODS, AND APPLICABLE TO BIG DATA.
utt_0025 utt 146.17 155.70 -X USING RANDOM FEATURES, WE CAN REPRESENT A NON-LINEAR FUNCTION TO BE LEARNED AS A LINEAR COMBINATION OF NON-LINEAR FEATURE MAPS, SUCH AS SINE AND COSINE.
utt_0027 utt 157.17 168.53 -X THE LEARNING IS ACHIEVED BY SAMPLING MANY FEATURES AT RANDOM FREQUENCY PARAMETERS, FOLLOWED BY PERFORMING REGRESSION OF COEFFICIENTS BY CONVEX OPTIMIZATION.
utt_0029 utt 168.53 175.35 -X CONVENTIONALLY, RANDOM FEATURES ARE SAMPLED FROM A DATA-INDEPENDENT PROBABILITY DISTRIBUTION,
utt_0030 utt 175.35 179.35 -X BUT THIS MAY REQUIRE A LARGE NUMBER OF FEATURES FOR ACHIEVING LEARNING.
utt_0031 utt 180.85 188.76 -X IN CONTRAST, RECENT WORK HAS PROPOSED TO SAMPLE FEATURES FROM A DATA-OPTIMIZED PROBABILITY DISTRIBUTION.
utt_0032 utt 189.20 199.51 -X THE USE OF THIS OPTIMIZED DISTRIBUTION CAN SIGNIFICANTLY REDUCE THE REQUIRED NUMBER OF FEATURES AND IS INDEED PROVABLY OPTIMAL, UP TO A LOGARITHMIC GAP.
utt_0034 utt 200.76 208.28 -X HOWEVER, A PROBLEMATIC BOTTLENECK ARISES FROM EACH SAMPLING STEP FROM THIS OPTIMIZED DISTRIBUTION,
utt_0035 utt 208.28 216.22 -X REQUIRING AN EXPONENTIAL RUNTIME IN DATA DIMENSION PER SAMPLING AS LONG AS WE USE THE EXISTING ALGORITHM.
utt_0036 utt 216.79 222.42 -X TO OVERCOME THIS BOTTLENECK, WE DISCOVER THAT WE CAN USE A QUANTUM ALGORITHM.
utt_0037 utt 222.42 228.27 -X OUR QUANTUM ALGORITHM ACHIEVES SAMPLING FROM THE OPTIMIZED DISTRIBUTION IN AS FAST AS
utt_0038 utt 228.37 237.05 -X LINEAR RUNTIME AND HENCE ACHIEVES AN EXPONENTIAL SPEEDUP COMPARED TO ANY KNOWN CLASSICAL ALGORITHM FOR THIS SAMPLING TASK.
utt_0040 utt 238.68 251.48 -X WE CAN ALSO COMBINE OUR QUANTUM ALGORITHM WITH REGRESSION BY A WELL-ESTABLISHED CLASSICAL ALGORITHM CALLED STOCHASTIC GRADIENT DESCENT TO ACHIEVE THE LEARNING AS A WHOLE, WITHOUT CANCELING OUT OUR EXPONENTIAL SPEEDUP.
utt_0043 utt 252.50 264.95 -X THE NOVELTY OF OUR QUANTUM ALGORITHM IS TO AVOID SPARSITY AND LOW-RANK ASSUMPTIONS THAT HAVE LIMITED APPLICABILITY OF QUANTUM MACHINE LEARNING, AS I WILL EXPLAIN IN THE FOLLOWING.
utt_0045 utt 264.95 271.51 -X THE DIFFICULTY IN SAMPLING FROM THE OPTIMIZED DISTRIBUTION ARISES FROM THE INVERSE OF AN OPERATOR.
utt_0046 utt 271.51 280.70 -X THE OPTIMIZED DISTRIBUTION IS DESCRIBED BY A WEIGHTED INNER PRODUCT OF TWO FUNCTIONS ON AN INFINITE-DIMENSIONAL SPACE OF FUNCTIONS.
utt_0048 utt 280.82 286.81 -X THIS INCLUDES AN INVERSE OF AN INFINITE-DIMENSIONAL OPERATOR.
utt_0049 utt 286.81 292.06 -X SINCE COMPUTERS USING BITS AND QUBITS CANNOT EXACTLY DEAL WITH INFINITE DIMENSION,
utt_0050 utt 292.70 298.75 -X WE REPRESENT THIS DISTRIBUTION BY A FINITE NUMBER OF BITS UP TO A SMALL PRECISION DELTA
utt_0051 utt 298.84 304.32 -X USING A WELL-ESTABLISHED WAY CALLED FIXED-POINT NUMBER REPRESENTATION.
utt_0052 utt 304.66 316.32 -X FOR SIMPLICITY OF OUR ANALYSIS, WE MAY RESCALE THE GIVEN DATA APPROPRIATELY, IN SUCH A WAY THAT THIS RESCALING IS EQUIVALENT TO CHOOSING AN APPROPRIATE PRECISION WITHOUT RESCALING.
utt_0054 utt 317.11 329.82 -X IN THIS WAY, WE REPRESENT A DISCRETIZED VERSION OF OPTIMIZED DISTRIBUTION AS A WEIGHTED INNER PRODUCT OF FINITE-DIMENSIONAL VECTORS THAT INCLUDES THE INVERSE OF A MATRIX.
utt_0056 utt 331.67 341.25 -X IN THE LIMIT OF GOOD APPROXIMATION, THIS DISCRETIZED DISTRIBUTION CONVERGES TO THE ORIGINAL OPTIMIZED DISTRIBUTION.
utt_0057 utt 341.50 347.89 -X BY A LINEAR ALGEBRAIC CALCULATION, WE PROVE THAT SAMPLING FROM THIS OPTIMIZED DISTRIBUTION
utt_0058 utt 347.93 353.12 -X CAN BE ACHIEVED BY PREPARING AND MEASURING A QUANTUM STATE DENOTED BY PSI.
utt_0059 utt 354.62 365.15 -X FOR THIS STATE, WE SHOW THAT SOME PARTS ARE IMPLEMENTABLE EFFICIENTLY USING CONVENTIONAL TECHNIQUES IN QUANTUM COMPUTATION SUCH AS QUANTUM FOURIER TRANSFORM.
utt_0061 utt 366.23 377.60 -X HOWEVER, THE PROBLEMATIC PART IS AN INVERSE OF EXPONENTIALLY LARGE OPERATOR SIGMA THAT CAN BE NON-SPARSE AND FULL-RANK IN GENERAL.
utt_0063 utt 377.60 387.55 -X IF WE COULD ASSUME SPARSITY OR LOW RANK OF SIGMA, CONVENTIONAL TECHNIQUES IN QUANTUM MACHINE LEARNING WOULD SUFFICE TO IMPLEMENT ITS INVERSE EFFICIENTLY.
utt_0065 utt 387.90 395.97 -X BUT WITHOUT SUCH ASSUMPTIONS, IMPLEMENTATION OF THIS INVERSE WOULD NEED EXPONENTIAL RUNTIME.
utt_0066 utt 395.97 409.06 -X BY CONTRAST, OUR TECHNIQUE DOES NOT DIRECTLY USE THE CONVENTIONAL TECHNIQUES THAT REQUIRE SPARSITY OR LOW RANK, BUT CAN STILL IMPLEMENT THIS INVERSE EFFICIENTLY.
utt_0068 utt 409.21 420.22 -X OUR TECHNIQUE CAN AVOID THE SPARSITY AND LOW-RANK ASSUMPTIONS SINCE WE SHOW THAT WE CAN DECOMPOSE THIS NON-SPARSE AND FULL-RANK OPERATOR INTO EFFICIENTLY IMPLEMENTABLE PARTS.
utt_0070 utt 421.34 432.83 -X TO SHOW SUCH A DECOMPOSITION, WE OBSERVE THE FACT THAT A NON-SPARSE PART OF SIGMA CORRESPONDING TO A KERNEL FUNCTION USED IN RANDOM FEATURES CAN BE REPRESENTED
utt_0072 utt 432.83 436.90 -X AS A DIAGONAL OPERATOR IN FOURIER BASIS.
utt_0073 utt 436.90 448.45 -X USING THIS OBSERVATION, WE CAN DECOMPOSE SIGMA INTO ADDITION AND MULTIPLICATION OF QUANTUM FOURIER TRANSFORMS AND SPARSE DIAGONAL OPERATORS THAT CAN BE IMPLEMENTED EFFICIENTLY.
utt_0075 utt 449.76 464.45 -X AS A RESULT, WE ACHIEVE LINEAR RUNTIME IN SAMPLING FROM THE OPTIMIZED DISTRIBUTION WITHOUT SPARSITY AND LOW-RANK ASSUMPTIONS. THIS ACHIEVES EXPONENTIAL SPEEDUP COMPARED ANY EXISTING CLASSICAL ALGORITHM FOR THIS SAMPLING TASK.
utt_0078 utt 465.79 472.61 -X WE ALSO REMARK THAT SIGMA CANNOT HAVE GOOD LOW-RANK APPROXIMATION, SO RECENT TECHNIQUES
utt_0079 utt 472.61 481.80 -X OF QUANTUM-INSPIRED CLASSICAL ALGORITHMS ASSUMING LOW RANK ARE NOT DIRECTLY APPLICABLE EITHER.
utt_0080 utt 481.89 489.19 -X OUR ALGORITHM MAY PROVIDE FOLLOWING INSIGHTS INTO DESIGNING FAST ALGORITHMS FOR QUANTUM MACHINE LEARNING.
utt_0081 utt 490.24 502.50 -X FIRSTLY, OUR TECHNIQUE CAN CIRCUMVENT SPARSITY AND LOW-RANK ASSUMPTIONS, AND THIS IS USEFUL FOR CONSTRUCTING WIDELY APPLICABLE ALGORITHMS FOR QUANTUM MACHINE LEARNING.
utt_0083 utt 502.50 514.58 -X INDEED, WE SHOW THAT OUR ALGORITHM IS APPLICABLE TO LEARNING WITH CONVENTIONAL KERNELS SUCH AS GAUSSIAN AND LAPLACIAN IN A REASONABLE PARAMETER REGION EVEN THOUGH THESE KERNELS
utt_0085 utt 514.58 520.87 -X DO NOT NECESSARILY SATISFY THE SPARSITY AND LOW-RANK ASSUMPTIONS IN GENERAL.
utt_0086 utt 520.87 527.43 -X SECONDLY, RATHER THAN JUST ASSUMING THE DATA INPUT MODEL WITHOUT DISCUSSING ITS IMPLEMENTATION,
utt_0087 utt 527.65 533.45 -X OUR ANALYSIS EXPLICITLY CLARIFIES TECHNOLOGIES REQUIRED FOR IMPLEMENTATION.
utt_0088 utt 533.63 539.93 -X IN PARTICULAR, WE ARGUE THAT OUR INPUT MODEL IS IMPLEMENTABLE FEASIBLY AND EFFICIENTLY
utt_0089 utt 540.03 547.05 -X BY USING A BINARY-TREE DATA STRUCTURE COMBINED WITH QUANTUM RANDOM ACCESS MEMORY, OR QRAM FOR SHORT.
utt_0090 utt 547.84 558.63 -X TO PREPARE THIS BINARY TREE, WE MAY NEED A CLASSICAL PREPROCESSING OF N DATA WITH RUNTIME OF ORDER N TO COUNT THE POPULATION OF DATA.
utt_0092 utt 558.63 563.81 -X BUT THIS IS THE SAME SCALING AS JUST COLLECTING N DATA.
utt_0093 utt 563.81 568.23 -X GIVEN THIS BINARY TREE, THE INPUT STATE CAN BE PREPARED EFFICIENTLY.
utt_0094 utt 569.67 576.81 -X TO ACCESS THIS BINARY TREE, WE MAY NEED QRAM THAT CAN LOAD IT WITHOUT DESTROYING THE COHERENCE.
utt_0095 utt 578.02 586.92 -X HOW TO IMPLEMENT QRAM EFFICIENTLY IS CLEARLY DESCRIBED BY RECENT RESEARCH IN CONCRETE EXPERIMENTAL SETTINGS.
utt_0096 utt 586.92 597.13 -X WITH OUR RESULTS, WE CAN MOTIVATE FURTHER TECHNOLOGICAL DEVELOPMENT TOWARD REALIZING SUCH QUANTUM MEMORY DEVICES IN ADDITION TO THOSE FOR QUANTUM COMPUTERS.
utt_0098 utt 598.85 611.46 -X FINALLY, SINCE WE APPLY QUANTUM COMPUTATION TO A SAMPLING PROBLEM, OUR ALGORITHM CAN AVOID OVERHEAD OF ESTIMATING EXPECTATION VALUES FROM MANY COPIES OF QUANTUM STATES.
utt_0100 utt 612.39 622.02 -X THE EXISTING CLASSICAL ALGORITHM ESTIMATES THE DESCRIPTION OF THE OPTIMIZED DISTRIBUTION USING MATRIX INVERSION AND THEN PERFORMS THE SAMPLING.
utt_0102 utt 622.02 628.81 -X IN CONTRAST, OUR QUANTUM ALGORITHM REPRESENTS THE DISTRIBUTION BY AMPLITUDES OF QUANTUM STATE PSI.
utt_0103 utt 629.64 638.44 -X WE NEVER ESTIMATE ITS CLASSICAL DESCRIPTION SINCE THE OVERHEAD OF ESTIMATING AMPLITUDES WOULD CANCEL OUT THE SPEEDUP.
utt_0105 utt 639.24 649.93 -X INSTEAD, OUR SPEEDUP IS ACHIEVED BY JUST PERFORMING A MEASUREMENT OF PSI, SO THAT WE CAN SAMPLE EACH FEATURE PER ONLY SINGLE STATE PREPARATION AND MEASUREMENT.
utt_0107 utt 650.66 660.23 -X IN THIS WAY, OUR RESULTS DISCOVER AN APPLICATION OF SAMPLING PROBLEMS TO DESIGNING FAST ALGORITHMS FOR QUANTUM MACHINE LEARNING.
utt_0109 utt 661.06 673.93 -X CONSEQUENTLY, OUR RESULTS LEAD TO A PROMISING CANDIDATE OF KILLER APPLICATIONS OF QUANTUM MACHINE LEARNING WITH BROADER APPLICABILITY BY AVOIDING SPARSITY AND LOW-RANK ASSUMPTIONS.
utt_0111 utt 674.95 686.57 -X OUR QUANTUM ALGORITHM ACHIEVES AN EXPONENTIAL SPEEDUP IN SAMPLING RANDOM FEATURES FROM THE OPTIMIZED DISTRIBUTION, TO ACHIEVE THE LEARNING WITH SIGNIFICANTLY SMALL NUMBER OF FEATURES.
utt_0113 utt 687.78 700.43 -X WE CAN COMBINE OUR QUANTUM ALGORITHM WITH CLASSICAL CONVEX OPTIMIZATION BY STOCHASTIC GRADIENT DESCENT, TO ACHIEVE THE LEARNING AS A WHOLE WITHOUT CANCELING OUT OUR EXPONENTIAL SPEEDUP.
utt_0115 utt 701.99 711.24 -X THIS OPENS UP A ROUTE TO A WIDELY APPLICABLE FRAMEWORK OF QUANTUM MACHINE LEARNING THAT TAKES ADVANTAGE OF AN EXPONENTIAL SPEEDUP.
utt_0117 utt 712.26 724.94 -X OUR RESULTS ESTABLISH A MORE SOLID MOTIVATION FOR FURTHER DEVELOPMENT OF QUANTUM TECHNOLOGIES IN THE LONG RUN SINCE HIGH-SPEED LARGE-SCALE MACHINE LEARNING WILL BE EVENTUALLY NEEDED IN PRACTICE.
utt_0119 utt 726.09 736.17 -X CONSIDERING MAJOR ROLES OF RANDOM FEATURES IN THE FIELD OF MACHINE LEARNING, OUR QUANTUM ALGORITHM MAY ALSO BE APPLICABLE TO OTHER MACHINE LEARNING TASKS.
utt_0121 utt 737.26 748.37 -X AND OUR TECHNIQUES FOR AVOIDING SPARSITY AND LOW-RANK ASSUMPTIONS ARE EXPECTED TO BE USEFUL FOR DESIGNING FURTHER QUANTUM MACHINE LEARNING ALGORITHMS THAT ACHIEVE
utt_0123 utt 748.37 752.30 -X HIGH SPEED AND WIDE APPLICABILITY AT THE SAME TIME.
utt_0124 utt 753.00 758.57 -X I WOULD BE HAPPY TO DISCUSS FURTHER APPLICATIONS TO PROGRESS THE FIELD OF QUANTUM MACHINE LEARNING.
utt_0125 utt 758.79 763.12 -4.3955 THANK YOU FOR YOUR ATTENTION.
