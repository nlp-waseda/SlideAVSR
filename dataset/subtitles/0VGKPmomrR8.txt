utt_0000 utt 0.02 4.02 -X WE PRESENT BEHIND THE SCENES: DENSITY FIELDS FOR SINGLE VIEW RECONSTRUCTION.
utt_0001 utt 4.59 11.76 -X BEHIND THE SCENES IS A NOVEL SELF-SUPERVISED METHOD FOR OBTAINING A VOLUMETRIC RECONSTRUCTION OF A SCENE FROM A SINGLE IMAGE.
utt_0003 utt 12.08 19.41 -X OUR RECONSTRUCTION IS REPRESENTED AS A DENSITY FIELD, WHICH MAPS EVERY LOCATION OF THE CAMERA FRUSTUM TO VOLUMETRIC DENSITY.
utt_0005 utt 19.50 23.89 -X OUR ENTIRE MODEL IS TRAINED ON ONLY STEREO VIDEO DATA IN A SELF-SUPERVISED WAY.
utt_0006 utt 24.40 32.02 -X UNLIKE METHODS FOR DEPTH MAP PREDICTION, FOR EXAMPLE MONODEPTHtwo, OUR METHOD CAN REASON ABOUT AREAS THAT ARE OCCLUDED IN THE INPUT IMAGE.
utt_0008 utt 32.33 40.82 -X COMPARED TO LEARNING-BASED NEURAL RADIANCE FIELDS, FOR EXAMPLE PIXELNERF, OUR REPRESENTATION IS MUCH EASIER TO PREDICT, IMPROVING GENERALIZATION CAPABILITIES.
utt_0010 utt 40.82 46.35 -X HERE, WE SHOW DIFFERENT VISUALIZATIONS OF THE RECONSTRUCTED DENSITY FIELDS FOR SEVERAL CHALLENGING SCENES FROM KITTIminus three hundred and sixty.
utt_0012 utt 47.09 51.06 -X NOTE THAT OUR METHOD EVEN RECOVERS GEOMETRY IN OCCLUDED AREAS.
utt_0013 utt 52.01 59.47 -X USING THE RECONSTRUCTED DENSITY FIELD, WE CAN ALSO PERFORM OTHER TASKS LIKE NOVEL VIEW SYNTHESIS, HERE SHOWN FOR THE KITTI AND REALESTATEtenK DATASETS.
utt_0015 utt 59.85 63.15 -X THE PREDICTION OF A DENSITY FIELD HAPPENS IN TWO STEPS.
utt_0016 utt 63.69 70.29 -X FIRST, WE PREDICT A PIXEL-ALIGNED FEATURE MAP FROM THE INPUT IMAGE USING A HIGH-CAPACITY ENCODER-DECODER NETWORK.
utt_0018 utt 70.77 77.46 -X EVERY FEATURE IN THIS FEATURE MAP IMPLICITLY DESCRIBES THE DENSITY DISTRIBUTION ALONG THE RAY THROUGH THE RESPECTIVE PIXEL.
utt_0020 utt 78.00 84.21 -X AS SHOWN IN THE FIGURE, THE DISTRIBUTION CAN MODEL GEOMETRY EVEN IN OCCLUDED REGIONS, FOR EXAMPLE BEHIND THE CAR.
utt_0022 utt 84.66 88.24 -X DENSITY ALONG THE RAY CAN BE EVALUATED BY A SMALL MLP.
utt_0023 utt 89.90 93.97 -X IN THE SECOND STEP, WE PERFORM VOLUME RENDERING WITH RAY CASTING.
utt_0024 utt 94.06 101.30 -X WHEN CONSIDERING A POINT X, WE FIRST REPROJECT X ONTO THE CAMERA PLANE AND SAMPLE THE CORRESPONDING FEATURE IN THE FEATURE MAP.
utt_0027 utt 106.52 107.92 -X AND FEED IT INTO A SMALL MLP.
utt_0028 utt 108.59 112.66 -X THIS SMALL MLP FINALLY OUTPUTS VOLUMETRIC DENSITY SIGMA.
utt_0029 utt 114.00 118.87 -X IN CONTRAST TO A NEURAL RADIANCE FIELD, OUR DENSITY FIELD DOES NOT STORE COLOR.
utt_0030 utt 119.19 126.07 -X THEREFORE, TO STILL PERFORM VOLUME RENDERING, WE AGAIN REPROJECT THE POINT AND SAMPLE COLOR DIRECTLY FROM THE IMAGE.
utt_0032 utt 126.51 137.21 -X NOTE, THAT UNLIKE THE FEATURE SAMPLING, WE CAN ALSO SAMPLE COLOR FROM ANY OTHER AVAILABLE VIEWS, FOR EXAMPLE Ione IN THE ILLUSTRATION, WHICH IS VERY IMPORTANT DURING TRAINING.
utt_0034 utt 138.29 145.14 -X COMPARED TO OTHER RELATED WORKS, FOR EXAMPLE PIXELNERF, THERE ARE TWO MAIN DIFFERENCES IN OUR PROPOSED ARCHITECTURE.
utt_0036 utt 146.74 152.47 -X WE DESIGN THE MLP TO BE VERY LIGHTWEIGHT AND GIVE THE ENCODER-DECODER A MUCH HIGHER CAPACITY.
utt_0037 utt 152.50 158.97 -X THROUGH THIS BOTTLENECK, THE MLP CANNOT REASON ABOUT GLOBAL SCENE PROPERTIES AND ONLY DEALS WITH LOCAL GEOMETRY.
utt_0039 utt 159.32 166.20 -X IN TURN, THE ENCODER-DECODER NETWORK HAS TO CAPTURE THE ENTIRE SCENE GEOMETRY AND PRODUCE VERY MEANINGFUL FEATURES.
utt_0041 utt 166.64 178.74 -X THIS CHANGE IS BENEFICIAL FOR LEARNING AND GENERALIZATION, AS THE ENCODER-DECODER IS MUCH BETTER AT CAPTURING THE CONTEXT OF AN ENTIRE IMAGE COMPARED TO THE MLP, WHICH ONLY RECEIVES LOCAL TRAINING SIGNALS.
utt_0044 utt 180.40 188.18 -X BY SAMPLING COLOR INSTEAD OF HAVING THE MLP PREDICT COLOR, THE IMPLICIT FUNCTION THE MLP HAS TO MODEL BECOMES SIGNIFICANTLY SIMPLER.
utt_0046 utt 188.50 193.40 -X IT THEREFORE FURTHER BOOSTS TRAINING STABILITY AND GENERALIZATION.
utt_0047 utt 193.40 197.69 -X ADDITIONALLY, COLOR SAMPLING ENFORCES MULTI-VIEW CONSISTENCY BETWEEN DIFFERENT FRAMES.
utt_0048 utt 197.84 203.16 -X THEREFORE, THE PREDICTED GEOMETRY IS TRAINED TO BE MORE ACCURATE AND WE GET FEWER ARTIFACTS,
utt_0049 utt 203.16 204.83 -X EVEN WITH A SMALL NUMBER OF VIEWS.
utt_0050 utt 206.39 210.36 -X WE TRAIN OUR METHOD THROUGH A NEW SELF-SUPERVISED RECONSTRUCTION LOSS.
utt_0051 utt 210.77 215.96 -X DURING TRAINING, WE HAVE AVAILABLE VIDEO DATA WITH ONE OR MULTIPLE VIEWS PER TIMESTEP.
utt_0052 utt 216.41 221.95 -X A SINGLE SAMPLE CONSISTS OF FRAMES FROM A FEW CONSECUTIVE TIMESTEPS, E.G. THREE.
utt_0053 utt 222.13 226.94 -X ONE OF THE FRAMES IS CONSIDERED THE INPUT FRAME, FROM WHICH WE RECONSTRUCT THE SCENE.
utt_0054 utt 227.16 233.66 -X WE PARTITION ALL FRAMES INTO TWO SETS: LOSS FRAMES AND RENDER FRAMES.
utt_0055 utt 233.66 238.23 -X WE PERFORM VOLUME RENDERING TO RECONSTRUCT THE FRAMES IN THE LOSS PARTITION.
utt_0056 utt 238.23 242.39 -X COLOR IS SAMPLED FROM THE FRAMES OF THE RENDER PARTITION USING THE PREDICTED DENSITIES.
utt_0057 utt 243.06 251.23 -X THE PHOTOMETRIC CONSISTENCY BETWEEN THE RECONSTRUCTED FRAMES AND THE FRAMES IN THE LOSS PARTITION SERVES AS THE SUPERVISION SIGNAL FOR THE DENSITY FIELD.
utt_0059 utt 253.30 261.53 -X ONE OF OUR KEY CONTRIBUTIONS IS THAT IN CONTRAST TO PREVIOUS WORKS, OUR LOSS FORMULATION GIVES SUPERVISION SIGNALS EVEN IN OCCLUDED REGIONS.
utt_0061 utt 262.20 267.84 -X THE CONTINUOUS NATURE OF THE DENSITY FIELD ALLOWS US TO RECONSTRUCT ANY FRAME FROM ANY OTHER FRAME.
utt_0063 utt 268.44 278.36 -X CONSIDER AN AREA P OF THE SCENE, WHICH IS OCCLUDED IN THE INPUT IMAGE, BUT VISIBLE IN TWO OTHER FRAMES Itwo , Ithree, AS DEPICTED IN THE FIGURE.
utt_0065 utt 279.03 282.27 -X DURING TRAINING, WE AIM TO RECONSTRUCT THIS AREA IN Itwo.
utt_0066 utt 283.32 295.42 -X THE RECONSTRUCTION BASED ON COLORS SAMPLED FROM Ithree WILL GIVE A CLEAR TRAINING SIGNAL TO CORRECTLY PREDICT THE GEOMETRIC STRUCTURE OF THIS AREA, EVEN THOUGH P IS OCCLUDED IN THE INPUT IMAGE.
utt_0069 utt 295.42 304.06 -X NOTE, THAT IN ORDER TO LEARN GEOMETRY ABOUT OCCLUDED AREAS, WE REQUIRE AT LEAST TWO ADDITIONAL VIEWS BESIDES THE INPUT IMAGE DURING TRAINING.
utt_0071 utt 304.57 313.57 -X THROUGH OUR PROPOSED ARCHITECTURE AND OUR NOVEL TRAINING SCHEME, WE CAN TRAIN OUR METHOD ON CHALLENGING DATASETS LIKE KITTIminus three hundred and sixty, KITTI, AND REALESTATEtenK.
utt_0073 utt 314.94 322.37 -X ONE OF THE KEY STRENGTHS OF OUR METHOD IS THAT OUR MODEL LEARNS TO RECONSTRUCT THE ENTIRE CAMERA FRUSTUM, INCLUDING OCCLUDED AREAS.
utt_0075 utt 322.97 327.84 -X TO DEMONSTRATE THIS EFFECT, WE PREDICT DENSITY FIELDS FOR THREE SCENES FROM THE KITTI DATASET.
utt_0076 utt 328.31 333.95 -X BELOW THE INPUT IMAGE, YOU CAN SEE THE EXPECTED RAY TERMINATION DEPTH RENDERED FROM THE DENSITY FIELD.
utt_0078 utt 334.40 339.94 -X IN THE CENTER, YOU CAN SEE BIRDS-EYE VIEWS OF THE SCENES, WHERE DARK AREAS DENOTE HIGH DENSITY.
utt_0080 utt 340.28 347.46 -X WE CAN CLEARLY SEE THAT THE RECONSTRUCTED DENSITY FIELD CORRECTLY MODELS THE SCENE AND RECOVERS THE AREAS BEHIND OBJECTS.
utt_0082 utt 347.68 355.46 -X FOR EXAMPLE, IN THE FIRST ROW BEHIND THE CARS ON THE LEFT SIDE, OR IN THE SECOND ROW BEHIND THE CYCLIST.
utt_0084 utt 357.21 362.78 -X WE ALSO VISUALIZE BIRDS-EYE VIEW FOR MONODEPTHtwo AS A REPRESENTATIVE DEPTH PREDICTION METHOD,
utt_0085 utt 362.97 364.58 -X PIXELNERF, AND MINE.
utt_0086 utt 364.58 367.14 -X MONODEPTHtwo DOES NOT PREDICT A FULL threeD VOLUME.
utt_0088 utt 368.77 373.18 -X PIXELNERF PRODUCES A VERY NOISY RECONSTRUCTION AND MINE CAN NOT WELL RECOVER OCCLUDED AREAS.
utt_0091 utt 381.54 382.18 -X PERFORMS BEST.
utt_0092 utt 382.18 389.19 -X EVEN WHEN EVALUATING THE EXPECTED RAY TERMINATION DEPTH AGAINST DEPTH GROUND TRUTH, WE ACHIEVE RESULTS COMPARABLE TO STATE-OF-THE-ART DEPTH PREDICTION METHODS.
utt_0095 utt 393.63 396.03 -6.4910 ARE VERY STABLE AND THE METHOD GENERALIZES VERY WELL.
