utt_0001 utt 0.78 3.28 -X MY NAME IS DENYS KATERENCHUK.
utt_0002 utt 4.14 8.14 -X I AM A PHD STUDENT AT THE GRADUATE CENTER, CITY UNIVERSITY OF NEW YORK.
utt_0003 utt 8.17 15.57 -X AND TODAY I'M GOING TO PRESENT OUR WORK DONE WITH PROFESSOR RIVKA LEVITAN ON HEDGE DETECTION IN TEXT
utt_0005 utt 17.52 28.34 -X . SO IN THIS PAPER THAT YOU SHOULD PROBABLY READ, WE WORK ON IMPROVING OUR CURRENT RESULTS FOR HEDGE DETECTION.
utt_0007 utt 28.34 30.93 -X SO WHAT ARE THE HEDGES?
utt_0008 utt 31.41 39.57 -X HEDGES ARE LINGUISTIC DEVICES THAT ARE USED TO MITIGATE DEGREE OF CERTAINTY IN THE STATEMENT.
utt_0010 utt 39.63 45.81 -X FOR EXAMPLE, LET'S SAY A DOCTOR SAYS TO A PATIENT, I THINK YOU NEED SURGERY IMMEDIATELY.
utt_0012 utt 45.81 52.72 -X HOWEVER, IF WE PAY CLOSER ATTENTION, THE PHRASE I THINK SHOULD SIGNIFY THAT THE
utt_0013 utt 54.16 57.11 -X PERSON IS NOT one hundred% SURE IN THE DIAGNOSIS.
utt_0014 utt 57.26 63.19 -X UH, IN GENERAL, HEDGE PHRASES ARE EXPRESSED THROUGH MODAL VERBS, SUCH:
utt_0015 utt 63.19 67.60 -X AS SHOULD, MIGHT PEACOCK EXPRESSIONS, SUCH AS VERY LIKELY, EVERYONE,
utt_0016 utt 67.60 73.24 -X I THINK AND WEASEL WORDS, SOME BELIEVE CLEARLY, EVERYONE, ET CETERA.
utt_0017 utt 73.24 84.60 -X WHAT'S INTERESTING AND DIFFICULT ABOUT THIS PROBLEM IS THAT THE PRESENCE OF THESE IN THE SENTENCE DOESN'T NECESSARILY MEAN THAT THE SENTENCE IS UNCERTAIN.
utt_0019 utt 85.75 89.43 -X AND THIS FACT MAKES THIS PROBLEM VERY CHALLENGING.
utt_0020 utt 90.35 96.25 -X UH, THERE ARE MULTIPLE APPLICATIONS WHERE THIS CAN BE VERY USEFUL.
utt_0021 utt 96.25 100.98 -X FOR EXAMPLE, AS WE SAW IN THE EARLIER EXAMPLE IN HEALTHCARE IN ENGINEERING,
utt_0022 utt 100.98 113.88 -X WHERE PRECISE LANGUAGE IS VERY IMPORTANT IN POLITICS TO PREVENT MISINFORMATION, OR IN FINANCE, DURING THE ANALYSIS OF, UH, DIFFERENT COMPANIES.
utt_0024 utt 113.88 120.98 -X FOR EXAMPLE, CONSIDER THIS SMALL INTERVIEW WITH ADAM NEWMAN RIGHT AFTER THE, UH,
utt_0025 utt 121.68 127.80 -X SOFTBANK PULLED OUT FROM THEIR DEAL TO INVEST MORE MONEY INTO THE COMPANY.
utt_0026 utt 127.83 134.23 -X LISTEN, HOW ADAM EXPRESSES HIS BELIEF IN THE FUTURE OF THEIR RELATIONSHIPS.
utt_0027 utt 135.60 137.62 -X AND IT'S A REAL PARTNERSHIP.
utt_0028 utt 137.62 140.47 -X AND I THINK THE LONGER WE KNOW EACH OTHER, THE MORE WE CAN BUILD IT.
utt_0029 utt 140.92 147.00 -X WHAT HE IS TRYING TO SAY THAT THE LONGER RELATIONSHIPS ARE GOING TO LAST AND THE STRONGER THEY'RE GOING TO BE.
utt_0031 utt 147.22 157.15 -X BUT HE ADDS, I THINK, WHICH PRETTY MUCH DIMINISHES THE STATEMENT THAT FOLLOWS RIGHT THAT, AND WE ALL KNOW WHAT HAPPENS LATER ON.
utt_0033 utt 158.45 161.34 -X IN THIS WORK, WE FOCUSED ONLY ON TEXTUAL DATA.
utt_0034 utt 164.02 167.19 -X IN PARTICULAR WE'LL LOOK INTO CONLL two thousand and ten WIKIPEDIA CHALLENGE.
utt_0035 utt 167.64 174.39 -X DURING THIS CHALLENGE, THE TEAM RELEASED THIS CORPUS OF, UH, WIKIPEDIA SENTENCES
utt_0036 utt 174.52 177.40 -X LABELED AS CERTAIN OR UNCERTAIN.
utt_0037 utt 177.62 183.93 -X WHAT'S INTERESTING THAT THE MAJORITY OF THE SENTENCES ARE CERTAIN AND ONLY AROUND twenty-two% ARE UNCERTAIN.
utt_0039 utt 185.27 196.73 -X ALSO THIS CORPUS IS ACTUALLY QUITE SMALL IN TODAY'S STANDARDS AND APPLYING THE STATE OF THE ART ALGORITHMS IS QUITE CHALLENGING BECAUSE THE MODEL IS SIMPLY OVER FITS ON THE DATA.
utt_0042 utt 197.85 202.33 -X THERE ARE ONLY eleven,zero TRAINING EXAMPLES UH, AND WE USE ten% OF
utt_0043 utt 202.42 205.21 -X THIS DATA AS OUR VALIDATION SET.
utt_0044 utt 206.90 215.26 -X UH, SO THE FIRST STEP THAT IS DONE DURING ANY RESEARCH IS TO LOOK WHAT HAS BEEN DONE IN THIS DOMAIN.
utt_0046 utt 215.32 220.67 -X SINCE THE CHALLENGE HAS BEEN AROUND FOR OVER ten YEARS, THERE'VE BEEN A FEW INTERESTING PAPERS.
utt_0048 utt 220.86 230.56 -X THE EARLY WORK WOULD MOST OF ALL, FOCUS ON SHALLOW LINGUISTIC FEATURES SUCH AS BAG OF WORDS AND PART OF SPEECH TAGS, DIFFERENTLY UH, VARIATIONS OF
utt_0050 utt 231.51 237.69 -X N-GRAMS TO, UH, EMPHASIZE AND TRYING TO CAPTURE THE CONTEXT WHERE THE WORDS,
utt_0051 utt 237.91 240.73 -X UH, THE TWO PRESENT HEDGES CAN APPEAR.
utt_0052 utt 241.02 255.84 -X LATER WORK WOULD INTRODUCE, UH, THE PROBABILISTIC, UH, NOTION THAT USES THE CONTEXT OF EACH WORD TO EVALUATE WHETHER THIS EXPRESSION CAN BE UNCERTAIN OR NOT.
utt_0054 utt 256.41 262.21 -X MORE RECENT WORKS WOULD USE DIFFERENT VARIATIONS OF NEURAL NETWORKS,
utt_0055 utt 262.21 268.19 -X SUCH AS CONVOLUTIONAL NEURAL NETWORKS, UH, RECURRENT NEURAL NETWORKS, AND OF COURSE ATTENTION.
utt_0057 utt 269.34 274.30 -X SO AFTER REVIEWING ALL THIS LITERATURE, WE DECIDED TO DO SOMETHING DIFFERENT.
utt_0058 utt 274.33 280.03 -X UM, WE WANTED TO SEE IF WE CAN FIND ADDITIONAL SIGNAL
utt_0059 utt 280.38 283.78 -X AND USE IT IN OUR MODEL.
utt_0060 utt 283.78 291.36 -X FOR THIS REASON, WE WANTED TO EXPLORE THE POSSIBILITY OF USING PARTS OF SPEECH TAGS AS ADDITIONAL SOURCE OF INFORMATION.
utt_0062 utt 292.16 299.49 -X SO TO START, WE LOOK AT VARIOUS WORDING, BEDDINGS, WORD EMBEDDINGS ARE TRAINED ON A HUGE AMOUNT OF DATA.
utt_0064 utt 299.68 311.28 -X HOWEVER, THE DIFFERENCE BETWEEN THE DOMAINS, UH, CAN INTRODUCE PROBLEMS FOR THIS REASON, WE SURVEY A NUMBER OF THE MOST POPULAR, EMBEDDINGS
utt_0066 utt 311.28 321.16 -X AS WELL AS TRAIN OUR OWN CUSTOM, UH, EMBEDDING MODEL ON ONE GIGABYTE OF A RANDOM SAMPLE TEXTS FROM WIKIPEDIA.
utt_0068 utt 322.24 328.29 -X AND WE NOTICED THAT TWO MODELS UH, OUTPERFORM THE REST.
utt_0069 utt 328.29 333.54 -X FIRST IS FAST TEXT MODEL AND THE GLOVE MODEL.
utt_0070 utt 333.73 340.93 -X HOWEVER, WE ALSO DECIDED TO INCLUDE OUR CUSTOM MODEL JUST BECAUSE IT'S MUCH MORE IN SIZE AND IDEAL FOR PROTOTYPING.
utt_0072 utt 341.34 346.28 -X THE SECOND STEP IS TO CHOOSE THE CORRECT, NEURAL NETWORK ARCHITECTURE.
utt_0073 utt 347.07 349.51 -X LOOKING AT THE RELATED WORK.
utt_0074 utt 349.54 353.41 -X MOST WORK IS DONE EITHER WITH THE USE OF CONDITIONAL NEURAL NETWORK
utt_0075 utt 353.76 356.29 -X OR RECURRENT NEURAL NETWORKS.
utt_0076 utt 356.99 362.37 -X UH, THAT'S WHY WE CREATE A SIMPLE TWO-LAYER NETWORK ARCHITECTURE
utt_0077 utt 362.59 370.05 -X UH, THAT IS BASED ON CNNS, GRUS, LSTMS AND THEIR VARIANTS WITH ATTENTION LAYER ON TOP OF IT.
utt_0079 utt 370.56 376.87 -X AND WE FIND THAT A GRU AND LSTM BASED MODELS TEND TO OUTPERFORM CNN.
utt_0080 utt 376.87 379.24 -X IT WASN'T THE CASE IN THE PREVIOUS WORK.
utt_0081 utt 379.33 390.95 -X HOWEVER, WE DIDN'T, UH, I SPEND MUCH TIME TUNING THIS ARCHITECTURE, SO WE DECIDED TO, UH, HAVE OUR FOCUS ON THE NETWORKS THAT CAN WORK THE BEST.
utt_0083 utt 392.00 399.24 -X SO AT THIS POINT, WE CHOSEN FOUR ARCHITECTURES, UH, GRU, LSTM, AND THEIR VARIANT WITH ATTENTIONAL LAYER.
utt_0085 utt 399.27 405.61 -X AND THE THIRD STEP IS TO SEE WHETHER OUR HYPOTHESIS, THAT PART OF THE SPEECH TAGS
utt_0086 utt 405.67 408.30 -X CAN INTRODUCE ADDITIONAL INFORMATION.
utt_0087 utt 409.03 414.38 -X SO WE, AGAIN, TRAIN A SIMPLE, UH, TWO LAYER, LSTM, GRU NETWORKS,
utt_0088 utt 414.38 418.86 -X AND THEIR VARIANTS WITH ATTENTION ONLY ON PART OF SPEECH TAGS.
utt_0089 utt 418.86 424.11 -X SO HERE, YOU CAN SEE AN EXAMPLE OF, UH, A TYPICAL INPUT SENTENCE.
utt_0090 utt 424.11 428.14 -X WE CAN SEE THAT THE RESULTS, NOT THAT GREAT.
utt_0091 utt 428.14 430.47 -X AND AT FIRST WE WERE KIND OF DISAPPOINTED.
utt_0092 utt 430.66 442.92 -X UH, HOWEVER WE WANTED TO TEST WHETHER THE TOP PERFORMING MODEL, IN THIS CASE LSTM WITH ATTENTION, CAN HAVE PREDICTIVE POWER ON THE REAL TEST DATA.
utt_0094 utt 442.92 448.75 -X FIRST OF ALL, THERE'S HUGE DISCREPANCY BETWEEN OUR DEVELOPMENT SET AND THE TEST SET.
utt_0096 utt 449.03 452.78 -X AND SECOND OF ALL, WE WERE SURPRISED BY HOW WELL IT PERFORMS.
utt_0097 utt 452.81 457.80 -X AND WE DO SEE THE POTENTIAL TO USE PART OF SPEECH TEXT IN OTHER PROBLEMS AS WELL.
utt_0098 utt 458.89 463.92 -X SO THE QUESTION BECOMES, HOW DO WE LEVERAGE THIS INFORMATION?
utt_0099 utt 463.92 466.44 -X UH, WE COME UP WITH TWO APPROACHES.
utt_0100 utt 466.44 475.44 -X FIRST IS TO COMBINE PART OF SPEECH TAGS, REPRESENTATIONS WITH WORD REPRESENTATIONS IN A SINGLE VECTOR.
utt_0102 utt 475.44 478.64 -X AND THIS VECTOR IS USED AS AN INPUT TO OUR MODEL.
utt_0103 utt 479.08 488.69 -X THE SECOND APPROACH IS TO ACTUALLY TRAIN A JOINT MODEL WHERE, UH, ONE PART OF THE NETWORK IS FOCUSED ON WORDS.
utt_0105 utt 488.69 492.01 -X THE OTHER PART OF THE NETWORK IS FOCUSED ON PARTS OF SPEECH TAGS.
utt_0106 utt 492.14 501.85 -X AND ONLY THE LAST LATENT SPACE IS JOINED, UH, TOGETHER TO PRESENT A SINGLE VECTOR FROM TWO DIFFERENT ARCHITECTURES
utt_0108 utt 501.96 504.69 -X SO NEXT STEP IS TO SEE HOW IT PERFORMS.
utt_0109 utt 504.75 508.62 -X AND FROM THIS GRAPH, WE CAN SEE THAT IT DOES IMPROVE THE PERFORMANCE.
utt_0110 utt 509.10 513.65 -X UH, THE BEST PERFORMING MODELS ARE GRU AND GRU WITH ATTENTION
utt_0111 utt 514.06 516.65 -X ON THE JOINT INPUT MODEL.
utt_0112 utt 516.65 526.45 -X AND IN THE CASE WHERE WE TWO SEPARATE MODELS, GRU WITH ATTENTION AND LSTM WITH ATTENTION, SEEM TO WORK THE BEST.
utt_0114 utt 526.73 535.98 -X SO WE USE THESE THREE MODELS AS OUR MAIN APPROACH, AND WE TRY TO TEST THESE RESULTS ON THE TEST DATA.
utt_0116 utt 535.98 539.12 -X SO HERE ARE OUR FINAL RESULTS.
utt_0117 utt 539.79 542.99 -X OUR MODELS SHOW REALLY GOOD PERFORMANCE.
utt_0118 utt 542.99 549.36 -X AND IN FACT, WE GOT AN, AN IMPROVEMENT BY A COUPLE OF PERCENT OVER MODELS WITHOUT,
utt_0119 utt 549.36 554.55 -X PARTS OF SPEECH TAGS, BUT WHAT WAS VERY INTERESTING AND SURPRISING IS THAT.
utt_0120 utt 554.55 560.85 -X SINCE WE WERE TRAINED ALL THESE DIFFERENT MODELS, WE WANTED TO SEE HOW THEY PERFORM ON OUR TEST DATA.
utt_0122 utt 561.20 570.23 -X AND AS SIMPLE GRU MODEL WITH CUSTOM PRE-TRAINED WORD EMBEDDINGS, SHOW THE BEST RESULTS.
utt_0124 utt 570.64 575.60 -X SO IT WAS QUITE SURPRISING AND ALSO BRINGS MORE QUESTIONS FOR THE FUTURE WORK.
utt_0125 utt 576.46 581.78 -X SO IN CONCLUSION, FIRST, WE FIND THAT THE JOINT MODEL PRESENTATION
utt_0126 utt 582.45 587.19 -X OF WORDS AND PARTS OF SPEECH TAGS DOES IMPROVE THE PERFORMANCE.
utt_0127 utt 587.19 591.89 -X HOWEVER, THE SURPRISING FACT THAT THE HIGHEST SCORE OF seventy point two four WAS ACHIVED
utt_0128 utt 593.33 599.35 -X BY THE DOMAIN SPECIFIC MODEL WITH A SIMPLE GRU, NETWORK ARCHITECTURE.
utt_0129 utt 599.35 600.92 -X IT WAS VERY SURPRISING.
utt_0130 utt 600.92 604.60 -X AND WE WOULD LIKE TO INVESTIGATE IT IN OUR FUTURE WORK.
utt_0131 utt 605.10 613.75 -X IN SUMMARY IN THIS WORK WE ANALYZE VARIOUS NETWORK ARCHITECTURES THAT THE PERFORMED ON THE PROBLEM OF HEDGE DETECTION.
utt_0133 utt 613.75 620.08 -X WE FORMULATE OUR NETWORK AS A JOINT MODEL APPROACH AND ACHIEVE A NEW HIGH SCORE ON THIS PROBLEM.
utt_0135 utt 620.08 624.98 -X THANK YOU VERY MUCH FOR ATTENTION, AND I HOPE TO SEE YOU AT OUR POSTER SESSION.
utt_0136 utt 624.98 625.43 -4.2885 THANK YOU.
