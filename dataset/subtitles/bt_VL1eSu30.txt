utt_0000 utt 0.24 9.10 -X HI, I AM ROMILA, AND I WILL PRESENT OUR WORK CALLED GOPHER, WHICH FOCUSES ON FINDING EFFICIENT WAYS TO GENERATE INTERPRETABLE DATA-BASED EXPLANATIONS OF MACHINE LEARNING MODELS FOR
utt_0002 utt 9.45 10.67 -X FAIRNESS DEBUGGING.
utt_0003 utt 10.99 16.91 -X THIS WORK IS A JOINT EFFORT BETWEEN MY COLLABORATORS AT UC SAN DIEGO AND IIT CHICAGO.
utt_0004 utt 16.91 29.81 -X MACHINE LEARNING SYSTEMS ARE INCREASINGLY USED FOR HIGH-STAKES DECISION MAKING IN DOMAINS SUCH AS LOAN APPLICATIONS, HIRING DECISIONS, HEALTHCARE, AND PRISON SENTENCES, AND HAVE BEEN SHOWN TO BE HIGHLY ACCURATE IN MANY OF THESE DOMAINS.
utt_0007 utt 30.32 32.88 -X ENSURING FAIRNESS OF THESE DECISIONS IS CRITICAL.
utt_0008 utt 32.94 42.22 -X THERE HAVE BEEN MANY RECENT EXAMPLES OF RACIAL DISCRIMINATION, FOR EXAMPLE, IN FRISKING DURING POLICE ROAD STOPS AND IN PREDICTING WHICH PATIENTS WOULD LIKELY NEED EXTRA MEDICAL CARE,
utt_0010 utt 42.22 44.56 -X AND GENDER BIAS IN HIRING DECISIONS.
utt_0011 utt 44.56 50.61 -X THERE IS GROWING CONCERN OVER THE TRANSPARENCY OF THESE MODELS SINCE THEY ARE OFTEN NON-INTUITIVE
utt_0012 utt 51.37 57.52 -X AND HARD TO INTERPRET EITHER BECAUSE THEY ARE HIGHLY COMPLEX OR BECAUSE OF THE PROPRIETARY NATURE OF THEIR LEARNING ALGORITHMS.
utt_0014 utt 59.79 74.77 -X EXPLAINABLE AI AIMS TO ADDRESS MANY OF THESE CONCERNS BY GENERATING HUMAN-UNDERSTANDABLE EXPLANATIONS FOR COMPLEX MODELS AND THEIR OUTPUTS FOR DIFFERENT STAKEHOLDERS OF THE SYSTEM SUCH AS THE MODEL DEVELOPERS, ML PRACTITIONERS, END-USERS AFFECTED BY THE SYSTEM’S DECISIONS,
utt_0017 utt 74.77 77.22 -X ALL THE WAY TO BUSINESSES THAT DEPLOY THE SYSTEMS
utt_0018 utt 77.61 80.72 -X EXPLAINABLE AI HAS THREE PRIMARY GOALS:
utt_0019 utt 82.54 85.83 -X TO DETECT BIAS AND DISCRIMINATION IN THE MODEL’S OUTCOMES
utt_0020 utt 87.09 91.31 -X TO PROVIDE ACTIONABLE INSIGHTS TO CHANGE THE OUTCOMES OF THE MODEL IN THE FUTURE
utt_0021 utt 92.11 98.13 -X AND TO DEBUG ALGORITHMS AND MODELS FOR UNEXPECTED OR DISCRIMINATORY MODEL BEHAVIOR
utt_0022 utt 99.41 109.78 -X TAKE THIS SCENARIO WHERE A MACHINE LEARNING MODEL IS USED TO PREDICT FOR A USER, DEPENDING UPON THEIR CHARACTERISTICS SUCH AS DEMOGRAPHIC AND FINANCIAL INFORMATION, WHETHER THEIR ANNUAL INCOME IS MORE OR LESS THAN fiftyK.
utt_0025 utt 110.61 116.91 -X NOW CONSIDER THIS MODEL IN PRODUCTION AND BEING USED FOR MAKING PREDICTIONS FOR A NUMBER OF INDIVIDUALS.
utt_0027 utt 116.91 125.65 -X THE PREDICTED OUTCOMES ARE THEN USED TO EVALUATE THE MODEL IN TERMS OF SOME FAIRNESS METRIC SUCH AS STATISTICAL PARITY, EQUALIZED ODDS, PREDICTIVE PARITY AND SO ON.
utt_0029 utt 125.90 139.22 -X IT IS FOUND THAT WHEN THE PREDICTIONS ARE USED TO COMPUTE SOME FAIRNESS METRIC, THE MODEL IS FOUND TO FAVOR CERTAIN GROUPS OF INDIVIDUALS OVER OTHERS BASED ON CERTAIN SENSITIVE ATTRIBUTES SUCH AS THEIR GENDER OR RACE OR MARITAL STATUS.
utt_0032 utt 140.30 148.56 -X IN THIS CASE, IT WAS FOUND THAT THE MODEL FAVORED PERSONS WHO IDENTIFIED THEMSELVES AS MALE AND THEY WERE MORE LIKELY TO RECEIVE HIGH SALARY PREDICTIONS.
utt_0034 utt 150.22 152.76 -X DEBUGGING SUCH SOURCES OF BIAS IS IMPORTANT.
utt_0035 utt 152.76 157.94 -X FOR EXAMPLE, CUSTOMERS MIGHT SUBMIT COMPLAINTS ABOUT UNEXPECTED OR DISCRIMINATORY MODEL BEHAVIOR,
utt_0036 utt 158.42 160.53 -X THAT MUST BE RESOLVED.
utt_0037 utt 160.53 168.34 -X DEBUGGING IS IMPORTANT NOT ONLY FOR PAST DECISIONS BUT ALSO FOR FUTURE MODEL PERFORMANCE BECAUSE DECISIONS MADE BY THE MODEL MAY INFLUENCE THE SELECTION OF TRAINING DATA.
utt_0039 utt 168.34 174.61 -X FOR EXAMPLE, PEOPLE WHO GOT REJECTED, MIGHT NOT APPLY AGAIN AND WE MAY SEE LESS OF THEIR DATA IN THE FUTURE.
utt_0041 utt 174.96 189.14 -X FURTHERMORE, SINCE DATA SCIENCE PIPELINES ARE ITERATIVE IN NATURE, COMPANIES CONTINUALLY GATHER MORE DATA AND RETRAIN THEIR MODELS ON THE UPDATED DATA, WHICH MIGHT BE DIFFERENT FROM THE ORIGINAL DATA AND AS A RESULT, MAY LEAD TO THE LEARNED MODEL PRODUCING UNEXPECTED
utt_0044 utt 189.14 191.25 -X RESULTS.
utt_0045 utt 191.25 196.69 -X RECALL THAT ONE OF THE PROMISES IN EXPLAINABLE AI IS FACILITATING DEBUGGING ERRONEOUS RESULTS.
utt_0046 utt 197.01 207.06 -X MOST OF THE EXISTING APPROACHES IN THIS SPACE ARE FOCUSED ON GENERATING FEATURE-BASED EXPLANATIONS THAT ATTRIBUTE THE DECISIONS OF AN ALGORITHM TO ITS INPUTS.
utt_0048 utt 207.18 213.24 -X LIME AND SHAP ARE TWO POPULAR TECHNIQUES THAT EXPLAIN THE MODEL BEHAVIOR FOR THE ENTIRE DATA OR FOR A SINGLE INSTANCE.
utt_0050 utt 213.97 225.88 -X HOWEVER, MOST OF THE FEATURE ATTRIBUTION APPROACHES PRIMARILY FOCUS ON FEATURES CORRELATED WITH THE ADVERSE OUTCOME AND CANNOT EXPLAIN WHY A MODEL EXHIBITS BIAS AND WHAT IS THE ROOT CAUSE OF THE UNEXPECTED OR DISCRIMINATORY OUTCOME.
utt_0053 utt 225.88 233.59 -X NOW, MANY OF THESE BIASES IN THE AI SYSTEM CAN ONLY BE DISCOVERED IN PRODUCTION, THAT IS THE MODEL IS FAIR ON TRAINING DATA BUT NOT FAIR ON TEST DATA.
utt_0055 utt 234.96 238.55 -X MOST OF THESE BIASES CAN BE TRACED BACK TO DATA THAT IT WAS TRAINED UPON.
utt_0056 utt 238.83 243.41 -X NOW THAT DATA MIGHT HAVE HISTORICAL PATTERNS OF BIAS ENCODED IN ITS OUTCOMES.
utt_0057 utt 243.41 247.77 -X FOR EXAMPLE, LOAN DECISIONS HISTORICALLY DISFAVORED AFRICAN AMERICANS.
utt_0058 utt 250.55 255.64 -X BIAS MIGHT ALSO BE A PRODUCT OF DATA COLLECTION OR SAMPLING METHODS MISREPRESENTING THE GROUND TRUTH.
utt_0060 utt 255.64 261.56 -X FOR EXAMPLE, BY COLLECTING DATA FROM GRADUATE STUDENTS, WE MAY SELECT AGAINST LOWER CLASS SUBJECTS.
utt_0062 utt 262.77 273.33 -X ANOTHER EXAMPLE OF BIAS IS CAUSED BY DATA IMBALANCE, FOR EXAMPLE, WHEN THE PREVALENCE OF ENGLISH TEXTS CAUSES A LANGUAGE MODEL TO CLASSIFY TEXT IN OTHER LANGUAGES AS GRAMMATICALLY INCORRECT.
utt_0065 utt 275.67 280.95 -X DATASET POISONING MAY ALSO INTRODUCE BIASES INTO MODELS THAT ARE HARD TO DETECT.
utt_0066 utt 282.10 286.10 -X BIAS MAY ALSO BE INTRODUCED IN THE FORM OF MEASUREMENT ERRORS AND MISCLASSIFICATION
utt_0067 utt 291.54 299.35 -X FOR EXAMPLE, THE INSENSITIVITY OF CHEAPER MEASUREMENT DEVICES MIGHT RESULT IN AN UNDERESTIMATION OF LEAD LEVEL IN THE WATER SUPPLY OF POORER COMMUNITIES.
utt_0069 utt 301.75 306.94 -X NOT ONLY THAT, BIAS MIGHT GET INTRODUCED IN DIFFERENT STAGES ALONG THE DATA SCIENCE PIPELINE.
utt_0070 utt 306.94 315.54 -X FOR EXAMPLE, DATA INTEGRATION FROM MULTIPLE SOURCES CAN LEAD TO MISINTERPRETATION OF SALARY CURRENCY CAUSING AN UNDERESTIMATION OF SALARIES FROM EUROPE.
utt_0072 utt 315.54 321.56 -X NOW THE QUESTION WE ARE INTERESTED IN IS HOW CAN WE DEBUG SUCH SOURCES OF MODEL BIAS.
utt_0073 utt 321.56 328.82 -X AND WE WANT TO BE ABLE TO DO THAT NOT IN THE TRAINING PHASE BUT ONCE THE MACHINE LEARNING MODEL IS IN PRODUCTION, AND IS ALREADY BEING USED FOR MAKING PREDICTIONS.
utt_0075 utt 330.48 338.49 -X A POPULAR TECHNIQUE FOR DEBUGGING MODEL ERROR IS SLICELINE, THAT IDENTIFIES PARTS OF THE DATA WHERE THE MODEL PERFORMS MUCH WORSE THAN ON THE ENTIRE DATA.
utt_0077 utt 339.54 346.46 -X IT IS NOT DIRECTLY APPLICABLE TO FAIRNESS WHERE, UNLIKE MODEL ERROR, BIAS DUE TO A SUBSET IS NOT ADDITIVE.
utt_0079 utt 347.60 352.54 -X ANOTHER RELATED WORK, RAIN, IDENTIFIES DATA POINTS RESPONSIBLE FOR USER CONSTRAINTS SPECIFIED
utt_0080 utt 352.72 356.02 -X BY A SQL QUERY USING THE CONCEPT OF INFLUENCE FUNCTIONS.
utt_0081 utt 362.10 366.52 -X DATA DEBUGGING HAS ALSO LONG BEEN A PART OF THE DATA CLEANING COMMUNITY WHERE THE FOCUS
utt_0082 utt 368.47 375.64 -X IS ON DETECTING VIOLATIONS OF INTEGRITY CONSTRAINTS AND STATISTICAL CONSTRAINTS AND IDENTIFYING DATA INSTANCES RESPONSIBLE FOR THE VIOLATIONS.
utt_0084 utt 375.73 381.91 -X HOWEVER, THESE TECHNIQUES HAVE BEEN INDEPENDENT OF THE DOWNSTREAM ML TASK, AND HAVE NOT BEEN STUDIED FOR THE TASK OF MODEL BIAS.
utt_0086 utt 385.11 394.07 -X THE ML COMMUNITY OFFERS PRE-PROCESSING TECHNIQUES THAT REFER TO BIAS MITIGATION METHODS APPLIED TO TRAINING DATA BEFORE A MODEL IS TRAINED ON IT.
utt_0088 utt 394.29 400.22 -X ONE EXAMPLE IS ALTERING WEIGHTS ON ROWS OF THE DATA TO ACHIEVE GREATER PARITY IN ASSIGNED OUTCOMES.
utt_0090 utt 400.37 404.22 -X AGAIN, THESE TECHNIQUES ARE INDEPENDENT OF THE DOWNSTREAM ML TASK.
utt_0091 utt 404.73 409.00 -X FURTHERMORE, THERE ARE SOLUTIONS TAILORED TO DATA POISONING ATTACKS THAT
utt_0092 utt 414.74 421.56 -X TYPICALLY IDENTIFY OUTLIERS AS POISONED DATA AND DO NOT DETECT TARGETED ATTACKS ON FAIRNESS.
utt_0093 utt 424.31 432.41 -X WE PRESENT GOPHER THAT GENERATES POST HOC DATA-BASED EXPLANATIONS THAT TRACE ADVERSE MODEL OUTCOMES BACK TO THE DATA THAT THE MODEL IS TRAINED UPON.
utt_0095 utt 433.27 437.79 -X AND IDENTIFIES THE TRAINING DATA INSTANCES CAUSALLY RESPONSIBLE FOR THE MODEL BIAS, I.E.,
utt_0096 utt 439.93 444.57 -X HAD THEY NOT BEEN IN TRAINING DATA, THE LEARNED MODEL WOULD HAVE MADE DIFFERENT DECISIONS.
utt_0097 utt 446.49 456.31 -X GOPHER FORMALIZES THE CONCEPT OF ROOT CAUSES OF BIAS WHERE A SUBSET OF TRAINING DATA IS A ROOT CAUSE IF RETRAINING THE MODEL AFTER INTERVENING ON IT REDUCES MODEL BIAS,
utt_0099 utt 457.17 465.27 -X AND CAPTURES ITS CONTRIBUTION TOWARD MODEL BIAS THROUGH WHAT WE CALL AS ITS CAUSAL RESPONSIBILITY.
utt_0100 utt 465.46 469.72 -X WE DEFINE AN INTERVENTION AS EITHER REMOVING OR UPDATING ONE OR MORE TRAINING DATA INSTANCES.
utt_0101 utt 469.85 477.07 -X FOR EXAMPLE, HERE WERE THIS INSTANCE REMOVED FROM TRAINING DATA AND A NEW MODEL LEARNED ON THE MODIFIED DATA
utt_0103 utt 477.46 479.48 -X THE MODEL BIAS WOULD HAVE REDUCED FROM twenty% TO eighteen%.
utt_0104 utt 480.63 484.76 -X THUS, THE CAUSAL RESPONSIBILITY OF THIS INSTANCE IS twenty%minus eighteen% = zero point zero two AND THIS INSTANCE IS A ROOT
utt_0105 utt 484.95 485.95 -X CAUSE OF BIAS.
utt_0106 utt 487.48 496.44 -X NOTE THAT TO COMPUTE THE CAUSAL RESPONSIBILITY OF EACH INSTANCE, WE NEED TO REMOVE OR UPWEIGHT IT, RETRAIN THE MODEL AND MEASURE CHANGE IN FAIRNESS.
utt_0108 utt 496.89 502.46 -X RETRAINING THE MODEL MIGHT BE COMPUTATIONALLY EXPENSIVE FOR COMPLEX MODELS AND FOR A LARGE NUMBER OF TRAINING INSTANCES.
utt_0110 utt 502.84 515.32 -X TO ADDRESS THIS CHALLENGE, WE OBSERVE THAT IN MACHINE LEARNING, THERE IS OFTEN A LOSS FUNCTION THAT EVALUATES HOW FAR A PREDICTION IS FROM THE ACTUAL OUTCOME, AND THE LEARNING ALGORITHM SEARCHES FOR A MODEL THAT MINIMIZES THIS LOSS FUNCTION.
utt_0113 utt 515.90 530.52 -X TO COMPUTE THE INFLUENCE OF A TRAINING DATA POINT ON THIS LOSS, WE USE FIRST-ORDER INFLUENCE FUNCTION APPROXIMATIONS THAT BORROW THE IDEA OF INFLUENCE FROM ROBUST STATISTICS TO COMPUTE THE CHANGE IN MODEL PARAMETERS WHEN THE TRAINING DATA POINT IN QUESTION IS UPWEIGHTED BY A VERY SMALL AMOUNT EPSILON.
utt_0117 utt 530.52 537.63 -X INFLUENCE FUNCTIONS HAVE PROVEN TO BE EFFECTIVE IN TRACING ERRORS IN ML MODELS AND ARE MUCH FASTER THAN RETRAINING.
utt_0119 utt 538.84 544.54 -X TRADITIONALLY, DATA-BASED EXPLANATIONS LIST INSTANCES RANKED IN DECREASING ORDER OF RESPONSIBILITY
utt_0120 utt 544.57 547.80 -X AND IDENTIFY THE TOP-K INFLUENTIAL DATA POINTS.
utt_0121 utt 547.80 550.84 -X BUT THERE IS NO WAY TO INTERPRET AND SUMMARIZE THOSE EXPLANATIONS.
utt_0122 utt 551.77 559.42 -X IN CONTRAST, GOPHER’S EXPLANATIONS ARE INTERPRETABLE, THAT IS, THEY CAN BE EXPRESSED IN THE FORM OF PREDICATE-BASED PATTERNS.
utt_0124 utt 559.42 565.57 -X FOR EXAMPLE, HERE WE CAN SAY THAT THE SUBSET OF TRAINING DATA WHERE GENDER=‘MALE’ IS THE MOST RESPONSIBLE TOWARD BIAS.
utt_0126 utt 566.59 573.18 -X TO GENERATE SUCH INTERPRETABLE EXPLANATIONS, WE NEED TO KNOW HOW MUCH SUCH A COHERENT SUBSET CONTRIBUTES TO BIAS.
utt_0128 utt 573.18 586.25 -X WHICH IS DIFFERENT FROM THE NAÏVE WAY OF SIMPLY ADDING THE FIRST-ORDER INFLUENCES OF INDIVIDUAL INSTANCES IN THE SET BECAUSE (A) THE CHANGE IN OPTIMAL MODEL PARAMETERS AFTER REMOVING THAT SUBSET CAN BE LARGE, AND (B) IT IGNORES POSSIBLE CROSS CORRELATIONS THAT
utt_0131 utt 586.25 588.32 -X MAY EXIST AMONG DATA POINTS IN THE SET.
utt_0132 utt 588.73 598.05 -X INSTEAD, WE USE SECOND-ORDER GROUP INFLUENCE FUNCTIONS THAT DOES NOT IGNORE SECOND-ORDER TERMS IN THE INFLUENCE APPROXIMATION AND CAPTURES DATA CORRELATIONS WITHIN A SUBSET.
utt_0134 utt 598.05 608.22 -X NOW COMPUTING THESE EXPLANATIONS REQUIRES COMPUTING THE CAUSAL RESPONSIBILITY OF ALL TRAINING DATA SUBSETS WHICH IS EXPONENTIAL IN THE NUMBER OF ATTRIBUTES AND ATTRIBUTE VALUES.
utt_0137 utt 609.47 621.15 -X TO ADDRESS THIS CHALLENGE, WE DEVELOP AN APPROACH INSPIRED FROM FREQUENT ITEMSET MINING IN DATA MINING WHERE ITEMSETS WITH N NUMBER OF ITEMS ARE GENERATED BY SUCCESSIVELY MERGING CANDIDATE ITEMSETS OF SMALLER SIZE.
utt_0140 utt 621.43 626.65 -X FOR EXAMPLE, THESE TWO SUBSETS WITH TWO PREDICATES ARE MERGED TO FORM A SUBSET WITH THREE PREDICATES.
utt_0141 utt 626.78 630.91 -X HOWEVER, BUILDING PATTERNS BOTTOM-UP DOES NOT REDUCE THE SIZE OF THE SEARCH SPACE.
utt_0142 utt 631.03 633.31 -X TO DO SO, WE ADOPT TWO PRUNING RULES.
utt_0143 utt 633.47 639.36 -X THE FIRST IS THAT WE ASSUME A SUPPORT THRESHOLD AS INPUT AND ONLY CONSIDER PATTERNS WHOSE SUPPORT IS ABOVE THAT THRESHOLD.
utt_0145 utt 639.36 645.73 -X FOR EXAMPLE, LET’S SAY THE THRESHOLD IS five% WHICH MEANS WE ARE ONLY INTERESTED IN PATTERNS THAT CONTAIN AT LEAST five% OF THE ENTIRE DATASET.
utt_0147 utt 645.98 654.14 -X THEN THIS SUBSET HERE WITH THRESHOLD LESS THAN five% AND THE ENTIRE SUBTREE CONSISTING
utt_0148 utt 655.00 660.48 -X OF SUBSETS WHERE THE SUBSET IS A PART OF, IS NOT CONSIDERED FOR COMPUTING CAUSAL RESPONSIBILITY.
utt_0149 utt 660.51 673.57 -X FOR THE SECOND RULE, WE COMPUTE THE AVERAGE RESPONSIBILITY PER INSTANCE OF A SUBSET, WHICH WE TERM AS INTERESTINGNESS OF THE SUBSET, AND ONLY CONSIDER A SUBSET THAT IS MORE INTERESTING THAN THE SUBSETS THAT IT WAS MERGED FROM.
utt_0152 utt 673.57 685.44 -X FOR EXAMPLE HERE, THE INTERESTINGNESS OF THE MERGED SUBSET IS LESS THAN THAT OF THE PARENT SUBSETS AND WE DO NOT CONSIDER IT AND ITS SUBTREE FURTHER.
utt_0154 utt 685.63 689.95 -X THE RATIONALE FOR THIS HEURISTIC IS THAT PATTERNS WITH MORE PREDICATES ARE HARDER TO INTERPRET.
utt_0155 utt 689.98 696.03 -X THUS, AN INCREASE IN THE NUMBER OF PREDICATES SHOULD BE JUSTIFIED BY AN INCREASED IMPACT ON THE BIAS OF THE MODEL.
utt_0157 utt 696.03 705.70 -X SO, AT THE END OF THESE PRUNING STEPS, WE HAVE A SMALLER SET OF SUBSETS FOR WHICH WE COMPUTE THE CAUSAL RESPONSIBILITY AND SORT IN DECREASING ORDER OF INTERESTINGNESS TO GENERATE THE TOP-K EXPLANATIONS.
utt_0160 utt 707.04 721.44 -X NOW TO EVALUATE GOPHER, WE GENERATED EXPLANATIONS FOR THE GERMAN CREDIT DATA THAT CONTAINS DEMOGRAPHIC AND FINANCIAL INFORMATION OF INDIVIDUALS AND THE TASK IS TO PREDICT, WHETHER A USER IS A HIGH CREDIT RISK AND SHOULD NOT BE GRANTED A LOAN OR IF THEY ARE A LOW CREDIT RISK AND SHOULD BE GRANTED A LOAN.
utt_0164 utt 722.40 731.49 -X THIS DATASET IS BIASED TOWARD OLDER INDIVIDUALS AND CONSIDERS THEM LESS LIKELY TO BE CHARACTERIZED AS HIGH CREDIT RISKS, AND WE CONSIDER AGE AS THE SENSITIVE ATTRIBUTE.
utt_0166 utt 731.80 736.48 -X HERE, WE SHOW THE TOPminus three EXPLANATIONS WHICH ARE COHERENT SUBSETS, ALONG WITH THEIR SUPPORT
utt_0167 utt 736.54 741.31 -X AND THE EXTENT TO WHICH THEY REDUCE MODEL BIAS WHEN REMOVED.
utt_0168 utt 741.31 745.59 -X WE OBSERVE THAT ONE SUBSET OF five% OF DATA POINTS EXPLAINS MORE THAN HALF OF THE MODEL BIAS
utt_0169 utt 746.21 750.11 -X WHEREAS ANOTHER SUBSET OF AROUND six% OF DATA POINTS REDUCES BIAS BY MORE THAN A THIRD.
utt_0170 utt 750.11 753.95 -X SO, THESE ARE VERY SMALL FRACTIONS OF DATA THAT EXPLAIN A LOT OF THE MODEL BIAS.
utt_0171 utt 754.21 760.64 -X THESE EXPLANATIONS HIGHLIGHT FRACTIONS OF TRAINING DATA THAT MAY HAVE POTENTIAL ERRORS AND HENCE, NEED ATTENTION.
utt_0173 utt 760.80 767.59 -X ON INSPECTION, WE FOUND THAT THEY CORRESPOND TO TRAINING DATA POINTS WHERE OLDER INDIVIDUALS ARE PRIMARILY LABELED AS LOW CREDIT RISKS.
utt_0175 utt 767.77 776.35 -X BY REMOVING THESE INSTANCES, THE PROBABILITY OF AN INDIVIDUAL BEING CLASSIFIED AS A HIGH/LOW CREDIT RISK IS UNIFORMLY DISTRIBUTED ACROSS THE SENSITIVE ATTRIBUTE AGE.
utt_0177 utt 776.64 781.57 -X AS A RESULT, THE MODEL’S DEPENDENCY ON AGE IS REDUCED, THUS REDUCING THE OVERALL MODEL BIAS.
utt_0179 utt 781.82 791.20 -X NOTE THAT THE TOPminus two EXPLANATIONS CONSIST OF PREDICATES WITH THE SENSITIVE ATTRIBUTE FOR THIS DATASET, SIGNIFYING ITS IMPORTANCE IN BIAS REDUCTION.
utt_0181 utt 791.20 797.38 -X WE ALSO GENERATED UPDATE-BASED EXPLANATIONS, WHERE INSTEAD OF DELETING A SUBSET, WE UPDATE IT HOMOGENEOUSLY SUCH THAT MODEL BIAS IS REDUCED.
utt_0183 utt 799.97 806.11 -X WE IDENTIFY A PERTURBATION VECTOR THAT UPDATES EACH INSTANCE IN THE SUBSET, IN THE SAME DIRECTION,
utt_0184 utt 806.11 807.30 -X BY THE SAME MAGNITUDE.
utt_0185 utt 807.30 813.47 -X FOR EXAMPLE, IF THE UPDATE TO HOURS IS FOUND TO BE eight HOURS, THEN THE WORKING HOURS OF ALL INSTANCES ARE INCREASED BY eight HOURS.
utt_0187 utt 813.50 817.39 -X MOREOVER, WE ADD DOMAIN CONSTRAINTS TO MAKE SURE THAT THE UPDATED INSTANCES LIE WITHIN
utt_0188 utt 817.50 819.17 -X THE INPUT DOMAIN.
utt_0189 utt 819.55 823.01 -X DETAILS ON THIS APPROACH ARE IN THE PAPER.
utt_0190 utt 823.01 833.92 -X SO FOR EXAMPLE HERE, IN THE HIGHLIGHTED EXPLANATION, WE SHOW THE ORIGINAL SUBSET, ITS SUPPORT AND BIAS REDUCTION WHEN REMOVED AND THE BIAS REDUCTION WHEN ITS INSTANCES ARE UPDATED HOMOGENEOUSLY.
utt_0192 utt 834.40 839.17 -X SO HERE, THE INSTANCES ARE UPDATED TO BE IN THE PROTECTED GROUP NOW (AGE &LT forty-five) AND ARE
utt_0193 utt 839.55 840.84 -X OF A DIFFERENT GENDER.
utt_0194 utt 841.12 846.80 -X WE FOUND THAT THIS UPDATE REDUCES BIAS SUBSTANTIALLY– UP TO forty-two% COMPARED TO fifty-five% WHEN WE REMOVE THIS
utt_0195 utt 847.52 848.90 -X SUBSET ALTOGETHER.
utt_0196 utt 850.37 863.17 -X IN SUMMARY, GOPHER IS A SYSTEM THAT PRODUCES COMPACT, INTERPRETABLE, AND CAUSAL EXPLANATIONS FOR BIAS OR UNEXPECTED MODEL BEHAVIOR BY IDENTIFYING COHERENT SUBSETS OF THE TRAINING DATA THAT ARE ROOT-CAUSES FOR THIS BEHAVIOR.
utt_0199 utt 863.55 871.88 -X WE INTRODUCE THE CONCEPT OF CAUSAL RESPONSIBILITY THAT QUANTIFIES THE EXTENT TO WHICH INTERVENING ON TRAINING DATA BY REMOVING OR UPDATING SUBSETS OF IT CAN RESOLVE THE BIAS.
utt_0201 utt 872.13 877.68 -X BUILDING ON THIS CONCEPT, WE DEVELOP AN EFFICIENT APPROACH FOR GENERATING THE TOP-K PATTERNS
utt_0202 utt 878.11 888.10 -X THAT EXPLAIN MODEL BIAS BY UTILIZING TECHNIQUES FROM THE MACHINE LEARNING (ML) COMMUNITY TO APPROXIMATE CAUSAL RESPONSIBILITY, AND BY USING PRUNING RULES TO MANAGE THE LARGE SEARCH SPACE FOR PATTERNS.
utt_0205 utt 888.10 894.53 -X WITH THIS, I WOULD LIKE TO CONCLUDE THE TALK AND WOULD ENCOURAGE YOU TO READ OUR PAPER FOR MORE DETAILS AND REACH OUT TO US IF YOU HAVE ANY QUESTIONS.
