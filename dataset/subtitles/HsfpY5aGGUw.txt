utt_0000 utt 0.13 6.19 -X HELLO, MY NAME IS GERARDO ROA DABIKE, AND I'M A PHD STUDENT AT THE UNIVERSITY OF SHEFFIELD.
utt_0002 utt 6.19 13.78 -X TODAY, I WILL BE PRESENTING OUR PAPER WITH JON BARKER TITLED THE USE OF VOICE SOURCE FEATURES FOR SUNG SPEECH RECOGNITION.
utt_0004 utt 13.87 20.46 -X IN THIS PAPER, WE ASK WHETHER VOICE SOURCE FEATURES SUCH AS THE PITCH, DEGREE OF VOICING,
utt_0005 utt 20.46 25.97 -X AND VOICE QUALITY FEATURES CAN HELP IMPROVE AUTOMATIC SUNG SPEECH RECOGNITION PERFORMANCES.
utt_0006 utt 26.16 33.43 -X WE ARGUE THAT CONCLUSIONS DRAWN FROM STUDIES ON SPOKEN SPEECH MAY NOT BE VALID IN THE SUNG SPEECH DOMAIN.
utt_0008 utt 33.97 36.31 -X WHY ARE WE WORKING ON SUNG SPEECH?
utt_0009 utt 36.31 41.33 -X WELL, THERE ARE SEVERAL APPLICATIONS FOR MUSIC INFORMATION RETRIEVAL, JUST A FEW EXAMPLES:
utt_0010 utt 41.33 45.62 -X RETRIEVE THE SONG LYRICS BY RECOGNISING A SEGMENT OF A SONG.
utt_0012 utt 45.87 55.91 -X OR RETRIEVE SONG INFORMATION LIKE THE AUTHOR, THE TITLE AND THE ALBUM JUST BY SINGING A SMALL PART OF THE SONG OR INDEXING A MUSIC COLLECTION USING LYRICS KEYWORDS
utt_0015 utt 56.40 65.62 -X BESIDES THE MUSIC INFORMATION RETRIEVAL APPLICATIONS, SUNG SPEECH IS INHERENTLY LESS INTELLIGIBLE THAN SPOKEN SPEECH, AND MISHEARD LYRICS ARE VERY COMMON.
utt_0017 utt 65.81 79.41 -X SO, LEARN HOW TO ADAPT SPEECH TECHNOLOGIES TO SUNG SPEECH MAY PROVIDE US WITH SOME INSIGHTS INTO HOW TO ADJUST TO OTHER TYPES OF LESS INTELLIGIBLE SPEECH SIGNALS LIKE DYSARTHRIC SPEECH AND PERHAPS EVEN CASUAL SPEECH.
utt_0020 utt 80.56 83.25 -X THE REST OF THE PRESENTATION IS STRUCTURED AS FOLLOW.
utt_0021 utt 83.50 86.58 -X FIRST, I WILL BE PRESENTING THE MOTIVATION OF OUR PAPER.
utt_0022 utt 86.83 95.89 -X THEN, I WILL BE PRESENTING AN ANALYSIS OF THE CHALLENGES OF SUNG SPEECH RECOGNITION AND HOW THE VOCAL SOURCE FEATURE MAY BE USEFUL TO ALLEVIATE THESE CHALLENGES.
utt_0024 utt 96.27 108.44 -X LAST, I WILL BE PRESENTING RECOGNITION EXPERIMENTS COMPARING THE PERFORMANCE OF A BASELINE SYSTEM USING TRADITIONAL ASR FEATURES WITH SYSTEMS USING AUGMENTED FEATURE VECTORS WITH VOICE-SOURCE BASED FEATURES.
utt_0027 utt 108.69 120.92 -X OUR MOTIVATIONS SUCCESSFUL ASR SYSTEMS FOR SPOKEN SPEECH ARE TRADITIONALLY TRAINED USING CEPSTRAL FEATURES LIKE MFCCS TO CAPTURE THE CHARACTERISTICS OF THE VOCAL FILTER.
utt_0030 utt 120.92 130.23 -X IN ADDITION, THESE SYSTEMS USE SOME SORT OF SPEAKER REPRESENTATION, LIKE I-VECTORS, THAT HELP THE ASR PERFORMANCES BY CONDITIONING THE MODELS.
utt_0032 utt 130.55 140.98 -X HOWEVER, SINCE SUNG SPEECH IS LESS INTELLIGIBLE THAN SPOKEN SPEECH, MUCH POORER PERFORMANCES ARE OBTAINED WHEN WE USE JUST THESE REPRESENTATIONS FOR SUNG SPEECH RECOGNITION.
utt_0034 utt 140.98 149.85 -X IN SPOKEN SPEECH SCENARIOS, VOICE SOURCE FEATURES LIKE PITCH AND DEGREE OF VOICING HELP TO IMPROVE THE DISCRIMINATION BETWEEN VOICED AND UNVOICED SOUNDS.
utt_0036 utt 149.85 155.45 -X OR, BY EXPLOITING THE RELATIONSHIP BETWEEN THE PITCH AND THE VOCAL TRACT FOR VOCAL TRACT NORMALISATION.
utt_0038 utt 156.47 164.05 -X NOW, I WILL PRESENT SOME OF THE CHALLENGES WE FACED WHEN DEALING WITH SUNG SPEECH AND HOW THE VOCAL SOURCE FEATURES MAY BE USEFUL FOR ASR SYSTEMS.
utt_0040 utt 166.13 172.44 -X FOR THIS ANALYSIS, WE UTILISED THE NUSminus forty-eightE SUNG AND SPOKEN LYRICS CORPUS.
utt_0041 utt 172.76 178.33 -X THE CORPUS IS COMPOSED OF SUNG AND SPOKEN LYRICS OF forty-eight SONGS FROM twelve SPEAKERS.
utt_0042 utt 178.80 183.06 -X THIS DATASET POSSESSES SPEAKERS WITH AMERICAN OR SINGAPOREAN ACCENTS.
utt_0043 utt 183.09 190.07 -X THE DIFFERENCES IN ACCENT MAY RESULT IN A LARGER VARIABILITY ON THE SPOKEN SPEECH PRODUCTION THAN ON THE SUNG SPEECH.
utt_0045 utt 190.07 199.38 -X THIS BECAUSE, WHEN SINGING, THE NATIVE ENGLISH SPEAKERS ACCENT TEND TO NEUTRALISE, AND THERE IS A TENDENCY TO MOVE TOWARDS AMERICAN PRONUNCIATION.
utt_0047 utt 199.38 205.50 -X FIRST, USING THE NUSminus forty-eightE CORPUS, WE COMPARED THE VOWEL DURATION BETWEEN SUNG AND SPOKEN SPEECH.
utt_0049 utt 206.26 211.99 -X IN SPOKEN ASR, THE DURATION OF A VOWEL CAN AID TO DISCRIMINATE BETWEEN SHORT AND LONG VOWELS.
utt_0051 utt 212.12 226.49 -X LIKE IN BIT VS BEAT IN THIS PLOT, WE CAN SEE THAT THE DURATION OF THE SPOKEN SHORT E IS CONCENTRATED IN ABOUT one hundred MILLISECONDS, BUT THE LONG E IS MORE SPREAD REACHING VALUES AS LONG AS five hundred MILLISECONDS.
utt_0054 utt 227.83 239.74 -X HOWEVER, IN SINGING, AS WE CAN SEE IN THE FIGURE ON THE RIGHT-HAND SIDE, A VOWEL CAN BE EXTENDED MUCH LONGER THAN WHEN SPEAKING, THIS TO ACHIEVE THE RHYTHM DICTATED BY THE SONG.
utt_0057 utt 239.74 245.08 -X THIS DIFFERENCE IN THE VOWEL DURATION INVALIDATES THE DURATION AS A DISCRIMINATOR BETWEEN SHORT AND LONG VOWELS.
utt_0059 utt 245.08 256.76 -X IT HAS BEEN SHOWN THAT EXTENDED SUNG SPEECH VOWELS DURATIONS CAN LEAD TO INSERTION OR SUBSTITUTIONS RECOGNITION ERRORS, ESPECIALLY IF PITCH VARIES DURING THE VOWEL.
utt_0061 utt 256.76 263.96 -X AS I MENTIONED BEFORE, THE VOICING DEGREE CAN HELP DISCRIMINATE BETWEEN VOICED AND UNVOICED SOUNDS- LIKE IN PRICE AND PRIZE.
utt_0063 utt 264.89 273.24 -X IN THIS FIGURE, WE CAN SEE THE “PROBABILITY OF VOICING” FROM THE SPOKEN VOICED, AND UNVOICED FRICATIVE PAIR MEASURED USING THE KALDI TOOLKIT,
utt_0065 utt 273.24 276.54 -X HERE, SMALLER VALUES MEAN MORE STRONGLY VOICED.
utt_0066 utt 276.95 281.85 -X WE CAN SEE THAT THERE IS A SIGNIFICANT OVERLAP BETWEEN THE DEGREE OF VOICING OF BOTH PHONEMES,
utt_0067 utt 282.17 285.28 -X WITH SEVERAL VOICED SOUNDS PRONOUNCED MORE VOICELESS.
utt_0068 utt 285.98 294.84 -X WE HYPOTHESIZED THAT IN SINGING, THE SPEECH MIGHT BE MORE HEAVILY VOICED IN OTHER TO CARRY THE MELODY, WHICH MAY DIFFICULTIES THE VOICED-UNVOICED DISCRIMINATION.
utt_0070 utt 295.03 304.67 -X HOWEVER, FOR SUNG SPEECH, VOICED SOUNDS SHOWED A GREATER VOICING DEGREE, WHICH LED TO A HIGHER BHATTACHARYYA DISTANCE IN COMPARISON WITH THE DISTANCE OF THE SPOKEN SOUNDS.
utt_0072 utt 304.95 311.29 -X THIS MAY SUGGEST THAT THE DEGREE OF VOICING CAN, IN FACT, BE MORE HELPFUL FOR SUNG SPEECH THAN FOR SPOKEN SPEECH.
utt_0074 utt 311.48 324.06 -X HOWEVER, FOR THE CASE OF PLOSIVE SOUNDS LIKE P AND B IN PEAK AND BEAK, WE FOUND THAT WHEN SINGING, THE PS ARE PRONOUNCED SLIGHTLY MORE VOICED THAN IN SPOKEN SPEECH.
utt_0076 utt 324.06 329.92 -X AND THE BS ARE PRONOUNCED WITH A MORE CONSISTENT VOICING DEGREE, MAKING BOTH SOUNDS LESS DISTINGUISHABLE
utt_0077 utt 330.14 331.93 -X IN SUNG SPEECH.
utt_0078 utt 331.93 342.05 -X THIS MAY LIMIT THE USE OF VOICED DEGREE AS A DISCRIMINATOR TO CERTAIN GROUPS OF PHONEMES LIKE FRICATIVE SOUNDS AND NOT BE AS HELPFUL FOR PLOSIVE SOUNDS.
utt_0080 utt 342.05 345.18 -X THEN, WE EVALUATED THE DIFFERENCES IN THE PITCH RANGES.
utt_0081 utt 346.01 351.26 -X THIS PLOT SHOWS THE PITCH RANGES BETWEEN GENDERS FOR THE TWO TYPES OF SPEECH STYLES.
utt_0082 utt 351.67 359.26 -X FROM THIS PLOT, WE CAN SEE THAT THERE IS A CLEAR DISTINCTION IN THE SPOKEN PITCH RANGE PER GENDER, WITH A SMALL OVERLAP..
utt_0084 utt 360.35 365.95 -X HOWEVER, THESE RANGES ARE MUCH LARGER WHEN SINGING, EXISTING A LARGE OVERLAP BETWEEN GENDERS.
utt_0086 utt 365.95 374.30 -X THIS LARGE RANGE IMPLIES THAT VIRTUALLY A SINGER CAN HAVE DIFFERENT VOICES WHEN SINGING AT DIFFERENT FREQUENCIES OF THEIR VOCAL RANGE.
utt_0088 utt 374.75 380.06 -X THIS CAN AFFECT THE ROLE OF THE I-VECTORS AS A SPEAKER NORMALISATION IN ASR SYSTEMS.
utt_0089 utt 380.41 386.75 -X THEREFORE, INFORM THE MODELS WITH THE PITCH CAN HELP TO CONDITIONING THE MODELS TO SPEAKERS AT DIFFERENT PITCHES.
utt_0091 utt 387.64 393.41 -X FOR THE VOICE QUALITY PARAMETERS, WE ANALYSED THE JITTER, SHIMMER AND HARMONIC TO NOISE RATIO.
utt_0093 utt 393.41 397.38 -X THESE ARE COMMON PARAMETERS FOR VOICE PATHOLOGY DETECTIONS.
utt_0094 utt 397.38 404.10 -X JITTER MEASURES THE FREQUENCY VARIATION BETWEEN CYCLE TO CYCLE SHIMMER MEASURES THE AMPLITUDE VARIATION OF THE SOUND WAVE.
utt_0096 utt 404.22 410.82 -X AND THE HARMONIC TO NOISE RATIO MEASURES THE RATIO BETWEEN THE HARMONICS AND THE GLOTTAL NOISE.
utt_0098 utt 410.82 416.99 -X THESE PLOTS SHOW THE PARAMETERS' CENTRAL TENDENCY AND ERROR BARS PER GENDER AND HOW THEY VARY
utt_0099 utt 417.28 420.23 -X BY SPEECH STYLE.
utt_0100 utt 420.23 431.87 -X THESE PARAMETERS SHOW SOME GENDER AND SPEAKER CORRELATION IN THE SPOKEN SPEECH, BEING THE JITTER AND SHIMMER RANGES HIGHER, AND HNR RANGE LOWER THAN IN SUNG SPEECH.
utt_0102 utt 432.73 440.80 -X THESE PARAMETERS LOST MOST OF THEIR GENDER DIFFERENTIATION IN SUNG SPEECH, POSSIBLY WEAKENING THEIR VALUE FOR SPEAKER NORMALISATION.
utt_0104 utt 440.80 452.77 -X THESE VARIATIONS BETWEEN SPEECH STYLES CAN BE EXPLAINED BY FIRST, AFTER THE VOCAL TRACT MUSCLES WARM-UP THE JITTER AND SHIMMER REDUCE THEIR VALUES AND HNR INCREASES.
utt_0106 utt 453.34 463.94 -X SECOND, JITTER AND SHIMMER HAVE A DIFFERENT LEVEL OF INTERACTION WITH DIFFERENT TYPES OF FREQUENCY OSCILLATION LIKE VIBRATO AND TRILLO, DEPENDING ON HOW TRAINED IS THE SINGER.
utt_0108 utt 463.94 470.56 -X LAST, THESE PARAMETERS ARE CORRELATED WITH THE FUNDAMENTAL FREQUENCY, JITTER REDUCES ITS VALUE AS HIGHER FREQUENCIES.
utt_0110 utt 471.04 476.20 -X THEREFORE, THESE PARAMETERS CAN HELP TO CONDITIONATE THE MODELS TO SINGING CONDITIONS.
utt_0111 utt 477.95 483.49 -X NOW, I WILL PRESENT THE EXPERIMENTS AND RESULTS ON USING THE VOCAL SOURCE FEATURES FOR ASR SYSTEMS.
utt_0112 utt 485.02 494.82 -X FOR THE ASR EXPERIMENTS, WE USED THE DSING DATASET, AN UNACCOMPANIED SUNG SPEECH DATASET FOR ASR BASED ON THE SMULE CORPUS WE PRESENTED IN PREVIOUS WORK.
utt_0114 utt 495.39 501.28 -X DSING PROVIDES THREE INCREASINGLY LARGER TRAINING SETS DSINGone, DSINGthree AND DSINGthree0.
utt_0115 utt 502.02 508.04 -X THE SMALLEST DSINGone POSSESSES one5 HOURS OF SPEECH OF SINGERS FROM GREAT BRITAIN.
utt_0116 utt 508.16 513.60 -X THE SECOND SET, DSINGthree, EXPANDS DSINGone BY INCLUDING SINGERS FROM AMERICA AND AUSTRALIA,
utt_0117 utt 513.63 516.16 -X INCREASING THE DATA TO forty-four HOURS.
utt_0118 utt 517.15 525.83 -X THE LARGEST DSINGthirty INCLUDES ALL ENGLISH SPOKEN SONGS FROM THE thirty COUNTRIES CONTAINED IN THE SMULE DATASET, TOTALLING one hundred and fifty HOURS OF SUNG SPEECH.
utt_0120 utt 528.10 531.62 -X DSING ALSO OFFERS TWO TESTING SETS OF ROUGHLY forty-five MIN EACH.
utt_0121 utt 533.67 536.61 -X ONE FOR DEVELOPMENT AND ONE FOR EVALUATION.
utt_0122 utt 536.61 540.52 -X ALL SINGERS IN BOTH TEST SETS ARE FROM GREAT BRITAIN.
utt_0123 utt 540.52 545.25 -X AND THE SINGERS AND SONG ARRANGEMENT ARE NOT INCLUDED IN THE TRAINING SETS.
utt_0124 utt 545.73 548.90 -X OUR BASELINE WAS CONSTRUCTED BY USING THE KALDI TOOLKIT.
utt_0125 utt 549.19 561.19 -X TO CONSTRUCT AN IN-DOMAIN LANGUAGE MODEL, WE COLLECTED forty-four THOUSAND LYRICS FROM UNSEEN SONGS FROM ALL ARTIST IN DSINGthree PLUS ARTIST IN THE BILLBOARD'S “THE HOT one hundred”’S.
utt_0127 utt 562.31 570.63 -X USING A VOCABULARY OF THE twenty-eightK MOST COMMON WORDS, WE TRAINED A three-GRAM AND four-GRAM MAXIMUM ENTROPY LANGUAGE MODEL.
utt_0129 utt 570.98 576.39 -X FOR THE ACOUSTIC MODEL, WE TRAINED A FACTORISED TDNN WITH LATTICE-FREE MMI OBJECTIVE FUNCTION,
utt_0130 utt 577.06 582.95 -X USING forty MFCC PLUS DELTAS AND one hundred I-VECTORS WITH THREE FRAMES OF CONTEXT.
utt_0131 utt 583.20 592.81 -X EXPERIMENT FOR THE BASELINE WAS TRAINED eleven TIMES TO OBTAIN A ninety-five CONFIDENCE INTERVAL AND WERE EVALUATED IN TERM OF THE WORD ERROR RATE.
utt_0133 utt 592.81 597.45 -X HERE, WE CAN SEE THAT four-GRAM MODELS PERFORM BETTER IN ALL SCENARIOS.
utt_0134 utt 597.83 603.08 -X ALSO, WE CAN SEE THAT THERE IS A CLEAR BENEFIT OF TRAINING THE MODEL USING THE LARGEST DSINGthirty,
utt_0135 utt 603.49 607.34 -X REDUCING THE ERROR FROM thirty-eight IN DSINGone TO one9 IN DSINGthirty.
utt_0136 utt 609.89 618.06 -X NOTE THAT THIS IMPROVEMENT WAS OBTAINED DESPITE THAT THE DSINGthirty CONTAINS A LARGE VARIETY OF ACCENTS FROM NATIVE AND NON-NATIVE ENGLISH SPEAKERS.
utt_0138 utt 618.41 619.40 -X FROM thirty COUNTRIES
utt_0139 utt 620.07 631.75 -X THE VOCAL SOURCE FEATURES WERE EVALUATED BY RETRAINING THE ACOUSTIC MODELS FROM THE BASELINE BY INCREASING THE MFCC FEATURE VECTORS USING THE PITCH AND THE DEGREE OF VOICING IN THE FIRST EXPERIMENT.
utt_0142 utt 631.81 637.90 -X THE SECOND EXPERIMENT INCREASES THE FEATURE VECTOR FURTHER BY INCLUDING THE THREE VOICE QUALITY FEATURES.
utt_0144 utt 637.93 643.50 -X THESE EXPERIMENTS WERE ALSO RUN eleven TIMES TO OBTAIN A CONFIDENCE INTERVAL.
utt_0145 utt 644.93 651.43 -X THIS TABLE SHOWS THE ASR RESULTS FROM THE BASELINE AND THE TWO EXPERIMENTS DESCRIBED BEFORE.
utt_0147 utt 651.43 656.20 -X FIRST, FOR THE DSINGone DATASET, USING THE PITCH AND DEGREE OF VOICING REDUCES THE ERROR BY
utt_0148 utt 657.22 659.82 -X one point five% WHEN USING THE four-GRAM LANGUAGE MODEL.
utt_0149 utt 660.29 666.28 -X SIMILAR PERFORMANCE WAS OBTAINED WHEN EXPANDING THE VECTOR FURTHER USING THE VOICE QUALITY FEATURES.
utt_0151 utt 667.21 674.28 -X FOR THE DSINGthree CASE, WE OBSERVED A REDUCTION OF zero point seven% WHEN USING THE PITCH AND VOICE QUALITY FEATURES.
utt_0153 utt 676.04 683.08 -X AND, FOR THE DSINGthirty, NO VOICE SOURCE FEATURES RESULTED IN A STATISTICALLY SIGNIFICANT REDUCTION IN THE ERROR.
utt_0155 utt 683.08 693.07 -X ONE POSSIBLE REASON FOR THIS IS THAT THE MODEL MAY BE ABLE TO LEARN PHONETICS CUES BEING CARRIED BY THE VOCAL SOURCE IN AN INDIRECT MANNER, FOR EXAMPLE, VIA THE TEMPORAL DYNAMICS OF THE MFCCS.
utt_0158 utt 693.93 707.44 -X CONCLUSION, SUNG SPEECH IS A LESS INTELLIGIBLE KIND OF SPEECH THAT DIFFER FROM SPOKEN SPEECH IN VOWELS DURATION, LEVEL OF VOICING IN VOICED AND UNVOICED PAIR OF CONSONANTS AND POSSESSES A LARGER PITCH RANGE.
utt_0161 utt 708.20 719.15 -X WHEN EVALUATING VOICE SOURCE FEATURES, WE FOUND THAT THE USE OF THE PITCH INFORMATION AND THE DEGREE OF VOICING HELPS TO REDUCE THE ERROR WHEN USING SMALL SIZE RESOURCES LIKE DSINGone.
utt_0164 utt 719.37 723.57 -X HOWEVER, WE DON’T OBSERVE THIS BENEFIT WHEN USING THE LARGEST DATASET.
utt_0165 utt 724.01 734.41 -X WE HYPOTHESES THAT GIVEN ENOUGH DATA, THE CUES BEING PROVIDED BY THE VOCAL SOURCE FEATURES CAN BE LEARNT INDIRECTLY FROM THE CONVENTIONAL MFCC PARAMETERS.
utt_0167 utt 734.54 739.73 -5.5915 THANK YOU SO MUCH FOR WATCHING, I HOPE YOU FOUNDED THIS WORK AS INTERESTING AS WE DO.
