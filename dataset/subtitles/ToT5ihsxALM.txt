utt_0000 utt 6.70 9.65 -X PATRICK: HI MICHELLE. WHAT’S UP?
utt_0001 utt 9.65 14.45 -X MICHELLE: I’M TRAVELING TO DUBLIN IN MAY AND HAVE BEEN LEARNING ABOUT VARIOUS PLACES TO VISIT.
utt_0002 utt 14.51 17.10 -X HOWEVER, I’M GETTING CONFUSED BY SOME OF THESE REVIEWS.
utt_0003 utt 17.10 20.40 -X PATRICK: WHAT DO YOU MEAN? COULD YOU EXPLAIN MORE?
utt_0004 utt 20.40 34.16 -X MICHELLE: THIS ONE REVIEW TALKS ABOUT THE GUINNESS BREWERY. THE TEXT MENTIONS A “STUDENT” AND I HAVE NO IDEA WHO THAT REFERS TO. [SWITCH] THIS OTHER REVIEW IS ABOUT THE “HA’PENNY BRIDGE” AND THEN ALSO MENTIONS THE “LIFFEY BRIDGE”. WHY IS IT SUDDENLY MENTIONING ANOTHER BRIDGE?
utt_0007 utt 34.16 38.03 -X PATRICK: THE “STUDENT” IN THE FIRST REVIEW IS WILLIAM GOSSET,
utt_0008 utt 38.03 41.04 -X WHO WAS THE HEAD BREWER OF GUINNESS AND A FAMOUS STATISTICIAN.
utt_0009 utt 41.29 49.71 -X HE COULD NOT PUBLISH PAPERS WITH HIS SURNAME, SO HE USED THE PSEUDONYM “STUDENT”. IN THE SECOND REVIEW, “HA’PENNY BRIDGE” AND “LIFFEY BRIDGE” REFER TO THE SAME BRIDGE.
utt_0011 utt 49.71 54.77 -X MICHELLE: THANKS FOR LETTING ME KNOW. I WISH THERE WAS SOME KIND
utt_0013 utt 56.85 62.13 -X PATRICK: THIS PROBLEM OF LINKING WORDS THAT REFER TO EACH OTHER IS CALLED COREFERENCE RESOLUTION.
utt_0014 utt 62.13 72.47 -X THERE ARE EXISTING NLP MODELS OUT THERE THAT CAN AUTOMATICALLY DETERMINE WHICH SPANS OF TEXT REFER TO THE SAME ENTITY. IN FACT, I HAVE DEVELOPED A MODEL CALLED ICOREF THAT COULD HELP YOU WITH THAT.
utt_0016 utt 72.47 74.96 -X MICHELLE: OH THANKS! LET ME TRY USING THE MODEL THEN.
utt_0017 utt 74.96 80.40 -X PATRICK: THERE MIGHT BE AN ISSUE WHEN YOU USE IT. LIKE OTHER COREFERENCE RESOLUTION MODELS,
utt_0018 utt 80.40 83.76 -X ICOREF WAS TRAINED ON ONTONOTES, A DATASET FILLED WITH NEWS ARTICLES.
utt_0019 utt 83.76 87.79 -X IT MIGHT NOT WORK FOR YOUR SETTING UNLESS YOU CONTINUE TRAINING THE MODEL ON TOURIST REVIEWS.
utt_0020 utt 87.79 93.11 -X MICHELLE: I DON’T THINK I HAVE THE TIME OR RESOURCES TO CREATE SUCH A DATASET.
utt_0021 utt 93.11 95.99 -X I NEED TO PLAN MY TRIP TO DUBLIN AS SOON AS POSSIBLE!
utt_0022 utt 95.99 99.80 -X PATRICK: MAYBE IF THERE WAS A WAY TO TRAIN THE MODEL ON A SMALL SAMPLE OF REVIEWS?
utt_0023 utt 99.80 102.39 -X I THINK THAT WOULD STILL BE HELPFUL TO ADAPT THE MODEL.
utt_0024 utt 102.39 105.68 -X MICHELLE: I WONDER IF I COULD USE ACTIVE LEARNING FOR THIS PROBLEM.
utt_0025 utt 105.68 109.62 -X I CAN SELECT DATA TO LABEL WITH THE GOAL OF REDUCING ANNOTATION TIME AND COSTS.
utt_0026 utt 109.68 113.33 -X WHAT IF I APPLY ACTIVE LEARNING TO ADAPT COREFERENCE RESOLUTION MODELS?
utt_0031 utt 125.44 129.30 -X TRAINED THEIR MODELS ON ONTONOTES AND MADE THEM PUBLICLY AVAILABLE FOR OTHERS TO USE.
utt_0032 utt 130.13 134.10 -X HOWEVER, THESE MODELS MAY NOT IMMEDIATELY GENERALIZE TO NEW DOMAINS.
utt_0033 utt 134.29 146.42 -X IN THIS EXAMPLE, THE SPAN “THE BONDS” NO LONGER REFERS TO “MUNICIPAL BONDS”. INSTEAD IT LINKS TO “CHEMICAL BONDS” IN A SCIENCE DOMAIN. MODELS THAT WERE ORIGINALLY TRAINED ON ONTONOTES MAY NOT
utt_0035 utt 146.42 153.56 -X BE ACCURATE FOR DOMAINS LIKE SCIENTIFIC ARTICLES OR TOURIST REVIEWS WITHOUT ADDITIONAL TRAINING.
utt_0036 utt 153.56 156.44 -X PATRICK, FROM EARLIER IN THE VIDEO, HAS SHOWN IN HIS PAPER
utt_0037 utt 156.50 163.22 -X THAT CONTINUED TRAINING THE MODEL ON THE TARGET DATASET IS USEFUL FOR DOMAIN TRANSFER.
utt_0038 utt 163.22 174.13 -X HOWEVER, THE WORK DOES NOT CONSIDER THE TIME AND COST OF OBTAINING LABELED DATA IN THE TARGET DOMAIN. FOR PRACTICAL APPLICATIONS OF COREFERENCE RESOLUTION MODELS, WE NEED METHODS
utt_0041 utt 180.43 184.09 -X FOR EXAMPLE, GIVEN THIS REVIEW ABOUT THE GUINNESS BREWERY,
utt_0043 utt 188.47 193.27 -X FIRST, WE CAN CONSIDER LABELING SPANS THAT CONFUSE THE MODEL’S MENTION DETECTOR.
utt_0044 utt 193.27 197.78 -X IN OTHER WORDS, THE MODEL DOES NOT KNOW WHETHER OR NOT THIS PARTICULAR SPAN IS AN ENTITY MENTION.
utt_0046 utt 201.65 206.87 -X NOUN, DEPENDING ON THE CONTEXT. ANOTHER EXAMPLE WOULD BE “TIME”.
utt_0047 utt 207.16 214.46 -X BECAUSE OF HOW THIS REVIEW IS WRITTEN, THE MODEL MAY CONFUSE THE WORD “TIME” AS AN ENTITY MENTION.
utt_0048 utt 214.46 225.66 -X SECOND, WE CAN CONSIDER LABELING SPANS THAT CONFUSE MENTION CLUSTERING. IN THE BEGINNING OF THE VIDEO, I DID NOT KNOW THAT “THE STUDENT” REFERRED TO “WILLIAM GOSSET”. SO, LABELING THE
utt_0050 utt 225.66 230.42 -X CLUSTER OF SPANS LIKE “THE STUDENT” AND “THE BREWERY” COULD HELP IMPROVE MENTION CLUSTERING.
utt_0051 utt 230.55 236.42 -X THIRD, WE MAY CONSIDER LABELING SPANS THAT CONFUSE MENTION CLUSTERING CONDITIONED ON
utt_0053 utt 240.57 247.03 -X BUT ARE HARD TO LINK TO AN ENTITY CLUSTER. THIS STRATEGY WOULD SAMPLE PRONOUNS LIKE “YOU”.
utt_0054 utt 248.95 258.10 -X LASTLY, WE CAN CONSIDER LABELING SPANS THAT CHALLENGE BOTH MENTION DETECTION AND MENTION CLUSTERING. THE SPAN “A WELL-KNOWN METHOD IN STATISTICAL INFERENCE”
utt_0056 utt 258.10 264.31 -X IS LENGTHY AND MAY BE DIFFICULT TO DETECT AS AN ENTITY MENTION IN ITS ENTIRETY.
utt_0057 utt 264.31 270.27 -X IT IS ALSO HARD TO LINK IT TO “STUDENT’S T-TEST” IF THE MODEL HASN’T TRAINED ON SIMILAR DATA.
utt_0058 utt 270.27 273.66 -X IN OUR EXPERIMENTS, WE TEST THESE VARIOUS UNCERTAINTY SAMPLING
utt_0059 utt 273.81 284.73 -X STRATEGIES. WE ALSO COMPARE AGAINST TWO RANDOM SAMPLING BASELINES. ONE BASELINE IS PURE RANDOM SAMPLING FROM ALL SPANS IN THE DOCUMENT. WE IMAGINE THAT THIS BASELINE WILL SAMPLE MANY
utt_0061 utt 284.73 290.68 -X SPANS THAT ARE NOT ENTITY MENTIONS AND PROVIDE UNINFORMATIVE LABELS. THEREFORE, WE HAVE ANOTHER
utt_0063 utt 296.37 300.86 -X THIS RANDOM SAMPLING BASELINE IS STRONGER AND MORE COMPARABLE TO THE OTHER STRATEGIES.
utt_0064 utt 301.81 309.37 -X FOR THIS VIDEO, WE FOCUS ON TRANSFERRING FROM THE SOURCE DOMAIN ONTONOTES TO A TARGET DOMAIN PRECO,
utt_0065 utt 309.37 320.28 -X WHICH IS A LARGE CORPUS OF GRADE-SCHOOL TEXTS. ONE MAIN DIFFERENCE IS THAT ONTONOTES DOES NOT ANNOTATE SINGLETONS, WHICH ARE ENTITY CLUSTERS WITH ONLY ONE ENTITY MENTION.
utt_0067 utt 320.28 324.51 -X SO, THE ONTONOTES MODEL MAY NOT DETECT THE SINGLETONS IN PRECO.
utt_0069 utt 330.26 337.34 -X BASED ON AVG Fone SCORE ON PRECO. ON EACH CYCLE, WE SAMPLE FIFTY SPANS FROM THE PRECO
utt_0071 utt 341.44 347.45 -X AND THEN TRAIN THE ONTONOTES MODEL ON THESE NEWLY LABELED SPANS FROM PRECO.
utt_0072 utt 347.45 356.51 -X FIRST MENTION DETECTION ENTROPY ACHIEVES AN AVG Fone SCORE OF ABOUT zero point seven three BY END OF THE SIMULATION.
utt_0073 utt 356.51 361.18 -X NEXT, MENTION CLUSTERING ENTROPY PERFORMS SIMILARLY.
utt_0074 utt 363.00 367.58 -X THEN, CONDITIONAL ENTROPY SHOWS MUCH LOWER ACCURACY.
utt_0075 utt 368.86 372.57 -X ACCURACY JUMPS BACK UP AGAIN WITH JOINT ENTROPY.
utt_0076 utt 374.65 378.24 -X RANDOM SAMPLING IS EVEN LOWER THAN CONDITIONAL ENTROPY.
utt_0077 utt 379.54 385.89 -X FINALLY, SAMPLING RANDOM ENTITY MENTIONS IS BETTER THAN RANDOM SAMPLING
utt_0078 utt 386.07 391.17 -X BUT STILL NOT AS STRONG AS SOME ACTIVE LEARNING STRATEGIES. OVERALL,
utt_0079 utt 391.17 397.53 -X WE SEE HIGH AVG Fone FROM STRATEGIES LIKE MENTION DETECTION ENTROPY, MENTION CLUSTERING ENTROPY,
utt_0080 utt 397.53 403.55 -X AND JOINT ENTROPY. AS WE HAVE PREDICTED, THE PURE RANDOM SAMPLING HURTS THE MODEL THE MOST.
utt_0081 utt 404.19 416.61 -X IN THE PREVIOUS EXPERIMENTS, WE LIMITED SPAN SAMPLING TO ONE DOCUMENT ON EACH ACTIVE LEARNING CYCLE. WHAT HAPPENS IF WE LABEL SPANS ACROSS DIFFERENT DOCUMENT CONTEXTS RATHER THAN
utt_0083 utt 416.61 423.26 -X STAYING WITHIN ONE PIECE OF TEXT? FOR EXAMPLE, SUPPOSE WE ONLY WANT TO LABEL THREE SPANS.
utt_0084 utt 423.80 427.79 -X IF WE CHOOSE TO STAY WITHIN THE SAME DOCUMENT, WE MAY WANT TO
utt_0085 utt 427.93 431.93 -X LABEL “EXPERIENCE”, “THE BREWERY”, AND “THE STUDENT” FROM THE FIRST REVIEW.
utt_0086 utt 432.86 447.55 -X HOWEVER, WHAT IF WE COULD LABEL COREFERENCE OF THREE SPANS ACROSS THREE DOCUMENTS? THEN WE MAY COVER COREFERENCE IN DIFFERENT CONTEXTS TO HELP TRANSFER THE MODEL TO THE TARGET DOMAIN.
utt_0088 utt 448.44 455.30 -X THE CAVEAT IS THAT LABELING THREE DIFFERENT DOCUMENTS REQUIRES READING ALL OF THOSE THREE
utt_0090 utt 461.54 465.76 -X TO UNDERSTAND THE EFFECTS OF SAMPLING FROM ONE DOCUMENT VERSUS MULTIPLE DOCUMENTS,
utt_0091 utt 465.76 470.08 -X WE SIMULATE SAMPLING FIFTY SPANS ACROSS ALL DOCUMENTS ON EACH CYCLE.
utt_0092 utt 471.64 482.08 -X WE SHOW A SIDE-BY-SIDE COMPARISON BETWEEN THE PREVIOUS ONE-DOCUMENT SAMPLING SIMULATION AND THE UNCONSTRAINED SAMPLING SIMULATION.
utt_0094 utt 482.17 490.53 -X FIRST, FOR MENTION DETECTION ENTROPY, THE AVG Fone SCORE DROPS DURING THE UNCONSTRAINED SIMULATION.
utt_0095 utt 494.49 502.21 -X NEXT, MENTION DETECTION ENTROPY SHOWS SIMILAR DEGENERATIVE BEHAVIOR WITH UNCONSTRAINED SAMPLING.
utt_0096 utt 502.81 507.68 -X FOR CONDITIONAL ENTROPY, THE UNCONSTRAINED SAMPLING CONVERGES TO ALMOST ZERO AVG Fone.
utt_0097 utt 507.90 511.39 -X WE SEE MORE DEGENERATIVE BEHAVIOR IN JOINT ENTROPY.
utt_0098 utt 512.54 518.59 -X RANDOM SAMPLING DOES EVEN WORSE FOR UNCONSTRAINED SAMPLING.
utt_0100 utt 522.15 526.08 -X FOR UNCONSTRAINED SAMPLING BUT STARTS TO DROP TOWARD THE END OF SIMULATION.
utt_0101 utt 529.05 536.55 -X OVERALL, WE NOTICE THAT THE UNCONSTRAINED SETTING IS NOT ONLY UNREALISTIC BUT ALSO UNSTABLE. UPON
utt_0102 utt 538.62 552.07 -X FURTHER ANALYSIS, WE OBSERVE THE MODEL TENDS TO PREDICT SINGLETONS RATHER THAN CLUSTER COREFERENT SPANS. TO PREVENT THIS FROM HAPPENING, WE MUST CONSTRAIN LABELING TO A SMALL SET OF DOCUMENTS.
utt_0104 utt 553.34 560.00 -X TO UNDERSTAND HOW READING AFFECTS LABELING COREFERENCE IN HUMANS,
utt_0105 utt 560.99 569.80 -X WE HOLD A USER STUDY WITH THREE PARTICIPANTS. WE DEVELOP A USER INTERFACE FOR LABELING COREFERENCE.
utt_0106 utt 569.80 575.30 -X WE HAVE USERS COMPLETING TWO TWENTY-FIVE MINUTE SESSIONS. IN THE FIRST SESSION,
utt_0107 utt 578.98 585.28 -X THEY LABEL MULTIPLE SPANS FROM A LIMITED NUMBER OF DOCUMENTS. IN THE SECOND SESSION,
utt_0108 utt 585.34 591.36 -X THEY LABEL ABOUT ONE SPAN PER DOCUMENT FROM A MUCH LARGER SET OF DOCUMENTS. WITH THIS SETUP,
utt_0109 utt 591.62 597.83 -X THEY WILL READ MANY MORE DOCUMENTS IN THE SECOND SESSION COMPARED TO THE FIRST.
utt_0110 utt 598.08 603.30 -X HERE, THE FIGURE SHOWS THE NUMBER OF SPANS LABELED BY THE USERS AS TIME PROGRESSES.
utt_0111 utt 603.30 607.14 -X THE SOLID LINES SHOW ANNOTATION PROGRESS DURING THE FEWDOCS SESSION,
utt_0112 utt 607.14 611.11 -X WHILE THE DASHED LINES INDICATE PROGRESS IN THE MANYDOCS SESSION.
utt_0113 utt 611.11 616.77 -X LABELING THROUGHPUT FROM USERS AT LEAST DOUBLES IN THE FEWDOCS SESSION COMPARED TO THE MANYDOCS
utt_0115 utt 624.74 630.95 -X ANNOTATION TO A FEW DOCUMENTS, RATHER THAN LABELING SPANS FROM SEVERAL DOCUMENTS.
utt_0116 utt 630.95 635.59 -5.7768 WITH ALL THESE EXPERIMENT RESULTS AND CONCLUSIONS, I NOW HAVE A BETTER IDEA OF
