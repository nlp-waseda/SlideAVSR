utt_0000 utt 1.26 6.10 -X HI EVERYONE TODAY I'M GOING TO PRESENT OUR WORK PLUG-AND-PLAY VQA:
utt_0001 utt 6.22 11.28 -X ZERO SHOT VQA BY CONJOINING LARGE PRE-TRAINED MODELS WITH ZERO TRAINING
utt_0002 utt 13.80 26.42 -X VQA REQUIRES A MODEL TO GENERATE AN ANSWER GIVEN AN IMAGE AND A QUESTION. IT IS A CHALLENGING TASK AND THE MODELS NEED TO EXHIBIT MULTIPLE CAPABILITIES TO DO SO. FOR EXAMPLE,
utt_0004 utt 26.42 32.31 -X FOR THE FOLLOWING QUESTION, THE MODEL NEEDS TO RECOGNIZE THE ACTION JUMPING AND ITS REASON,
utt_0005 utt 32.37 39.86 -X AND IDENTIFY THE MAN AND FRISBEE AS OBJECTS IN ORDER TO GENERATE THE ANSWER: TO CATCH FRISBEE.
utt_0006 utt 41.45 47.41 -X FOR THIS EXAMPLE, THE MODEL REQUIRES SPATIAL UNDERSTANDING TO IDENTIFY ON TOP OF,
utt_0007 utt 48.05 59.38 -X AND RECOGNIZING THE CHARACTER T AND ITS COLOR ATTRIBUTE IN ORDER TO GENERATE RED AS THE ANSWER. IT IS ESPECIALLY CHALLENGING IN THE ZERO-SHOT SETTING,
utt_0009 utt 59.41 64.37 -X WHERE THE MODEL NEEDS TO SOLVE VQA WITHOUT HAVING BEEN TRAINED ON ANY VQA DATA.
utt_0010 utt 66.16 77.48 -X RECENTLY, LARGE-SCALE PRE-TRAINED LANGUAGE MODEL (PLM) HAS DEMONSTRATED IMPRESSIVE PERFORMANCE AND PROPERTIES SUCH AS ZERO SHORT LEARNING AND VARIOUS WORK HAVE LEVERAGED PLM FOR
utt_0012 utt 77.48 85.56 -X ZERO SHOT VQA. HOWEVER, MOST OF EXISTING METHODS REQUIRE ADDITIONAL ADAPTATIONS BECAUSE PLM CANNOT
utt_0013 utt 85.56 100.34 -X UNDERSTAND THE IMAGE, SO A VISION ENCODER IS OFTEN TRAINED ON IMAGE AND TEXT PAIRS USING THE LANGUAGE MODELING OBJECTIVE IN ORDER TO INCORPORATE THE VISUAL MODALITY INTO THE PLM. AND THIS
utt_0015 utt 100.34 107.96 -X ADAPTATION PROCESS IS NON-TRIVIAL, AND IT OFTEN REQUIRES NEW NETWORK COMPONENTS AND NEW TRAINING
utt_0016 utt 107.96 115.41 -X OBJECTIVE DESIGN AND OUR WORK AIMS TO ADDRESS THIS LIMITATION BY PROPOSING A MODULAR FRAMEWORK.
utt_0017 utt 116.27 129.75 -X IN A MODULAR FRAMEWORK, EACH OF THE COMPONENTS IS RESPONSIBLE FOR CERTAIN FUNCTION. FROM THE PERSPECTIVE OF GENERAL PURPOSE AND PRACTICAL AI, IT IS HIGHLY DESIRABLE FOR THE SYSTEM TO LEARN
utt_0019 utt 129.75 142.74 -X NEW TASKS BY RECOMBINING THE MODULES WITHOUT ANY TRAINING OR ARCHITECTURE CHANGE. A MODULAR FRAMEWORK ALLOWS US TO REPLACE ITS COMPONENTS WITHOUT AFFECTING THE OTHER MODULES. THE KEY
utt_0021 utt 142.74 156.15 -X ADVANTAGE IS THAT WE CAN LEVERAGE IMPROVED MODULE FOR OVERALL PERFORMANCE GAIN OF THE SYSTEM. THIS ALLOWS THE SYSTEM TO KEEP EVOLVING AS THE MODULES CONTINUE TO ADVANCE. BESIDES,
utt_0023 utt 156.15 162.20 -X WE ARE ALSO MOTIVATED BY STUDIES THAT SHOW THE HUMAN COGNITIVE SYSTEM IS LARGELY MODULAR.
utt_0024 utt 162.97 173.95 -X PREVIOUS WORKS HAVE DEMONSTRATED THAT IT IS DIFFICULT AND BORDERLINE IMPOSSIBLE TO OBTAIN HIGH PERFORMANCE WITHOUT ANY KIND OF END-TO-END TRAINING. ON THE CONTRARY,
utt_0026 utt 174.20 185.91 -X WE PRESENT OUR WORK, A HIGH PERFORMING SYSTEM, PNP-VQA, WHICH CONJOINS PRE-TRAINED MODELS WITH NATURAL LANGUAGE AND NETWORK INTERPRETATION TECHNIQUE WITH NO TRAINING.
utt_0028 utt 188.12 200.60 -X OUR CONTRIBUTIONS ARE AS FOLLOW. WE PROPOSE A MODULAR FRAMEWORK WITH NO TRAINING. TO THE BEST OF OUR KNOWLEDGE, OUR WORK IS THE FIRST THAT USES NETWORK INTERPRETATIONS AS THE INTERFACE BETWEEN
utt_0030 utt 200.60 206.81 -X PRETRAINED MODELS AND OUR METHOD ALSO ACHIEVES STATE-OF-THE-ART PERFORMANCE OF VQAVtwo AND GQA.
utt_0031 utt 210.04 221.37 -X HERE IS PNP-VQA, THE OVERALL FRAMEWORK. IT CONSISTS OF MULTIPLE PRETRAINED OFF-THE-SHELF MODEL IN THREE DIFFERENT MODULES: IMAGE QUESTION MATCHING, IMAGE CAPTIONING,
utt_0033 utt 221.37 230.05 -X AND QUESTION ANSWERING MODULES. NOW LET ME GO INTO EACH OF THIS MODULE IN DETAIL.
utt_0034 utt 230.05 243.92 -X AN IMAGE IS A RICH SOURCE OF INFORMATION AND THE QUESTION IS OFTEN FOCUSED ON A PARTICULAR REGION OR OBJECT. THEREFORE WE DESIGN AN IMAGE QUESTION MATCHING MODULE TO IDENTIFY IMAGE PATCHES THAT ARE
utt_0036 utt 243.92 250.40 -X RELEVANT TO THE QUESTION. THE PRE-TRAINED MODEL WE ADOPT IS BLIP-ITM, A VISION LANGUAGE MODEL THAT
utt_0037 utt 250.40 264.83 -X IS TRAINED TO DETERMINE WHETHER A TEXT MATCHES AN IMAGE. WE ADOPT GRADCAM WHOSE SCORES DENOTE THE RELEVANCE AND IMPORTANCE OF IMAGE PATCHES WITH RESPECT TO THE QUESTION. AS SHOWN IN THIS
utt_0039 utt 264.83 273.89 -X EQUATION, GIVEN AN IMAGE AND A QUESTION, WE OBTAIN THE GRADIENT FROM THE CROSS ENTROPY LOSS, AND THE
utt_0040 utt 273.89 283.84 -X GRADIENT IS USED AS WEIGHT WHEN AGGREGATING THE CROSS ATTENTION MAP FOR ALL THE TOKENS.
utt_0041 utt 283.84 289.28 -X EVEN WITH RELEVANT IMAGE REGIONS, THERE ARE STILL MULTIPLE WAYS TO DESCRIBE THE REGIONS.
utt_0042 utt 289.28 295.14 -X SOME DESCRIPTIONS COULD BE USEFUL FOR QUESTION ANSWERING WHEREAS OTHER MAY NOT. THEREFORE,
utt_0043 utt 295.14 308.95 -X GIVEN SAMPLE PATCHES BASED ON GRADCAM, WE GENERATE MULTIPLE CAPTIONS THROUGH STOCHASTIC DECODING. WE DENOTE THE GENERATED CAPTIONS AS QUESTION-GUIDED CAPTIONS. HAVING MULTIPLE CAPTIONS ENCOURAGES
utt_0045 utt 308.95 322.56 -X THE DIVERSITY OF CAPTIONS AND THE COVERAGE OF VISUAL INFORMATION. OUR DESIGN BYPASSES THE NEED OF IMAGE FOR QUESTION ANSWERING MODULE WE ADOPT A PRE-TRAINED MODEL KNOWN AS BLIP-CAPTION
utt_0047 utt 325.82 337.99 -X THEN FOR OUR QUESTION ANSWERING MODULE, IT WILL GENERATE THE ANSWER GIVEN THE QUESTION AND CAPTION AS THE INPUT. THE PRE-TRAINED MODEL IS CALLED UNIFIEDQAVtwo,
utt_0049 utt 338.53 347.24 -X AN ENCODER-DECODER LANGUAGE TRANSFORMER THAT IS TRAINED FOR READING COMPREHENSION [BASED QA].
utt_0050 utt 347.24 357.83 -X THERE IS A MAXIMUM INPUT LENGTH RESTRICTION ON THE ENCODER, DUE TO THE POSITIONAL ENCODING. IT COULD LIMIT THE NUMBER OF CAPTIONS THAT THAT WE CAN USE FOR QUESTION ANSWERING.
utt_0052 utt 358.15 370.73 -X TO OVERCOME THIS LIMITATION, WE ADOPT FUSION-IN-DECODER, A METHOD PROPOSED BY META RESEARCH. ESSENTIALLY WE ENCODE A QUESTION WITH EACH OF THE CAPTIONS SEPARATELY. THEN WE
utt_0054 utt 370.73 376.39 -X CONCATENATE THE ENCODED REPRESENTATION OF ALL THE TOKENS AS INPUT TO THE DECODER.
utt_0055 utt 379.65 386.36 -X HERE IS OUR COMPARISON WITH STATES OF THE ART. WE COMPARE WITH METHODS THAT REQUIRE END-TO-END
utt_0056 utt 386.36 396.36 -X VISION LANGUAGE TRAINING AND THOSE WITHOUT ANY TRAINING. EVEN WITH FEWER PARAMETERS, OUR eleven
utt_0057 utt 396.39 404.10 -X BILLION MODEL ACTUALLY OUTPERFORMS FLAMINGO eighty BILLION MODEL OR THE VQAVtwo DATASET BY eight point five PERCENT.
utt_0058 utt 407.04 410.44 -X WHEN COMPARING WITH SIMILAR NUMBER OF TOTAL PARAMETERS,
utt_0059 utt 410.44 418.89 -X PNP-VQA STILL OUTPERFORMS FEWVLM ON ALL THE THREE BENCHMARKS, VGAVtwo, OK-VQA, AND GQA.
utt_0060 utt 421.48 427.69 -X WE PERFORM ABLATION ON IMAGE PATCH SAMPLING STRATEGY DURING CAPTION GENERATION.
utt_0061 utt 429.51 443.59 -X BY HAVING one hundred QUESTION-GUIDED PATCH SAMPLING CAPTION THE PERFORMANCE SURPASSES FIVE HUMAN-WRITTEN CAPTIONS ON VQAVtwo AND OK-VQA. THE QUESTION GUIDED PATCH SAMPLING IS BETTER
utt_0063 utt 443.59 455.31 -X THAN GENERIC CAPTION THAT USE ALL THE IMAGE PATCHES FOR GENERATION. WE ALSO COMPARE WITH UNIFORM RENDERED SAMPLING AND OUR QUESTION GUIDED CAPTION IS ALSO SHOWN TO BE BETTER.
utt_0065 utt 457.96 464.04 -X HERE ARE SOME QUALITATIVE EXAMPLES TO COMPARE QUESTION GUIDED CAPTION AND GENERIC CAPTIONS.
utt_0066 utt 464.04 475.95 -X WE SHOW THE ORIGINAL IMAGES AND IMAGES WITH HEATMAPS. THE HEATMAP SHOWS THAT THE GRADCAM CAN HIGHLIGHT THE IMPORTANT PORTION OF THE IMAGE THAT IS RELEVANT TO THE QUESTION,
utt_0068 utt 475.95 488.65 -X EVEN THOUGH IT MIGHT NOT BE OBVIOUS. IN THIS EXAMPLE, THE AUDIENCE CORRESPONDS TO A CROWD OF PEOPLE. AND IN THIS EXAMPLE, IT'S DOGS. THEREFORE, THE GENERATED QUESTION GUIDED
utt_0070 utt 488.65 502.93 -X CAPTION CAN CONTAIN ANSWER CUES THAT ARE HELPFUL FOR QUESTION ANSWERING. TO MEASURE QUANTITATIVELY HOW INFORMATIVE QUESTION GUIDED CAPTIONS ARE, WE COMPUTE THE ANSWER HIT RATE, WHICH IS DEFINED
utt_0072 utt 502.93 508.30 -X AS THE FREQUENCY OF QUESTIONS WITH THE GRANT TRUTH ANSWER APPEAR IN THE GENERATED CAPTIONS.
utt_0073 utt 509.19 513.77 -X THE FIGURE SHOWS THAT THE ACCURACY INCREASES WITH ANSWER HIT RATE,
utt_0074 utt 513.77 518.67 -X AND THE NUMBER OF CAPTIONS. THEREFORE, IT SUPPORTS THE PNP-VQA DESIGN,
utt_0075 utt 518.67 525.26 -X WHICH UTILIZES MULTIPLE QUESTION GUIDED CAPTIONS TO COVER DIVERSE AND RELEVANT VISUAL CONTENT.
utt_0076 utt 525.67 537.55 -X TO CONCLUDE, WE DEVELOP PNP-VQA, A MODULAR FRAMEWORK WITH ZERO TRAINING. IT CONJOINS MULTIPLE PRETRAINED MODELS USING NATURAL LANGUAGE AND NETWORK INTERPRETATION AS THE INTERFACE.
utt_0078 utt 538.25 549.94 -X IT ACHIEVES STATE-OF-THE-ART RESULT ON VQAVtwo AND GQA. WE HOPE THAT OUR WORK CAN INSPIRE MORE RESEARCH IN EXPLORING MODULAR FRAMEWORKS TO SOLVE VISUAL LANGUAGE TASKS. HERE ARE MY
utt_0080 utt 549.94 556.11 -2.7571 EMAIL ADDRESS AND THE QR CODES FOR THE PAPER AND THE CODE. THANK YOU FOR YOUR ATTENTION.
