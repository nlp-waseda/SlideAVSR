utt_0000 utt 0.11 14.26 -X HELLO! MY NAME IS RODRIGO CANAAN AND I'M GOING TO TALK ABOUT MY PAPER BEHAVIORAL EVALUATION OF HANABI RAINBOW DQN AGENTS AND RULE-BASED AGENTS. YOU CAN SEE MY CO-AUTHORS ON THE SCREEN AND A QR CODE FOR THE PAPER AND GITHUB IF YOU'RE INTERESTED.
utt_0003 utt 14.48 25.43 -X THE MAIN QUESTION THAT MOTIVATES THIS PAPER IS: HOW WELL DO EXISTING REINFORCEMENT LEARNING AGENTS IN HANABI ADAPT TO PARTNERS THAT WERE NOT SEEN DURING TRAINING?
utt_0005 utt 25.43 29.20 -X TO SEE WHAT WHY THIS IS INTERESTING LET'S TALK A BIT ABOUT HANABI.
utt_0006 utt 29.65 39.38 -X HANABI IS A COOPERATIVE CARD GAME FOR TWO TO FIVE PLAYERS THAT IS A GREAT TESTBED FOR COOPERATIVE AI THAT MODELS OTHER PLAYERS OR USES THEORY OF MIND.
utt_0008 utt 39.38 50.27 -X IN HANABI, THE OBJECTIVE OF THE GAME IS TO BUILD COLORED PILES OF CARDS BY PLAYING CARDS OF EACH COLOR IN ASCENDING ORDER OF VALUE. HOWEVER WHAT MAKES THE GAME UNIQUE IS THAT PLAYERS
utt_0010 utt 50.27 65.16 -X DO NOT SEE THEIR OWN HANDS. INSTEAD THEY PLAY WITH THEIR HANDS FACING OUTWARDS. THE ONLY WAY TO GET INFORMATION ABOUT THE COLORS AND VALUES OF YOUR CARDS IS THROUGH A SYSTEM OF HINTS, WHICH IS THE ONLY METHOD OF COMMUNICATION ALLOWED IN THE GAME. AND BECAUSE THESE HINTS ARE RESTRICTED
utt_0013 utt 65.71 71.16 -X PLAYERS WILL OFTEN ACT ON INCOMPLETE INFORMATION AND MUST DO THEIR BEST TO FIGURE OUT, FOR EXAMPLE,
utt_0014 utt 71.16 75.83 -X WHICH CARDS IN THEIR HANDS ARE IMMEDIATELY PLAYABLE AND WHICH SHOULD BE SAVED FOR LATER.
utt_0015 utt 76.43 87.09 -X IN EACH PLAYER'S TURN, THEY PICK BETWEEN THREE ACTIONS: PLAYING A CARD, GIVING A HINT (WHICH CONSUMES A TOKEN FROM A SHARED POOL) AND DISCARDING A CARD (WHICH RECOVERS A TOKEN).
utt_0017 utt 87.09 97.11 -X THEY ALTERNATE TURNS UNTIL EITHER THE DECK RUNS OUT, OR THE TEAM RUNS OUT OF LIVES OR ALL PILES ARE COMPLETE. AS AN EXAMPLE LET'S SAY IT'S MY TURN AND THE BOARD IS AS SEEN ON THE RIGHT.
utt_0019 utt 97.23 103.22 -X I COULD USE A HINT TOKEN TO TELL MY PARTNER THAT THIS CARD IS YELLOW IN THE HOPES THAT THEY PLAY IT.
utt_0020 utt 103.22 113.17 -X BUT REMEMBER THAT THEIR PERSPECTIVE LOOKS LIKE THIS! AT THIS POINT THEY DO NOT KNOW THE CARD'S VALUE, ONLY ITS COLOR. IF THIS WAS ANYTHING BUT THE YELLOW FOUR AND THEY ATTEMPTED TO PLAY IT
utt_0022 utt 113.17 118.39 -X IT WOULD BE A MISTAKE AND THE GROUP WOULD LOSE A LIFE. BUT IF I'M PLAYING WITH A HUMAN PARTNER,
utt_0023 utt 118.39 129.18 -X THEY'RE LIKELY TO PUT THEMSELVES IN MY POSITION AND REALIZE THAT THE ONLY REASON I'M GIVING A HINT ABOUT YELLOW IS BECAUSE IT IS THE FOUR. IN PRACTICE THERE'S AN IMPLICIT CONVENTION IN PLACE
utt_0025 utt 129.18 143.99 -X THAT SAYS PLAYERS SHOULD PRIORITIZE HINTS ABOUT PLAYABLE CARDS. IF THEY DID DECIDE TO PLAY THE CARD, THEY WOULD REVEAL IT, SEE THAT IT FITS THE BOARD AND WE WOULD SCORE A POINT AND THEY WOULD DRAW A REPLACEMENT CARD. BUT IF I WERE NOT FOLLOWING THIS CONVENTION AND GIVING HINTS
utt_0028 utt 143.99 154.49 -X AT RANDOM, THIS MIGHT HAVE BEEN A VERY BAD MOVE. NOW, IF IT WERE MY TURN AGAIN I MIGHT WANT TO TRY TO GET THEM TO PLAY THEIR NEW GREEN TWO, BUT NOTE THAT TELLING THEM JUST ABOUT THIS TWO WOULD BE
utt_0030 utt 154.49 165.55 -X ILLEGAL AS THEY HAVE ANOTHER TWO IN HAND AND THE RULES REQUIRE ME TO GIVE FULL INFORMATION. IF I WANTED, I COULD POINT AT BOTH TWOS BUT ONLY ONE OF THEM WOULD BE PLAYABLE. THEY MIGHT REASON THAT THE
utt_0032 utt 165.55 179.19 -X RECENTLY DRAWN TWO IS MORE RELEVANT, EFFECTIVELY ADDING ANOTHER LAYER OF CONVENTION AND PLAYER MODELING. WITH MORE AND MORE ASSUMPTIONS, EVEN MORE EFFICIENT PLAYS ARE POSSIBLE, BUT THIS CAN GET VERY COMPLEX AND GENERALIZE POORLY TO AGENTS THAT ARE NOT PLAYING BY THE SAME CONVENTIONS.
utt_0035 utt 180.63 193.75 -X AND THE INTERESTING THING IS THAT CURRENT STATE-OF-THE-ART REINFORCEMENT LEARNING AGENTS FOR THE GAME CAN LEARN SOME VERY COMPLEX AND ARBITRARY CONVENTIONS VIA SELF-PLAY AND DO EXTREMELY WELL WITH THEM. OF COURSE THIS IS GOOD FOR SELF-PLAY, BUT BAD FOR MORE GENERAL
utt_0038 utt 193.75 197.82 -X SCENARIOS WHERE WE'RE INTERESTED IN PLAYING WITH UNKNOWN AGENTS OR EVEN WITH HUMANS.
utt_0039 utt 199.22 204.67 -X BUT WHAT IF INSTEAD OF SELF-PLAY, WE TRAINED THEM BY PAIRING THEM WITH SIMPLER PARTNERS?
utt_0040 utt 204.79 208.86 -X SO IN THIS PAPER, WE STUDY A SPECIFIC REINFORCEMENT LEARNING AGENT,
utt_0041 utt 208.86 219.45 -X CALLED RAINBOW, AND HOW IT PERFORMS WHEN WE SELECT TRAINING PARTNERS FROM A POOL OF SIMPLER RULE-BASED AGENTS AND EVALUATE BASED ON THE PERFORMANCE ON THE ENTIRE UNSEEN POOL.
utt_0043 utt 220.15 224.80 -X SOME TRAINING PARTNERS LEAD TO MORE GENERAL TRAINED POLICIES THAN OTHERS,
utt_0044 utt 224.80 229.44 -X SO WE ALSO PERFORM A BEHAVIORAL ANALYSIS TO SHED LIGHT ON WHY THIS MIGHT BE THE CASE.
utt_0045 utt 229.46 240.33 -X LET ME START BY DESCRIBING THESE SIMPLER AGENTS WE USE AS PARTNERS. THEY ARE CALLED RULE-BASE AGENTS BECAUSE, AS YOU CAN SEE, THEY PLAY BY FOLLOWING RULES IN A CERTAIN ORDER. EACH RULE
utt_0047 utt 240.33 246.43 -X HAS A CONDITION WHICH, IF APPLICABLE, RESULTS IN AN ACTION, OTHERWISE THE AGENT MOVES TO THE NEXT.
utt_0048 utt 246.55 258.24 -X MOST OF THESE RULES DESCRIBE SIMPLE HEURISTICS FOR WHEN TO PLAY, DISCARD OR HINT A CARD AND DIFFERENT COMBINATIONS OF THESE RULES OR REORDERINGS OF THE RULES WILL GIVE RISE TO DIFFERENT AGENTS.
utt_0050 utt 258.75 270.29 -X SO, INTERNAL IS ONE EXAMPLE OF RULE-BASED AGENT, BUT IN THIS PAPER WE FOCUS ON SIX SUCH AGENTS THAT WERE PART OF THE EARLY HANABI LITERATURE AND WERE COMPILED IN A two thousand and seventeen PAPER BY WALTON-RIVERS
utt_0052 utt 270.29 274.18 -X AND DISTRIBUTED IN JAVA AS PART OF THEIR HANABI COMPETITION FRAMEWORK.
utt_0053 utt 274.91 285.09 -X FOR THIS PAPER WE RE-IMPLEMENT THEM IN THE HANABI LEARNING ENVIRONMENT, WHICH IS ANOTHER FRAMEWORK I'LL INTRODUCE IN A MINUTE. IN THE TOP OF THE SCREEN YOU CAN SEE THE SELF-PLAY SCORE
utt_0055 utt 285.09 289.98 -X OF EACH AGENT IN THE TWO-PLAYER VERSION OF THE GAME AND A REFERENCE TO THEIR ORIGINAL PAPERS.
utt_0056 utt 289.98 293.22 -X IN THE BOTTOM YOU SEE THEIR SCORES WHEN PLAYING WITH EACH OTHER.
utt_0057 utt 293.95 298.72 -X WITH THE EXCEPTION OF FLAWED, WHICH WAS DESIGNED TO BE TRICKY TO PLAY WITH ON PURPOSE,
utt_0058 utt 299.61 304.58 -X THEY SCORE FROM ten TO seventeen POINTS ON AVERAGE AND THEIR STRATEGIES SEEM BROADLY COMPATIBLE
utt_0059 utt 304.77 319.31 -X BASED ON THE SCORES THEY ACHIEVE WHEN PAIRED WITH ONE ANOTHER. THIS BROAD COMPATIBILITY OF STRATEGIES QUICKLY VANISHES THOUGH WHEN WE LOOK INSTEAD AT STATE-OF-THE-ART REINFORCEMENT LEARNING AGENTS. HERE WE CAN SEE FOUR OF THEM. IN COMMENTS IN THE ORIGINAL PAPERS THE
utt_0062 utt 319.31 329.86 -X AUTHORS OF THREE OF THEM NOTED THAT THEY LEARNED CONVENTIONS THAT MAP ARBITRARY COLORS TO ARBITRARY ACTIONS. THIS LEADS THEM TO PERFORM POORLY EVEN AMONG INDEPENDENTLY TRAINED INSTANCES OF THE SAME
utt_0064 utt 329.86 339.65 -X ARCHITECTURE. RAINBOW, ON THE OTHER HAND, WAS NOTED BY ITS AUTHORS TO CONVERGE EACH TIME TO SIMILAR STRATEGIES AND IT SEEMS TO NOT LEARN THESE TYPES OF UNDESIRED CONVENTIONS.
utt_0066 utt 339.65 341.92 -X THIS IS WHY WE PICKED RAINBOW FOR THIS PAPER.
utt_0067 utt 342.46 350.66 -X THE HANABI RAINBOW AGENT WAS INCLUDED AS A SAMPLE AGENT IN THE HANABI LEARNING ENVIRONMENT, A PYTHON FRAMEWORK DESCRIBED IN A PAPER BY BARD ET AL.
utt_0069 utt 351.10 362.40 -X IT IS TRAINED USING A MODIFICATION OF THE DQN ALGORITHM. THIS IS THE VERSION WE USE IN OUR EXPERIMENTS WITH THE SAME HYPER-PARAMETERS, CHANGING ONLY THE CHOICE OF TRAINING PARTNERS. THE AGENT ITSELF
utt_0071 utt 362.43 373.19 -X USES A SIMPLE FEED-FORWARD NEURAL NETWORK TO ESTIMATE VALUE DISTRIBUTIONS FOR EACH ACTION. THE INPUT SPACE INCLUDES ONLY ONE TURN OF GAME HISTORY BUT PROVIDES FEATURES REPRESENTING THE INFORMATION
utt_0073 utt 373.19 386.18 -X CURRENTLY KNOWN TO EACH AGENT. SINCE THIS AGENT HAS LIMITED MEMORY AND NO DIRECT MODELING OF THE OTHER PLAYER, WE DID NOT EXPECT IT TO LEARN TO IDENTIFY OR ADAPT TO ITS PARTNERS. RATHER WE KNEW IT SEEMED TO
utt_0075 utt 386.18 395.64 -X AVOID SOME PROBLEMATIC CONVENTIONS SO THERE WAS A CHANCE ITS POLICY MIGHT BE BROADLY COMPATIBLE WITH THE ONES USED BY THE RULE BASE AGENTS. SO WHAT WE DID IS WE TOOK EACH RULE-BASED AGENT
utt_0077 utt 395.71 408.01 -X AND WE TRAINED A FEW INSTANCES OF RAINBOW USING THAT SPECIFIC AGENT AS A PARTNER. WE ALSO TRAINED A FEW INSTANCES OF RAINBOW PURELY VIA SELF-PLAY AND WE TRAINED A FEW INSTANCES THAT SAMPLED EVENLY
utt_0079 utt 408.01 418.95 -X FROM ALL SIX RULE-BASED AGENTS DURING TRAINING (OPTIONALLY SOME VERSIONS COULD ALSO INCLUDE SELF-PLAY IN THIS MIX). BUT WHAT WE'RE INTERESTED ON IS WHAT HAPPENS WHEN WE TAKE THESE AGENTS TRAINED
utt_0081 utt 418.95 429.42 -X WITH SOME PARTNERS AND PUT THEM TO PLAY WITH THE UNSEEN ONES IN THE POOL. WE ARE ALSO INTERESTED IN THEIR OWN SELF-PLAY BEHAVIORS. SO IF WE DO THIS FOR EVERY POSSIBLE MATCHUP WE GET SOMETHING LIKE THIS,
utt_0083 utt 429.42 444.01 -X WHERE THE DOTTED LINES REPRESENT TRAINING PARTNERS AND THE RED LINES REPRESENT MATCHUPS PLAYED DURING EVALUATION. YOU CAN SEE THE RESULTS ON THIS TABLE. HERE IN THE LEFT COLUMN WE REPRESENT THE RAINBOW AGENTS THAT TRAINED WITH A SINGLE PARTNER IN BLUE, THE ONES THAT
utt_0086 utt 444.01 455.48 -X WERE TRAINED IN SELF-PLAY IN YELLOW AND THE ONES THAT WERE TRAINED WITH ALL THE RULES RULE-BASED PARTNERS (WITH OR WITHOUT SELF-PLAY) IN RED. ALL THE SCORES ARE THE AVERAGE OF EACH MATCH-UP
utt_0088 utt 455.65 464.59 -X OVER FOUR INDEPENDENTLY TRAINED INSTANCES OF THE CORRESPONDING RAINBOW AGENT IN THE TWO-PLAYER VERSION OF THE GAME. AS YOU CAN SEE IN THE MAIN DIAGONAL THE AGENTS THAT TRAINED WITH A SINGLE
utt_0090 utt 464.59 477.16 -X PARTNER SCORED THE HIGHEST WITH THAT PARTNER. THEY ALSO SCORE HIGHER THAN THAT PARTNERS' SELF-PLAY SO FOR EXAMPLE WHERE PIERS HAS A SELF-PLAY SCORE OF sixteen point six RAINBOW[PIERS] SCORED seventeen point six IN THAT MATCHUP
utt_0092 utt 479.02 491.47 -X THE RAINBOW[SELF-PLAY] AGENTS SCORED PRETTY WELL IN SELF-PLAY AS SHOULD BE EXPECTED, BUT THEY PERFORMED VERY POORLY WITH THE REMAINING PARTNERS. APPARENTLY THE LEARNED STRATEGY WASN'T VERY GENERAL AFTER ALL.
utt_0094 utt 491.94 500.78 -X IN ALL FAIRNESS THE AGENTS THAT TRAINED WITH ONE OF THE RULE-BASED PARTNERS DO NOT FARE WELL IN SELF-PLAY EITHER, WITH THE EXCEPTION OF RAINBOW[INTERNAL].
utt_0096 utt 501.29 508.75 -X INTERESTINGLY, RAINBOWIGGI HAD ONE OF THE HIGHEST SCORES WITH ITS INTENDED PARTNER BUT THE LOWEST SELF-PLAY AND AGGREGATE SCORE.
utt_0098 utt 508.81 514.51 -X FINALLY, RAINBOW[INTERNAL] ACHIEVED SOME SURPRISINGLY HIGH SELF-PLAY AND AGGREGATE SCORE.
utt_0099 utt 514.51 519.47 -X SOMETHING ABOUT INTERNAL SEEMS TO MAKE RAINBOW LEARN A ROBUST GENERAL POLICY WHEN TRAINED WITH IT.
utt_0100 utt 520.20 531.85 -X SO WHAT MAKES AGENTS TRAINED WITH SOME PARTNERS LEARN MORE GENERAL POLICIES THAN AGENTS TRAINED WITH OTHERS? TO START ADDRESSING THIS QUESTION WE LOOK FOR SOME METRICS TO HELP CHARACTERIZE HOW
utt_0102 utt 531.85 543.02 -X AN AGENT PLAYS RATHER THAN JUST JUST HOW WELL IT SCORES. WE STARTED WITH METRICS DEVELOPED IN ONE OF OUR PREVIOUS PAPERS: COMMUNICATIVENESS MEASURES HOW OFTEN THE AGENT CHOOSES A HINT ACTION
utt_0104 utt 543.02 554.51 -X GIVEN THAT AT LEAST ONE HINT TOKEN IS AVAILABLE AND INFORMATION PER PLAY (IPP) MEASURES HOW MANY PIECES OF INFORMATION (THAT IS, COLOR AND VALUE) THE AGENT KNOWS ON AVERAGE AT THE TIME OF PLAYING A CARD.
utt_0106 utt 555.21 559.17 -X WE ALSO LOOKED INTO HOW MANY CORRECT PLAYS AND MISTAKES EACH AGENT MAKES
utt_0107 utt 559.18 570.13 -X IN EACH OF EACH MATCH-UPS. THIS HELPS US DISCERN WHETHER AN AGENT IS DOING POORLY BECAUSE IT CAN'T UNDERSTAND ITS PARTNER'S HINTS OR IF IT CAN'T GIVE HINTS THAT ITS PARTNERS UNDERSTAND.
utt_0109 utt 570.64 580.86 -X GAMES BOMBED OUT REFERS TO GAMES WHERE THREE LIVES ARE LOST DUE TO MISTAKES. IN THESE GAMES THE TEAM SCORES ZERO REGARDLESS OF HOW MANY CARDS WERE SUCCESSFULLY PLAYED. ALL THESE
utt_0111 utt 580.86 590.67 -X METRICS WERE COLLECTED FOR ALL THE MATCHUPS AND AGGREGATED IN IT IN THIS TABLE. WHILE THERE IS A LOT OF INFORMATION TO DIGEST, LET ME GUIDE YOU THROUGH SOME OF THE MOST INTERESTING FINDINGS.
utt_0113 utt 592.43 607.36 -X RAINBOW[SELF-PLAY] HAS AN IPP OF ALMOST EXACTLY ONE. THIS IS THE LOWEST OF ANY RAINBOW AGENT AND SEEMS TO IMPLY IT LEARNS SOME SORT OF VERY OPTIMIZED ONE HINT, ONE PLAY CONVENTION BUT THIS DOES NOT TRANSLATE TO MANY CORRECT PLAYS WITH THE REMAINING PARTNERS
utt_0116 utt 608.21 612.21 -X AT AROUND four point three AND IT IS UNABLE TO GIVE HINTS THAT OTHER PLAYERS UNDERSTAND
utt_0117 utt 612.81 618.85 -X AT ONLY three point seven. RAINBOW[INTERNAL] AND RAINBOW[IGGI] SHOW AN INTERESTING CONTRAST: RAINBOW[INTERNAL]
utt_0118 utt 618.85 623.73 -X MAKES PLENTY OF MISTAKES WITH IT WITH ITS INTENDED PARTNER BUT FEW MISTAKES IN AGGREGATE.
utt_0119 utt 624.11 635.99 -X RAINBOW[IGGI] IS THE REVERSE, MAKING AMONG THE FEWEST MISTAKES WITH ITS INTENDED PARTNER BUT THE HIGHEST NUMBER OF MISTAKES IN AGGREGATE. IN FACT, RAINBOW[IGGI] MANAGED TO BOMB OUT ALMOST
utt_0121 utt 636.46 646.72 -X seventy PERCENT OF ITS GAMES ACROSS ALL MATCHUPS. IN THE PAPER WE SPECULATE THAT THIS MIGHT BE LINKED TO THE FACT THAT IGGI GIVES HINTS IN A VERY PREDICTABLE WAY ABOUT PLAYABLE CARDS
utt_0123 utt 646.72 658.42 -X WHILE INTERNAL WILL SOMETIMES GIVE RANDOM HINTS. TO SUMMARIZE THESE RESULTS, WE FOUND THAT TRAINING WITH A SINGLE PARTNER (INCLUDING SELF-PLAY) USUALLY GENERALIZES POORLY, WITH ONE IMPORTANT EXCEPTION,
utt_0125 utt 658.64 670.69 -X AND THE TYPE OF BEHAVIORAL ANALYSIS WE PERFORM HELPS RAISE HYPOTHESIS FOR IDENTIFYING GOOD PARTNERS FOR POLICY GENERALITY. IN TERMS OF FUTURE WORK, NONE OF THE AGENTS DISCUSSED ARE TRYING TO
utt_0127 utt 670.69 676.89 -X IDENTIFY OR ADAPT TO ITS PARTNERS, SO EXPLORING MECHANISMS TO DO THAT WOULD BE INTERESTING.
utt_0128 utt 676.89 687.48 -X THIS COULD PERHAPS BE ACHIEVED BY AN EXPLICIT MECHANISM OR SIMPLY THROUGH DIFFERENT NEURAL NETWORK ARCHITECTURES, ESPECIALLY RECURRENT ONES. WE WOULD ALSO LIKE TO FURTHER INVESTIGATE THE IMPACT
utt_0130 utt 687.48 698.68 -X OF PARTNER SELECTION INTO POLICY GENERALITY AND INVESTIGATE THE RELATIONSHIP BETWEEN PERFORMANCE WITH ONE OR MORE PARTNERS AND PERFORMANCE WITH HUMANS. TO CONCLUDE, HERE
utt_0132 utt 698.68 703.61 -X ARE THE LINKS TO THE PAPER AND THE CODE AS WELL AS SOME REFERENCES I MENTIONED DURING THIS TALK.
utt_0133 utt 703.80 714.57 -X AS A DISCLAIMER, AN EARLIER VERSION OF THIS WORK WAS PRESENTED AT A AAAI WORKSHOP EARLIER THIS YEAR, AND SOME VALUES IN THIS PRESENTATION MAY BE DIFFERENT THAN THE PAPER DUE TO A CORRECTION OF
utt_0135 utt 714.57 719.35 -4.0684 A SMALL ERROR INVOLVING THE IPP METRIC. THANK YOU FOR LISTENING!
