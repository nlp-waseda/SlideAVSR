utt_0001 utt 0.34 4.88 -X I'M CHRIS POTTS, HERE ON BEHALF OF MY FULL TEAM – ZHENGXUAN WU, ATTICUS GEIGER, AND DOUWE KIELA.
utt_0003 utt 4.88 8.59 -X WE ARE INTRODUCING DYNASENT, A NEW DYNAMIC BENCHMARK FOR SENTIMENT ANALYSIS.
utt_0004 utt 9.20 17.42 -X SENTIMENT WAS ONE OF THE FIRST AREAS OF NATURAL LANGUAGE UNDERSTANDING TO BE REVOLUTIONIZED BY DATA-DRIVEN METHODS, AND AMONG THE FIRST TO ACHIEVE WIDESPREAD INDUSTRY ADOPTION.
utt_0006 utt 17.93 20.79 -X IN two thousand and fourteen, SETH GRIMES SURVEYED INDUSTRY PRACTITIONERS.
utt_0007 utt 20.79 25.62 -X IN RESPONSE TO A PROMPT ABOUT WHAT THEY CURRENTLY NEED (OR EXPECT TO NEED) TO EXTRACT AND ANALYZE,
utt_0008 utt 25.62 28.31 -X SENTIMENT HAD THE SECOND HIGHEST RATE OF POSITIVE RESPONSES.
utt_0009 utt 28.85 32.66 -X BUT THESE APPLICATIONS ARE NOT ALWAYS CLEAR WINS FOR INDUSTRY.
utt_0010 utt 32.66 39.89 -X IN THAT SAME SURVEY, ONE RESPONDENT SAID THAT SENTIMENT SYSTEMS WILL ALWAYS BE FLAWED AND MANY SOLUTIONS ARE SO BAD THEY'RE NOT WORTH USING.
utt_0012 utt 39.89 48.91 -X THE DISCONNECT HERE IS CLEAR: IN NLP, WE MIGHT TALK ABOUT SENTIMENT ANALYSIS AS BEING EASY OR SOLVED, AND YET WE ARE FALLING SHORT WHEN IT COMES TO SUCCESSFUL APPLICATIONS.
utt_0014 utt 48.91 50.71 -X WE BELIEVE DATA IS A KEY ASPECT TO THIS.
utt_0015 utt 50.71 56.40 -X AS A COMMUNITY, WE THINK WE'VE MADE MORE PROGRESS THAN WE ACTUALLY HAVE BECAUSE AVAILABLE SENTIMENT BENCHMARKS ARE LIMITED.
utt_0017 utt 56.40 65.30 -X AND THAT'S THE CENTRAL MOTIVATION BEHIND DYNASENT: WE AIM TO CREATE A DYNAMIC BENCHMARK FOR SENTIMENT THAT CAN RESPOND TO EVOLVING NEEDS FROM INDUSTRY AND RESEARCH.
utt_0019 utt 65.58 70.58 -X WITH THE PRESENT PAPER, WE ARE INTRODUCING THE FIRST TWO OF WHAT WE HOPE ARE MANY ROUNDS OF DYNASENT.
utt_0021 utt 70.58 73.88 -X AND THIS DIAGRAM PROVIDES AN OVERVIEW OF THE PROJECT SO FAR.
utt_0022 utt 73.88 83.89 -X WE'RE GOING TO LEVERAGE TWO VERY STRONG MODELS TO FIND OR CREATE CHALLENGING SENTENCES, WHICH ARE VALIDATED BY HUMANS TO PROVIDE TWO ROUNDS FOR THE DATASET.
utt_0024 utt 83.89 89.72 -X BY WEAVING MODELS INTO THIS PROCESS, WE HELP ENSURE THAT WE HAVE A DATASET THAT IS VERY CHALLENGING FOR CURRENT APPROACHES.
utt_0026 utt 89.87 100.98 -X YOU CAN SEE WE’RE ALSO EXPLORING TWO METHODS FOR OBTAINING THESE HARD EXAMPLES: HARVESTING THEM FROM NATURALLY OCCURRING DATA, AND CREATING THEM AS PART OF AN ADVERSARIAL, MODEL-AND-HUMAN-IN-THE-LOOP PROCESS.
utt_0029 utt 100.98 104.76 -X LET'S LOOK MORE CLOSELY AT THE PROCESS OF CREATING DYNASENT, STARTING WITH ROUND one.
utt_0030 utt 105.23 111.77 -X THE FIRST PLAYER IN OUR STORY IS MODEL zero, WHICH WE'RE GOING TO USE AS A DEVICE FOR FINDING THESE HARD EXAMPLES.
utt_0032 utt 111.77 117.65 -X MODEL zero BEGINS WITH ROBERTA PRETRAINED PARAMETERS AND IS FINE-TUNED ON THESE LARGE DATASETS THAT YOU SEE HERE.
utt_0035 utt 121.59 126.84 -X THAT’S THE SIMPLEST FORMULATION THAT STILL DOES JUSTICE TO THE FACT THAT NOT ALL TEXTS EXPRESS SENTIMENT.
utt_0037 utt 127.09 132.44 -X PERHAPS THE MAIN THING TO POINT OUT IS THAT OUR NEUTRAL CATEGORY IS THE SMALLEST OF THE THREE.
utt_0039 utt 132.44 138.10 -X THAT'S SORT OF MISALIGNED WITH REAL-WORLD DISTRIBUTIONS OF SENTIMENT, AND IT WILL AFFECT OUR MODELS AND RESULTS, AS YOU'LL SEE.
utt_0041 utt 138.80 145.02 -X FOR TRACKING PERFORMANCE THROUGHOUT THE PROJECT, WE USE THESE THREE EXTERNAL TEST SETS, IN ADDITION TO THOSE CREATED FOR DYNASENT.
utt_0043 utt 145.27 154.74 -X MODEL zero PERFORMS WELL ON ALL THREE, THOUGH THE NEUTRAL CATEGORY IS ALREADY ARGUABLY EMERGING AS A TROUBLEMAKER – IT HAS THE LOWEST PERFORMANCE ACROSS-THE-BOARD, AND BY WIDE MARGINS.
utt_0045 utt 155.60 159.51 -X NOW, AS I SAID, MODEL zero IS REALLY JUST A DEVICE FOR FINDING HARD EXAMPLES.
utt_0046 utt 159.51 163.51 -X AND OUR HUNT FOR THOSE EXAMPLES WAS CONDUCTED IN THE YELP ACADEMIC DATASET.
utt_0047 utt 163.51 174.81 -X WE EMPLOY A SIMPLE PROCEDURE: FROM THE OCEAN OF AVAILABLE SENTENCES, WE OVER-SAMPLE SENTENCES WHERE THERE IS A SENTIMENT TENSION BETWEEN THE SENTENCE-LEVEL MODEL PREDICTION AND THE REVIEW-LEVEL RATING OF THE TEXT.
utt_0050 utt 175.09 181.08 -X THE IDEA IS THAT THIS IS LIKELY TO HELP US LOCATE SENTENCES THAT THE MODEL IS NOT PROPERLY UNDERSTANDING FOR WHATEVER REASON.
utt_0052 utt 181.21 189.05 -X IT’S MERELY A HEURISTIC, SINCE OF COURSE POSITIVE SENTENCES CAN APPEAR IN NEGATIVE REVIEWS, AND VICE VERSA, BUT WE DON'T NEED IT TO BE PERFECT.
utt_0054 utt 189.05 192.76 -X ALL THE SENTENCES IN DYNASENT ARE MULTIPLY VALIDATED BY CROWDWORKERS.
utt_0055 utt 192.76 202.56 -X IN THAT TASK, AFTER BEING PRESENTED WITH GUIDELINES, WORKERS WERE SHOWN A SENTENCE AND ASKED TO LABEL IT AS POSITIVE, NEGATIVE, NO SENTIMENT (THAT IS, NEUTRAL), OR MIXED SENTIMENT.
utt_0057 utt 202.68 211.52 -X ALL THE EXAMPLES IN DYNASENT HAVE LABELS FROM FIVE CROWDWORKERS, AND DO SEE OUR PAPER FOR THE METHODS WE USED TO IDENTIFY AND RETAIN HIGH QUALITY LABELS AND WORKERS.
utt_0059 utt 211.64 214.36 -X THAT LABELING PROCEDURE LEADS US TO THIS DATASET.
utt_0060 utt 214.39 218.46 -X BECAUSE WE HAVE FIVE LABELS PER EXAMPLE, THERE ARE TWO PERSPECTIVES WE CAN TAKE.
utt_0061 utt 218.46 223.55 -X IN DISTRIBUTIONAL TRAINING, WE REPEAT EACH EXAMPLE FIVE TIMES WITH EACH OF ITS LABELS.
utt_0062 utt 223.55 230.94 -X THIS ALLOWS US TO USE ALL THE EXAMPLES, EVEN THOSE WITHOUT A MAJORITY, AND IT LETS US EMBRACE THE UNCERTAINTY THAT IS PRESENT IN THOSE LABELS.
utt_0064 utt 230.94 234.59 -X IN PRACTICE, WE FOUND THAT THIS KIND OF TRAINING LED TO CONSISTENTLY BETTER RESULTS.
utt_0065 utt 234.74 236.92 -X WE CAN ALSO TAKE A MORE TRADITIONAL ROUTE.
utt_0066 utt 236.92 240.51 -X HERE, AN EXAMPLE HAS A LABEL X IF AT LEAST THREE OF THE FIVE WORKERS CHOSE X.
utt_0067 utt 241.27 247.71 -X OUR DEV AND TEST SETS ARE BALANCED ACROSS THE THREE INFERRED LABEL CATEGORIES, WITH NO MIXED OR NON-MAJORITY EXAMPLES.
utt_0069 utt 248.12 257.28 -X YOU'LL NOTICE THAT, IN OUR TRAIN SETS, THE CLASS DISTRIBUTION HAS SHIFTED: IN OUR EXTERNAL BENCHMARKS, NEUTRAL IS THE SMALLEST CATEGORY, AND NOW IT’S THE LARGEST.
utt_0071 utt 257.28 261.95 -X THIS IS MORE REALISTIC, BUT IT IS LIKELY TO INTRODUCE NEW CHALLENGES.
utt_0072 utt 261.95 266.05 -X FINALLY, IT LOOKS LIKE OUR ATTEMPT TO FIND HARD NATURALLY OCCURRING EXAMPLES WAS SUCCESSFUL.
utt_0074 utt 272.32 278.82 -X NOW, WE MADE SURE THAT MODEL zero IS TOTALLY STUMPED BY OUR TEST SETS: BY DESIGN, IT PERFORMS AT CHANCE ON ROUND one DEV AND TEST.
utt_0076 utt 279.20 285.38 -X FOR BENCHMARKING, IT'S USEFUL TO HAVE AN ESTIMATE OF HUMAN PERFORMANCE USING THE SAME METRIC WE USE FOR MODEL EVALUATIONS.
utt_0078 utt 285.40 294.27 -X AND TO CREATE THOSE, WE SYNTHESIZED FIVE COMPREHENSIVE HUMAN LABELERS FROM OUR RESPONSE DISTRIBUTIONS AND COMPARED EACH TO OUR INFERRED LABELS, AVERAGING THE RESULTS.
utt_0080 utt 294.30 300.54 -X THIS LEADS US TO ESTIMATES OF HUMAN PERFORMANCE ON ROUND one THAT ARE AROUND zero point eight eight MACRO-AVERAGED Fone.
utt_0082 utt 301.05 303.68 -X WE EMPHASIZE THAT THIS IS A CONSERVATIVE ESTIMATE.
utt_0083 utt 303.68 312.74 -X NEARLY HALF OF OUR WORKERS NEVER DISAGREED WITH THE INFERRED MAJORITY LABEL ON ANY EXAMPLE THEY SAW, SUGGESTING THAT HUMAN PERFORMANCE IS NEARER TO PERFECTION, AT LEAST FOR SOME HUMANS.
utt_0086 utt 313.15 318.34 -X NONETHELESS, THOUGH, LET'S SAY THIS: IF MODELS START TO SURPASS THIS ESTIMATE OF HUMAN PERFORMANCE,
utt_0087 utt 318.34 323.04 -X THEN IT IS TIME TO CONSIDER USING THOSE MODELS TO CREATE NEW ROUNDS OF DYNASENT.
utt_0088 utt 323.04 327.01 -X FURTHER HILL-CLIMBING ON THE EXISTING ONES MIGHT NOT BE ALL THAT PRODUCTIVE.
utt_0089 utt 327.01 327.55 -X NOW TO ROUND two.
utt_0090 utt 327.55 336.10 -X IN THIS ROUND, WE ARE GOING TO TRAIN A MODEL ON OUR ROUND one DATA, PLUS A LOT OF EXTERNAL DATA, AND THEN HAVE CROWDWORKERS TRY TO FOOL IT, WITH A SEPARATE HUMAN VALIDATION STEP AS BEFORE.
utt_0093 utt 336.96 340.51 -X SO, MODEL one: THIS IS AGAIN A ROBERTA-BASED SENTIMENT CLASSIFIER.
utt_0094 utt 340.51 354.85 -X WE'RE LEVERAGING THOSE EXTERNAL DATASETS FROM BEFORE TO FINE-TUNE IT, BUT NOW WITH SOMEWHAT DIFFERENT PROTOCOLS: WE’VE OVER-SAMPLED THE SSTminus three, WE’VE UNDER-SAMPLED YELP AND AMAZON, WHILE ALSO BALANCING OUT THEIR LABEL IMBALANCES, AND WE’VE INCLUDED TWO COPIES
utt_0097 utt 354.85 357.41 -X OF OUR ROUND one DATA, WITH DISTRIBUTIONAL LABELS.
utt_0098 utt 357.41 361.09 -X THIS IS MEANT TO ENCODE OUR VALUES, ESPECIALLY OUR GOAL OF DOING WELL ON DYNASENT.
utt_0099 utt 361.09 363.52 -X AND HERE'S HOW MODEL one DOES.
utt_0101 utt 366.82 371.51 -X THERE’S A BIT OF A CONCERN, THOUGH: MODEL one HAS DROPPED IN PERFORMANCE RELATIVE TO MODEL
utt_0102 utt 371.55 373.35 -X zero ON YELP AND AMAZON.
utt_0103 utt 373.38 386.31 -X WE THINK THIS IS MIGHT BE UNAVOIDABLE GIVEN OUR GOALS OF DOING WELL ON DYNASENT: WE WANT TO START SHIFTING TO THE DYNASENT LABELS AND LABEL DISTRIBUTIONS, AND WE ARE STARTING TO SEE SOME TRADE-OFFS IN DOING THIS WHEN IT COMES TO THE EXTERNAL BENCHMARKS.
utt_0106 utt 386.31 391.81 -X NOW THAT WE HAVE MODEL one, WE WANT TO START USING IT TO OBTAIN HARD SENTIMENT EXAMPLES.
utt_0107 utt 391.81 394.73 -X AND TO DO THIS, WE RELY ON THE NEW DYNABENCH PLATFORM.
utt_0108 utt 394.73 400.87 -X DYNABENCH IS OPEN-SOURCE PLATFORM, AND MAKES IT REMARKABLY EASY TO PUT MODELS IN THE LOOP DURING DATASET CREATION.
utt_0110 utt 401.25 409.06 -X DYNASENT IS IN FACT ONE OF THE FIRST DYNABENCH DATASETS, ALONG WITH THOSE FOR NLI, HATE SPEECH DETECTION, AND QUESTION ANSWERING.
utt_0112 utt 409.09 415.65 -X FOR DYNASENT, WE INITIALLY USED DYNABENCH OUT OF THE BOX: WORKERS SIMPLY HAD TO TRY TO FOOL THE MODEL BY WRITING EXAMPLES FROM SCRATCH.
utt_0114 utt 416.00 423.62 -X WE SAW, THOUGH, THAT THIS WAS LEADING TO EXAMPLES THAT TENDED TO BE SHORT AND EMPLOYED A SMALL SET OF TECHNIQUES THAT WERE GOING TO LEAD TO DATASET ARTIFACTS.
utt_0116 utt 423.62 427.34 -X AND TO HELP AVOID THIS, WE MOVED TO WHAT WE CALL OUR PROMPT CONDITION.
utt_0117 utt 427.46 436.55 -X IN THIS CONDITION, THE WORKER SETS UP THEIR TASK AS USUAL – FOR INSTANCE, THIS WORKER IS GOING TO TRY TO WRITE A SENTENCE THAT IS NEGATIVE BUT THAT THE MODEL ASSIGNS A DIFFERENT LABEL.
utt_0120 utt 436.64 441.53 -X HOWEVER, NOW WE OFFER THE WORKER A PROMPT SENTENCE, DRAWN FROM THE YELP DATASET, THAT
utt_0121 utt 443.01 447.98 -X THEY CAN, IF THEY CHOOSE, USE AS A STARTER, TO MODIFY IT TO ACHIEVE THEIR GOAL.
utt_0122 utt 447.98 458.86 -X WE FOUND THAT THIS LED TO MORE NATURALISTIC EXAMPLES, AND SO WE USED THE PROMPT CONDITION FOR ESSENTIALLY THE ENTIRE ROUND, RELEGATING OUR SMALL NUMBER OF NO PROMPT EXAMPLES TO THE TRAIN SET, WHERE THEY CAN EASILY BE EXCLUDED.
utt_0125 utt 459.97 462.60 -X OUR VALIDATION PROCESS WAS THE SAME AS FOR ROUND one.
utt_0126 utt 462.60 465.77 -X AND THIS LEADS US TO OUR ROUND two DATASET.
utt_0127 utt 465.77 472.55 -X AS BEFORE, ALL EXAMPLES HAVE FIVE LABELS, SO WE CAN USE DISTRIBUTIONAL TRAINING, OR WE CAN INFER LABELS AND TRAIN ON THOSE.
utt_0129 utt 472.81 478.95 -X AND, AS FOR ROUND one, THE DEV AND TEST SETS ARE DESIGNED TO BE BALANCED ACROSS OUR THREE CLASSES, AND MAXIMALLY HARD FOR MODEL one.
utt_0131 utt 479.75 483.31 -X NOW, THE OVERALL RATE OF ADVERSARIAL EXAMPLES IS ONLY nineteen% FOR THIS ROUND.
utt_0132 utt 483.88 485.90 -X THAT'S MUCH LOWER THAN FOR ROUND one.
utt_0133 utt 485.90 488.17 -X WE THINK TWO MAJOR FACTORS ARE IN PLAY HERE.
utt_0134 utt 488.17 495.85 -X FIRST, THE COGNITIVE DEMANDS OF OUR TASK ARE HIGH, AND SO SOME WORKERS LOSE TRACK OF WHAT THEIR GOAL IS AND WRITE SENTENCES THAT THE MODEL ACTUALLY GETS RIGHT.
utt_0136 utt 496.36 498.28 -X SECOND, MODEL one IS HARD TO FOOL.
utt_0137 utt 498.28 499.12 -X IT’S GOOD MODEL!
utt_0138 utt 499.12 505.10 -X AND I KNOW THIS FROM EXPERIENCE, BECAUSE I'VE CONFIDENTLY TRIED TO FOOL IT AND BEEN LESS SUCCESSFUL THAN I WOULD HAVE LIKED!
utt_0140 utt 505.10 507.69 -X NONETHELESS, WE HAVE A SOLID ROUND two DATASET.
utt_0141 utt 508.14 516.65 -X AS BEFORE, WE ENSURE THAT MODEL one PERFORMS AT CHANCE ON THIS ROUND, AND OUR ESTIMATE OF HUMAN PERFORMANCE IS HIGH – IN FACT HIGHER THAN ROUND one, AT AROUND zero point nine – AND THIS
utt_0143 utt 517.10 520.91 -X IGNORES THE FACT THAT MANY WORKERS ARE ARGUABLY CLOSE TO PERFECTION.
utt_0144 utt 521.83 523.66 -X SO, THAT'S DYNASENT THUS FAR.
utt_0145 utt 523.66 528.27 -X LET ME CLOSE WITH SOME GENERAL LESSONS THAT WE'RE APPLYING TO FUTURE ROUNDS AND FUTURE MODELING EFFORTS.
utt_0147 utt 528.27 530.03 -X AS I SAID BEFORE, PROMPTS ARE USEFUL.
utt_0148 utt 530.03 536.01 -X WE FOUND THAT WORKERS MADE GOOD USE OF THEM AS MEASURED BY EDIT DISTANCE, THEY TENDED TO MAKE A SUBSTANTIAL NUMBERS OF EDITS, WITHOUT,
utt_0150 utt 536.01 537.71 -X THOUGH, DISCARDING THE PROMPT ENTIRELY.
utt_0151 utt 537.93 547.95 -X IN ADDITION, PROMPT-DERIVED SENTENCES HAVE GREATER VOCABULARY DIVERSITY: NO PROMPT SENTENCES, IN GRAY HERE, HAVE VERY SMALL VOCABULARIES OVERALL, WHEREAS PROMPT SENTENCES, IN BLUE,
utt_0153 utt 547.95 552.33 -X HAVE OVERALL A VOCAB THAT IS MORE LIKE THAT OF OUR ROUND one NATURALLY OCCURRING CASES,
utt_0154 utt 552.33 553.23 -X WHICH YOU SEE IN RED.
utt_0155 utt 553.23 557.87 -X THUS, WE THINK PROMPTS COULD BE VALUABLE FOR ADVERSARIAL EXAMPLE CREATION IN GENERAL.
utt_0156 utt 557.87 561.84 -X BASICALLY, IT REDUCES THE BURDEN OF CREATIVE WRITING.
utt_0157 utt 561.84 564.69 -X A SECOND INSIGHT FROM DYNASENT CONCERNS THE NEUTRAL CATEGORY.
utt_0158 utt 564.69 569.65 -X OUR NEUTRAL CATEGORY HAS A SEMANTICS THAT IS DETERMINED, OR AT LEAST CONSTRAINED, BY OUR VALIDATION TASK.
utt_0160 utt 569.65 575.28 -X BY CONTRAST, NEUTRAL CATEGORIES THAT WE INFER FROM STAR RATINGS HAVE MUCH MORE DIVERSITY TO THEM.
utt_0162 utt 575.40 581.07 -X TO SUPPORT THIS INTUITION, WE RELABELED THE DEV SET OF THE SST USING OUR VALIDATION PROTOCOLS.
utt_0163 utt 581.07 583.98 -X HERE'S A CONFUSION MATRIX FOR THE TWO LABELING EFFORTS.
utt_0164 utt 583.98 587.34 -X IT'S REASSURING THAT THERE ARE RELATIVELY FEW SENTIMENT CONFUSIONS.
utt_0165 utt 587.34 590.51 -X AND, BY THE WAY, WE THINK ALL OF THESE FAVOR OUR OWN LABELS OVER SST.
utt_0167 utt 595.70 596.53 -X OUR CLASSES.
utt_0168 utt 596.53 602.54 -X AND THE LESSON SEEMS CLEAR: THREE-STAR REVIEWS MIX NEUTRAL, MIXED, AND UNCERTAIN SENTIMENT.
utt_0169 utt 602.54 608.05 -X SO WE SHOULD BE CAUTIOUS WHEN USING SUCH REVIEWS TO TRAIN MODELS TO FIND NEUTRAL EXAMPLES IN THE WILD.
utt_0171 utt 608.49 611.22 -X AND THIS BRINGS ME TO OUR FINAL ANALYSIS FOR THE TALK.
utt_0172 utt 611.22 616.03 -X YOU MIGHT BE WONDERING WHY WE DIDN'T FINE-TUNE MODEL zero ON OUR ROUND one DATA TO OBTAIN MODEL
utt_0173 utt 616.30 616.82 -X one.
utt_0174 utt 616.82 618.45 -X WE RETRAINED EVERYTHING FROM SCRATCH.
utt_0175 utt 618.45 624.40 -X THE REASON IS THAT WE OFTEN EXPERIENCED CATASTROPHIC FAILURES FOR THE NEUTRAL CATEGORY WHEN TRYING TO FINE-TUNE.
utt_0177 utt 624.56 631.28 -X TO DIAGNOSE THE PROBLEM, WE EMPLOYED THE INOCULATION BY FINE-TUNING METHOD, AND IT GAVE WHAT YOU MIGHT CALL TEXTBOOK RESULTS.
utt_0179 utt 631.28 634.03 -X CONSIDER THE LEFT PANEL HERE, FOR OUR POSITIVE CLASS.
utt_0180 utt 634.09 644.98 -X AS WE FINE-TUNE MODEL zero ON MORE AND MORE ROUND one DATA, PERFORMANCE ON THE POSITIVE CLASS STEADILY IMPROVES, AND WE MAINTAIN PERFORMANCE ON OUR THREE EXTERNAL BENCHMARKS.
utt_0182 utt 645.10 647.35 -X THE SAME IS TRUE FOR THE NEGATIVE CLASS.
utt_0183 utt 647.35 658.00 -X BUT THE PICTURE FOR THE NEUTRAL CATEGORY IS MUCH MORE WORRISOME: AS WE DO BETTER ON OUR NEUTRAL CATEGORY, WITH MORE FINE-TUNING, WE DO WORSE FOR THIS CATEGORY ON THE EXTERNAL BENCHMARKS.
utt_0186 utt 658.22 665.40 -X THIS PATTERN IS INDICATIVE OF LABEL SHIFT – THE SAME SORT OF LABEL SHIFT THAT WE JUST SAW WITH THE SST A MOMENT AGO.
utt_0188 utt 665.58 676.79 -X NOW, WE FIRMLY BELIEVE DYNASENT HAS THE BETTER NOTION OF NEUTRAL, SO WE HOPE TO CONTINUE TO LEAN ON DYNASENT TO CREATE MODELS THAT CAN MAKE GOOD NEUTRAL PREDICTIONS, EVEN IF IT COSTS US ON THESE EXTERNAL BENCHMARKS.
utt_0191 utt 677.84 679.06 -X OKAY, TO WRAP UP:
utt_0192 utt 679.06 684.57 -X OUR BEST MODEL SO FAR, USING ALL OF DYNASENT, GETS ABOUT zero point eight three Fone ON ROUND one AND 0.7one ON
utt_0193 utt 684.88 685.69 -X ROUND two.
utt_0194 utt 685.97 695.16 -X BUT WE'RE SURE THESE AREN'T THE BEST MODELS CONCEIVABLE, AND WE WOULD LOVE TO SEE SOMEONE DO BETTER – AFTER ALL, THAT WOULD GIVE US A MODEL THAT WE COULD USE TO POWER ANOTHER ROUND OF DYNASENT.
utt_0197 utt 695.60 703.19 -X WE OURSELVES ARE ALREADY AT WORK ON NEW ROUNDS OF DYNASENT, FOCUSING ON MORE EMOTIONAL DIMENSIONS AND DOMAINS OUTSIDE OF PRODUCT REVIEWS.
utt_0199 utt 703.19 713.17 -X AND, FINALLY, HERE'S A LINK TO THE PROJECT REPOSITORY, WHICH PROVIDES THE DATASET, STARTER CODE, MODELS, A DATASHEET, A MODEL CARD, AND NOTEBOOKS TO REPRODUCE EVERYTHING FROM THE PAPER.
utt_0202 utt 713.43 716.86 -X PLEASE DO LET US KNOW WHAT YOU LEARN ABOUT DYNASENT FROM WORKING WITH IT!
utt_0203 utt 716.86 718.68 -2.8730 AND THANK YOU FOR WATCHING!
