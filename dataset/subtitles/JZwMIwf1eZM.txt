utt_0000 utt 5.61 18.58 -X HELLO EVERYONE. THIS IS JIEYU ZHAO. I’M GLAD TO TALK OUR WORK ON GENDER BIAS IN MULTILINGUAL WORD EMBEDDINGS AND CROSS-LINGUAL TRANSFER. IT’S A JOINT WORK WITH SUBHABRATA, SAGHAR,
utt_0002 utt 18.58 20.05 -X KAI-WEI AND AHMED.
utt_0003 utt 22.67 36.95 -X WE KNOW THAT WORD EMBEDDING HAS BECOME A FUNDAMENTAL TECHNIQUE IN NLP. IT MAPS WORDS WITH SIMILAR SEMANTICS TO CLOSE POSITIONS IN THE EMBEDDING SPACE AND SUPPORTS MANY DOWNSTREAM APPLICATIONS.
utt_0005 utt 38.54 53.04 -X HOWEVER, RECENTLY, SEVERAL STUDIES REPORT THAT EMBEDDINGS REFLECT THE SOCIETAL BIASES FROM THE TEXT THAT THEY TRAINED ON. FOR EXAMPLE, WHEN FINDING WORD PAIRS THAT PARALLEL TO MAN
utt_0007 utt 53.04 62.51 -X VERSUS WOMAN TO REPRESENT GENDER, THE MODEL NOT ONLY IDENTIFIES KING TO QUEEN BUT ALSO COMPUTER PROGRAMMER TO HOMEMAKER.
utt_0009 utt 66.45 79.99 -X IN THIS PAPER, WE ANALYZE THE GENDER BIAS IN MULTILINGUAL WORD EMBEDDINGS. MULTILINGUAL WORD EMBEDDINGS ALIGN THE WORD VECTORS IN DIFFERENT LANGUAGES TO THE SAME SHARED SPACE.
utt_0011 utt 81.36 87.47 -X IT ENABLES CROSS-LINGUAL TRANSFER LEARNING WHERE WE CAN TRAIN A MODEL IN ONE LANGUAGE
utt_0012 utt 87.57 101.72 -X AND ADAPT IT OTHER LANGUAGES. VARIOUS METHODS HAVE BEEN PROPOSED TO CREATE MULTILINGUAL WORD EMBEDDINGS. ONE COMMON WAY IS TO FIRST TRAIN MONOLINGUAL WORD EMBEDDINGS SEPARATELY
utt_0014 utt 101.72 110.77 -X IN EVERY LANGUAGE AND THEN ALIGN THEM TO THE SAME SPACE. EVEN THOUGH GREAT EFFORTS HAVE
utt_0015 utt 110.77 121.46 -X FOCUSED ON IMPROVING THE MODEL’S PERFORMANCE ON TARGET LANGUAGES, LESS ATTENTION IS GIVEN TO UNDERSTANDING THE BIAS IN CROSS-LINGUAL TRANSFER LEARNING SETTINGS.
utt_0017 utt 125.30 140.12 -X IN THIS WORK, WE AIM TO UNDERSTAND THE FOLLOWING QUESTIONS: FIRST, WE WOULD LIKE TO KNOW DOES THE LANGUAGE PROPERTY AFFECT BIAS EXHIBITION? FOR EXAMPLE, THE DIFFERENT GENDER GRAMMATICAL
utt_0019 utt 140.14 154.93 -X SYSTEMS IN DIFFERENT LANGUAGES. DOES SUCH PROPERTY CAUSE DIFFERENT BIAS IN ENGLISH AND SPANISH? SECOND, DOES THE CHOICE OF THE TARGET SPACE FOR THE EMBEDDING ALIGNMENT AFFECT THE
utt_0021 utt 154.93 169.81 -X BIAS? THIRD, DOES THE CROSS-LINGUAL TRANSFER LEARNING ALSO INHERIT THE BIAS FROM THE MULTILINGUAL WORD EMBEDDINGS? IN THIS WORK, WE USE GENDER BIAS AS AN EXAMPLE FOR TO DO THE ANALYSIS
utt_0023 utt 170.23 174.13 -X BUT OUR METHODS CAN ALSO BE USED FOR OTHER BIAS ANALYSIS.
utt_0024 utt 177.68 184.12 -X IN THE FOLLOWING, I WILL DISCUSS OUR ANALYSIS ABOUT GENDER BIAS IN MULTILINGUAL WORD EMBEDDINGS
utt_0025 utt 184.18 191.57 -X FROM THE INTRINSIC AND EXTRINSIC PERSPECTIVES. THEN, I WILL TALK ABOUT BIAS MITIGATION METHOD
utt_0026 utt 191.63 201.85 -X FOR MULTILINGUAL EMBEDDINGS. LET’S FIRST LOOK AT THE INTRINSIC BIAS ANALYSIS.
utt_0027 utt 201.85 207.48 -X INTRINSIC BIAS AIMS AT UNDERSTANDING BIASES IN MULTILINGUAL EMBEDDINGS FROM WORD LEVEL.
utt_0028 utt 208.27 216.69 -X IN THIS PAPER, WE STUDY LANGUAGES INCLUDING ENGLISH, SPANISH, GERMAN AND FRENCH. ALL THE
utt_0029 utt 216.69 224.76 -X EMBEDDINGS ARE OBTAINED FROM FASTTEXT. TO MEASURE THE BIAS, WE PROPOSE INBIAS SCORE,
utt_0030 utt 224.95 239.45 -X WHICH IS CALCULATED BY THE AVERAGE DISTANCE GAP BETWEEN A TARGET WORD AND AN ATTRIBUTE WORD FROM DIFFERENT GENDER GROUPS. THE TARGET WORDS CONTAIN PAIRS OF FEMININE AND MASCULINE
utt_0032 utt 239.45 246.66 -X OCCUPATIONS, FOR EXAMPLE MEDICA AND MEDICO IN THE FIGURE. THE ATTRIBUTES REFER TO THE
utt_0033 utt 246.66 254.87 -X CORRESPONDING GENDER SEED WORDS, SUCH AS ELLA AND EL HERE. IF THERE IS A GAP BETWEEN THE
utt_0034 utt 254.87 262.71 -X DISTANCE OF OCCUPATION AND CORRESPONDING GENDER, IT MEANS THIS OCCUPATION SHOWS DISCRIMINATION
utt_0035 utt 262.71 271.86 -X AGAINST GENDER. TO CONDUCT THIS ANALYSIS, WE COLLECT A DATASET CONTAINING OVER two hundred PAIRS
utt_0036 utt 272.12 277.34 -X OF OCCUPATIONS AND ten PAIRS OF GENDER SEED WORDS FOR EACH LANGUAGE.
utt_0037 utt 281.52 289.27 -X HOW DOES THE BIAS EXHIBIT IN DEFERENT LANGUAGES? HERE X-AXIS IS THE DIFFERENT LANGUAGES AND
utt_0038 utt 289.27 297.56 -X THE Y-AXIS IS THE INTRINSIC BIAS SCORE WHICH IS SHOWN PREVIOUSLY. ORIGINAL HERE MEANS THESE
utt_0039 utt 297.56 306.07 -X ARE MONOLINGUAL EMBEDDINGS BEFORE ALIGNMENT. WE FIND THAT BIAS COMMONLY EXISTS ACROSS ALL
utt_0040 utt 306.07 319.83 -X THE LANGUAGES. AND AMONG THEM, GERMAN AND FRENCH SHOW SLIGHTLY STRONGER BIAS THAN ENGLISH AND SPANISH.
utt_0042 utt 319.83 325.87 -X WE ALSO SHOW THAT THE LEVELS OF GENDER BIAS IN MULTI-LINGUAL EMBEDDINGS ARE DIFFERENT
utt_0043 utt 326.01 338.71 -X WHEN USING DIFFERENT LANGUAGES AS THE TARGET SHARED SPACE. FOR EXAMPLE, HERE, THE BLUE BAR STANDS FOR THE BIAS IN THE MONOLINGUAL EMBEDDINGS BEFORE THE ALIGNMENT JUST AS THE
utt_0045 utt 338.71 347.55 -X PREVIOUS SLIDE. WHEN WE ALIGN THEM TO THE EN SPACE, THE BIAS INCREASES AS SHOWN IN GREEN
utt_0046 utt 347.55 357.53 -X BARS AND DECREASES WHEN USING ES AS THE TARGET, SHOWN IN GREY. WE BELIEVE THIS IS RELATED
utt_0047 utt 357.53 366.88 -X TO THE DIFFERENT GENDER GRAMMAR SYSTEMS IN DIFFERENT LANGUAGES. NEXT, WE WILL DISCUSS THE EXTRINSIC BIAS ANALYSIS.
utt_0049 utt 371.77 379.84 -X IN THIS PART, WE STUDY HOW THE BIAS IN MULTILINGUAL EMBEDDINGS AFFECTS THE DOWNSTREAM TASKS. TO
utt_0050 utt 380.09 388.09 -X CONDUCT THE EXTRINSIC BIAS ANALYSIS, WE CREATE A MULTILINGUAL BIOSBIAS DATASET FOLLOWING THE
utt_0051 utt 390.55 397.31 -X two thousand and nineteen PAPER TO EXTRACT BIOGRAPHIES FROM COMMON CRWAL BASED ON THE PATTERN “NAME IS AN OCCUPATION”.
utt_0052 utt 399.06 404.54 -X AND THEN EXTRACT THE BINARY GENDERS BASED ON THE GENDERED PRONOUNS. IN THIS WORK, WE
utt_0053 utt 408.66 422.14 -X USE THE OCCUPATION PREDICTION AS THE EVALUATION TASK AND USE THE AVERAGE PERFORMANCE GAP BETWEEN DIFFERENT GENDER GROUPS ACROSS ALL OCCUPATIONS AS THE EXTRINSIC BIAS EVALUATION METRIC.
utt_0055 utt 426.36 439.26 -X WE FIRST ANALYZE THE MONOLINGUAL PORTIONS OF BIOSBIAS USING DIFFERENT EMBEDDINGS. FOR EXAMPLE, HERE WE SHOW THE RESULTS ON EN AND ES PORTIONS OF THE BIOSBIAS RESPECTIVELY.
utt_0057 utt 439.87 454.27 -X THE TOP ROW IS THE RESULT ON EN DATASET AND THE BOTTOM IS THE RESULTS ON ES PORTION. THE LEFT FIGURE IS THE OVERALL PERFORMANCE OF THE OCCUPATION PREDICTION TASK, WHERE Y-AXIS
utt_0059 utt 454.27 469.05 -X IS THE ACCURACY AND X-AXIS STANDS FOR DIFFERENT EMBEDDINGS AND FOR EXAMPLE, HERE, WE SHOW THE RESULTS USING ORIGINAL EN EMBEDDING AND THE EN EMBEDDINGS ALIGNED TO ES SPACE. THE
utt_0061 utt 471.39 478.88 -X FIGURE ON THE RIGHT IS THE BIAS SCORE FOR THE DIFFERENT EMBEDDINGS. FROM BOTH EXPERIMENTS,
utt_0062 utt 478.88 487.46 -X WE SHOW THAT WITH EMBEDDINGS ALIGNED TO DIFFERENT TARGET SPACE, IT INFLUENCES THE BIAS IN THIS OCCUPATION PREDICTION TASK.
utt_0064 utt 493.31 505.18 -X NOW, LET’S LOOK AT THE BIAS IN CROSS-LINGUAL TRANSFER LEARNING WHERE WE TRANSFER A MODEL THAT IS TRAINED ON EN DATASET TO ES DATASET AND VICE VERSA USING THE MULTILINGUAL EMBEDDINGS.
utt_0066 utt 507.13 514.50 -X THE ARROW IN X-AXIS STANDS FOR THE TRANSFER DIRECTION, FOR EXAMPLE, THE LEFT BAR IS TRANSFERRING
utt_0067 utt 514.91 522.88 -X FROM EN TO ES. AND THE RIGHT ONE IS FROM SPANISH TO EN. WE FIND THAT THE EXTRINSIC BIAS EXISTS
utt_0068 utt 523.48 530.85 -X REGARDLESS OF THE TRANSFER DIRECTION.
utt_0069 utt 531.07 545.63 -X WE ALSO DO THE ANALYSIS WITH CONTEXTUALIZED EMBEDDINGS BY REPLACING THE FASTTEXT EMBEDDING WITH MULTILINGUAL BERT EMBEDDINGS. THE TOP ROW IS THE RESULT IN EN PORTION OF THE BIOSBIAS
utt_0071 utt 545.63 557.12 -X DATASET AND THE BOTTOW ROW IS THE RESULTS WHEN WE DO THE CROSS-LINGUAL TRANSFER LEARNING FROM EN TO ES. WE FIND THAT USING BERT CAN HELP TO IMPROVE THE PERFORMANCE IN BOTH CASES.
utt_0074 utt 561.54 563.87 -X LEARNING WHEN USING MULTILINGUAL BERT.
utt_0077 utt 571.71 583.52 -X AND THE BOTTOM ROW IS THE RESULT FOR EXTRINSIC BIAS WHERE WE TRANSFER FROM EN TO SPANISH.
utt_0080 utt 590.69 599.33 -X IN CONCLUSION, IN THIS PAPER WE ANALYZE BIAS IN MULTILINGUAL WORD EMBEDDINGS FROM BOTH INTRINSIC AND EXTRINSIC PERSPECTIVES. WE COLLECT NEW DATASETS FOR SUCH BIAS ANALYSIS AND SHOW
utt_0082 utt 599.33 607.40 -X THAT BIAS COMMONLY EXISTS IN DIFFERENT LANGUAGES AND THE BIAS IN MULTILINGUAL WORD EMBEDDINGS
utt_0086 utt 625.32 634.08 -X TARGET SPACE SUCH AS ES OR DEBIASED EN HELPS TO REDUCE BIAS BUT IT CANNOT COMPLETELY REMOVE THAT. WE ENCOURAGE MORE NOVEL ALGORITHMS TO BE PROPOSED. OUR DATA AND CODES CAN BE ACCESSED
utt_0088 utt 635.87 645.70 -X THROUGH THIS LINK. AND IN THIS REPRESENTATION, I’M NOT ABLE TO COVER ALL THE EXPERIMENTS AND PLEASE REFER TO OUR PAPER FOR MORE ANALYSIS. THANK YOU!
