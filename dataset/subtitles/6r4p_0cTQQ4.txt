utt_0000 utt 0.88 15.57 -X HELLO, I AM HARSHIL JAIN, A SENIOR UNDERGRADUATE IN COMPUTER SCIENCE AND ENGINEERING AT IIT GANDHINAGAR AND I WILL BE PRESENTING MY WORK ON END-TO-END BINARIZED NEURAL NETWORKS FOR TEXT CLASSIFICATION WHICH I DID AS AN INTERNSHIP AT NEURALSPACE DURING THE SUMMER.
utt_0003 utt 15.57 28.11 -X THIS WORK GOT ACCEPTED AT THE SUSTAINLP WORKSHOP AT EMNLP two thousand and twenty, ONE OF THE PREMIER CONFERENCES IN NATURAL LANGUAGE PROCESSING ACROSS THE GLOBE. SO COMING TO THE MAIN IDEA, RECENT ADVANCES IN DEEP
utt_0005 utt 28.11 42.99 -X LEARNING HAVE LED TO A SIGNIFICANT PERFORMANCE INCREASE IN SEVERAL NLP TASKS. HOWEVER, THE MODELS HAVE BECOME MORE AND MORE COMPUTATIONALLY DEMANDING. THE STATE-OF-THE-ART TRANSFORMER MODELS ARE A GOOD EXAMPLE. IN THIS WORK, WE INTRODUCE A FULLY END TO END BINARIZED NEURAL NETWORK ARCHITECTURE
utt_0008 utt 43.09 56.18 -X FOR THE TASK OF INTENT CLASSIFICATION THAT USES BINARY REPRESENTATIONS FOLLOWED BY A BINARIZED CLASSIFIER. SO COMING TO THE N-GRAM STATISTICS, LET'S TAKE THE EXAMPLE OF SUPERVISED LEARNING.
utt_0010 utt 56.18 68.02 -X A DATA SET CONSISTS OF SAMPLES AND THEIR CORRESPONDING LABELS. FOR EACH SAMPLE, WE NORMALIZE THE SAMPLES AND CREATE TOKENS. AN N-GRAM VOCABULARY IS CREATED FOR EACH TOKEN WHERE N CAN
utt_0012 utt 68.02 73.40 -X BE ANY POSITIVE INTEGER. A VALUE OF two TO four WORKS WELL IN REAL LIFE AND IS USED IN OUR EXPERIMENTS.
utt_0013 utt 73.49 84.52 -X WITH THE CREATED VOCABULARY WE VECTORIZE THE VOCABULARY TO OBTAIN THE FEATURE REPRESENTATION OF EACH SAMPLE WITH A VECTOR SIZE EQUAL TO THE NUMBER OF N-GRAMS. THIS IS SHOWN PICTORIALLY IN THE SLIDE
utt_0015 utt 84.52 96.02 -X SHOWN. NOW COMING TO THE EMBEDDING INTO THE HYPER-DIMENSIONAL VECTOR, WE ASSIGN EACH SYMBOL WITH A RANDOM D DIMENSIONAL BIPOLAR HD VECTOR AND THESE VECTORS ARE STORED IN A MATRIX WHICH
utt_0017 utt 96.02 101.53 -X IS DENOTED BY H WHICH IS REFERRED TO AS THE ITEM MEMORY. SO FIRST H IS GENERATED FOR THE ALPHABET
utt_0018 utt 101.62 114.07 -X AND THE POSITION OF THE SYMBOL S_I IS GENERATED BY APPLYING PERMUTATION TO THE CORRESPONDING HD VECTOR I TIMES AND A UNIQUE HD VECTOR IS THEN FORMED FOR EACH N-GRAM VOCABULARY USING THE
utt_0020 utt 114.07 125.37 -X BINDING AND PERMUTATION OPERATION AS SHOWN IN THE RIGHT. IT IS WORTH MENTIONING HOWEVER THAT INTUITIVELY THE ENTIRE APPROACH WORKS BECAUSE THE EMBEDDING IS DONE IN SUCH A WAY THAT IN THE
utt_0022 utt 125.37 130.58 -X PROJECTED HIGH DIMENSIONAL SPACE THE TWO SIMILAR N-GRAM STATISTICS STILL REMAIN VERY SIMILAR.
utt_0023 utt 131.57 135.67 -X NOW BINARIZED NEURAL NETWORKS, SO ONCE WE HAVE THE BINARIZED EMBEDDING,
utt_0024 utt 135.67 147.70 -X WE USE A BINARY NEURAL NETWORK AS A CLASSIFIER AND WE CONSTRAIN THE WEIGHTS AND ACTIVATIONS OF THE NETWORK LAYER TO BE EITHER one OR minus one. WE USE A SIGN FUNCTION FOR THAT AND FOR ALL THE VALUES
utt_0026 utt 147.70 153.75 -X GREATER THAN OR EQUAL TO zero WE GET one AS OUTPUT AND minus one OTHERWISE. THIS IS STATED IN THE FORMULA BELOW.
utt_0027 utt 154.52 164.44 -X NOW COMING TO THE ENTIRE END-TO-END BINARIZED PIPELINE, LET'S LOOK AT THE FINAL PROPOSED PIPELINE. WE GET THE INPUT TEXT DOCUMENT, WE DO SOME PRE-PROCESSING OVER IT THEN WE TOKENIZE THE
utt_0029 utt 164.44 176.79 -X INPUT DOCUMENT USING THE N-GRAM STATISTICS. WE CONSTRUCT THE COUNT-BASED VECTORIZER AND WE EMBED THE TOKENS INTO BINARIZED HIGH DIMENSIONAL VECTORS AND FINALLY, HD VECTORS ARE USED WITH THE BINARIZED
utt_0031 utt 176.79 182.14 -X NEURAL NETWORK AS A CLASSIFIER TO CLASSIFY THE INTENT. NOW THE DATASETS USED FOR OUR
utt_0032 utt 182.36 194.33 -X EVALUATION ARE three SMALL DATA SETS WITH two TO eight INTENT CLASSES AND one LARGE DATA SET WITH two0 INTENT CLASSES. THE TRAINING AND TEST SAMPLE VARY A LOT AND A DATA SET IS CHOSEN TO SHOW THE
utt_0034 utt 194.33 199.36 -X APPLICABILITY OF OUR METHODS ON SMALL AS WELL AS LARGE DATASETS. THE SMALLER DATASET IS LIMITED TO
utt_0035 utt 199.80 206.01 -X three TRAINING SAMPLES WHILE THE LARGE DATA SET USES eleven thousand, five hundred SAMPLES TO TRAIN. NOW COMING TO THE RESULTS,
utt_0036 utt 206.01 210.11 -X SO IF WE COMPARE THE RESULTS OF OTHER PLATFORMS TO OUR BINARY ARCHITECTURE
utt_0037 utt 210.14 217.02 -X WE SEE THAT IT HAS ACHIEVED NEAR TO THE STATE OF THE ART COMPARED TO OTHER PLATFORMS AND WE CAN
utt_0038 utt 217.69 221.37 -X SHOW THAT OUR METHOD PERFORMS EQUALLY WELL COMPARED TO OTHER PLATFORMS.
utt_0039 utt 221.66 232.99 -X NOW COMING TO THE COMPARISON OF THE NORMAL BNN ARCHITECTURE WITH THE FULLY BINARIZED ARCHITECTURE WE CAN SEE THAT MOST OF THE TIME THE FULLY BINARIZED ARCHITECTURE ACHIEVES GREATER Fone
utt_0041 utt 232.99 238.37 -X SCORES FOR MOST OF THE DATA SETS AS COMPARED TO THE NON-BINARIZED VERSION OF THE ARCHITECTURES.
utt_0042 utt 239.13 251.46 -X NOW WE ALSO PERFORM ABLATION STUDIES TO CONFIRM THAT THE HD VECTORS PERFORM WELL OR WORSE COMPARED TO GLOVE OR SEMHASH AND WE FOUND THAT THE BINARIZED HYPER-DIMENSIONAL VECTORS PERFORMED WELL
utt_0044 utt 251.46 265.09 -X ON three DATASETS AND PERFORMED EQUALLY WELL ON THE OTHER DATA SET. SO WE PROVED THAT THE BINARIZED HYPER-DIMENSIONAL VECTORS ARE USEFUL AS EMBEDDINGS FOR ALL four DATA SETS. NOW COMING TO THE PRIMARY
utt_0046 utt 266.24 272.16 -X REASON FOR THE PAPER, THE MEMORY COMPARISON. SO WE SHOW THAT THE BINARIZED HYPERDIMENSIONAL VECTORS
utt_0047 utt 274.81 280.29 -X USE ABOUT twenty TO forty % LESS MEMORY AS COMPARED TO THEIR NON-BINARIZED COUNTERPARTS ON ALL four DATA
utt_0048 utt 282.27 287.65 -X SETS AND THE TRAINING TIME COMPARISON FOR ALL FOUR DATA SETS. SO WE CAN SEE THAT WE ACHIEVE A
utt_0049 utt 289.76 301.36 -X one point fiveX TO twoX GAIN PER EPOCH WHEN WE TRAIN THE BINARIZED VERSION OF THE ARCHITECTURE USING THE BINARIZED EMBEDDINGS FOR ALL four DATA SETS. WE CAN SHOW THAT WE ACHIEVE EFFICIENCY BOTH IN TERMS OF
utt_0051 utt 301.36 307.39 -X MEMORY AND TRAINING TIME WHEN WE USE THIS FULLY BINARIZED END-TO-END PIPELINE FOR THE TASK OF
utt_0052 utt 308.29 315.14 -3.3730 INTENT CLASSIFICATION. THANK YOU! THE PAPER CAN BE SEEN HERE WHICH IS PUBLISHED IN ACL ANTHOLOGY
